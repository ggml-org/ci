### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.83 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.50 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.03 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.29 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.25 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.24 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  175.18 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.90 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   26.37 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 218.44 sec*proc (27 tests)

Total Test time (real) = 218.45 sec

real	3m38.481s
user	7m23.576s
sys	0m5.467s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.24 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.94 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.47 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.30 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.11 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.28 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.24 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.44 sec*proc (27 tests)

Total Test time (real) =  50.45 sec

real	0m50.464s
user	1m10.036s
sys	0m5.072s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.135 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.623 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.634 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.635 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.636 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.637 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.638 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.639 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.640 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.641 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.642 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.642 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.646 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.647 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.648 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.649 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.649 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.650 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.654 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.845 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.294 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.297 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.297 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.298 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.299 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.299 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.299 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.300 I llama_model_loader: - type  f32:  124 tensors
0.00.032.301 I llama_model_loader: - type  f16:   73 tensors
0.00.037.670 I llm_load_vocab: special tokens cache size = 5
0.00.039.953 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.039.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.039.957 I llm_load_print_meta: arch             = bert
0.00.039.958 I llm_load_print_meta: vocab type       = WPM
0.00.039.958 I llm_load_print_meta: n_vocab          = 30522
0.00.039.959 I llm_load_print_meta: n_merges         = 0
0.00.039.959 I llm_load_print_meta: vocab_only       = 0
0.00.039.959 I llm_load_print_meta: n_ctx_train      = 512
0.00.039.959 I llm_load_print_meta: n_embd           = 384
0.00.039.960 I llm_load_print_meta: n_layer          = 12
0.00.039.963 I llm_load_print_meta: n_head           = 12
0.00.039.964 I llm_load_print_meta: n_head_kv        = 12
0.00.039.964 I llm_load_print_meta: n_rot            = 32
0.00.039.965 I llm_load_print_meta: n_swa            = 0
0.00.039.965 I llm_load_print_meta: n_embd_head_k    = 32
0.00.039.965 I llm_load_print_meta: n_embd_head_v    = 32
0.00.039.966 I llm_load_print_meta: n_gqa            = 1
0.00.039.967 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.039.968 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.039.969 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.039.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.039.972 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.039.972 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.039.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.039.974 I llm_load_print_meta: n_ff             = 1536
0.00.039.974 I llm_load_print_meta: n_expert         = 0
0.00.039.974 I llm_load_print_meta: n_expert_used    = 0
0.00.039.975 I llm_load_print_meta: causal attn      = 0
0.00.039.975 I llm_load_print_meta: pooling type     = 2
0.00.039.975 I llm_load_print_meta: rope type        = 2
0.00.039.976 I llm_load_print_meta: rope scaling     = linear
0.00.039.976 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.039.979 I llm_load_print_meta: freq_scale_train = 1
0.00.039.980 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.039.980 I llm_load_print_meta: rope_finetuned   = unknown
0.00.039.980 I llm_load_print_meta: ssm_d_conv       = 0
0.00.039.980 I llm_load_print_meta: ssm_d_inner      = 0
0.00.039.981 I llm_load_print_meta: ssm_d_state      = 0
0.00.039.981 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.039.981 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.039.988 I llm_load_print_meta: model type       = 33M
0.00.039.988 I llm_load_print_meta: model ftype      = F16
0.00.039.989 I llm_load_print_meta: model params     = 33.21 M
0.00.039.990 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.039.990 I llm_load_print_meta: general.name     = Bge Small
0.00.039.990 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.039.991 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.039.991 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.039.991 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.039.992 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.039.992 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.039.992 I llm_load_print_meta: max token length = 21
0.00.042.148 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.042.150 I llm_load_tensors: offloading output layer to GPU
0.00.042.151 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.042.175 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.177 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.042.856 I llama_new_context_with_model: n_seq_max     = 1
0.00.042.857 I llama_new_context_with_model: n_ctx         = 512
0.00.042.858 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.042.858 I llama_new_context_with_model: n_batch       = 2048
0.00.042.858 I llama_new_context_with_model: n_ubatch      = 2048
0.00.042.859 I llama_new_context_with_model: flash_attn    = 0
0.00.042.859 I llama_new_context_with_model: freq_base     = 10000.0
0.00.042.860 I llama_new_context_with_model: freq_scale    = 1
0.00.042.861 I ggml_metal_init: allocating
0.00.042.875 I ggml_metal_init: found device: Apple M4
0.00.042.882 I ggml_metal_init: picking default device: Apple M4
0.00.043.843 I ggml_metal_init: using embedded metal library
0.00.047.912 I ggml_metal_init: GPU name:   Apple M4
0.00.047.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.916 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.917 I ggml_metal_init: simdgroup reduction   = true
0.00.047.917 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.917 I ggml_metal_init: has bfloat            = true
0.00.047.917 I ggml_metal_init: use bfloat            = true
0.00.047.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.060.095 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.098 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.100 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.061.044 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.061.046 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.061.047 I llama_new_context_with_model: graph nodes  = 429
0.00.061.047 I llama_new_context_with_model: graph splits = 2
0.00.061.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.472 I 
0.00.071.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.072.166 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.713 I llama_perf_context_print:        load time =      50.82 ms
0.00.076.714 I llama_perf_context_print: prompt eval time =       4.41 ms /     9 tokens (    0.49 ms per token,  2041.28 tokens per second)
0.00.076.715 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.715 I llama_perf_context_print:       total time =       5.24 ms /    10 tokens
0.00.076.856 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.065s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.987 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.109 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.114 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.115 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.115 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.116 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.116 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.117 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.117 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.117 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.118 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.119 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.120 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.120 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.120 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.121 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.121 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.121 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.409 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.411 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.411 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.411 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.412 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.412 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.412 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.412 I llama_model_loader: - type  f32:  124 tensors
0.00.015.413 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.921 I llm_load_vocab: special tokens cache size = 5
0.00.019.249 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.251 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.252 I llm_load_print_meta: arch             = bert
0.00.019.252 I llm_load_print_meta: vocab type       = WPM
0.00.019.252 I llm_load_print_meta: n_vocab          = 30522
0.00.019.253 I llm_load_print_meta: n_merges         = 0
0.00.019.253 I llm_load_print_meta: vocab_only       = 0
0.00.019.253 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.253 I llm_load_print_meta: n_embd           = 384
0.00.019.253 I llm_load_print_meta: n_layer          = 12
0.00.019.255 I llm_load_print_meta: n_head           = 12
0.00.019.256 I llm_load_print_meta: n_head_kv        = 12
0.00.019.256 I llm_load_print_meta: n_rot            = 32
0.00.019.256 I llm_load_print_meta: n_swa            = 0
0.00.019.256 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.256 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.257 I llm_load_print_meta: n_gqa            = 1
0.00.019.257 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.258 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.259 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.259 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.259 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.259 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.259 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.260 I llm_load_print_meta: n_ff             = 1536
0.00.019.260 I llm_load_print_meta: n_expert         = 0
0.00.019.260 I llm_load_print_meta: n_expert_used    = 0
0.00.019.260 I llm_load_print_meta: causal attn      = 0
0.00.019.260 I llm_load_print_meta: pooling type     = 2
0.00.019.261 I llm_load_print_meta: rope type        = 2
0.00.019.261 I llm_load_print_meta: rope scaling     = linear
0.00.019.261 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.261 I llm_load_print_meta: freq_scale_train = 1
0.00.019.261 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.263 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.263 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.264 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.264 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.270 I llm_load_print_meta: model type       = 33M
0.00.019.270 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.271 I llm_load_print_meta: model params     = 33.21 M
0.00.019.271 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.272 I llm_load_print_meta: general.name     = Bge Small
0.00.019.272 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.272 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.272 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.272 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.273 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.273 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.273 I llm_load_print_meta: max token length = 21
0.00.020.594 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.594 I llm_load_tensors: offloading output layer to GPU
0.00.020.595 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.601 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.602 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.970 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.971 I llama_new_context_with_model: n_ctx         = 512
0.00.020.971 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.971 I llama_new_context_with_model: n_batch       = 2048
0.00.020.972 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.972 I llama_new_context_with_model: flash_attn    = 0
0.00.020.972 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.973 I llama_new_context_with_model: freq_scale    = 1
0.00.020.973 I ggml_metal_init: allocating
0.00.020.979 I ggml_metal_init: found device: Apple M4
0.00.020.981 I ggml_metal_init: picking default device: Apple M4
0.00.021.494 I ggml_metal_init: using embedded metal library
0.00.023.612 I ggml_metal_init: GPU name:   Apple M4
0.00.023.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.614 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.614 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.615 I ggml_metal_init: simdgroup reduction   = true
0.00.023.615 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.615 I ggml_metal_init: has bfloat            = true
0.00.023.615 I ggml_metal_init: use bfloat            = true
0.00.023.615 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.616 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.000 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.001 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.002 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.596 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.597 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.598 I llama_new_context_with_model: graph nodes  = 429
0.00.032.598 I llama_new_context_with_model: graph splits = 2
0.00.032.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.098 I 
0.00.037.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.037.641 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.050 I llama_perf_context_print:        load time =      27.11 ms
0.00.042.051 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2106.25 tokens per second)
0.00.042.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.052 I llama_perf_context_print:       total time =       4.95 ms /    10 tokens
0.00.042.226 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.185 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.850 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.429 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.436 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.441 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.442 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.442 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.444 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.033.445 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.033.445 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.033.446 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.033.447 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.033.450 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.033.451 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.033.451 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.033.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.306 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.307 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.307 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.308 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.308 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.308 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.309 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.309 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.309 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.310 I llama_model_loader: - type  f32:   41 tensors
0.00.048.310 I llama_model_loader: - type  f16:   29 tensors
0.00.066.662 W llm_load_vocab: empty token at index 5
0.00.071.123 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.072.407 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.072.432 I llm_load_vocab: special tokens cache size = 5
0.00.315.535 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.315.542 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.315.543 I llm_load_print_meta: arch             = jina-bert-v2
0.00.315.544 I llm_load_print_meta: vocab type       = BPE
0.00.315.544 I llm_load_print_meta: n_vocab          = 61056
0.00.315.544 I llm_load_print_meta: n_merges         = 39382
0.00.315.545 I llm_load_print_meta: vocab_only       = 0
0.00.315.545 I llm_load_print_meta: n_ctx_train      = 8192
0.00.315.545 I llm_load_print_meta: n_embd           = 384
0.00.315.545 I llm_load_print_meta: n_layer          = 4
0.00.315.553 I llm_load_print_meta: n_head           = 12
0.00.315.553 I llm_load_print_meta: n_head_kv        = 12
0.00.315.553 I llm_load_print_meta: n_rot            = 32
0.00.315.554 I llm_load_print_meta: n_swa            = 0
0.00.315.554 I llm_load_print_meta: n_embd_head_k    = 32
0.00.315.554 I llm_load_print_meta: n_embd_head_v    = 32
0.00.315.554 I llm_load_print_meta: n_gqa            = 1
0.00.315.555 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.315.556 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.315.557 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.315.558 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.315.558 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.315.558 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.315.558 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.315.559 I llm_load_print_meta: n_ff             = 1536
0.00.315.559 I llm_load_print_meta: n_expert         = 0
0.00.315.560 I llm_load_print_meta: n_expert_used    = 0
0.00.315.566 I llm_load_print_meta: causal attn      = 0
0.00.315.566 I llm_load_print_meta: pooling type     = -1
0.00.315.566 I llm_load_print_meta: rope type        = -1
0.00.315.566 I llm_load_print_meta: rope scaling     = linear
0.00.315.567 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.315.567 I llm_load_print_meta: freq_scale_train = 1
0.00.315.567 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.315.568 I llm_load_print_meta: rope_finetuned   = unknown
0.00.315.568 I llm_load_print_meta: ssm_d_conv       = 0
0.00.315.570 I llm_load_print_meta: ssm_d_inner      = 0
0.00.315.571 I llm_load_print_meta: ssm_d_state      = 0
0.00.315.571 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.315.571 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.315.596 I llm_load_print_meta: model type       = 33M
0.00.315.597 I llm_load_print_meta: model ftype      = F16
0.00.315.597 I llm_load_print_meta: model params     = 32.90 M
0.00.315.598 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.315.598 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.315.598 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.315.598 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.315.598 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.315.599 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.315.599 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.315.599 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.315.599 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.315.601 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.315.601 I llm_load_print_meta: max token length = 45
0.00.316.859 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.316.859 I llm_load_tensors: offloading output layer to GPU
0.00.316.859 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.316.880 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.316.881 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.318.019 I llama_new_context_with_model: n_seq_max     = 1
0.00.318.020 I llama_new_context_with_model: n_ctx         = 8192
0.00.318.021 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.318.021 I llama_new_context_with_model: n_batch       = 2048
0.00.318.021 I llama_new_context_with_model: n_ubatch      = 2048
0.00.318.021 I llama_new_context_with_model: flash_attn    = 0
0.00.318.022 I llama_new_context_with_model: freq_base     = 10000.0
0.00.318.022 I llama_new_context_with_model: freq_scale    = 1
0.00.318.023 I ggml_metal_init: allocating
0.00.318.035 I ggml_metal_init: found device: Apple M4
0.00.318.038 I ggml_metal_init: picking default device: Apple M4
0.00.319.101 I ggml_metal_init: using embedded metal library
0.00.321.579 I ggml_metal_init: GPU name:   Apple M4
0.00.321.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.321.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.321.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.321.582 I ggml_metal_init: simdgroup reduction   = true
0.00.321.582 I ggml_metal_init: simdgroup matrix mul. = true
0.00.321.582 I ggml_metal_init: has bfloat            = true
0.00.321.583 I ggml_metal_init: use bfloat            = true
0.00.321.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.321.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.331.892 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.331.894 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.331.895 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.332.468 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.332.469 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.332.469 I llama_new_context_with_model: graph nodes  = 154
0.00.332.469 I llama_new_context_with_model: graph splits = 2
0.00.332.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.344.025 I 
0.00.344.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.344.196 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.344.197 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.344.199 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.344.199 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.344.203 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.344.203 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.344.748 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.348.495 I llama_perf_context_print:        load time =     321.17 ms
0.00.348.496 I llama_perf_context_print: prompt eval time =       3.74 ms /    62 tokens (    0.06 ms per token, 16595.29 tokens per second)
0.00.348.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.348.498 I llama_perf_context_print:       total time =       4.47 ms /    63 tokens
0.00.348.664 I ggml_metal_free: deallocating

real	0m1.062s
user	0m0.323s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.153 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.268 I main: llama backend init
0.00.000.292 I main: load the model and apply lora adapter, if any
0.00.026.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.303 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.314 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.315 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.044 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.046 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.047 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.047 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.048 I llama_model_loader: - type  f32:  194 tensors
0.00.056.048 I llama_model_loader: - type  f16:   98 tensors
0.00.084.281 I llm_load_vocab: special tokens cache size = 25
0.00.090.883 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.885 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.885 I llm_load_print_meta: arch             = gptneox
0.00.090.886 I llm_load_print_meta: vocab type       = BPE
0.00.090.886 I llm_load_print_meta: n_vocab          = 50304
0.00.090.887 I llm_load_print_meta: n_merges         = 50009
0.00.090.887 I llm_load_print_meta: vocab_only       = 0
0.00.090.887 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.887 I llm_load_print_meta: n_embd           = 2048
0.00.090.887 I llm_load_print_meta: n_layer          = 24
0.00.090.890 I llm_load_print_meta: n_head           = 16
0.00.090.890 I llm_load_print_meta: n_head_kv        = 16
0.00.090.890 I llm_load_print_meta: n_rot            = 32
0.00.090.891 I llm_load_print_meta: n_swa            = 0
0.00.090.891 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.891 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.893 I llm_load_print_meta: n_gqa            = 1
0.00.090.894 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.895 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.895 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.896 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.896 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.896 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.896 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.897 I llm_load_print_meta: n_ff             = 8192
0.00.090.897 I llm_load_print_meta: n_expert         = 0
0.00.090.897 I llm_load_print_meta: n_expert_used    = 0
0.00.090.897 I llm_load_print_meta: causal attn      = 1
0.00.090.897 I llm_load_print_meta: pooling type     = 0
0.00.090.899 I llm_load_print_meta: rope type        = 2
0.00.090.899 I llm_load_print_meta: rope scaling     = linear
0.00.090.899 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.899 I llm_load_print_meta: freq_scale_train = 1
0.00.090.899 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.900 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.900 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.900 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.900 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.900 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.900 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.913 I llm_load_print_meta: model type       = 1.4B
0.00.090.913 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.914 I llm_load_print_meta: model params     = 1.41 B
0.00.090.914 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.914 I llm_load_print_meta: general.name     = 1.4B
0.00.090.914 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.915 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.915 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.916 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.916 I llm_load_print_meta: LF token         = 128 ''
0.00.090.916 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.916 I llm_load_print_meta: max token length = 1024
0.00.093.350 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.350 I llm_load_tensors: offloading output layer to GPU
0.00.093.350 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.368 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.369 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.277 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.278 I llama_new_context_with_model: n_ctx         = 2048
0.00.094.278 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.094.278 I llama_new_context_with_model: n_batch       = 2048
0.00.094.278 I llama_new_context_with_model: n_ubatch      = 512
0.00.094.278 I llama_new_context_with_model: flash_attn    = 0
0.00.094.279 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.279 I llama_new_context_with_model: freq_scale    = 1
0.00.094.280 I ggml_metal_init: allocating
0.00.094.283 I ggml_metal_init: found device: Apple M4
0.00.094.285 I ggml_metal_init: picking default device: Apple M4
0.00.094.849 I ggml_metal_init: using embedded metal library
0.00.101.925 I ggml_metal_init: GPU name:   Apple M4
0.00.101.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.928 I ggml_metal_init: simdgroup reduction   = true
0.00.101.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.928 I ggml_metal_init: has bfloat            = true
0.00.101.928 I ggml_metal_init: use bfloat            = true
0.00.101.928 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.483 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.136.489 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.507 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.137.459 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.137.460 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.137.461 I llama_new_context_with_model: graph nodes  = 967
0.00.137.461 I llama_new_context_with_model: graph splits = 2
0.00.137.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.210.910 I main: llama threadpool init, n_threads = 4
0.00.210.943 I 
0.00.210.961 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.210.961 I 
0.00.211.040 I sampler seed: 1234
0.00.211.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.211.069 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.211.071 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.211.071 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.067.251 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.02.067.252 I llama_perf_context_print:        load time =     184.17 ms
0.02.067.252 I llama_perf_context_print: prompt eval time =      37.68 ms /     7 tokens (    5.38 ms per token,   185.77 tokens per second)
0.02.067.253 I llama_perf_context_print:        eval time =    1815.61 ms /    63 runs   (   28.82 ms per token,    34.70 tokens per second)
0.02.067.254 I llama_perf_context_print:       total time =    1856.34 ms /    70 tokens
0.02.067.451 I ggml_metal_free: deallocating

real	0m2.362s
user	0m0.141s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.713 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.355 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.603 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.625 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.627 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.628 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.346 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.057.348 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.348 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.349 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.349 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.350 I llama_model_loader: - type  f32:  194 tensors
0.00.057.350 I llama_model_loader: - type  f16:   98 tensors
0.00.085.982 I llm_load_vocab: special tokens cache size = 25
0.00.092.558 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.560 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.561 I llm_load_print_meta: arch             = gptneox
0.00.092.561 I llm_load_print_meta: vocab type       = BPE
0.00.092.561 I llm_load_print_meta: n_vocab          = 50304
0.00.092.561 I llm_load_print_meta: n_merges         = 50009
0.00.092.562 I llm_load_print_meta: vocab_only       = 0
0.00.092.562 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.562 I llm_load_print_meta: n_embd           = 2048
0.00.092.562 I llm_load_print_meta: n_layer          = 24
0.00.092.565 I llm_load_print_meta: n_head           = 16
0.00.092.565 I llm_load_print_meta: n_head_kv        = 16
0.00.092.566 I llm_load_print_meta: n_rot            = 32
0.00.092.566 I llm_load_print_meta: n_swa            = 0
0.00.092.566 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.566 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.567 I llm_load_print_meta: n_gqa            = 1
0.00.092.567 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.570 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.570 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.571 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.571 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.571 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.571 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.572 I llm_load_print_meta: n_ff             = 8192
0.00.092.572 I llm_load_print_meta: n_expert         = 0
0.00.092.572 I llm_load_print_meta: n_expert_used    = 0
0.00.092.572 I llm_load_print_meta: causal attn      = 1
0.00.092.572 I llm_load_print_meta: pooling type     = 0
0.00.092.573 I llm_load_print_meta: rope type        = 2
0.00.092.573 I llm_load_print_meta: rope scaling     = linear
0.00.092.574 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.574 I llm_load_print_meta: freq_scale_train = 1
0.00.092.574 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.575 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.575 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.575 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.575 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.575 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.575 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.580 I llm_load_print_meta: model type       = 1.4B
0.00.092.581 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.581 I llm_load_print_meta: model params     = 1.41 B
0.00.092.582 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.582 I llm_load_print_meta: general.name     = 1.4B
0.00.092.582 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.582 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.582 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.582 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.584 I llm_load_print_meta: LF token         = 128 ''
0.00.092.584 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.584 I llm_load_print_meta: max token length = 1024
0.00.095.025 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.025 I llm_load_tensors: offloading output layer to GPU
0.00.095.025 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.035 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.036 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.965 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.966 I llama_new_context_with_model: n_ctx         = 128
0.00.095.966 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.966 I llama_new_context_with_model: n_batch       = 128
0.00.095.967 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.967 I llama_new_context_with_model: flash_attn    = 0
0.00.095.967 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.968 I llama_new_context_with_model: freq_scale    = 1
0.00.095.968 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.968 I ggml_metal_init: allocating
0.00.095.974 I ggml_metal_init: found device: Apple M4
0.00.095.976 I ggml_metal_init: picking default device: Apple M4
0.00.096.529 I ggml_metal_init: using embedded metal library
0.00.098.588 I ggml_metal_init: GPU name:   Apple M4
0.00.098.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.590 I ggml_metal_init: simdgroup reduction   = true
0.00.098.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.590 I ggml_metal_init: has bfloat            = true
0.00.098.590 I ggml_metal_init: use bfloat            = true
0.00.098.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.888 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.892 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.906 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.832 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.833 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.833 I llama_new_context_with_model: graph nodes  = 967
0.00.108.833 I llama_new_context_with_model: graph splits = 2
0.00.108.846 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.532 I 
0.00.936.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.936.573 I perplexity: tokenizing the input ..
0.00.950.436 I perplexity: tokenization took 13.859 ms
0.00.950.481 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.084.211 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.085.665 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.085.686 I llama_perf_context_print:        load time =     910.16 ms
0.01.085.688 I llama_perf_context_print: prompt eval time =     132.77 ms /   128 tokens (    1.04 ms per token,   964.04 tokens per second)
0.01.085.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.085.689 I llama_perf_context_print:       total time =     149.16 ms /   129 tokens
0.01.086.185 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.122s
sys	0m0.194s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.993 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.999 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.999 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.999 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.000 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.001 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.001 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.002 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.002 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.004 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.918 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.828 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.832 I llama_model_loader: - type  f32:  194 tensors
0.00.037.832 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.576 I llm_load_vocab: special tokens cache size = 25
0.00.069.630 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.634 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.634 I llm_load_print_meta: arch             = gptneox
0.00.069.635 I llm_load_print_meta: vocab type       = BPE
0.00.069.635 I llm_load_print_meta: n_vocab          = 50304
0.00.069.636 I llm_load_print_meta: n_merges         = 50009
0.00.069.637 I llm_load_print_meta: vocab_only       = 0
0.00.069.637 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.637 I llm_load_print_meta: n_embd           = 2048
0.00.069.637 I llm_load_print_meta: n_layer          = 24
0.00.069.643 I llm_load_print_meta: n_head           = 16
0.00.069.644 I llm_load_print_meta: n_head_kv        = 16
0.00.069.644 I llm_load_print_meta: n_rot            = 32
0.00.069.645 I llm_load_print_meta: n_swa            = 0
0.00.069.645 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.645 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.647 I llm_load_print_meta: n_gqa            = 1
0.00.069.648 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.649 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.650 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.650 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.650 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.650 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.651 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.651 I llm_load_print_meta: n_ff             = 8192
0.00.069.652 I llm_load_print_meta: n_expert         = 0
0.00.069.652 I llm_load_print_meta: n_expert_used    = 0
0.00.069.652 I llm_load_print_meta: causal attn      = 1
0.00.069.652 I llm_load_print_meta: pooling type     = 0
0.00.069.652 I llm_load_print_meta: rope type        = 2
0.00.069.653 I llm_load_print_meta: rope scaling     = linear
0.00.069.653 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.653 I llm_load_print_meta: freq_scale_train = 1
0.00.069.653 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.653 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.654 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.654 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.654 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.654 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.655 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.669 I llm_load_print_meta: model type       = 1.4B
0.00.069.669 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.670 I llm_load_print_meta: model params     = 1.41 B
0.00.069.670 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.670 I llm_load_print_meta: general.name     = 1.4B
0.00.069.671 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.671 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.671 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.671 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.672 I llm_load_print_meta: LF token         = 128 ''
0.00.069.672 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.672 I llm_load_print_meta: max token length = 1024
0.00.071.649 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.649 I llm_load_tensors: offloading output layer to GPU
0.00.071.650 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.659 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.661 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.644 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.645 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.645 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.645 I llama_new_context_with_model: n_batch       = 2048
0.00.072.645 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.646 I llama_new_context_with_model: flash_attn    = 0
0.00.072.646 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.646 I llama_new_context_with_model: freq_scale    = 1
0.00.072.647 I ggml_metal_init: allocating
0.00.072.650 I ggml_metal_init: found device: Apple M4
0.00.072.652 I ggml_metal_init: picking default device: Apple M4
0.00.073.419 I ggml_metal_init: using embedded metal library
0.00.075.829 I ggml_metal_init: GPU name:   Apple M4
0.00.075.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.832 I ggml_metal_init: simdgroup reduction   = true
0.00.075.832 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.832 I ggml_metal_init: has bfloat            = true
0.00.075.833 I ggml_metal_init: use bfloat            = true
0.00.075.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.833 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.588 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.598 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.635 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.678 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.680 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.681 I llama_new_context_with_model: graph nodes  = 967
0.00.111.681 I llama_new_context_with_model: graph splits = 2
0.00.111.706 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.328.195 I main: llama threadpool init, n_threads = 4
0.01.328.260 I 
0.01.328.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.328.302 I 
0.01.328.843 I sampler seed: 1234
0.01.328.848 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.328.923 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.328.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.328.925 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.429.527 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.02.429.527 I llama_perf_context_print:        load time =    1318.19 ms
0.02.429.529 I llama_perf_context_print: prompt eval time =      42.43 ms /     7 tokens (    6.06 ms per token,   165.00 tokens per second)
0.02.429.529 I llama_perf_context_print:        eval time =    1055.14 ms /    63 runs   (   16.75 ms per token,    59.71 tokens per second)
0.02.429.530 I llama_perf_context_print:       total time =    1101.34 ms /    70 tokens
0.02.429.717 I ggml_metal_free: deallocating

real	0m2.449s
user	0m0.127s
sys	0m0.260s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.461 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.133 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.134 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.135 I llama_model_loader: - type  f32:  194 tensors
0.00.029.135 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.430 I llm_load_vocab: special tokens cache size = 25
0.00.058.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.409 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.409 I llm_load_print_meta: arch             = gptneox
0.00.058.409 I llm_load_print_meta: vocab type       = BPE
0.00.058.409 I llm_load_print_meta: n_vocab          = 50304
0.00.058.409 I llm_load_print_meta: n_merges         = 50009
0.00.058.410 I llm_load_print_meta: vocab_only       = 0
0.00.058.410 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.410 I llm_load_print_meta: n_embd           = 2048
0.00.058.410 I llm_load_print_meta: n_layer          = 24
0.00.058.414 I llm_load_print_meta: n_head           = 16
0.00.058.415 I llm_load_print_meta: n_head_kv        = 16
0.00.058.415 I llm_load_print_meta: n_rot            = 32
0.00.058.415 I llm_load_print_meta: n_swa            = 0
0.00.058.416 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.416 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.417 I llm_load_print_meta: n_gqa            = 1
0.00.058.418 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.419 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.420 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.420 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.420 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.420 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.421 I llm_load_print_meta: n_ff             = 8192
0.00.058.421 I llm_load_print_meta: n_expert         = 0
0.00.058.421 I llm_load_print_meta: n_expert_used    = 0
0.00.058.422 I llm_load_print_meta: causal attn      = 1
0.00.058.422 I llm_load_print_meta: pooling type     = 0
0.00.058.422 I llm_load_print_meta: rope type        = 2
0.00.058.422 I llm_load_print_meta: rope scaling     = linear
0.00.058.424 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.424 I llm_load_print_meta: freq_scale_train = 1
0.00.058.424 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.425 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.425 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.425 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.425 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.425 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.437 I llm_load_print_meta: model type       = 1.4B
0.00.058.437 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.438 I llm_load_print_meta: model params     = 1.41 B
0.00.058.438 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.438 I llm_load_print_meta: general.name     = 1.4B
0.00.058.438 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.439 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.439 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.439 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.439 I llm_load_print_meta: LF token         = 128 ''
0.00.058.441 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.441 I llm_load_print_meta: max token length = 1024
0.00.060.159 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.159 I llm_load_tensors: offloading output layer to GPU
0.00.060.159 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.169 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.170 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.061.308 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.309 I llama_new_context_with_model: n_ctx         = 128
0.00.061.309 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.061.309 I llama_new_context_with_model: n_batch       = 128
0.00.061.309 I llama_new_context_with_model: n_ubatch      = 128
0.00.061.309 I llama_new_context_with_model: flash_attn    = 0
0.00.061.310 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.310 I llama_new_context_with_model: freq_scale    = 1
0.00.061.311 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.311 I ggml_metal_init: allocating
0.00.061.315 I ggml_metal_init: found device: Apple M4
0.00.061.317 I ggml_metal_init: picking default device: Apple M4
0.00.061.903 I ggml_metal_init: using embedded metal library
0.00.064.129 I ggml_metal_init: GPU name:   Apple M4
0.00.064.131 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.132 I ggml_metal_init: simdgroup reduction   = true
0.00.064.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.132 I ggml_metal_init: has bfloat            = true
0.00.064.132 I ggml_metal_init: use bfloat            = true
0.00.064.133 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.015 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.021 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.038 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.970 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.971 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.971 I llama_new_context_with_model: graph nodes  = 967
0.00.074.972 I llama_new_context_with_model: graph splits = 2
0.00.074.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.218 I 
0.00.817.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.817.243 I perplexity: tokenizing the input ..
0.00.824.697 I perplexity: tokenization took 7.452 ms
0.00.824.707 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.946.421 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.947.627 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.947.642 I llama_perf_context_print:        load time =     806.75 ms
0.00.947.643 I llama_perf_context_print: prompt eval time =     121.49 ms /   128 tokens (    0.95 ms per token,  1053.60 tokens per second)
0.00.947.644 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.947.645 I llama_perf_context_print:       total time =     130.43 ms /   129 tokens
0.00.948.016 I ggml_metal_free: deallocating

real	0m0.963s
user	0m0.086s
sys	0m0.157s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.407 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.268 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.268 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.269 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.269 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.270 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.271 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.272 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.272 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.279 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.280 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.280 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.321 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.338 I llama_model_loader: - type  f32:  194 tensors
0.00.027.339 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.339 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.622 I llm_load_vocab: special tokens cache size = 25
0.00.054.657 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.661 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.661 I llm_load_print_meta: arch             = gptneox
0.00.054.662 I llm_load_print_meta: vocab type       = BPE
0.00.054.662 I llm_load_print_meta: n_vocab          = 50304
0.00.054.662 I llm_load_print_meta: n_merges         = 50009
0.00.054.662 I llm_load_print_meta: vocab_only       = 0
0.00.054.662 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.663 I llm_load_print_meta: n_embd           = 2048
0.00.054.665 I llm_load_print_meta: n_layer          = 24
0.00.054.669 I llm_load_print_meta: n_head           = 16
0.00.054.670 I llm_load_print_meta: n_head_kv        = 16
0.00.054.670 I llm_load_print_meta: n_rot            = 32
0.00.054.670 I llm_load_print_meta: n_swa            = 0
0.00.054.670 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.671 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.671 I llm_load_print_meta: n_gqa            = 1
0.00.054.672 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.673 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.674 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.674 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.675 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.675 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.675 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.676 I llm_load_print_meta: n_ff             = 8192
0.00.054.676 I llm_load_print_meta: n_expert         = 0
0.00.054.676 I llm_load_print_meta: n_expert_used    = 0
0.00.054.676 I llm_load_print_meta: causal attn      = 1
0.00.054.676 I llm_load_print_meta: pooling type     = 0
0.00.054.677 I llm_load_print_meta: rope type        = 2
0.00.054.677 I llm_load_print_meta: rope scaling     = linear
0.00.054.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.678 I llm_load_print_meta: freq_scale_train = 1
0.00.054.680 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.681 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.681 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.681 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.681 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.681 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.681 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.689 I llm_load_print_meta: model type       = 1.4B
0.00.054.690 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.691 I llm_load_print_meta: model params     = 1.41 B
0.00.054.691 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.691 I llm_load_print_meta: general.name     = 1.4B
0.00.054.691 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.692 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.692 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.692 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.692 I llm_load_print_meta: LF token         = 128 ''
0.00.054.692 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.693 I llm_load_print_meta: max token length = 1024
0.00.056.464 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.464 I llm_load_tensors: offloading output layer to GPU
0.00.056.465 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.469 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.470 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.401 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.402 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.402 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.402 I llama_new_context_with_model: n_batch       = 2048
0.00.057.402 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.402 I llama_new_context_with_model: flash_attn    = 0
0.00.057.403 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.403 I llama_new_context_with_model: freq_scale    = 1
0.00.057.404 I ggml_metal_init: allocating
0.00.057.408 I ggml_metal_init: found device: Apple M4
0.00.057.410 I ggml_metal_init: picking default device: Apple M4
0.00.058.101 I ggml_metal_init: using embedded metal library
0.00.060.221 I ggml_metal_init: GPU name:   Apple M4
0.00.060.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.223 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.224 I ggml_metal_init: simdgroup reduction   = true
0.00.060.224 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.224 I ggml_metal_init: has bfloat            = true
0.00.060.224 I ggml_metal_init: use bfloat            = true
0.00.060.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.225 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.377 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.384 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.406 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.530 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.531 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.532 I llama_new_context_with_model: graph nodes  = 967
0.00.093.532 I llama_new_context_with_model: graph splits = 2
0.00.093.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.947 I main: llama threadpool init, n_threads = 4
0.00.677.984 I 
0.00.678.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.678.010 I 
0.00.678.245 I sampler seed: 1234
0.00.678.249 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.678.293 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.678.294 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.678.294 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.358.521 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.358.522 I llama_perf_context_print:        load time =     666.53 ms
0.01.358.523 I llama_perf_context_print: prompt eval time =      40.23 ms /     7 tokens (    5.75 ms per token,   173.99 tokens per second)
0.01.358.524 I llama_perf_context_print:        eval time =     637.07 ms /    63 runs   (   10.11 ms per token,    98.89 tokens per second)
0.01.358.525 I llama_perf_context_print:       total time =     680.58 ms /    70 tokens
0.01.358.696 I ggml_metal_free: deallocating

real	0m1.376s
user	0m0.112s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.158 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.929 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.933 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.937 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.942 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.945 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.940 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.814 I llama_model_loader: - type  f32:  194 tensors
0.00.024.814 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.815 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.448 I llm_load_vocab: special tokens cache size = 25
0.00.051.543 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.546 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.546 I llm_load_print_meta: arch             = gptneox
0.00.051.547 I llm_load_print_meta: vocab type       = BPE
0.00.051.547 I llm_load_print_meta: n_vocab          = 50304
0.00.051.547 I llm_load_print_meta: n_merges         = 50009
0.00.051.547 I llm_load_print_meta: vocab_only       = 0
0.00.051.548 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.548 I llm_load_print_meta: n_embd           = 2048
0.00.051.548 I llm_load_print_meta: n_layer          = 24
0.00.051.551 I llm_load_print_meta: n_head           = 16
0.00.051.553 I llm_load_print_meta: n_head_kv        = 16
0.00.051.553 I llm_load_print_meta: n_rot            = 32
0.00.051.553 I llm_load_print_meta: n_swa            = 0
0.00.051.553 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.554 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.554 I llm_load_print_meta: n_gqa            = 1
0.00.051.560 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.561 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.561 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.563 I llm_load_print_meta: n_ff             = 8192
0.00.051.563 I llm_load_print_meta: n_expert         = 0
0.00.051.563 I llm_load_print_meta: n_expert_used    = 0
0.00.051.563 I llm_load_print_meta: causal attn      = 1
0.00.051.563 I llm_load_print_meta: pooling type     = 0
0.00.051.564 I llm_load_print_meta: rope type        = 2
0.00.051.564 I llm_load_print_meta: rope scaling     = linear
0.00.051.567 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.568 I llm_load_print_meta: freq_scale_train = 1
0.00.051.568 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.569 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.569 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.569 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.572 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.572 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.573 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.585 I llm_load_print_meta: model type       = 1.4B
0.00.051.585 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.585 I llm_load_print_meta: model params     = 1.41 B
0.00.051.586 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.586 I llm_load_print_meta: general.name     = 1.4B
0.00.051.586 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.587 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.587 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.587 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.587 I llm_load_print_meta: LF token         = 128 ''
0.00.051.587 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.587 I llm_load_print_meta: max token length = 1024
0.00.053.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.599 I llm_load_tensors: offloading output layer to GPU
0.00.053.599 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.609 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.610 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.642 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.643 I llama_new_context_with_model: n_ctx         = 128
0.00.054.643 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.643 I llama_new_context_with_model: n_batch       = 128
0.00.054.643 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.644 I llama_new_context_with_model: flash_attn    = 0
0.00.054.644 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.644 I llama_new_context_with_model: freq_scale    = 1
0.00.054.645 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.645 I ggml_metal_init: allocating
0.00.054.648 I ggml_metal_init: found device: Apple M4
0.00.054.650 I ggml_metal_init: picking default device: Apple M4
0.00.055.212 I ggml_metal_init: using embedded metal library
0.00.057.142 I ggml_metal_init: GPU name:   Apple M4
0.00.057.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.143 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.144 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.144 I ggml_metal_init: simdgroup reduction   = true
0.00.057.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.144 I ggml_metal_init: has bfloat            = true
0.00.057.144 I ggml_metal_init: use bfloat            = true
0.00.057.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.145 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.728 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.734 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.749 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.657 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.658 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.658 I llama_new_context_with_model: graph nodes  = 967
0.00.067.659 I llama_new_context_with_model: graph splits = 2
0.00.067.671 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.374 I 
0.00.609.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.609.393 I perplexity: tokenizing the input ..
0.00.617.406 I perplexity: tokenization took 8.011 ms
0.00.617.419 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.055 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.741.332 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.741.349 I llama_perf_context_print:        load time =     599.21 ms
0.00.741.354 I llama_perf_context_print: prompt eval time =     122.41 ms /   128 tokens (    0.96 ms per token,  1045.67 tokens per second)
0.00.741.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.355 I llama_perf_context_print:       total time =     131.97 ms /   129 tokens
0.00.741.747 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.077s
sys	0m0.112s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.111 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.562 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.397 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.397 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.398 I llama_model_loader: - type  f32:  194 tensors
0.00.024.398 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.398 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.887 I llm_load_vocab: special tokens cache size = 25
0.00.051.945 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.948 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.949 I llm_load_print_meta: arch             = gptneox
0.00.051.949 I llm_load_print_meta: vocab type       = BPE
0.00.051.949 I llm_load_print_meta: n_vocab          = 50304
0.00.051.949 I llm_load_print_meta: n_merges         = 50009
0.00.051.950 I llm_load_print_meta: vocab_only       = 0
0.00.051.950 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.950 I llm_load_print_meta: n_embd           = 2048
0.00.051.950 I llm_load_print_meta: n_layer          = 24
0.00.051.953 I llm_load_print_meta: n_head           = 16
0.00.051.954 I llm_load_print_meta: n_head_kv        = 16
0.00.051.954 I llm_load_print_meta: n_rot            = 32
0.00.051.957 I llm_load_print_meta: n_swa            = 0
0.00.051.957 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.957 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.958 I llm_load_print_meta: n_gqa            = 1
0.00.051.959 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.959 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.960 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.960 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.961 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.961 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.961 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.962 I llm_load_print_meta: n_ff             = 8192
0.00.051.962 I llm_load_print_meta: n_expert         = 0
0.00.051.962 I llm_load_print_meta: n_expert_used    = 0
0.00.051.964 I llm_load_print_meta: causal attn      = 1
0.00.051.965 I llm_load_print_meta: pooling type     = 0
0.00.051.966 I llm_load_print_meta: rope type        = 2
0.00.051.966 I llm_load_print_meta: rope scaling     = linear
0.00.051.966 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.967 I llm_load_print_meta: freq_scale_train = 1
0.00.051.967 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.967 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.967 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.967 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.967 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.967 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.968 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.980 I llm_load_print_meta: model type       = 1.4B
0.00.051.980 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.981 I llm_load_print_meta: model params     = 1.41 B
0.00.051.981 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.981 I llm_load_print_meta: general.name     = 1.4B
0.00.051.982 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.984 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.984 I llm_load_print_meta: LF token         = 128 ''
0.00.051.984 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.984 I llm_load_print_meta: max token length = 1024
0.00.053.850 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.851 I llm_load_tensors: offloading output layer to GPU
0.00.053.851 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.856 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.857 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.745 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.745 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.746 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.746 I llama_new_context_with_model: n_batch       = 2048
0.00.054.746 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.746 I llama_new_context_with_model: flash_attn    = 0
0.00.054.747 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.747 I llama_new_context_with_model: freq_scale    = 1
0.00.054.747 I ggml_metal_init: allocating
0.00.054.751 I ggml_metal_init: found device: Apple M4
0.00.054.753 I ggml_metal_init: picking default device: Apple M4
0.00.055.314 I ggml_metal_init: using embedded metal library
0.00.057.293 I ggml_metal_init: GPU name:   Apple M4
0.00.057.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.295 I ggml_metal_init: simdgroup reduction   = true
0.00.057.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.296 I ggml_metal_init: has bfloat            = true
0.00.057.297 I ggml_metal_init: use bfloat            = true
0.00.057.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.299 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.249 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.261 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.288 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.286 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.287 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.288 I llama_new_context_with_model: graph nodes  = 967
0.00.087.288 I llama_new_context_with_model: graph splits = 2
0.00.087.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.397 I main: llama threadpool init, n_threads = 4
0.00.644.426 I 
0.00.644.443 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.644.444 I 
0.00.644.627 I sampler seed: 1234
0.00.644.632 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.644.667 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.644.670 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.644.670 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.369.628 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.369.628 I llama_perf_context_print:        load time =     635.28 ms
0.01.369.629 I llama_perf_context_print: prompt eval time =      32.73 ms /     7 tokens (    4.68 ms per token,   213.85 tokens per second)
0.01.369.630 I llama_perf_context_print:        eval time =     689.26 ms /    63 runs   (   10.94 ms per token,    91.40 tokens per second)
0.01.369.630 I llama_perf_context_print:       total time =     725.23 ms /    70 tokens
0.01.369.791 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.716 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.650 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.650 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.650 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.652 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.652 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.652 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.653 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.654 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.658 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.386 I llama_model_loader: - type  f32:  194 tensors
0.00.023.386 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.387 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.832 I llm_load_vocab: special tokens cache size = 25
0.00.049.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.844 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.844 I llm_load_print_meta: arch             = gptneox
0.00.049.845 I llm_load_print_meta: vocab type       = BPE
0.00.049.845 I llm_load_print_meta: n_vocab          = 50304
0.00.049.845 I llm_load_print_meta: n_merges         = 50009
0.00.049.845 I llm_load_print_meta: vocab_only       = 0
0.00.049.845 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.846 I llm_load_print_meta: n_embd           = 2048
0.00.049.846 I llm_load_print_meta: n_layer          = 24
0.00.049.848 I llm_load_print_meta: n_head           = 16
0.00.049.849 I llm_load_print_meta: n_head_kv        = 16
0.00.049.849 I llm_load_print_meta: n_rot            = 32
0.00.049.849 I llm_load_print_meta: n_swa            = 0
0.00.049.850 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.850 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.851 I llm_load_print_meta: n_gqa            = 1
0.00.049.851 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.852 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.852 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.853 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.853 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.856 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.856 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.857 I llm_load_print_meta: n_ff             = 8192
0.00.049.857 I llm_load_print_meta: n_expert         = 0
0.00.049.857 I llm_load_print_meta: n_expert_used    = 0
0.00.049.857 I llm_load_print_meta: causal attn      = 1
0.00.049.857 I llm_load_print_meta: pooling type     = 0
0.00.049.858 I llm_load_print_meta: rope type        = 2
0.00.049.858 I llm_load_print_meta: rope scaling     = linear
0.00.049.860 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.861 I llm_load_print_meta: freq_scale_train = 1
0.00.049.861 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.862 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.862 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.862 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.862 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.862 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.862 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.875 I llm_load_print_meta: model type       = 1.4B
0.00.049.875 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.875 I llm_load_print_meta: model params     = 1.41 B
0.00.049.876 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.876 I llm_load_print_meta: general.name     = 1.4B
0.00.049.876 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.876 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.876 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.877 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.877 I llm_load_print_meta: LF token         = 128 ''
0.00.049.877 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.877 I llm_load_print_meta: max token length = 1024
0.00.051.874 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.874 I llm_load_tensors: offloading output layer to GPU
0.00.051.874 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.884 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.885 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.817 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.818 I llama_new_context_with_model: n_ctx         = 128
0.00.052.818 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.818 I llama_new_context_with_model: n_batch       = 128
0.00.052.818 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.819 I llama_new_context_with_model: flash_attn    = 0
0.00.052.819 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.819 I llama_new_context_with_model: freq_scale    = 1
0.00.052.820 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.820 I ggml_metal_init: allocating
0.00.052.823 I ggml_metal_init: found device: Apple M4
0.00.052.825 I ggml_metal_init: picking default device: Apple M4
0.00.053.355 I ggml_metal_init: using embedded metal library
0.00.055.269 I ggml_metal_init: GPU name:   Apple M4
0.00.055.270 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.270 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.271 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.271 I ggml_metal_init: simdgroup reduction   = true
0.00.055.271 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.271 I ggml_metal_init: has bfloat            = true
0.00.055.271 I ggml_metal_init: use bfloat            = true
0.00.055.272 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.272 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.595 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.597 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.610 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.545 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.546 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.547 I llama_new_context_with_model: graph nodes  = 967
0.00.065.547 I llama_new_context_with_model: graph splits = 2
0.00.065.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.537 I 
0.00.659.553 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.659.560 I perplexity: tokenizing the input ..
0.00.667.542 I perplexity: tokenization took 7.981 ms
0.00.667.552 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.890 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.791.030 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.791.044 I llama_perf_context_print:        load time =     650.82 ms
0.00.791.045 I llama_perf_context_print: prompt eval time =     122.12 ms /   128 tokens (    0.95 ms per token,  1048.14 tokens per second)
0.00.791.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.046 I llama_perf_context_print:       total time =     131.51 ms /   129 tokens
0.00.791.411 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.077s
sys	0m0.112s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.032 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.755 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.764 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.765 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.747 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.748 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.749 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.749 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.749 I llama_model_loader: - type  f32:  194 tensors
0.00.025.750 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.750 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.236 I llm_load_vocab: special tokens cache size = 25
0.00.052.221 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.224 I llm_load_print_meta: arch             = gptneox
0.00.052.224 I llm_load_print_meta: vocab type       = BPE
0.00.052.224 I llm_load_print_meta: n_vocab          = 50304
0.00.052.224 I llm_load_print_meta: n_merges         = 50009
0.00.052.225 I llm_load_print_meta: vocab_only       = 0
0.00.052.225 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.225 I llm_load_print_meta: n_embd           = 2048
0.00.052.225 I llm_load_print_meta: n_layer          = 24
0.00.052.228 I llm_load_print_meta: n_head           = 16
0.00.052.229 I llm_load_print_meta: n_head_kv        = 16
0.00.052.229 I llm_load_print_meta: n_rot            = 32
0.00.052.229 I llm_load_print_meta: n_swa            = 0
0.00.052.229 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.229 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.230 I llm_load_print_meta: n_gqa            = 1
0.00.052.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.232 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.232 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.233 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.233 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.233 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.234 I llm_load_print_meta: n_ff             = 8192
0.00.052.236 I llm_load_print_meta: n_expert         = 0
0.00.052.237 I llm_load_print_meta: n_expert_used    = 0
0.00.052.238 I llm_load_print_meta: causal attn      = 1
0.00.052.239 I llm_load_print_meta: pooling type     = 0
0.00.052.240 I llm_load_print_meta: rope type        = 2
0.00.052.240 I llm_load_print_meta: rope scaling     = linear
0.00.052.240 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.240 I llm_load_print_meta: freq_scale_train = 1
0.00.052.241 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.241 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.241 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.241 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.241 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.241 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.241 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.253 I llm_load_print_meta: model type       = 1.4B
0.00.052.254 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.254 I llm_load_print_meta: model params     = 1.41 B
0.00.052.255 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.255 I llm_load_print_meta: general.name     = 1.4B
0.00.052.255 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.256 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.258 I llm_load_print_meta: LF token         = 128 ''
0.00.052.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.258 I llm_load_print_meta: max token length = 1024
0.00.054.244 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.244 I llm_load_tensors: offloading output layer to GPU
0.00.054.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.254 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.255 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.275 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.276 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.276 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.276 I llama_new_context_with_model: n_batch       = 2048
0.00.055.277 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.277 I llama_new_context_with_model: flash_attn    = 0
0.00.055.277 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.277 I llama_new_context_with_model: freq_scale    = 1
0.00.055.278 I ggml_metal_init: allocating
0.00.055.281 I ggml_metal_init: found device: Apple M4
0.00.055.283 I ggml_metal_init: picking default device: Apple M4
0.00.055.833 I ggml_metal_init: using embedded metal library
0.00.057.746 I ggml_metal_init: GPU name:   Apple M4
0.00.057.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.748 I ggml_metal_init: simdgroup reduction   = true
0.00.057.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.748 I ggml_metal_init: has bfloat            = true
0.00.057.749 I ggml_metal_init: use bfloat            = true
0.00.057.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.376 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.382 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.402 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.455 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.456 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.456 I llama_new_context_with_model: graph nodes  = 967
0.00.086.456 I llama_new_context_with_model: graph splits = 2
0.00.086.479 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.972 I main: llama threadpool init, n_threads = 4
0.00.689.008 I 
0.00.689.045 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.689.048 I 
0.00.689.277 I sampler seed: 1234
0.00.689.282 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.293 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.293 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.293 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.473.175 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.473.176 I llama_perf_context_print:        load time =     678.94 ms
0.01.473.177 I llama_perf_context_print: prompt eval time =      36.59 ms /     7 tokens (    5.23 ms per token,   191.28 tokens per second)
0.01.473.178 I llama_perf_context_print:        eval time =     744.37 ms /    63 runs   (   11.82 ms per token,    84.64 tokens per second)
0.01.473.178 I llama_perf_context_print:       total time =     784.21 ms /    70 tokens
0.01.473.335 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.373 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.269 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.273 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.275 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.278 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.278 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.279 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.279 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.279 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.280 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.282 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.180 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.117 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.118 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.119 I llama_model_loader: - type  f32:  194 tensors
0.00.025.119 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.119 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.136 I llm_load_vocab: special tokens cache size = 25
0.00.052.396 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.399 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.399 I llm_load_print_meta: arch             = gptneox
0.00.052.399 I llm_load_print_meta: vocab type       = BPE
0.00.052.400 I llm_load_print_meta: n_vocab          = 50304
0.00.052.400 I llm_load_print_meta: n_merges         = 50009
0.00.052.400 I llm_load_print_meta: vocab_only       = 0
0.00.052.400 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.400 I llm_load_print_meta: n_embd           = 2048
0.00.052.401 I llm_load_print_meta: n_layer          = 24
0.00.052.404 I llm_load_print_meta: n_head           = 16
0.00.052.405 I llm_load_print_meta: n_head_kv        = 16
0.00.052.405 I llm_load_print_meta: n_rot            = 32
0.00.052.405 I llm_load_print_meta: n_swa            = 0
0.00.052.405 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.405 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.408 I llm_load_print_meta: n_gqa            = 1
0.00.052.409 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.410 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.410 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.412 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.413 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.413 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.413 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.414 I llm_load_print_meta: n_ff             = 8192
0.00.052.414 I llm_load_print_meta: n_expert         = 0
0.00.052.414 I llm_load_print_meta: n_expert_used    = 0
0.00.052.414 I llm_load_print_meta: causal attn      = 1
0.00.052.414 I llm_load_print_meta: pooling type     = 0
0.00.052.415 I llm_load_print_meta: rope type        = 2
0.00.052.415 I llm_load_print_meta: rope scaling     = linear
0.00.052.415 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.415 I llm_load_print_meta: freq_scale_train = 1
0.00.052.416 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.416 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.420 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.420 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.420 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.421 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.422 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.433 I llm_load_print_meta: model type       = 1.4B
0.00.052.434 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.435 I llm_load_print_meta: model params     = 1.41 B
0.00.052.435 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.435 I llm_load_print_meta: general.name     = 1.4B
0.00.052.436 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.436 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.437 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.437 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: LF token         = 128 ''
0.00.052.438 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: max token length = 1024
0.00.054.089 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.089 I llm_load_tensors: offloading output layer to GPU
0.00.054.089 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.099 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.100 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.944 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.945 I llama_new_context_with_model: n_ctx         = 128
0.00.054.945 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.945 I llama_new_context_with_model: n_batch       = 128
0.00.054.946 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.946 I llama_new_context_with_model: flash_attn    = 0
0.00.054.946 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.946 I llama_new_context_with_model: freq_scale    = 1
0.00.054.947 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.947 I ggml_metal_init: allocating
0.00.054.951 I ggml_metal_init: found device: Apple M4
0.00.054.953 I ggml_metal_init: picking default device: Apple M4
0.00.055.534 I ggml_metal_init: using embedded metal library
0.00.057.507 I ggml_metal_init: GPU name:   Apple M4
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.510 I ggml_metal_init: simdgroup reduction   = true
0.00.057.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.510 I ggml_metal_init: has bfloat            = true
0.00.057.510 I ggml_metal_init: use bfloat            = true
0.00.057.510 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.211 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.213 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.227 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.111 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.113 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.113 I llama_new_context_with_model: graph nodes  = 967
0.00.068.113 I llama_new_context_with_model: graph splits = 2
0.00.068.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.025 I 
0.00.690.052 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.690.057 I perplexity: tokenizing the input ..
0.00.697.784 I perplexity: tokenization took 7.726 ms
0.00.697.797 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.439 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.833.546 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.833.562 I llama_perf_context_print:        load time =     679.65 ms
0.00.833.563 I llama_perf_context_print: prompt eval time =     134.39 ms /   128 tokens (    1.05 ms per token,   952.47 tokens per second)
0.00.833.564 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.565 I llama_perf_context_print:       total time =     143.54 ms /   129 tokens
0.00.834.006 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.079s
sys	0m0.119s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.940 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.966 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.977 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.979 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.980 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.981 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.981 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.985 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.986 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.988 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.923 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.923 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.924 I llama_model_loader: - type  f32:  194 tensors
0.00.026.925 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.925 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.428 I llm_load_vocab: special tokens cache size = 25
0.00.054.612 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.615 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.615 I llm_load_print_meta: arch             = gptneox
0.00.054.615 I llm_load_print_meta: vocab type       = BPE
0.00.054.616 I llm_load_print_meta: n_vocab          = 50304
0.00.054.616 I llm_load_print_meta: n_merges         = 50009
0.00.054.616 I llm_load_print_meta: vocab_only       = 0
0.00.054.616 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.616 I llm_load_print_meta: n_embd           = 2048
0.00.054.617 I llm_load_print_meta: n_layer          = 24
0.00.054.619 I llm_load_print_meta: n_head           = 16
0.00.054.620 I llm_load_print_meta: n_head_kv        = 16
0.00.054.620 I llm_load_print_meta: n_rot            = 32
0.00.054.620 I llm_load_print_meta: n_swa            = 0
0.00.054.620 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.620 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.621 I llm_load_print_meta: n_gqa            = 1
0.00.054.622 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.623 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.623 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.624 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.624 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.625 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.625 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.626 I llm_load_print_meta: n_ff             = 8192
0.00.054.626 I llm_load_print_meta: n_expert         = 0
0.00.054.626 I llm_load_print_meta: n_expert_used    = 0
0.00.054.628 I llm_load_print_meta: causal attn      = 1
0.00.054.629 I llm_load_print_meta: pooling type     = 0
0.00.054.629 I llm_load_print_meta: rope type        = 2
0.00.054.630 I llm_load_print_meta: rope scaling     = linear
0.00.054.630 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.630 I llm_load_print_meta: freq_scale_train = 1
0.00.054.631 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.631 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.631 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.631 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.631 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.631 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.631 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.643 I llm_load_print_meta: model type       = 1.4B
0.00.054.643 I llm_load_print_meta: model ftype      = Q5_1
0.00.054.644 I llm_load_print_meta: model params     = 1.41 B
0.00.054.644 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.054.644 I llm_load_print_meta: general.name     = 1.4B
0.00.054.645 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.645 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.645 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.645 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.645 I llm_load_print_meta: LF token         = 128 ''
0.00.054.646 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.646 I llm_load_print_meta: max token length = 1024
0.00.056.732 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.732 I llm_load_tensors: offloading output layer to GPU
0.00.056.732 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.742 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.744 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.057.724 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.725 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.725 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.725 I llama_new_context_with_model: n_batch       = 2048
0.00.057.725 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.725 I llama_new_context_with_model: flash_attn    = 0
0.00.057.726 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.726 I llama_new_context_with_model: freq_scale    = 1
0.00.057.726 I ggml_metal_init: allocating
0.00.057.730 I ggml_metal_init: found device: Apple M4
0.00.057.732 I ggml_metal_init: picking default device: Apple M4
0.00.058.305 I ggml_metal_init: using embedded metal library
0.00.060.328 I ggml_metal_init: GPU name:   Apple M4
0.00.060.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.330 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.330 I ggml_metal_init: simdgroup reduction   = true
0.00.060.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.332 I ggml_metal_init: has bfloat            = true
0.00.060.333 I ggml_metal_init: use bfloat            = true
0.00.060.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.158 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.166 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.193 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.257 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.258 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.258 I llama_new_context_with_model: graph nodes  = 967
0.00.091.258 I llama_new_context_with_model: graph splits = 2
0.00.091.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.920 I main: llama threadpool init, n_threads = 4
0.00.788.956 I 
0.00.788.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.788.975 I 
0.00.789.250 I sampler seed: 1234
0.00.789.255 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.295 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.295 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.295 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.630.304 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.630.305 I llama_perf_context_print:        load time =     777.98 ms
0.01.630.306 I llama_perf_context_print: prompt eval time =      36.60 ms /     7 tokens (    5.23 ms per token,   191.24 tokens per second)
0.01.630.307 I llama_perf_context_print:        eval time =     801.38 ms /    63 runs   (   12.72 ms per token,    78.61 tokens per second)
0.01.630.307 I llama_perf_context_print:       total time =     841.39 ms /    70 tokens
0.01.630.475 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.111s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.986 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.887 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.893 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.894 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.895 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.895 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.896 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.667 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.668 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.668 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.669 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.669 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.669 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.670 I llama_model_loader: - type  f32:  194 tensors
0.00.023.670 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.670 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.151 I llm_load_vocab: special tokens cache size = 25
0.00.050.120 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.123 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.124 I llm_load_print_meta: arch             = gptneox
0.00.050.124 I llm_load_print_meta: vocab type       = BPE
0.00.050.124 I llm_load_print_meta: n_vocab          = 50304
0.00.050.124 I llm_load_print_meta: n_merges         = 50009
0.00.050.125 I llm_load_print_meta: vocab_only       = 0
0.00.050.125 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.125 I llm_load_print_meta: n_embd           = 2048
0.00.050.125 I llm_load_print_meta: n_layer          = 24
0.00.050.130 I llm_load_print_meta: n_head           = 16
0.00.050.131 I llm_load_print_meta: n_head_kv        = 16
0.00.050.131 I llm_load_print_meta: n_rot            = 32
0.00.050.131 I llm_load_print_meta: n_swa            = 0
0.00.050.132 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.132 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.133 I llm_load_print_meta: n_gqa            = 1
0.00.050.133 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.134 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.134 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.135 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.135 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.135 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.135 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.136 I llm_load_print_meta: n_ff             = 8192
0.00.050.136 I llm_load_print_meta: n_expert         = 0
0.00.050.136 I llm_load_print_meta: n_expert_used    = 0
0.00.050.137 I llm_load_print_meta: causal attn      = 1
0.00.050.137 I llm_load_print_meta: pooling type     = 0
0.00.050.137 I llm_load_print_meta: rope type        = 2
0.00.050.137 I llm_load_print_meta: rope scaling     = linear
0.00.050.139 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.139 I llm_load_print_meta: freq_scale_train = 1
0.00.050.139 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.139 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.140 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.140 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.140 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.140 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.140 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.152 I llm_load_print_meta: model type       = 1.4B
0.00.050.152 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.153 I llm_load_print_meta: model params     = 1.41 B
0.00.050.154 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.154 I llm_load_print_meta: general.name     = 1.4B
0.00.050.154 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.155 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.155 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.155 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.156 I llm_load_print_meta: LF token         = 128 ''
0.00.050.156 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.156 I llm_load_print_meta: max token length = 1024
0.00.051.767 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.767 I llm_load_tensors: offloading output layer to GPU
0.00.051.767 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.777 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.778 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.670 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.670 I llama_new_context_with_model: n_ctx         = 128
0.00.052.671 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.671 I llama_new_context_with_model: n_batch       = 128
0.00.052.671 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.671 I llama_new_context_with_model: flash_attn    = 0
0.00.052.671 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.672 I llama_new_context_with_model: freq_scale    = 1
0.00.052.672 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.672 I ggml_metal_init: allocating
0.00.052.676 I ggml_metal_init: found device: Apple M4
0.00.052.678 I ggml_metal_init: picking default device: Apple M4
0.00.053.231 I ggml_metal_init: using embedded metal library
0.00.055.142 I ggml_metal_init: GPU name:   Apple M4
0.00.055.143 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.144 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.144 I ggml_metal_init: simdgroup reduction   = true
0.00.055.144 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.145 I ggml_metal_init: has bfloat            = true
0.00.055.145 I ggml_metal_init: use bfloat            = true
0.00.055.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.424 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.437 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.340 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.341 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.341 I llama_new_context_with_model: graph nodes  = 967
0.00.065.341 I llama_new_context_with_model: graph splits = 2
0.00.065.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.151 I 
0.00.745.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.745.175 I perplexity: tokenizing the input ..
0.00.753.007 I perplexity: tokenization took 7.831 ms
0.00.753.018 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.887.815 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.888.987 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.009 I llama_perf_context_print:        load time =     736.16 ms
0.00.889.010 I llama_perf_context_print: prompt eval time =     134.55 ms /   128 tokens (    1.05 ms per token,   951.31 tokens per second)
0.00.889.011 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.011 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.889.424 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.077s
sys	0m0.125s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.017.669 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.110 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.025.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.121 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.127 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.129 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.693 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.695 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.696 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.696 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.697 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.697 I llama_model_loader: - type  f32:  194 tensors
0.00.036.698 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.698 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.698 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.407 I llm_load_vocab: special tokens cache size = 25
0.00.080.051 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.055 I llm_load_print_meta: arch             = gptneox
0.00.080.056 I llm_load_print_meta: vocab type       = BPE
0.00.080.056 I llm_load_print_meta: n_vocab          = 50304
0.00.080.056 I llm_load_print_meta: n_merges         = 50009
0.00.080.057 I llm_load_print_meta: vocab_only       = 0
0.00.080.057 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.057 I llm_load_print_meta: n_embd           = 2048
0.00.080.057 I llm_load_print_meta: n_layer          = 24
0.00.080.061 I llm_load_print_meta: n_head           = 16
0.00.080.062 I llm_load_print_meta: n_head_kv        = 16
0.00.080.062 I llm_load_print_meta: n_rot            = 32
0.00.080.062 I llm_load_print_meta: n_swa            = 0
0.00.080.063 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.063 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.064 I llm_load_print_meta: n_gqa            = 1
0.00.080.065 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.066 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.067 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.067 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.067 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.068 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.068 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.069 I llm_load_print_meta: n_ff             = 8192
0.00.080.069 I llm_load_print_meta: n_expert         = 0
0.00.080.069 I llm_load_print_meta: n_expert_used    = 0
0.00.080.069 I llm_load_print_meta: causal attn      = 1
0.00.080.070 I llm_load_print_meta: pooling type     = 0
0.00.080.070 I llm_load_print_meta: rope type        = 2
0.00.080.070 I llm_load_print_meta: rope scaling     = linear
0.00.080.071 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.071 I llm_load_print_meta: freq_scale_train = 1
0.00.080.071 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.072 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.072 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.072 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.073 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.073 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.073 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.085 I llm_load_print_meta: model type       = 1.4B
0.00.080.086 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.080.087 I llm_load_print_meta: model params     = 1.41 B
0.00.080.087 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.080.088 I llm_load_print_meta: general.name     = 1.4B
0.00.080.088 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.088 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.089 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.089 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.089 I llm_load_print_meta: LF token         = 128 ''
0.00.080.091 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.091 I llm_load_print_meta: max token length = 1024
0.00.082.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.685 I llm_load_tensors: offloading output layer to GPU
0.00.082.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.696 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.082.697 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.084.121 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.122 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.122 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.123 I llama_new_context_with_model: n_batch       = 2048
0.00.084.123 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.123 I llama_new_context_with_model: flash_attn    = 0
0.00.084.124 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.124 I llama_new_context_with_model: freq_scale    = 1
0.00.084.125 I ggml_metal_init: allocating
0.00.084.128 I ggml_metal_init: found device: Apple M4
0.00.084.131 I ggml_metal_init: picking default device: Apple M4
0.00.084.897 I ggml_metal_init: using embedded metal library
0.00.087.748 I ggml_metal_init: GPU name:   Apple M4
0.00.087.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.751 I ggml_metal_init: simdgroup reduction   = true
0.00.087.751 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.752 I ggml_metal_init: has bfloat            = true
0.00.087.752 I ggml_metal_init: use bfloat            = true
0.00.087.752 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.088 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.093 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.112 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.115 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.116 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.116 I llama_new_context_with_model: graph nodes  = 967
0.00.120.117 I llama_new_context_with_model: graph splits = 2
0.00.120.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.248 I main: llama threadpool init, n_threads = 4
0.00.540.299 I 
0.00.540.333 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.540.337 I 
0.00.540.658 I sampler seed: 1234
0.00.540.663 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.540.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.540.692 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.540.692 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.229.391 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.229.393 I llama_perf_context_print:        load time =     522.58 ms
0.01.229.393 I llama_perf_context_print: prompt eval time =      41.27 ms /     7 tokens (    5.90 ms per token,   169.61 tokens per second)
0.01.229.394 I llama_perf_context_print:        eval time =     644.35 ms /    63 runs   (   10.23 ms per token,    97.77 tokens per second)
0.01.229.395 I llama_perf_context_print:       total time =     689.15 ms /    70 tokens
0.01.229.564 I ggml_metal_free: deallocating

real	0m1.267s
user	0m0.136s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.496 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.292 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.298 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.298 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.299 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.300 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.082 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.083 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.084 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.084 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.085 I llama_model_loader: - type  f32:  194 tensors
0.00.025.085 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.086 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.086 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.482 I llm_load_vocab: special tokens cache size = 25
0.00.052.613 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.616 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.616 I llm_load_print_meta: arch             = gptneox
0.00.052.616 I llm_load_print_meta: vocab type       = BPE
0.00.052.617 I llm_load_print_meta: n_vocab          = 50304
0.00.052.617 I llm_load_print_meta: n_merges         = 50009
0.00.052.617 I llm_load_print_meta: vocab_only       = 0
0.00.052.617 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.617 I llm_load_print_meta: n_embd           = 2048
0.00.052.617 I llm_load_print_meta: n_layer          = 24
0.00.052.620 I llm_load_print_meta: n_head           = 16
0.00.052.621 I llm_load_print_meta: n_head_kv        = 16
0.00.052.621 I llm_load_print_meta: n_rot            = 32
0.00.052.621 I llm_load_print_meta: n_swa            = 0
0.00.052.624 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.624 I llm_load_print_meta: n_gqa            = 1
0.00.052.625 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.626 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.628 I llm_load_print_meta: n_ff             = 8192
0.00.052.628 I llm_load_print_meta: n_expert         = 0
0.00.052.630 I llm_load_print_meta: n_expert_used    = 0
0.00.052.630 I llm_load_print_meta: causal attn      = 1
0.00.052.630 I llm_load_print_meta: pooling type     = 0
0.00.052.630 I llm_load_print_meta: rope type        = 2
0.00.052.630 I llm_load_print_meta: rope scaling     = linear
0.00.052.631 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.631 I llm_load_print_meta: freq_scale_train = 1
0.00.052.631 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.631 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.632 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.632 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.632 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.632 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.632 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.639 I llm_load_print_meta: model type       = 1.4B
0.00.052.639 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.639 I llm_load_print_meta: model params     = 1.41 B
0.00.052.640 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.640 I llm_load_print_meta: general.name     = 1.4B
0.00.052.640 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.641 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.641 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.641 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.642 I llm_load_print_meta: LF token         = 128 ''
0.00.052.642 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.642 I llm_load_print_meta: max token length = 1024
0.00.054.403 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.403 I llm_load_tensors: offloading output layer to GPU
0.00.054.403 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.408 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.409 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.300 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.301 I llama_new_context_with_model: n_ctx         = 128
0.00.055.301 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.301 I llama_new_context_with_model: n_batch       = 128
0.00.055.301 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.301 I llama_new_context_with_model: flash_attn    = 0
0.00.055.302 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.302 I llama_new_context_with_model: freq_scale    = 1
0.00.055.303 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.303 I ggml_metal_init: allocating
0.00.055.306 I ggml_metal_init: found device: Apple M4
0.00.055.308 I ggml_metal_init: picking default device: Apple M4
0.00.055.856 I ggml_metal_init: using embedded metal library
0.00.057.809 I ggml_metal_init: GPU name:   Apple M4
0.00.057.810 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.811 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.811 I ggml_metal_init: simdgroup reduction   = true
0.00.057.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.811 I ggml_metal_init: has bfloat            = true
0.00.057.812 I ggml_metal_init: use bfloat            = true
0.00.057.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.048 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.055 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.073 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.947 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.948 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.948 I llama_new_context_with_model: graph nodes  = 967
0.00.067.948 I llama_new_context_with_model: graph splits = 2
0.00.067.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.435.526 I 
0.00.435.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.435.587 I perplexity: tokenizing the input ..
0.00.443.659 I perplexity: tokenization took 8.073 ms
0.00.443.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.576.127 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.577.388 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.577.406 I llama_perf_context_print:        load time =     425.02 ms
0.00.577.408 I llama_perf_context_print: prompt eval time =     132.22 ms /   128 tokens (    1.03 ms per token,   968.12 tokens per second)
0.00.577.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.577.410 I llama_perf_context_print:       total time =     141.88 ms /   129 tokens
0.00.577.880 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.078s
sys	0m0.082s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.220 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.350 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.021.354 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.360 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.360 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.030.143 I llama_model_loader: - type  f32:  194 tensors
0.00.030.144 I llama_model_loader: - type q3_K:   25 tensors
0.00.030.144 I llama_model_loader: - type q4_K:   71 tensors
0.00.030.144 I llama_model_loader: - type q5_K:    1 tensors
0.00.030.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.106 I llm_load_vocab: special tokens cache size = 25
0.00.058.163 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.165 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.166 I llm_load_print_meta: arch             = gptneox
0.00.058.166 I llm_load_print_meta: vocab type       = BPE
0.00.058.166 I llm_load_print_meta: n_vocab          = 50304
0.00.058.166 I llm_load_print_meta: n_merges         = 50009
0.00.058.166 I llm_load_print_meta: vocab_only       = 0
0.00.058.167 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.167 I llm_load_print_meta: n_embd           = 2048
0.00.058.167 I llm_load_print_meta: n_layer          = 24
0.00.058.170 I llm_load_print_meta: n_head           = 16
0.00.058.171 I llm_load_print_meta: n_head_kv        = 16
0.00.058.171 I llm_load_print_meta: n_rot            = 32
0.00.058.171 I llm_load_print_meta: n_swa            = 0
0.00.058.171 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.171 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.172 I llm_load_print_meta: n_gqa            = 1
0.00.058.173 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.174 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.175 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.179 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.180 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.180 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.180 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.181 I llm_load_print_meta: n_ff             = 8192
0.00.058.181 I llm_load_print_meta: n_expert         = 0
0.00.058.181 I llm_load_print_meta: n_expert_used    = 0
0.00.058.181 I llm_load_print_meta: causal attn      = 1
0.00.058.181 I llm_load_print_meta: pooling type     = 0
0.00.058.182 I llm_load_print_meta: rope type        = 2
0.00.058.182 I llm_load_print_meta: rope scaling     = linear
0.00.058.182 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.183 I llm_load_print_meta: freq_scale_train = 1
0.00.058.183 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.183 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.184 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.184 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.184 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.196 I llm_load_print_meta: model type       = 1.4B
0.00.058.197 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.058.197 I llm_load_print_meta: model params     = 1.41 B
0.00.058.197 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.058.197 I llm_load_print_meta: general.name     = 1.4B
0.00.058.198 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.198 I llm_load_print_meta: LF token         = 128 ''
0.00.058.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.199 I llm_load_print_meta: max token length = 1024
0.00.059.744 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.745 I llm_load_tensors: offloading output layer to GPU
0.00.059.745 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.754 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.059.755 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.060.581 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.582 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.582 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.582 I llama_new_context_with_model: n_batch       = 2048
0.00.060.582 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.582 I llama_new_context_with_model: flash_attn    = 0
0.00.060.583 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.583 I llama_new_context_with_model: freq_scale    = 1
0.00.060.583 I ggml_metal_init: allocating
0.00.060.587 I ggml_metal_init: found device: Apple M4
0.00.060.589 I ggml_metal_init: picking default device: Apple M4
0.00.061.132 I ggml_metal_init: using embedded metal library
0.00.063.103 I ggml_metal_init: GPU name:   Apple M4
0.00.063.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.106 I ggml_metal_init: simdgroup reduction   = true
0.00.063.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.106 I ggml_metal_init: has bfloat            = true
0.00.063.106 I ggml_metal_init: use bfloat            = true
0.00.063.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.073 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.078 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.098 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.226 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.228 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.228 I llama_new_context_with_model: graph nodes  = 967
0.00.093.228 I llama_new_context_with_model: graph splits = 2
0.00.093.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.361 I main: llama threadpool init, n_threads = 4
0.00.692.406 I 
0.00.692.425 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.692.425 I 
0.00.692.642 I sampler seed: 1234
0.00.692.646 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.693 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.437.888 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63791.55 tokens per second)
0.01.437.889 I llama_perf_context_print:        load time =     683.14 ms
0.01.437.890 I llama_perf_context_print: prompt eval time =      35.65 ms /     7 tokens (    5.09 ms per token,   196.36 tokens per second)
0.01.437.890 I llama_perf_context_print:        eval time =     706.76 ms /    63 runs   (   11.22 ms per token,    89.14 tokens per second)
0.01.437.894 I llama_perf_context_print:       total time =     745.53 ms /    70 tokens
0.01.438.075 I ggml_metal_free: deallocating

real	0m1.457s
user	0m0.112s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.415 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.148 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.152 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.155 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.156 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.157 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.158 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.159 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.162 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.163 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.983 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.822 I llama_model_loader: - type  f32:  194 tensors
0.00.022.823 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.823 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.823 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.091 I llm_load_vocab: special tokens cache size = 25
0.00.049.324 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.327 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.327 I llm_load_print_meta: arch             = gptneox
0.00.049.327 I llm_load_print_meta: vocab type       = BPE
0.00.049.328 I llm_load_print_meta: n_vocab          = 50304
0.00.049.328 I llm_load_print_meta: n_merges         = 50009
0.00.049.328 I llm_load_print_meta: vocab_only       = 0
0.00.049.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.328 I llm_load_print_meta: n_embd           = 2048
0.00.049.328 I llm_load_print_meta: n_layer          = 24
0.00.049.331 I llm_load_print_meta: n_head           = 16
0.00.049.332 I llm_load_print_meta: n_head_kv        = 16
0.00.049.332 I llm_load_print_meta: n_rot            = 32
0.00.049.332 I llm_load_print_meta: n_swa            = 0
0.00.049.332 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.332 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.333 I llm_load_print_meta: n_gqa            = 1
0.00.049.334 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.335 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.335 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.335 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.336 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.338 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.338 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.339 I llm_load_print_meta: n_ff             = 8192
0.00.049.339 I llm_load_print_meta: n_expert         = 0
0.00.049.339 I llm_load_print_meta: n_expert_used    = 0
0.00.049.339 I llm_load_print_meta: causal attn      = 1
0.00.049.341 I llm_load_print_meta: pooling type     = 0
0.00.049.341 I llm_load_print_meta: rope type        = 2
0.00.049.341 I llm_load_print_meta: rope scaling     = linear
0.00.049.341 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.342 I llm_load_print_meta: freq_scale_train = 1
0.00.049.342 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.342 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.342 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.342 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.343 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.343 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.343 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.354 I llm_load_print_meta: model type       = 1.4B
0.00.049.354 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.355 I llm_load_print_meta: model params     = 1.41 B
0.00.049.355 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.355 I llm_load_print_meta: general.name     = 1.4B
0.00.049.356 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.356 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.356 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.356 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.356 I llm_load_print_meta: LF token         = 128 ''
0.00.049.356 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.357 I llm_load_print_meta: max token length = 1024
0.00.050.867 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.867 I llm_load_tensors: offloading output layer to GPU
0.00.050.867 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.877 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.878 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.734 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.734 I llama_new_context_with_model: n_ctx         = 128
0.00.051.734 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.734 I llama_new_context_with_model: n_batch       = 128
0.00.051.735 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.735 I llama_new_context_with_model: flash_attn    = 0
0.00.051.735 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.735 I llama_new_context_with_model: freq_scale    = 1
0.00.051.736 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.736 I ggml_metal_init: allocating
0.00.051.741 I ggml_metal_init: found device: Apple M4
0.00.051.744 I ggml_metal_init: picking default device: Apple M4
0.00.052.273 I ggml_metal_init: using embedded metal library
0.00.054.171 I ggml_metal_init: GPU name:   Apple M4
0.00.054.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.174 I ggml_metal_init: simdgroup reduction   = true
0.00.054.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.174 I ggml_metal_init: has bfloat            = true
0.00.054.174 I ggml_metal_init: use bfloat            = true
0.00.054.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.383 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.397 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.269 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.270 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.270 I llama_new_context_with_model: graph nodes  = 967
0.00.064.270 I llama_new_context_with_model: graph splits = 2
0.00.064.282 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.439 I 
0.00.501.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.501.466 I perplexity: tokenizing the input ..
0.00.509.287 I perplexity: tokenization took 7.82 ms
0.00.509.298 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.640.984 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.642.219 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.642.237 I llama_perf_context_print:        load time =     493.02 ms
0.00.642.238 I llama_perf_context_print: prompt eval time =     131.46 ms /   128 tokens (    1.03 ms per token,   973.67 tokens per second)
0.00.642.239 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.642.239 I llama_perf_context_print:       total time =     140.80 ms /   129 tokens
0.00.642.749 I ggml_metal_free: deallocating

real	0m0.655s
user	0m0.076s
sys	0m0.097s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.017.144 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.031.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.222 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.224 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.224 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.231 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.799 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.813 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.043.814 I llama_model_loader: - type  f32:  194 tensors
0.00.043.814 I llama_model_loader: - type q4_K:   61 tensors
0.00.043.815 I llama_model_loader: - type q5_K:   24 tensors
0.00.043.815 I llama_model_loader: - type q6_K:   13 tensors
0.00.081.651 I llm_load_vocab: special tokens cache size = 25
0.00.091.085 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.088 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.089 I llm_load_print_meta: arch             = gptneox
0.00.091.089 I llm_load_print_meta: vocab type       = BPE
0.00.091.089 I llm_load_print_meta: n_vocab          = 50304
0.00.091.089 I llm_load_print_meta: n_merges         = 50009
0.00.091.090 I llm_load_print_meta: vocab_only       = 0
0.00.091.090 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.090 I llm_load_print_meta: n_embd           = 2048
0.00.091.090 I llm_load_print_meta: n_layer          = 24
0.00.091.094 I llm_load_print_meta: n_head           = 16
0.00.091.095 I llm_load_print_meta: n_head_kv        = 16
0.00.091.096 I llm_load_print_meta: n_rot            = 32
0.00.091.096 I llm_load_print_meta: n_swa            = 0
0.00.091.096 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.096 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.097 I llm_load_print_meta: n_gqa            = 1
0.00.091.098 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.099 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.099 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.100 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.100 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.100 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.100 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.101 I llm_load_print_meta: n_ff             = 8192
0.00.091.101 I llm_load_print_meta: n_expert         = 0
0.00.091.103 I llm_load_print_meta: n_expert_used    = 0
0.00.091.105 I llm_load_print_meta: causal attn      = 1
0.00.091.105 I llm_load_print_meta: pooling type     = 0
0.00.091.105 I llm_load_print_meta: rope type        = 2
0.00.091.105 I llm_load_print_meta: rope scaling     = linear
0.00.091.106 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.106 I llm_load_print_meta: freq_scale_train = 1
0.00.091.106 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.107 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.107 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.107 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.107 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.107 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.107 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.120 I llm_load_print_meta: model type       = 1.4B
0.00.091.120 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.091.121 I llm_load_print_meta: model params     = 1.41 B
0.00.091.121 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.091.123 I llm_load_print_meta: general.name     = 1.4B
0.00.091.124 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.124 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.124 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.124 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.124 I llm_load_print_meta: LF token         = 128 ''
0.00.091.125 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.125 I llm_load_print_meta: max token length = 1024
0.00.093.717 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.718 I llm_load_tensors: offloading output layer to GPU
0.00.093.718 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.729 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.093.730 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.094.973 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.974 I llama_new_context_with_model: n_ctx         = 2048
0.00.094.974 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.094.975 I llama_new_context_with_model: n_batch       = 2048
0.00.094.975 I llama_new_context_with_model: n_ubatch      = 512
0.00.094.975 I llama_new_context_with_model: flash_attn    = 0
0.00.094.976 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.976 I llama_new_context_with_model: freq_scale    = 1
0.00.094.976 I ggml_metal_init: allocating
0.00.094.984 I ggml_metal_init: found device: Apple M4
0.00.094.986 I ggml_metal_init: picking default device: Apple M4
0.00.095.689 I ggml_metal_init: using embedded metal library
0.00.098.369 I ggml_metal_init: GPU name:   Apple M4
0.00.098.371 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.372 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.372 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.374 I ggml_metal_init: simdgroup reduction   = true
0.00.098.374 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.374 I ggml_metal_init: has bfloat            = true
0.00.098.374 I ggml_metal_init: use bfloat            = true
0.00.098.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.129.273 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.129.284 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.129.307 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.130.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.130.279 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.130.279 I llama_new_context_with_model: graph nodes  = 967
0.00.130.280 I llama_new_context_with_model: graph splits = 2
0.00.130.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.714 I main: llama threadpool init, n_threads = 4
0.00.819.791 I 
0.00.819.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.819.837 I 
0.00.820.338 I sampler seed: 1234
0.00.820.346 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.820.418 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.820.424 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.820.424 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.584.488 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.584.489 I llama_perf_context_print:        load time =     802.56 ms
0.01.584.489 I llama_perf_context_print: prompt eval time =      46.17 ms /     7 tokens (    6.60 ms per token,   151.61 tokens per second)
0.01.584.490 I llama_perf_context_print:        eval time =     714.88 ms /    63 runs   (   11.35 ms per token,    88.13 tokens per second)
0.01.584.491 I llama_perf_context_print:       total time =     764.78 ms /    70 tokens
0.01.584.666 I ggml_metal_free: deallocating

real	0m1.624s
user	0m0.146s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.942 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.067 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.069 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.070 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.078 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.078 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.813 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.816 I llama_model_loader: - type  f32:  194 tensors
0.00.024.816 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.816 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.817 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.951 I llm_load_vocab: special tokens cache size = 25
0.00.052.026 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.029 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.029 I llm_load_print_meta: arch             = gptneox
0.00.052.029 I llm_load_print_meta: vocab type       = BPE
0.00.052.030 I llm_load_print_meta: n_vocab          = 50304
0.00.052.030 I llm_load_print_meta: n_merges         = 50009
0.00.052.030 I llm_load_print_meta: vocab_only       = 0
0.00.052.030 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.030 I llm_load_print_meta: n_embd           = 2048
0.00.052.030 I llm_load_print_meta: n_layer          = 24
0.00.052.033 I llm_load_print_meta: n_head           = 16
0.00.052.034 I llm_load_print_meta: n_head_kv        = 16
0.00.052.034 I llm_load_print_meta: n_rot            = 32
0.00.052.034 I llm_load_print_meta: n_swa            = 0
0.00.052.034 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.034 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.035 I llm_load_print_meta: n_gqa            = 1
0.00.052.036 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.037 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.037 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.038 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.039 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.039 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.039 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.040 I llm_load_print_meta: n_ff             = 8192
0.00.052.040 I llm_load_print_meta: n_expert         = 0
0.00.052.040 I llm_load_print_meta: n_expert_used    = 0
0.00.052.041 I llm_load_print_meta: causal attn      = 1
0.00.052.041 I llm_load_print_meta: pooling type     = 0
0.00.052.041 I llm_load_print_meta: rope type        = 2
0.00.052.041 I llm_load_print_meta: rope scaling     = linear
0.00.052.041 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.042 I llm_load_print_meta: freq_scale_train = 1
0.00.052.042 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.042 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.042 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.042 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.043 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.043 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.043 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.054 I llm_load_print_meta: model type       = 1.4B
0.00.052.055 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.055 I llm_load_print_meta: model params     = 1.41 B
0.00.052.056 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.056 I llm_load_print_meta: general.name     = 1.4B
0.00.052.057 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.057 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.057 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.057 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.057 I llm_load_print_meta: LF token         = 128 ''
0.00.052.058 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.058 I llm_load_print_meta: max token length = 1024
0.00.053.684 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.684 I llm_load_tensors: offloading output layer to GPU
0.00.053.685 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.694 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.695 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.552 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.552 I llama_new_context_with_model: n_ctx         = 128
0.00.054.552 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.553 I llama_new_context_with_model: n_batch       = 128
0.00.054.553 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.553 I llama_new_context_with_model: flash_attn    = 0
0.00.054.553 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.554 I llama_new_context_with_model: freq_scale    = 1
0.00.054.554 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.554 I ggml_metal_init: allocating
0.00.054.558 I ggml_metal_init: found device: Apple M4
0.00.054.560 I ggml_metal_init: picking default device: Apple M4
0.00.055.110 I ggml_metal_init: using embedded metal library
0.00.057.095 I ggml_metal_init: GPU name:   Apple M4
0.00.057.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.097 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.097 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.098 I ggml_metal_init: simdgroup reduction   = true
0.00.057.098 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.098 I ggml_metal_init: has bfloat            = true
0.00.057.098 I ggml_metal_init: use bfloat            = true
0.00.057.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.618 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.622 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.635 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.514 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.515 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.516 I llama_new_context_with_model: graph nodes  = 967
0.00.067.516 I llama_new_context_with_model: graph splits = 2
0.00.067.528 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.369 I 
0.00.584.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.584.419 I perplexity: tokenizing the input ..
0.00.592.446 I perplexity: tokenization took 8.026 ms
0.00.592.457 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.726.313 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.727.461 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.727.475 I llama_perf_context_print:        load time =     574.42 ms
0.00.727.476 I llama_perf_context_print: prompt eval time =     133.63 ms /   128 tokens (    1.04 ms per token,   957.84 tokens per second)
0.00.727.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.480 I llama_perf_context_print:       total time =     143.11 ms /   129 tokens
0.00.727.905 I ggml_metal_free: deallocating

real	0m0.744s
user	0m0.079s
sys	0m0.116s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.000 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.005 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.007 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.008 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.010 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.013 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.553 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.584 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.585 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.585 I llama_model_loader: - type  f32:  194 tensors
0.00.027.585 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.586 I llama_model_loader: - type q6_K:   37 tensors
0.00.054.894 I llm_load_vocab: special tokens cache size = 25
0.00.061.743 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.746 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.746 I llm_load_print_meta: arch             = gptneox
0.00.061.746 I llm_load_print_meta: vocab type       = BPE
0.00.061.747 I llm_load_print_meta: n_vocab          = 50304
0.00.061.747 I llm_load_print_meta: n_merges         = 50009
0.00.061.747 I llm_load_print_meta: vocab_only       = 0
0.00.061.747 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.747 I llm_load_print_meta: n_embd           = 2048
0.00.061.747 I llm_load_print_meta: n_layer          = 24
0.00.061.750 I llm_load_print_meta: n_head           = 16
0.00.061.750 I llm_load_print_meta: n_head_kv        = 16
0.00.061.751 I llm_load_print_meta: n_rot            = 32
0.00.061.751 I llm_load_print_meta: n_swa            = 0
0.00.061.751 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.751 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.752 I llm_load_print_meta: n_gqa            = 1
0.00.061.752 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.753 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.753 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.754 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.754 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.756 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.756 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.756 I llm_load_print_meta: n_ff             = 8192
0.00.061.757 I llm_load_print_meta: n_expert         = 0
0.00.061.757 I llm_load_print_meta: n_expert_used    = 0
0.00.061.757 I llm_load_print_meta: causal attn      = 1
0.00.061.757 I llm_load_print_meta: pooling type     = 0
0.00.061.757 I llm_load_print_meta: rope type        = 2
0.00.061.757 I llm_load_print_meta: rope scaling     = linear
0.00.061.758 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.758 I llm_load_print_meta: freq_scale_train = 1
0.00.061.758 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.758 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.758 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.759 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.759 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.759 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.759 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.771 I llm_load_print_meta: model type       = 1.4B
0.00.061.771 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.061.772 I llm_load_print_meta: model params     = 1.41 B
0.00.061.772 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.061.772 I llm_load_print_meta: general.name     = 1.4B
0.00.061.772 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.773 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.773 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.773 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.773 I llm_load_print_meta: LF token         = 128 ''
0.00.061.773 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.774 I llm_load_print_meta: max token length = 1024
0.00.063.935 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.936 I llm_load_tensors: offloading output layer to GPU
0.00.063.936 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.945 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.063.947 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.064.967 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.968 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.968 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.968 I llama_new_context_with_model: n_batch       = 2048
0.00.064.968 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.968 I llama_new_context_with_model: flash_attn    = 0
0.00.064.969 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.969 I llama_new_context_with_model: freq_scale    = 1
0.00.064.969 I ggml_metal_init: allocating
0.00.064.972 I ggml_metal_init: found device: Apple M4
0.00.064.974 I ggml_metal_init: picking default device: Apple M4
0.00.065.554 I ggml_metal_init: using embedded metal library
0.00.067.670 I ggml_metal_init: GPU name:   Apple M4
0.00.067.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.673 I ggml_metal_init: simdgroup reduction   = true
0.00.067.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.673 I ggml_metal_init: has bfloat            = true
0.00.067.673 I ggml_metal_init: use bfloat            = true
0.00.067.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.797 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.813 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.812 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.813 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.814 I llama_new_context_with_model: graph nodes  = 967
0.00.099.814 I llama_new_context_with_model: graph splits = 2
0.00.099.837 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.085 I main: llama threadpool init, n_threads = 4
0.00.798.120 I 
0.00.798.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.798.149 I 
0.00.798.375 I sampler seed: 1234
0.00.798.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.391 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.391 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.391 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.641.689 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59513.83 tokens per second)
0.01.641.690 I llama_perf_context_print:        load time =     788.21 ms
0.01.641.690 I llama_perf_context_print: prompt eval time =      38.72 ms /     7 tokens (    5.53 ms per token,   180.77 tokens per second)
0.01.641.691 I llama_perf_context_print:        eval time =     801.47 ms /    63 runs   (   12.72 ms per token,    78.61 tokens per second)
0.01.641.692 I llama_perf_context_print:       total time =     843.61 ms /    70 tokens
0.01.641.874 I ggml_metal_free: deallocating

real	0m1.675s
user	0m0.122s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.456 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.266 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.122 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.122 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.123 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.123 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.123 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.124 I llama_model_loader: - type  f32:  194 tensors
0.00.023.124 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.124 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.224 I llm_load_vocab: special tokens cache size = 25
0.00.050.367 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.370 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.370 I llm_load_print_meta: arch             = gptneox
0.00.050.370 I llm_load_print_meta: vocab type       = BPE
0.00.050.371 I llm_load_print_meta: n_vocab          = 50304
0.00.050.371 I llm_load_print_meta: n_merges         = 50009
0.00.050.371 I llm_load_print_meta: vocab_only       = 0
0.00.050.371 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.371 I llm_load_print_meta: n_embd           = 2048
0.00.050.372 I llm_load_print_meta: n_layer          = 24
0.00.050.374 I llm_load_print_meta: n_head           = 16
0.00.050.375 I llm_load_print_meta: n_head_kv        = 16
0.00.050.375 I llm_load_print_meta: n_rot            = 32
0.00.050.375 I llm_load_print_meta: n_swa            = 0
0.00.050.376 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.376 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.379 I llm_load_print_meta: n_gqa            = 1
0.00.050.379 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.381 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.381 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.382 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.382 I llm_load_print_meta: n_ff             = 8192
0.00.050.383 I llm_load_print_meta: n_expert         = 0
0.00.050.383 I llm_load_print_meta: n_expert_used    = 0
0.00.050.383 I llm_load_print_meta: causal attn      = 1
0.00.050.383 I llm_load_print_meta: pooling type     = 0
0.00.050.383 I llm_load_print_meta: rope type        = 2
0.00.050.383 I llm_load_print_meta: rope scaling     = linear
0.00.050.393 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.396 I llm_load_print_meta: freq_scale_train = 1
0.00.050.396 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.396 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.398 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.398 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.398 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.398 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.398 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.410 I llm_load_print_meta: model type       = 1.4B
0.00.050.410 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.411 I llm_load_print_meta: model params     = 1.41 B
0.00.050.411 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.411 I llm_load_print_meta: general.name     = 1.4B
0.00.050.411 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.412 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.413 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.413 I llm_load_print_meta: LF token         = 128 ''
0.00.050.413 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.414 I llm_load_print_meta: max token length = 1024
0.00.051.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.998 I llm_load_tensors: offloading output layer to GPU
0.00.051.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.007 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.008 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.811 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.811 I llama_new_context_with_model: n_ctx         = 128
0.00.052.811 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.812 I llama_new_context_with_model: n_batch       = 128
0.00.052.812 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.812 I llama_new_context_with_model: flash_attn    = 0
0.00.052.812 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.813 I llama_new_context_with_model: freq_scale    = 1
0.00.052.813 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.813 I ggml_metal_init: allocating
0.00.052.818 I ggml_metal_init: found device: Apple M4
0.00.052.821 I ggml_metal_init: picking default device: Apple M4
0.00.053.366 I ggml_metal_init: using embedded metal library
0.00.055.343 I ggml_metal_init: GPU name:   Apple M4
0.00.055.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.345 I ggml_metal_init: simdgroup reduction   = true
0.00.055.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.345 I ggml_metal_init: has bfloat            = true
0.00.055.346 I ggml_metal_init: use bfloat            = true
0.00.055.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.405 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.419 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.270 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.271 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.271 I llama_new_context_with_model: graph nodes  = 967
0.00.065.272 I llama_new_context_with_model: graph splits = 2
0.00.065.284 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.783 I 
0.00.677.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.677.805 I perplexity: tokenizing the input ..
0.00.685.418 I perplexity: tokenization took 7.612 ms
0.00.685.429 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.447 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.827.709 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.827.725 I llama_perf_context_print:        load time =     669.32 ms
0.00.827.725 I llama_perf_context_print: prompt eval time =     140.78 ms /   128 tokens (    1.10 ms per token,   909.19 tokens per second)
0.00.827.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.727 I llama_perf_context_print:       total time =     149.94 ms /   129 tokens
0.00.828.191 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.078s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.012.432 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.589 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.021.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.600 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.602 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.604 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.604 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.606 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.607 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.607 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.008 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.010 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.010 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.011 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.033.012 I llama_model_loader: - type  f32:  194 tensors
0.00.033.012 I llama_model_loader: - type q6_K:   98 tensors
0.00.068.585 I llm_load_vocab: special tokens cache size = 25
0.00.079.061 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.065 I llm_load_print_meta: arch             = gptneox
0.00.079.065 I llm_load_print_meta: vocab type       = BPE
0.00.079.066 I llm_load_print_meta: n_vocab          = 50304
0.00.079.066 I llm_load_print_meta: n_merges         = 50009
0.00.079.066 I llm_load_print_meta: vocab_only       = 0
0.00.079.066 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.066 I llm_load_print_meta: n_embd           = 2048
0.00.079.067 I llm_load_print_meta: n_layer          = 24
0.00.079.070 I llm_load_print_meta: n_head           = 16
0.00.079.071 I llm_load_print_meta: n_head_kv        = 16
0.00.079.071 I llm_load_print_meta: n_rot            = 32
0.00.079.071 I llm_load_print_meta: n_swa            = 0
0.00.079.072 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.072 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.075 I llm_load_print_meta: n_gqa            = 1
0.00.079.076 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.079 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.080 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.080 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.080 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.081 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.081 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.082 I llm_load_print_meta: n_ff             = 8192
0.00.079.082 I llm_load_print_meta: n_expert         = 0
0.00.079.082 I llm_load_print_meta: n_expert_used    = 0
0.00.079.082 I llm_load_print_meta: causal attn      = 1
0.00.079.083 I llm_load_print_meta: pooling type     = 0
0.00.079.083 I llm_load_print_meta: rope type        = 2
0.00.079.083 I llm_load_print_meta: rope scaling     = linear
0.00.079.084 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.084 I llm_load_print_meta: freq_scale_train = 1
0.00.079.084 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.084 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.085 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.085 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.085 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.085 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.085 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.098 I llm_load_print_meta: model type       = 1.4B
0.00.079.098 I llm_load_print_meta: model ftype      = Q6_K
0.00.079.099 I llm_load_print_meta: model params     = 1.41 B
0.00.079.100 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.079.100 I llm_load_print_meta: general.name     = 1.4B
0.00.079.100 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.100 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.100 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.101 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.103 I llm_load_print_meta: LF token         = 128 ''
0.00.079.103 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.104 I llm_load_print_meta: max token length = 1024
0.00.081.975 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.975 I llm_load_tensors: offloading output layer to GPU
0.00.081.975 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.986 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.081.988 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.083.380 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.381 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.382 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.382 I llama_new_context_with_model: n_batch       = 2048
0.00.083.382 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.382 I llama_new_context_with_model: flash_attn    = 0
0.00.083.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.383 I llama_new_context_with_model: freq_scale    = 1
0.00.083.384 I ggml_metal_init: allocating
0.00.083.391 I ggml_metal_init: found device: Apple M4
0.00.083.393 I ggml_metal_init: picking default device: Apple M4
0.00.084.121 I ggml_metal_init: using embedded metal library
0.00.086.957 I ggml_metal_init: GPU name:   Apple M4
0.00.086.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.961 I ggml_metal_init: simdgroup reduction   = true
0.00.086.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.961 I ggml_metal_init: has bfloat            = true
0.00.086.961 I ggml_metal_init: use bfloat            = true
0.00.086.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.728 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.736 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.756 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.743 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.744 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.744 I llama_new_context_with_model: graph nodes  = 967
0.00.118.745 I llama_new_context_with_model: graph splits = 2
0.00.118.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.954.232 I main: llama threadpool init, n_threads = 4
0.00.954.284 I 
0.00.954.317 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.954.317 I 
0.00.954.671 I sampler seed: 1234
0.00.954.678 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.954.734 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.954.736 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.954.736 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.828.090 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.828.090 I llama_perf_context_print:        load time =     941.79 ms
0.01.828.091 I llama_perf_context_print: prompt eval time =      38.90 ms /     7 tokens (    5.56 ms per token,   179.93 tokens per second)
0.01.828.092 I llama_perf_context_print:        eval time =     831.57 ms /    63 runs   (   13.20 ms per token,    75.76 tokens per second)
0.01.828.092 I llama_perf_context_print:       total time =     873.87 ms /    70 tokens
0.01.828.275 I ggml_metal_free: deallocating

real	0m1.872s
user	0m0.140s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4177 (811872a5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.852 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.551 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.562 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.563 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.563 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.564 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.338 I llama_model_loader: - type  f32:  194 tensors
0.00.024.338 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.811 I llm_load_vocab: special tokens cache size = 25
0.00.051.056 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.059 I llm_load_print_meta: arch             = gptneox
0.00.051.059 I llm_load_print_meta: vocab type       = BPE
0.00.051.059 I llm_load_print_meta: n_vocab          = 50304
0.00.051.060 I llm_load_print_meta: n_merges         = 50009
0.00.051.060 I llm_load_print_meta: vocab_only       = 0
0.00.051.060 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.060 I llm_load_print_meta: n_embd           = 2048
0.00.051.060 I llm_load_print_meta: n_layer          = 24
0.00.051.063 I llm_load_print_meta: n_head           = 16
0.00.051.064 I llm_load_print_meta: n_head_kv        = 16
0.00.051.064 I llm_load_print_meta: n_rot            = 32
0.00.051.064 I llm_load_print_meta: n_swa            = 0
0.00.051.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.065 I llm_load_print_meta: n_gqa            = 1
0.00.051.066 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.067 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.067 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.068 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.068 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.068 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.068 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.069 I llm_load_print_meta: n_ff             = 8192
0.00.051.069 I llm_load_print_meta: n_expert         = 0
0.00.051.070 I llm_load_print_meta: n_expert_used    = 0
0.00.051.070 I llm_load_print_meta: causal attn      = 1
0.00.051.070 I llm_load_print_meta: pooling type     = 0
0.00.051.072 I llm_load_print_meta: rope type        = 2
0.00.051.072 I llm_load_print_meta: rope scaling     = linear
0.00.051.073 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.074 I llm_load_print_meta: freq_scale_train = 1
0.00.051.075 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.075 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.075 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.075 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.075 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.075 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.076 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.083 I llm_load_print_meta: model type       = 1.4B
0.00.051.083 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.084 I llm_load_print_meta: model params     = 1.41 B
0.00.051.084 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.084 I llm_load_print_meta: general.name     = 1.4B
0.00.051.084 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.086 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.086 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.087 I llm_load_print_meta: LF token         = 128 ''
0.00.051.087 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.087 I llm_load_print_meta: max token length = 1024
0.00.052.905 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.905 I llm_load_tensors: offloading output layer to GPU
0.00.052.905 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.910 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.911 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.925 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.926 I llama_new_context_with_model: n_ctx         = 128
0.00.053.927 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.927 I llama_new_context_with_model: n_batch       = 128
0.00.053.927 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.927 I llama_new_context_with_model: flash_attn    = 0
0.00.053.927 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.928 I llama_new_context_with_model: freq_scale    = 1
0.00.053.928 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.928 I ggml_metal_init: allocating
0.00.053.931 I ggml_metal_init: found device: Apple M4
0.00.053.933 I ggml_metal_init: picking default device: Apple M4
0.00.054.542 I ggml_metal_init: using embedded metal library
0.00.056.525 I ggml_metal_init: GPU name:   Apple M4
0.00.056.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.527 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.527 I ggml_metal_init: simdgroup reduction   = true
0.00.056.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.528 I ggml_metal_init: has bfloat            = true
0.00.056.528 I ggml_metal_init: use bfloat            = true
0.00.056.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.680 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.690 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.710 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.615 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.616 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.616 I llama_new_context_with_model: graph nodes  = 967
0.00.066.616 I llama_new_context_with_model: graph splits = 2
0.00.066.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.179.684 I 
0.00.179.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.179.720 I perplexity: tokenizing the input ..
0.00.187.343 I perplexity: tokenization took 7.621 ms
0.00.187.361 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.327.508 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.328.706 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.328.719 I llama_perf_context_print:        load time =     169.83 ms
0.00.328.720 I llama_perf_context_print: prompt eval time =     139.82 ms /   128 tokens (    1.09 ms per token,   915.44 tokens per second)
0.00.328.721 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.328.723 I llama_perf_context_print:       total time =     149.04 ms /   129 tokens
0.00.328.998 I ggml_metal_free: deallocating

real	0m0.342s
user	0m0.077s
sys	0m0.045s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4177 (811872a5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e908350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e908a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e909030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e9095e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e909b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e90a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e90a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e90aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e90b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e90b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e90bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e90c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e90cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e90d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e90dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e90e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e90ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e90f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e90f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e910080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e9107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e910ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e9115e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e911e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e9125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e912860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e912e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e913ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e914020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e9142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e914780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e914a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e9152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e915810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e915ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e915f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e916410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e9168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e916d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e9171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e917690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e917b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e917fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e918470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e918730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e918d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e919350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e919c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e91a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e91a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e91aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e91b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e91bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e91c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e91c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e91cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e91d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e91d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e91dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e91e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e91e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e91ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e91eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e91f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e91f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e91fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e920140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e9205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e920a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e920f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e9213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e921860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e921d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e9221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e922640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e922ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e922f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e923420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e9238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e923d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e924200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e9246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e924b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e924fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e925480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e925920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e925dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e926260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e926700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e926ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e927040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e9274e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e927980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e927e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e9282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e928760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e928c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e919960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e929250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e9296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e929b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e92a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e92a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e92a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e92ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e92b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e92b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e92bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e92c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e92c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e92c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e92ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e92d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e92d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e92dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e92e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e92e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e92ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e92eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e92f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e92f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e92fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e930150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e9305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e930a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e930f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e9313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e931870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e931d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e9321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e932650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e932af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e932f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e933430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e9338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e933d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e934210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e9346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e934b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e934ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e935490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e935930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e935dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e936270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e936710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e936bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e937050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e9374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e937990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e937e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e9382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e938770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e938c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e939160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e9396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e939c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e93a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e93a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e93aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e93b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e93b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e93bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e93c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e93ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e93cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e93d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e93d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e93dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e93e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e93ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e93efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e93f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e93fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e93ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e940510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e940a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e940fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e941500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e941a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e941fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e9424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e942a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e942f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e9434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e943a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e943f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e9444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e944a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e944f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e9454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e945a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e945f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e9464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e946a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e946f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e9474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e9479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e947f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e948490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e9489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e948f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e949480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e9499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e949f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e94a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e94a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e94af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e94b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e94b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e94bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e94c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e94c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e94cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e94d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e94d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e94dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e94e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e94e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e94eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e94f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e94f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e94fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e950410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e950960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e950e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e9512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e951740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e951be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e952080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e952520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e9529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e952e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e953300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e9537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e953c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e9540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e954580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e954ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e9551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e955910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e956030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e956750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e956a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e957020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e957630 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.132.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e947c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e9480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e948550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e9489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e948e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e9492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e949710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e949b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e949ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e94a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e94a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e94aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e94b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e94bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e94c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e94cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e94d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e94dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e94e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e94ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e94f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e94fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e950110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e950800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e950ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e951360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e9517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e951c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e9520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e952520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e952990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e952e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e953270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e953530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e9539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e953e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e954280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e9546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e954b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e954fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e955440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e9558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e955d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e956190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e956600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e956a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e956ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e957350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e9577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e9095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e90a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e909030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e909b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e907c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e908500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e9159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e915e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e916290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e916700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e916b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e916fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e917450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e9178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e917d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e9181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e918610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e918a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e918ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e919360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e9197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e919c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e91a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e91a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e91a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e91ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e91b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e91b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e91bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e91bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e91c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e91c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e91cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e91d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e91d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e91da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e91ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e91e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e91e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e91ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e91f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e91f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e91f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e91fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e920250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e9206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e920b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e920fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e921410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e921880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e921cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e922160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e9225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e922a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e922eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e923320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e923790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e923c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e924070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e9244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e924950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e924dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e925230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e9256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e925b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e925f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e9263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e926860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e926cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e927140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e9275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e927a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e927e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e928300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e928770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e928be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e929050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e9294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e929930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e929da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e92a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e92a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e92aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e92af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e92b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e92b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e92bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e92c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e92c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e92ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e92ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e92d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e92d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e92dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e92e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e92e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e92e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e92ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e92f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e92f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e92fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e92ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e9303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e930820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e930c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e931100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e931570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e9319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e931e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e9322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e932730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e932ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e933010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e933480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e9338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e933d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e9341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e934640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e934dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e935230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e9356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e935b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e935f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e9363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e936860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e936cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e937140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e9375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e937a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e937e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e938300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e938770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e938be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e939050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e9394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e939930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e939da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e93a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e93a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e93aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e93af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e93b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e93b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e93bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e93c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e93c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e93ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e93ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e93d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e93d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e93dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e93e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e93e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e93e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e93ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e93f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e93f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e93fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e93ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e9403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e940820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e940c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e941100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e941570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e9419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e941e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e9422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e942730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e942ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e943010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e943480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e9438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e943d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e9441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e944640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e944ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e944f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e945390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e945800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e945c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e9460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e946550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e9469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e946e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e9141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e914630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e914aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e914f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e90b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e90bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e90c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e90ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e90d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e90d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e90da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e90de70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ea08340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ea087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ea08c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ea09090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ea09500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ea09970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ea09de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ea0a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ea0a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ea0ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ea0b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ea0b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ea0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ea0c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ea0d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ea0d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ea0e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ea0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ea0ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ea0f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ea0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ea10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ea10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ea112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ea119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ea11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ea11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ea123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ea12850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ea12cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ea13130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ea13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ea13ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ea13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ea14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ea14670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ea14ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ea14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ea153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ea15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ea15ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ea16110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ea16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ea169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ea16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ea172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ea17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ea17bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ea18020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ea18490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ea18900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ea18d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ea191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ea19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ea19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ea19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ea1a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ea1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ea1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ea1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ea1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ea1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ea1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ea1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ea1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ea1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ea1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ea1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ea1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ea1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ea1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ea1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ea1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ea1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ea1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ea1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ea1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ea20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ea206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ea20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ea20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ea21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ea21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ea21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ea22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ea225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ea22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ea22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ea23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ea237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ea23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ea24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ea244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ea24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ea24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ea25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ea256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ea25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ea25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ea26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ea26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ea26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ea27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ea275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ea27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ea27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ea28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ea28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ea28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ea29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ea294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ea29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ea29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ea2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ea2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ea2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ea2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ea2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ea2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ea2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ea2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ea2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ea2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ea2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ea2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ea2d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ea2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ea2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ea2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ea2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ea2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ea2f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ea2f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ea2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ea2ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ea303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ea30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ea30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ea31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ea31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ea319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ea31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ea322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ea32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ea32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ea33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ea33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ea33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ea33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ea341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ea34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ea34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ea34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ea353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ea35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ea35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ea360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ea36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ea369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ea36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ea372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ea37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ea37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ea38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ea38470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ea388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ea38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ea398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ea39ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ea39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ea3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ea3a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ea3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ea3b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ea3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ea3b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ea3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ea3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ea3c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ea3cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ea3cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ea3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ea3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ea3dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ea3e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ea3e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ea3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ea3ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ea3f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ea3f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ea3fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ea40000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ea40470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ea408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ea40d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ea411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ea41630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ea41aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ea41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ea42380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ea427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ea42c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ea430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ea43540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ea439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ea43e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ea44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ea44700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ea44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ea44fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ea45450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ea458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ea45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ea461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ea46610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ea46a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ea46ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ea47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ea477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ea47c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ea480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ea48520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ea48990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ea48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ea49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ea496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ea49b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ea49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ea4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ea4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ea4ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ea4b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ea4b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ea4ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ea4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ea4c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ea4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ea4cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ea4d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ea4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ea4e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ea4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ea4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ea4f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ea4f6b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.792s
user	0m0.287s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4177 (811872a5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153f10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153f109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x153f10f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153f11520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153f11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153f12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153f12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153f12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153f13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153f13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153f13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153f14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153f14bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153f15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153f16290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153f169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153f177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153f17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153f186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153f18e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153f19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153f19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153f1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153f1a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153f1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153f1bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153f1c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153f1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153f1c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153f1d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153f1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153f1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153f1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153f1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153f1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153f1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153f1fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153f1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153f203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153f20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153f21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153f21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153f221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153f227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153f22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153f233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153f23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153f24010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153f24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153f24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153f25140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153f25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153f25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153f26200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153f264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153f26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153f26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153f272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153f27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153f27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153f28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153f28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153f289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153f28e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153f297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153f29c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153f2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153f2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153f2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153f2aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153f2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153f2b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153f2bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153f2c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153f2c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153f2ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153f2cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153f2d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153f2d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153f2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153f2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153f2e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153f2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153f2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153f2f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153f2f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153f2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153f30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153f306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153f30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153f218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153f31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153f31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153f31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153f31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153f32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153f328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153f32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153f331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153f33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153f33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153f33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153f34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153f34910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153f34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153f35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153f356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153f35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153f36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153f364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153f36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153f36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153f372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153f37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153f37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153f38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153f38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153f389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153f38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153f39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153f397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153f39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153f3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153f3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153f3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153f3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153f3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153f3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153f3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153f3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153f3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153f3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153f3cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153f3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153f3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153f3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153f3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153f3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153f3eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153f3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153f3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153f3f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153f3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153f40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153f406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153f40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153f410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153f415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153f41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153f42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153f42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153f42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153f42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153f43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153f43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153f441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153f44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153f44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153f452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153f45770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153f46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153f469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153f46f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153f47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153f479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153f47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153f48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153f489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153f48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153f49440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153f49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153f49ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153f4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153f4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153f4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153f4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153f4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153f4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153f4c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153f4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153f4d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153f4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153f4dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153f4e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153f4e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153f4ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153f4f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153f4f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153f4fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153f503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153f50920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153f50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153f513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153f51910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153f51e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153f523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153f52900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153f52e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153f533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153f538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153f53e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153f54390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153f548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153f54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153f55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153f558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153f55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153f56370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153f568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153f56e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153f57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153f578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153f57e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153f58350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153f588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153f591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153f59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153f59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153f59fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153f5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153f5a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153f5ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153f5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153f5b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153f5bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153f5c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153f5c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153f5ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153f5d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153f5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153f5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153f5e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153f5e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153f5ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153f5f570 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1550053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1550069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1550072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1550090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15500a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15500a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15500ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15500b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15500bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15500c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15500cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15500d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15500d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15500e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15500e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15500e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15500eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15500ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15500f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15500f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15500fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1550101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1550111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1550123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1550130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1550139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1550142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1550158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1550161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1550170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1550186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15501a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15501a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15501aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15501aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15501b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15501b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15501bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15501c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15501c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15501c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15501cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15501d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15501d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15501db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15501df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15501e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15501e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15501ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15501f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15501f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15501fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15501fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1550214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1550233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1550245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1550252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1550264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1550283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1550299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15502a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15502a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15502abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15502b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15502b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15502b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15502bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15502c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15502c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15502cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15502cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15502d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15502d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15502dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15502e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15502e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15502e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15502ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15502f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15502f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15502fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1550308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1550311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1550327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1550330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1550339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155035450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155035fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1550362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155036560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1550369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155036e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1550372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155037720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155037b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155038000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155038470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1550388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155038d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1550391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155039aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155039f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15503a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15503a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15503ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15503b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15503b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15503b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15503be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15503c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15503c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15503cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15503cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15503d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15503d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15503dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15503e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15503e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15503ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15503eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15503f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15503f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15503fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1550400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155040990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155040e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155041270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1550416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155041b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155042430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1550428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155042d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1550435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155043a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155043ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1550447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155044c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155045090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155045500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155045970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155045de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1550466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155046b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155046fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155047410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155047880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155047cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155048160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1550485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155048a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155048eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155049320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155049e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15504a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15504aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15504b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15504b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15504b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15504bdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1083044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108304dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108305230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1083056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108305b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108305f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1083063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108306860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108306cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108307140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108307860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108308380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108308b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108309340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108309a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10830a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10830a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10830afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10830b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10830be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10830c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10830cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10830d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10830da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10830dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10830e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10830e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10830e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10830ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10830f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10830f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10830fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10830fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1083102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108310710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108310b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108310ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108311460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1083118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108311d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1083121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108312a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108312f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1083137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108313c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1083140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108314530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1083149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108314e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108315280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1083156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108316a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108316eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108317320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108317790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108317c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108318070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1083184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108318950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108318dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108319230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1083196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108319b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108319f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10831a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10831a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10831acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10831b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10831b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10831ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10831be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10831c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10831c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10831cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10831d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10831d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10831d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10831dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10831e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10831e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10831eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10831ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10831f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10831f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10831fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108320120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108320590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108320a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108320e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1083212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108321750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108321bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108322030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1083224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108322910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108322d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1083231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108323660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108323ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108323f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1083243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108324820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108324c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108325100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108325570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1083259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108325e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1083262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108326730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108326ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108327010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108327480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1083278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108327d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1083281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108328640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108328ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108328f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108329390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108329800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108329c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10832a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10832a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10832a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10832ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10832b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10832b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10832bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10832bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10832c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10832c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10832cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10832d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10832d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10832da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10832df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10832e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10832e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10832ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10832f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10832f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10832f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10832fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108330280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1083306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108330b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108330fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108331440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1083318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108331d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108332190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108332600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108332a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108332ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108333350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1083337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108333c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1083340a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108334510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108334980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108334df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108335980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108335c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108335f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108336370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1083367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108336c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1083370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108337530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1083379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108337e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108338280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1083386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108338b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108338fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108339440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1083398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108339d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10833a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10833a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10833aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10833aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10833b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10833b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10833bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10833c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10833c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10833c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10833cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10833d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10833d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10833db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10833dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10833e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10833e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10833ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10833f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10833f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10833fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10833fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108340330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1083407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108340c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108341080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1083414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108341960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108341dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108342240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1083426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108342b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108342f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108343400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108343870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108343ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108344150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1083445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108344a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108344ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108345310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108345780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108345bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108346060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1083464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108346940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108346db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108347220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108347690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108347b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108347f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1083483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108348850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108348cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108349800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108349f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10834a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10834ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10834b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10834b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10834b750 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.910s
user	0m0.241s
sys	0m0.125s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.53 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.11 sec
        1.13 real         0.71 user         0.04 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.29 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.15 user         0.04 sys
```
