Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.650s
user	0m0.910s
sys	0m1.290s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-cpu
[ 12%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-barrier
[ 63%] Built target test-gguf
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-backend-ops
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Built target test-quantize-fns
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-eval-callback
[ 70%] Built target llama-batched-bench
[ 70%] Built target llama-batched
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-embedding
[ 72%] Built target llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-imatrix
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-cli
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Built target llama-passkey
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-perplexity
[ 87%] Built target llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-save-load-state
[ 88%] Built target llama-retrieval
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-cvector-generator
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.180s
user	0m6.584s
sys	0m10.027s

main: quantize time =  3941.35 ms
main:    total time =  3941.35 ms

main: quantize time =  2168.87 ms
main:    total time =  2168.87 ms

main: quantize time =  2370.63 ms
main:    total time =  2370.63 ms

main: quantize time =  1886.51 ms
main:    total time =  1886.51 ms

main: quantize time =  2019.42 ms
main:    total time =  2019.42 ms

main: quantize time =  5262.06 ms
main:    total time =  5262.06 ms

main: quantize time =  5867.30 ms
main:    total time =  5867.30 ms

main: quantize time =  7364.01 ms
main:    total time =  7364.01 ms

main: quantize time =  6277.41 ms
main:    total time =  6277.41 ms

main: quantize time =  4738.21 ms
main:    total time =  4738.21 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.234 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.391 I main: llama backend init
0.00.000.398 I main: load the model and apply lora adapter, if any
0.00.076.505 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.088.911 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.088.925 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.088.929 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.088.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.088.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.088.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.088.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.088.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.088.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.088.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.088.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.088.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.088.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.088.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.088.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.088.944 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.088.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.095.750 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.097.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.104.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.104.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.104.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.104.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.104.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.104.763 I llama_model_loader: - type  f32:  194 tensors
0.00.104.763 I llama_model_loader: - type  f16:   98 tensors
0.00.104.765 I print_info: file format = GGUF V3 (latest)
0.00.104.770 I print_info: file type   = all F32 (guessed)
0.00.104.773 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.122.968 I load: special tokens cache size = 25
0.00.132.841 I load: token to piece cache size = 0.2984 MB
0.00.132.846 I print_info: arch             = gptneox
0.00.132.847 I print_info: vocab_only       = 0
0.00.132.847 I print_info: n_ctx_train      = 2048
0.00.132.847 I print_info: n_embd           = 2048
0.00.132.847 I print_info: n_layer          = 24
0.00.132.854 I print_info: n_head           = 16
0.00.132.855 I print_info: n_head_kv        = 16
0.00.132.855 I print_info: n_rot            = 32
0.00.132.855 I print_info: n_swa            = 0
0.00.132.855 I print_info: n_embd_head_k    = 128
0.00.132.856 I print_info: n_embd_head_v    = 128
0.00.132.856 I print_info: n_gqa            = 1
0.00.132.857 I print_info: n_embd_k_gqa     = 2048
0.00.132.861 I print_info: n_embd_v_gqa     = 2048
0.00.132.862 I print_info: f_norm_eps       = 1.0e-05
0.00.132.862 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.132.862 I print_info: f_clamp_kqv      = 0.0e+00
0.00.132.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.132.863 I print_info: f_logit_scale    = 0.0e+00
0.00.132.864 I print_info: n_ff             = 8192
0.00.132.864 I print_info: n_expert         = 0
0.00.132.864 I print_info: n_expert_used    = 0
0.00.132.865 I print_info: causal attn      = 1
0.00.132.865 I print_info: pooling type     = 0
0.00.132.865 I print_info: rope type        = 2
0.00.132.867 I print_info: rope scaling     = linear
0.00.132.868 I print_info: freq_base_train  = 10000.0
0.00.132.868 I print_info: freq_scale_train = 1
0.00.132.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.132.869 I print_info: rope_finetuned   = unknown
0.00.132.869 I print_info: ssm_d_conv       = 0
0.00.132.869 I print_info: ssm_d_inner      = 0
0.00.132.869 I print_info: ssm_d_state      = 0
0.00.132.869 I print_info: ssm_dt_rank      = 0
0.00.132.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.132.870 I print_info: model type       = 1.4B
0.00.132.870 I print_info: model params     = 1.41 B
0.00.132.870 I print_info: general.name     = 1.4B
0.00.132.877 I print_info: vocab type       = BPE
0.00.132.877 I print_info: n_vocab          = 50304
0.00.132.877 I print_info: n_merges         = 50009
0.00.132.879 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.132.880 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.132.880 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.132.880 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.132.880 I print_info: LF token         = 187 'Ċ'
0.00.132.881 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.132.881 I print_info: max token length = 1024
0.00.132.882 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.176.183 I load_tensors: offloading 24 repeating layers to GPU
0.00.176.187 I load_tensors: offloading output layer to GPU
0.00.176.187 I load_tensors: offloaded 25/25 layers to GPU
0.00.176.208 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.176.209 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.176.643 I llama_init_from_model: n_seq_max     = 1
0.00.176.644 I llama_init_from_model: n_ctx         = 2048
0.00.176.644 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.176.644 I llama_init_from_model: n_batch       = 2048
0.00.176.645 I llama_init_from_model: n_ubatch      = 512
0.00.176.645 I llama_init_from_model: flash_attn    = 0
0.00.176.646 I llama_init_from_model: freq_base     = 10000.0
0.00.176.646 I llama_init_from_model: freq_scale    = 1
0.00.176.647 I ggml_metal_init: allocating
0.00.176.681 I ggml_metal_init: found device: Apple M4
0.00.176.688 I ggml_metal_init: picking default device: Apple M4
0.00.177.302 I ggml_metal_init: using embedded metal library
0.00.216.796 I ggml_metal_init: GPU name:   Apple M4
0.00.216.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.216.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.216.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.216.799 I ggml_metal_init: simdgroup reduction   = true
0.00.216.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.216.799 I ggml_metal_init: has residency sets    = true
0.00.216.799 I ggml_metal_init: has bfloat            = true
0.00.216.799 I ggml_metal_init: use bfloat            = true
0.00.216.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.216.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.253 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.377.343 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.377.350 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.377.373 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.381.170 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.381.172 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.381.173 I llama_init_from_model: graph nodes  = 967
0.00.381.173 I llama_init_from_model: graph splits = 2
0.00.381.176 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.381.300 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.381.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.994 I main: llama threadpool init, n_threads = 4
0.00.447.035 I 
0.00.447.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.447.066 I 
0.00.447.109 I sampler seed: 1234
0.00.447.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.447.139 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.447.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.447.141 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.299.255 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.02.299.256 I llama_perf_context_print:        load time =     369.64 ms
0.02.299.257 I llama_perf_context_print: prompt eval time =      53.73 ms /     7 tokens (    7.68 ms per token,   130.27 tokens per second)
0.02.299.258 I llama_perf_context_print:        eval time =    1795.50 ms /    63 runs   (   28.50 ms per token,    35.09 tokens per second)
0.02.299.258 I llama_perf_context_print:       total time =    1853.10 ms /    70 tokens
0.02.299.519 I ggml_metal_free: deallocating

real	0m2.625s
user	0m0.136s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.010.021 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.226 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.612 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.613 I llama_model_loader: - type  f32:  194 tensors
0.00.037.613 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.614 I print_info: file format = GGUF V3 (latest)
0.00.037.614 I print_info: file type   = Q8_0
0.00.037.616 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.984 I load: special tokens cache size = 25
0.00.054.482 I load: token to piece cache size = 0.2984 MB
0.00.054.488 I print_info: arch             = gptneox
0.00.054.490 I print_info: vocab_only       = 0
0.00.054.491 I print_info: n_ctx_train      = 2048
0.00.054.491 I print_info: n_embd           = 2048
0.00.054.491 I print_info: n_layer          = 24
0.00.054.497 I print_info: n_head           = 16
0.00.054.498 I print_info: n_head_kv        = 16
0.00.054.499 I print_info: n_rot            = 32
0.00.054.499 I print_info: n_swa            = 0
0.00.054.499 I print_info: n_embd_head_k    = 128
0.00.054.499 I print_info: n_embd_head_v    = 128
0.00.054.500 I print_info: n_gqa            = 1
0.00.054.500 I print_info: n_embd_k_gqa     = 2048
0.00.054.501 I print_info: n_embd_v_gqa     = 2048
0.00.054.502 I print_info: f_norm_eps       = 1.0e-05
0.00.054.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.503 I print_info: f_logit_scale    = 0.0e+00
0.00.054.504 I print_info: n_ff             = 8192
0.00.054.504 I print_info: n_expert         = 0
0.00.054.504 I print_info: n_expert_used    = 0
0.00.054.504 I print_info: causal attn      = 1
0.00.054.505 I print_info: pooling type     = 0
0.00.054.505 I print_info: rope type        = 2
0.00.054.507 I print_info: rope scaling     = linear
0.00.054.507 I print_info: freq_base_train  = 10000.0
0.00.054.508 I print_info: freq_scale_train = 1
0.00.054.508 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.508 I print_info: rope_finetuned   = unknown
0.00.054.508 I print_info: ssm_d_conv       = 0
0.00.054.509 I print_info: ssm_d_inner      = 0
0.00.054.509 I print_info: ssm_d_state      = 0
0.00.054.509 I print_info: ssm_dt_rank      = 0
0.00.054.509 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.509 I print_info: model type       = 1.4B
0.00.054.510 I print_info: model params     = 1.41 B
0.00.054.510 I print_info: general.name     = 1.4B
0.00.054.510 I print_info: vocab type       = BPE
0.00.054.511 I print_info: n_vocab          = 50304
0.00.054.511 I print_info: n_merges         = 50009
0.00.054.511 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.511 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.512 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.512 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.512 I print_info: LF token         = 187 'Ċ'
0.00.054.512 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.513 I print_info: max token length = 1024
0.00.054.513 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.107.825 I load_tensors: offloading 24 repeating layers to GPU
0.01.107.830 I load_tensors: offloading output layer to GPU
0.01.107.832 I load_tensors: offloaded 25/25 layers to GPU
0.01.107.856 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.107.858 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.108.592 I llama_init_from_model: n_seq_max     = 1
0.01.108.593 I llama_init_from_model: n_ctx         = 2048
0.01.108.594 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.108.594 I llama_init_from_model: n_batch       = 2048
0.01.108.595 I llama_init_from_model: n_ubatch      = 512
0.01.108.595 I llama_init_from_model: flash_attn    = 0
0.01.108.596 I llama_init_from_model: freq_base     = 10000.0
0.01.108.596 I llama_init_from_model: freq_scale    = 1
0.01.108.597 I ggml_metal_init: allocating
0.01.108.613 I ggml_metal_init: found device: Apple M4
0.01.108.620 I ggml_metal_init: picking default device: Apple M4
0.01.109.798 I ggml_metal_init: using embedded metal library
0.01.115.401 I ggml_metal_init: GPU name:   Apple M4
0.01.115.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.115.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.115.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.115.406 I ggml_metal_init: simdgroup reduction   = true
0.01.115.407 I ggml_metal_init: simdgroup matrix mul. = true
0.01.115.407 I ggml_metal_init: has residency sets    = true
0.01.115.407 I ggml_metal_init: has bfloat            = true
0.01.115.407 I ggml_metal_init: use bfloat            = true
0.01.115.408 I ggml_metal_init: hasUnifiedMemory      = true
0.01.115.409 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.131.416 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.183.845 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.183.850 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.183.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.188.471 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.188.474 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.188.474 I llama_init_from_model: graph nodes  = 967
0.01.188.474 I llama_init_from_model: graph splits = 2
0.01.188.481 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.188.609 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.188.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.243.544 I main: llama threadpool init, n_threads = 4
0.01.243.589 I 
0.01.243.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.243.612 I 
0.01.243.771 I sampler seed: 1234
0.01.243.776 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.243.787 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.243.787 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.243.787 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.338.946 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.02.338.947 I llama_perf_context_print:        load time =    1232.83 ms
0.02.338.948 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.32 tokens per second)
0.02.338.948 I llama_perf_context_print:        eval time =    1043.47 ms /    63 runs   (   16.56 ms per token,    60.38 tokens per second)
0.02.338.949 I llama_perf_context_print:       total time =    1096.09 ms /    70 tokens
0.02.339.188 I ggml_metal_free: deallocating

real	0m2.358s
user	0m0.111s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.013.435 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.583 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.595 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.552 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.822 I llama_model_loader: - type  f32:  194 tensors
0.00.043.822 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.823 I print_info: file format = GGUF V3 (latest)
0.00.043.824 I print_info: file type   = Q4_0
0.00.043.825 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.052.811 I load: special tokens cache size = 25
0.00.061.005 I load: token to piece cache size = 0.2984 MB
0.00.061.009 I print_info: arch             = gptneox
0.00.061.009 I print_info: vocab_only       = 0
0.00.061.009 I print_info: n_ctx_train      = 2048
0.00.061.009 I print_info: n_embd           = 2048
0.00.061.010 I print_info: n_layer          = 24
0.00.061.013 I print_info: n_head           = 16
0.00.061.014 I print_info: n_head_kv        = 16
0.00.061.014 I print_info: n_rot            = 32
0.00.061.015 I print_info: n_swa            = 0
0.00.061.017 I print_info: n_embd_head_k    = 128
0.00.061.017 I print_info: n_embd_head_v    = 128
0.00.061.018 I print_info: n_gqa            = 1
0.00.061.019 I print_info: n_embd_k_gqa     = 2048
0.00.061.019 I print_info: n_embd_v_gqa     = 2048
0.00.061.020 I print_info: f_norm_eps       = 1.0e-05
0.00.061.021 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.021 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.021 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.021 I print_info: f_logit_scale    = 0.0e+00
0.00.061.023 I print_info: n_ff             = 8192
0.00.061.023 I print_info: n_expert         = 0
0.00.061.024 I print_info: n_expert_used    = 0
0.00.061.024 I print_info: causal attn      = 1
0.00.061.024 I print_info: pooling type     = 0
0.00.061.024 I print_info: rope type        = 2
0.00.061.024 I print_info: rope scaling     = linear
0.00.061.025 I print_info: freq_base_train  = 10000.0
0.00.061.025 I print_info: freq_scale_train = 1
0.00.061.025 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.026 I print_info: rope_finetuned   = unknown
0.00.061.026 I print_info: ssm_d_conv       = 0
0.00.061.026 I print_info: ssm_d_inner      = 0
0.00.061.026 I print_info: ssm_d_state      = 0
0.00.061.026 I print_info: ssm_dt_rank      = 0
0.00.061.026 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.027 I print_info: model type       = 1.4B
0.00.061.027 I print_info: model params     = 1.41 B
0.00.061.027 I print_info: general.name     = 1.4B
0.00.061.028 I print_info: vocab type       = BPE
0.00.061.028 I print_info: n_vocab          = 50304
0.00.061.028 I print_info: n_merges         = 50009
0.00.061.029 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.029 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.029 I print_info: LF token         = 187 'Ċ'
0.00.061.030 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.030 I print_info: max token length = 1024
0.00.061.030 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.281 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.297 I load_tensors: offloading output layer to GPU
0.00.649.298 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.331 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.649.332 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.650.671 I llama_init_from_model: n_seq_max     = 1
0.00.650.674 I llama_init_from_model: n_ctx         = 2048
0.00.650.675 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.675 I llama_init_from_model: n_batch       = 2048
0.00.650.676 I llama_init_from_model: n_ubatch      = 512
0.00.650.676 I llama_init_from_model: flash_attn    = 0
0.00.650.678 I llama_init_from_model: freq_base     = 10000.0
0.00.650.679 I llama_init_from_model: freq_scale    = 1
0.00.650.681 I ggml_metal_init: allocating
0.00.650.755 I ggml_metal_init: found device: Apple M4
0.00.650.769 I ggml_metal_init: picking default device: Apple M4
0.00.652.510 I ggml_metal_init: using embedded metal library
0.00.658.154 I ggml_metal_init: GPU name:   Apple M4
0.00.658.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.160 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.161 I ggml_metal_init: simdgroup reduction   = true
0.00.658.161 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.161 I ggml_metal_init: has residency sets    = true
0.00.658.162 I ggml_metal_init: has bfloat            = true
0.00.658.162 I ggml_metal_init: use bfloat            = true
0.00.658.163 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.329 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.337 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.361 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.312 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.315 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.315 I llama_init_from_model: graph nodes  = 967
0.00.741.315 I llama_init_from_model: graph splits = 2
0.00.741.322 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.021 I main: llama threadpool init, n_threads = 4
0.00.798.069 I 
0.00.798.091 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.092 I 
0.00.798.243 I sampler seed: 1234
0.00.798.248 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.279 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.280 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.280 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.482.581 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.482.582 I llama_perf_context_print:        load time =     783.89 ms
0.01.482.582 I llama_perf_context_print: prompt eval time =      44.39 ms /     7 tokens (    6.34 ms per token,   157.70 tokens per second)
0.01.482.583 I llama_perf_context_print:        eval time =     636.99 ms /    63 runs   (   10.11 ms per token,    98.90 tokens per second)
0.01.482.583 I llama_perf_context_print:       total time =     685.25 ms /    70 tokens
0.01.482.810 I ggml_metal_free: deallocating

real	0m1.501s
user	0m0.114s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.059 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.203 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.493 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.499 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.018 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.019 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.019 I llama_model_loader: - type  f32:  194 tensors
0.00.031.020 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.020 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.021 I print_info: file format = GGUF V3 (latest)
0.00.031.021 I print_info: file type   = Q4_1
0.00.031.022 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.039.054 I load: special tokens cache size = 25
0.00.044.839 I load: token to piece cache size = 0.2984 MB
0.00.044.842 I print_info: arch             = gptneox
0.00.044.842 I print_info: vocab_only       = 0
0.00.044.842 I print_info: n_ctx_train      = 2048
0.00.044.842 I print_info: n_embd           = 2048
0.00.044.842 I print_info: n_layer          = 24
0.00.044.845 I print_info: n_head           = 16
0.00.044.846 I print_info: n_head_kv        = 16
0.00.044.846 I print_info: n_rot            = 32
0.00.044.846 I print_info: n_swa            = 0
0.00.044.846 I print_info: n_embd_head_k    = 128
0.00.044.846 I print_info: n_embd_head_v    = 128
0.00.044.847 I print_info: n_gqa            = 1
0.00.044.848 I print_info: n_embd_k_gqa     = 2048
0.00.044.850 I print_info: n_embd_v_gqa     = 2048
0.00.044.851 I print_info: f_norm_eps       = 1.0e-05
0.00.044.851 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.851 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.851 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.851 I print_info: f_logit_scale    = 0.0e+00
0.00.044.852 I print_info: n_ff             = 8192
0.00.044.856 I print_info: n_expert         = 0
0.00.044.856 I print_info: n_expert_used    = 0
0.00.044.857 I print_info: causal attn      = 1
0.00.044.857 I print_info: pooling type     = 0
0.00.044.857 I print_info: rope type        = 2
0.00.044.857 I print_info: rope scaling     = linear
0.00.044.858 I print_info: freq_base_train  = 10000.0
0.00.044.858 I print_info: freq_scale_train = 1
0.00.044.858 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.858 I print_info: rope_finetuned   = unknown
0.00.044.859 I print_info: ssm_d_conv       = 0
0.00.044.859 I print_info: ssm_d_inner      = 0
0.00.044.859 I print_info: ssm_d_state      = 0
0.00.044.861 I print_info: ssm_dt_rank      = 0
0.00.044.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.861 I print_info: model type       = 1.4B
0.00.044.862 I print_info: model params     = 1.41 B
0.00.044.862 I print_info: general.name     = 1.4B
0.00.044.862 I print_info: vocab type       = BPE
0.00.044.863 I print_info: n_vocab          = 50304
0.00.044.863 I print_info: n_merges         = 50009
0.00.044.864 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.864 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.864 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.864 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.864 I print_info: LF token         = 187 'Ċ'
0.00.044.865 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.865 I print_info: max token length = 1024
0.00.044.865 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.832.031 I load_tensors: offloading 24 repeating layers to GPU
0.00.832.037 I load_tensors: offloading output layer to GPU
0.00.832.037 I load_tensors: offloaded 25/25 layers to GPU
0.00.832.055 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.832.056 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.832.928 I llama_init_from_model: n_seq_max     = 1
0.00.832.933 I llama_init_from_model: n_ctx         = 2048
0.00.832.934 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.832.934 I llama_init_from_model: n_batch       = 2048
0.00.832.935 I llama_init_from_model: n_ubatch      = 512
0.00.832.935 I llama_init_from_model: flash_attn    = 0
0.00.832.936 I llama_init_from_model: freq_base     = 10000.0
0.00.832.937 I llama_init_from_model: freq_scale    = 1
0.00.832.938 I ggml_metal_init: allocating
0.00.832.974 I ggml_metal_init: found device: Apple M4
0.00.832.985 I ggml_metal_init: picking default device: Apple M4
0.00.834.056 I ggml_metal_init: using embedded metal library
0.00.838.268 I ggml_metal_init: GPU name:   Apple M4
0.00.838.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.838.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.838.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.838.278 I ggml_metal_init: simdgroup reduction   = true
0.00.838.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.838.278 I ggml_metal_init: has residency sets    = true
0.00.838.279 I ggml_metal_init: has bfloat            = true
0.00.838.279 I ggml_metal_init: use bfloat            = true
0.00.838.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.838.283 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.849.713 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.880.593 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.880.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.880.629 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.885.883 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.885.885 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.885.885 I llama_init_from_model: graph nodes  = 967
0.00.885.885 I llama_init_from_model: graph splits = 2
0.00.885.891 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.886.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.886.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.943.928 I main: llama threadpool init, n_threads = 4
0.00.943.978 I 
0.00.943.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.944.001 I 
0.00.944.155 I sampler seed: 1234
0.00.944.160 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.944.178 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.944.179 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.944.179 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.682.161 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.682.162 I llama_perf_context_print:        load time =     934.03 ms
0.01.682.163 I llama_perf_context_print: prompt eval time =      49.12 ms /     7 tokens (    7.02 ms per token,   142.52 tokens per second)
0.01.682.164 I llama_perf_context_print:        eval time =     686.11 ms /    63 runs   (   10.89 ms per token,    91.82 tokens per second)
0.01.682.164 I llama_perf_context_print:       total time =     738.93 ms /    70 tokens
0.01.682.378 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.100s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.077 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.109 I main: llama backend init
0.00.000.111 I main: load the model and apply lora adapter, if any
0.00.009.771 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.219 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.230 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.231 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.236 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.061 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.798 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.801 I llama_model_loader: - type  f32:  194 tensors
0.00.025.801 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.802 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.802 I print_info: file format = GGUF V3 (latest)
0.00.025.803 I print_info: file type   = Q5_0
0.00.025.804 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.540 I load: special tokens cache size = 25
0.00.039.592 I load: token to piece cache size = 0.2984 MB
0.00.039.594 I print_info: arch             = gptneox
0.00.039.595 I print_info: vocab_only       = 0
0.00.039.595 I print_info: n_ctx_train      = 2048
0.00.039.595 I print_info: n_embd           = 2048
0.00.039.595 I print_info: n_layer          = 24
0.00.039.598 I print_info: n_head           = 16
0.00.039.599 I print_info: n_head_kv        = 16
0.00.039.599 I print_info: n_rot            = 32
0.00.039.599 I print_info: n_swa            = 0
0.00.039.599 I print_info: n_embd_head_k    = 128
0.00.039.600 I print_info: n_embd_head_v    = 128
0.00.039.600 I print_info: n_gqa            = 1
0.00.039.601 I print_info: n_embd_k_gqa     = 2048
0.00.039.602 I print_info: n_embd_v_gqa     = 2048
0.00.039.602 I print_info: f_norm_eps       = 1.0e-05
0.00.039.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.605 I print_info: f_logit_scale    = 0.0e+00
0.00.039.605 I print_info: n_ff             = 8192
0.00.039.606 I print_info: n_expert         = 0
0.00.039.606 I print_info: n_expert_used    = 0
0.00.039.606 I print_info: causal attn      = 1
0.00.039.606 I print_info: pooling type     = 0
0.00.039.606 I print_info: rope type        = 2
0.00.039.606 I print_info: rope scaling     = linear
0.00.039.607 I print_info: freq_base_train  = 10000.0
0.00.039.607 I print_info: freq_scale_train = 1
0.00.039.607 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.608 I print_info: rope_finetuned   = unknown
0.00.039.608 I print_info: ssm_d_conv       = 0
0.00.039.608 I print_info: ssm_d_inner      = 0
0.00.039.608 I print_info: ssm_d_state      = 0
0.00.039.608 I print_info: ssm_dt_rank      = 0
0.00.039.608 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.608 I print_info: model type       = 1.4B
0.00.039.609 I print_info: model params     = 1.41 B
0.00.039.609 I print_info: general.name     = 1.4B
0.00.039.609 I print_info: vocab type       = BPE
0.00.039.610 I print_info: n_vocab          = 50304
0.00.039.610 I print_info: n_merges         = 50009
0.00.039.610 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.610 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.610 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: LF token         = 187 'Ċ'
0.00.039.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: max token length = 1024
0.00.039.612 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.682.364 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.381 I load_tensors: offloading output layer to GPU
0.00.682.382 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.418 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.682.419 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.683.871 I llama_init_from_model: n_seq_max     = 1
0.00.683.874 I llama_init_from_model: n_ctx         = 2048
0.00.683.874 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.683.875 I llama_init_from_model: n_batch       = 2048
0.00.683.876 I llama_init_from_model: n_ubatch      = 512
0.00.683.876 I llama_init_from_model: flash_attn    = 0
0.00.683.877 I llama_init_from_model: freq_base     = 10000.0
0.00.683.877 I llama_init_from_model: freq_scale    = 1
0.00.683.882 I ggml_metal_init: allocating
0.00.683.943 I ggml_metal_init: found device: Apple M4
0.00.683.953 I ggml_metal_init: picking default device: Apple M4
0.00.685.483 I ggml_metal_init: using embedded metal library
0.00.691.798 I ggml_metal_init: GPU name:   Apple M4
0.00.691.802 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.804 I ggml_metal_init: simdgroup reduction   = true
0.00.691.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.805 I ggml_metal_init: has residency sets    = true
0.00.691.805 I ggml_metal_init: has bfloat            = true
0.00.691.805 I ggml_metal_init: use bfloat            = true
0.00.691.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.454 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.766.401 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.766.407 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.766.432 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.770.608 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.770.609 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.770.610 I llama_init_from_model: graph nodes  = 967
0.00.770.610 I llama_init_from_model: graph splits = 2
0.00.770.616 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.770.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.770.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.859 I main: llama threadpool init, n_threads = 4
0.00.828.908 I 
0.00.828.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.930 I 
0.00.829.082 I sampler seed: 1234
0.00.829.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.107 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.108 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.108 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.629.591 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.629.592 I llama_perf_context_print:        load time =     818.37 ms
0.01.629.594 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.36 tokens per second)
0.01.629.595 I llama_perf_context_print:        eval time =     744.69 ms /    63 runs   (   11.82 ms per token,    84.60 tokens per second)
0.01.629.596 I llama_perf_context_print:       total time =     801.45 ms /    70 tokens
0.01.629.846 I ggml_metal_free: deallocating

real	0m1.651s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.740 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.740 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.742 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.216 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.217 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.218 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.218 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.219 I llama_model_loader: - type  f32:  194 tensors
0.00.025.220 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.220 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.220 I print_info: file format = GGUF V3 (latest)
0.00.025.221 I print_info: file type   = Q5_1
0.00.025.223 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.294 I load: special tokens cache size = 25
0.00.039.435 I load: token to piece cache size = 0.2984 MB
0.00.039.438 I print_info: arch             = gptneox
0.00.039.438 I print_info: vocab_only       = 0
0.00.039.438 I print_info: n_ctx_train      = 2048
0.00.039.438 I print_info: n_embd           = 2048
0.00.039.439 I print_info: n_layer          = 24
0.00.039.441 I print_info: n_head           = 16
0.00.039.442 I print_info: n_head_kv        = 16
0.00.039.442 I print_info: n_rot            = 32
0.00.039.442 I print_info: n_swa            = 0
0.00.039.443 I print_info: n_embd_head_k    = 128
0.00.039.443 I print_info: n_embd_head_v    = 128
0.00.039.444 I print_info: n_gqa            = 1
0.00.039.445 I print_info: n_embd_k_gqa     = 2048
0.00.039.446 I print_info: n_embd_v_gqa     = 2048
0.00.039.446 I print_info: f_norm_eps       = 1.0e-05
0.00.039.447 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.447 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.447 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.447 I print_info: f_logit_scale    = 0.0e+00
0.00.039.450 I print_info: n_ff             = 8192
0.00.039.450 I print_info: n_expert         = 0
0.00.039.450 I print_info: n_expert_used    = 0
0.00.039.450 I print_info: causal attn      = 1
0.00.039.450 I print_info: pooling type     = 0
0.00.039.452 I print_info: rope type        = 2
0.00.039.453 I print_info: rope scaling     = linear
0.00.039.454 I print_info: freq_base_train  = 10000.0
0.00.039.454 I print_info: freq_scale_train = 1
0.00.039.454 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.454 I print_info: rope_finetuned   = unknown
0.00.039.455 I print_info: ssm_d_conv       = 0
0.00.039.455 I print_info: ssm_d_inner      = 0
0.00.039.455 I print_info: ssm_d_state      = 0
0.00.039.455 I print_info: ssm_dt_rank      = 0
0.00.039.455 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.455 I print_info: model type       = 1.4B
0.00.039.456 I print_info: model params     = 1.41 B
0.00.039.456 I print_info: general.name     = 1.4B
0.00.039.456 I print_info: vocab type       = BPE
0.00.039.457 I print_info: n_vocab          = 50304
0.00.039.457 I print_info: n_merges         = 50009
0.00.039.457 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.457 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: LF token         = 187 'Ċ'
0.00.039.458 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.458 I print_info: max token length = 1024
0.00.039.459 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.700 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.706 I load_tensors: offloading output layer to GPU
0.00.600.707 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.727 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.600.730 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.602.159 I llama_init_from_model: n_seq_max     = 1
0.00.602.161 I llama_init_from_model: n_ctx         = 2048
0.00.602.162 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.602.162 I llama_init_from_model: n_batch       = 2048
0.00.602.163 I llama_init_from_model: n_ubatch      = 512
0.00.602.163 I llama_init_from_model: flash_attn    = 0
0.00.602.164 I llama_init_from_model: freq_base     = 10000.0
0.00.602.164 I llama_init_from_model: freq_scale    = 1
0.00.602.165 I ggml_metal_init: allocating
0.00.602.183 I ggml_metal_init: found device: Apple M4
0.00.602.192 I ggml_metal_init: picking default device: Apple M4
0.00.603.686 I ggml_metal_init: using embedded metal library
0.00.609.876 I ggml_metal_init: GPU name:   Apple M4
0.00.609.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.882 I ggml_metal_init: simdgroup reduction   = true
0.00.609.882 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.882 I ggml_metal_init: has residency sets    = true
0.00.609.882 I ggml_metal_init: has bfloat            = true
0.00.609.883 I ggml_metal_init: use bfloat            = true
0.00.609.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.461 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.274 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.280 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.301 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.835 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.837 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.838 I llama_init_from_model: graph nodes  = 967
0.00.684.838 I llama_init_from_model: graph splits = 2
0.00.684.844 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.778 I main: llama threadpool init, n_threads = 4
0.00.744.822 I 
0.00.744.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.850 I 
0.00.745.000 I sampler seed: 1234
0.00.745.005 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.025 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.025 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.591.724 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.01.591.724 I llama_perf_context_print:        load time =     735.20 ms
0.01.591.725 I llama_perf_context_print: prompt eval time =      50.69 ms /     7 tokens (    7.24 ms per token,   138.09 tokens per second)
0.01.591.726 I llama_perf_context_print:        eval time =     793.07 ms /    63 runs   (   12.59 ms per token,    79.44 tokens per second)
0.01.591.726 I llama_perf_context_print:       total time =     847.65 ms /    70 tokens
0.01.591.991 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.107s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.310 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.316 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.317 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.322 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.326 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.142 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.961 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.962 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.963 I llama_model_loader: - type  f32:  194 tensors
0.00.024.964 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.964 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.965 I print_info: file format = GGUF V3 (latest)
0.00.024.965 I print_info: file type   = Q2_K - Medium
0.00.024.966 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.670 I load: special tokens cache size = 25
0.00.038.684 I load: token to piece cache size = 0.2984 MB
0.00.038.687 I print_info: arch             = gptneox
0.00.038.687 I print_info: vocab_only       = 0
0.00.038.687 I print_info: n_ctx_train      = 2048
0.00.038.687 I print_info: n_embd           = 2048
0.00.038.688 I print_info: n_layer          = 24
0.00.038.690 I print_info: n_head           = 16
0.00.038.691 I print_info: n_head_kv        = 16
0.00.038.691 I print_info: n_rot            = 32
0.00.038.691 I print_info: n_swa            = 0
0.00.038.692 I print_info: n_embd_head_k    = 128
0.00.038.692 I print_info: n_embd_head_v    = 128
0.00.038.693 I print_info: n_gqa            = 1
0.00.038.693 I print_info: n_embd_k_gqa     = 2048
0.00.038.694 I print_info: n_embd_v_gqa     = 2048
0.00.038.695 I print_info: f_norm_eps       = 1.0e-05
0.00.038.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.696 I print_info: f_logit_scale    = 0.0e+00
0.00.038.698 I print_info: n_ff             = 8192
0.00.038.698 I print_info: n_expert         = 0
0.00.038.698 I print_info: n_expert_used    = 0
0.00.038.699 I print_info: causal attn      = 1
0.00.038.699 I print_info: pooling type     = 0
0.00.038.700 I print_info: rope type        = 2
0.00.038.700 I print_info: rope scaling     = linear
0.00.038.701 I print_info: freq_base_train  = 10000.0
0.00.038.701 I print_info: freq_scale_train = 1
0.00.038.701 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.701 I print_info: rope_finetuned   = unknown
0.00.038.702 I print_info: ssm_d_conv       = 0
0.00.038.702 I print_info: ssm_d_inner      = 0
0.00.038.702 I print_info: ssm_d_state      = 0
0.00.038.702 I print_info: ssm_dt_rank      = 0
0.00.038.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.702 I print_info: model type       = 1.4B
0.00.038.703 I print_info: model params     = 1.41 B
0.00.038.703 I print_info: general.name     = 1.4B
0.00.038.703 I print_info: vocab type       = BPE
0.00.038.704 I print_info: n_vocab          = 50304
0.00.038.704 I print_info: n_merges         = 50009
0.00.038.704 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.705 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.705 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.705 I print_info: LF token         = 187 'Ċ'
0.00.038.706 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: max token length = 1024
0.00.038.707 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.344.306 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.322 I load_tensors: offloading output layer to GPU
0.00.344.322 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.355 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.361 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.346.105 I llama_init_from_model: n_seq_max     = 1
0.00.346.109 I llama_init_from_model: n_ctx         = 2048
0.00.346.110 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.346.110 I llama_init_from_model: n_batch       = 2048
0.00.346.111 I llama_init_from_model: n_ubatch      = 512
0.00.346.111 I llama_init_from_model: flash_attn    = 0
0.00.346.113 I llama_init_from_model: freq_base     = 10000.0
0.00.346.114 I llama_init_from_model: freq_scale    = 1
0.00.346.115 I ggml_metal_init: allocating
0.00.346.229 I ggml_metal_init: found device: Apple M4
0.00.346.242 I ggml_metal_init: picking default device: Apple M4
0.00.348.156 I ggml_metal_init: using embedded metal library
0.00.353.623 I ggml_metal_init: GPU name:   Apple M4
0.00.353.639 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.641 I ggml_metal_init: simdgroup reduction   = true
0.00.353.642 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.642 I ggml_metal_init: has residency sets    = true
0.00.353.642 I ggml_metal_init: has bfloat            = true
0.00.353.643 I ggml_metal_init: use bfloat            = true
0.00.353.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.650 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.919 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.433.815 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.433.822 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.844 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.025 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.438.028 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.438.028 I llama_init_from_model: graph nodes  = 967
0.00.438.028 I llama_init_from_model: graph splits = 2
0.00.438.033 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.438.169 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.438.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.550 I main: llama threadpool init, n_threads = 4
0.00.499.594 I 
0.00.499.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.618 I 
0.00.499.773 I sampler seed: 1234
0.00.499.778 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.801 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.802 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.803 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.177.856 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.177.858 I llama_perf_context_print:        load time =     489.00 ms
0.01.177.858 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.79 tokens per second)
0.01.177.859 I llama_perf_context_print:        eval time =     633.02 ms /    63 runs   (   10.05 ms per token,    99.52 tokens per second)
0.01.177.859 I llama_perf_context_print:       total time =     679.00 ms /    70 tokens
0.01.178.115 I ggml_metal_free: deallocating

real	0m1.196s
user	0m0.112s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.261 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.972 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.805 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.570 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.571 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.573 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.573 I llama_model_loader: - type  f32:  194 tensors
0.00.025.574 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.574 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.574 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.575 I print_info: file format = GGUF V3 (latest)
0.00.025.575 I print_info: file type   = Q3_K - Medium
0.00.025.576 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.612 I load: special tokens cache size = 25
0.00.039.756 I load: token to piece cache size = 0.2984 MB
0.00.039.759 I print_info: arch             = gptneox
0.00.039.759 I print_info: vocab_only       = 0
0.00.039.759 I print_info: n_ctx_train      = 2048
0.00.039.759 I print_info: n_embd           = 2048
0.00.039.760 I print_info: n_layer          = 24
0.00.039.762 I print_info: n_head           = 16
0.00.039.763 I print_info: n_head_kv        = 16
0.00.039.763 I print_info: n_rot            = 32
0.00.039.764 I print_info: n_swa            = 0
0.00.039.764 I print_info: n_embd_head_k    = 128
0.00.039.766 I print_info: n_embd_head_v    = 128
0.00.039.766 I print_info: n_gqa            = 1
0.00.039.767 I print_info: n_embd_k_gqa     = 2048
0.00.039.768 I print_info: n_embd_v_gqa     = 2048
0.00.039.768 I print_info: f_norm_eps       = 1.0e-05
0.00.039.769 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.769 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.769 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.769 I print_info: f_logit_scale    = 0.0e+00
0.00.039.770 I print_info: n_ff             = 8192
0.00.039.770 I print_info: n_expert         = 0
0.00.039.770 I print_info: n_expert_used    = 0
0.00.039.770 I print_info: causal attn      = 1
0.00.039.770 I print_info: pooling type     = 0
0.00.039.771 I print_info: rope type        = 2
0.00.039.773 I print_info: rope scaling     = linear
0.00.039.773 I print_info: freq_base_train  = 10000.0
0.00.039.773 I print_info: freq_scale_train = 1
0.00.039.773 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.774 I print_info: rope_finetuned   = unknown
0.00.039.774 I print_info: ssm_d_conv       = 0
0.00.039.774 I print_info: ssm_d_inner      = 0
0.00.039.774 I print_info: ssm_d_state      = 0
0.00.039.774 I print_info: ssm_dt_rank      = 0
0.00.039.774 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.774 I print_info: model type       = 1.4B
0.00.039.775 I print_info: model params     = 1.41 B
0.00.039.775 I print_info: general.name     = 1.4B
0.00.039.775 I print_info: vocab type       = BPE
0.00.039.776 I print_info: n_vocab          = 50304
0.00.039.776 I print_info: n_merges         = 50009
0.00.039.776 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.776 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.776 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.777 I print_info: LF token         = 187 'Ċ'
0.00.039.777 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: max token length = 1024
0.00.039.778 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.455.312 I load_tensors: offloading output layer to GPU
0.00.455.313 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.347 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.349 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.456.901 I llama_init_from_model: n_seq_max     = 1
0.00.456.904 I llama_init_from_model: n_ctx         = 2048
0.00.456.905 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.456.905 I llama_init_from_model: n_batch       = 2048
0.00.456.906 I llama_init_from_model: n_ubatch      = 512
0.00.456.906 I llama_init_from_model: flash_attn    = 0
0.00.456.908 I llama_init_from_model: freq_base     = 10000.0
0.00.456.909 I llama_init_from_model: freq_scale    = 1
0.00.456.911 I ggml_metal_init: allocating
0.00.456.990 I ggml_metal_init: found device: Apple M4
0.00.457.003 I ggml_metal_init: picking default device: Apple M4
0.00.458.864 I ggml_metal_init: using embedded metal library
0.00.464.491 I ggml_metal_init: GPU name:   Apple M4
0.00.464.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.464.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.464.498 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.464.498 I ggml_metal_init: simdgroup reduction   = true
0.00.464.499 I ggml_metal_init: simdgroup matrix mul. = true
0.00.464.499 I ggml_metal_init: has residency sets    = true
0.00.464.499 I ggml_metal_init: has bfloat            = true
0.00.464.500 I ggml_metal_init: use bfloat            = true
0.00.464.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.464.502 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.483.933 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.536 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.542.543 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.542.572 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.547.317 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.547.319 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.547.319 I llama_init_from_model: graph nodes  = 967
0.00.547.319 I llama_init_from_model: graph splits = 2
0.00.547.324 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.547.454 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.547.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.605.600 I main: llama threadpool init, n_threads = 4
0.00.605.646 I 
0.00.605.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.605.669 I 
0.00.605.819 I sampler seed: 1234
0.00.605.824 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.605.835 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.605.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.605.835 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.352.166 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48630.14 tokens per second)
0.01.352.167 I llama_perf_context_print:        load time =     595.65 ms
0.01.352.168 I llama_perf_context_print: prompt eval time =      49.72 ms /     7 tokens (    7.10 ms per token,   140.78 tokens per second)
0.01.352.168 I llama_perf_context_print:        eval time =     693.91 ms /    63 runs   (   11.01 ms per token,    90.79 tokens per second)
0.01.352.168 I llama_perf_context_print:       total time =     747.25 ms /    70 tokens
0.01.352.426 I ggml_metal_free: deallocating

real	0m1.369s
user	0m0.111s
sys	0m0.197s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.933 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.071 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.867 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.903 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.633 I llama_model_loader: - type  f32:  194 tensors
0.00.024.633 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.633 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.633 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.634 I print_info: file format = GGUF V3 (latest)
0.00.024.634 I print_info: file type   = Q4_K - Medium
0.00.024.635 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.355 I load: special tokens cache size = 25
0.00.038.494 I load: token to piece cache size = 0.2984 MB
0.00.038.496 I print_info: arch             = gptneox
0.00.038.497 I print_info: vocab_only       = 0
0.00.038.497 I print_info: n_ctx_train      = 2048
0.00.038.497 I print_info: n_embd           = 2048
0.00.038.497 I print_info: n_layer          = 24
0.00.038.500 I print_info: n_head           = 16
0.00.038.500 I print_info: n_head_kv        = 16
0.00.038.500 I print_info: n_rot            = 32
0.00.038.501 I print_info: n_swa            = 0
0.00.038.501 I print_info: n_embd_head_k    = 128
0.00.038.501 I print_info: n_embd_head_v    = 128
0.00.038.502 I print_info: n_gqa            = 1
0.00.038.502 I print_info: n_embd_k_gqa     = 2048
0.00.038.503 I print_info: n_embd_v_gqa     = 2048
0.00.038.504 I print_info: f_norm_eps       = 1.0e-05
0.00.038.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.507 I print_info: f_logit_scale    = 0.0e+00
0.00.038.508 I print_info: n_ff             = 8192
0.00.038.508 I print_info: n_expert         = 0
0.00.038.508 I print_info: n_expert_used    = 0
0.00.038.508 I print_info: causal attn      = 1
0.00.038.510 I print_info: pooling type     = 0
0.00.038.511 I print_info: rope type        = 2
0.00.038.511 I print_info: rope scaling     = linear
0.00.038.511 I print_info: freq_base_train  = 10000.0
0.00.038.512 I print_info: freq_scale_train = 1
0.00.038.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.512 I print_info: rope_finetuned   = unknown
0.00.038.512 I print_info: ssm_d_conv       = 0
0.00.038.512 I print_info: ssm_d_inner      = 0
0.00.038.513 I print_info: ssm_d_state      = 0
0.00.038.513 I print_info: ssm_dt_rank      = 0
0.00.038.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.513 I print_info: model type       = 1.4B
0.00.038.513 I print_info: model params     = 1.41 B
0.00.038.513 I print_info: general.name     = 1.4B
0.00.038.514 I print_info: vocab type       = BPE
0.00.038.514 I print_info: n_vocab          = 50304
0.00.038.518 I print_info: n_merges         = 50009
0.00.038.518 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.521 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.522 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.522 I print_info: LF token         = 187 'Ċ'
0.00.038.522 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.523 I print_info: max token length = 1024
0.00.038.523 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.207 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.219 I load_tensors: offloading output layer to GPU
0.00.544.220 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.256 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.257 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.545.817 I llama_init_from_model: n_seq_max     = 1
0.00.545.822 I llama_init_from_model: n_ctx         = 2048
0.00.545.822 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.545.823 I llama_init_from_model: n_batch       = 2048
0.00.545.823 I llama_init_from_model: n_ubatch      = 512
0.00.545.823 I llama_init_from_model: flash_attn    = 0
0.00.545.825 I llama_init_from_model: freq_base     = 10000.0
0.00.545.825 I llama_init_from_model: freq_scale    = 1
0.00.545.827 I ggml_metal_init: allocating
0.00.545.878 I ggml_metal_init: found device: Apple M4
0.00.545.890 I ggml_metal_init: picking default device: Apple M4
0.00.547.665 I ggml_metal_init: using embedded metal library
0.00.553.355 I ggml_metal_init: GPU name:   Apple M4
0.00.553.361 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.553.361 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.553.362 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.553.363 I ggml_metal_init: simdgroup reduction   = true
0.00.553.363 I ggml_metal_init: simdgroup matrix mul. = true
0.00.553.363 I ggml_metal_init: has residency sets    = true
0.00.553.364 I ggml_metal_init: has bfloat            = true
0.00.553.364 I ggml_metal_init: use bfloat            = true
0.00.553.365 I ggml_metal_init: hasUnifiedMemory      = true
0.00.553.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.609 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.211 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.634.217 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.634.243 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.340 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.639.343 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.639.343 I llama_init_from_model: graph nodes  = 967
0.00.639.343 I llama_init_from_model: graph splits = 2
0.00.639.348 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.639.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.639.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.287 I main: llama threadpool init, n_threads = 4
0.00.699.332 I 
0.00.699.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.357 I 
0.00.699.509 I sampler seed: 1234
0.00.699.513 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.524 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.524 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.525 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.453.928 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.453.928 I llama_perf_context_print:        load time =     689.66 ms
0.01.453.929 I llama_perf_context_print: prompt eval time =      50.61 ms /     7 tokens (    7.23 ms per token,   138.32 tokens per second)
0.01.453.931 I llama_perf_context_print:        eval time =     700.91 ms /    63 runs   (   11.13 ms per token,    89.88 tokens per second)
0.01.453.931 I llama_perf_context_print:       total time =     755.33 ms /    70 tokens
0.01.454.152 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.110s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.228 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.797 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.803 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.804 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.804 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.805 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.808 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.811 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.811 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.262 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.264 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.265 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.265 I llama_model_loader: - type  f32:  194 tensors
0.00.026.265 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.266 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.266 I print_info: file format = GGUF V3 (latest)
0.00.026.267 I print_info: file type   = Q5_K - Medium
0.00.026.272 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.297 I load: special tokens cache size = 25
0.00.040.371 I load: token to piece cache size = 0.2984 MB
0.00.040.374 I print_info: arch             = gptneox
0.00.040.374 I print_info: vocab_only       = 0
0.00.040.374 I print_info: n_ctx_train      = 2048
0.00.040.374 I print_info: n_embd           = 2048
0.00.040.375 I print_info: n_layer          = 24
0.00.040.377 I print_info: n_head           = 16
0.00.040.378 I print_info: n_head_kv        = 16
0.00.040.378 I print_info: n_rot            = 32
0.00.040.378 I print_info: n_swa            = 0
0.00.040.379 I print_info: n_embd_head_k    = 128
0.00.040.379 I print_info: n_embd_head_v    = 128
0.00.040.380 I print_info: n_gqa            = 1
0.00.040.380 I print_info: n_embd_k_gqa     = 2048
0.00.040.381 I print_info: n_embd_v_gqa     = 2048
0.00.040.382 I print_info: f_norm_eps       = 1.0e-05
0.00.040.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.382 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.384 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.384 I print_info: f_logit_scale    = 0.0e+00
0.00.040.384 I print_info: n_ff             = 8192
0.00.040.385 I print_info: n_expert         = 0
0.00.040.387 I print_info: n_expert_used    = 0
0.00.040.387 I print_info: causal attn      = 1
0.00.040.387 I print_info: pooling type     = 0
0.00.040.387 I print_info: rope type        = 2
0.00.040.388 I print_info: rope scaling     = linear
0.00.040.388 I print_info: freq_base_train  = 10000.0
0.00.040.388 I print_info: freq_scale_train = 1
0.00.040.388 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.389 I print_info: rope_finetuned   = unknown
0.00.040.389 I print_info: ssm_d_conv       = 0
0.00.040.389 I print_info: ssm_d_inner      = 0
0.00.040.389 I print_info: ssm_d_state      = 0
0.00.040.389 I print_info: ssm_dt_rank      = 0
0.00.040.389 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.389 I print_info: model type       = 1.4B
0.00.040.390 I print_info: model params     = 1.41 B
0.00.040.390 I print_info: general.name     = 1.4B
0.00.040.391 I print_info: vocab type       = BPE
0.00.040.391 I print_info: n_vocab          = 50304
0.00.040.391 I print_info: n_merges         = 50009
0.00.040.391 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.391 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.392 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.392 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.392 I print_info: LF token         = 187 'Ċ'
0.00.040.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.396 I print_info: max token length = 1024
0.00.040.397 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.388 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.400 I load_tensors: offloading output layer to GPU
0.00.607.401 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.430 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.607.432 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.608.782 I llama_init_from_model: n_seq_max     = 1
0.00.608.784 I llama_init_from_model: n_ctx         = 2048
0.00.608.785 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.608.785 I llama_init_from_model: n_batch       = 2048
0.00.608.786 I llama_init_from_model: n_ubatch      = 512
0.00.608.786 I llama_init_from_model: flash_attn    = 0
0.00.608.787 I llama_init_from_model: freq_base     = 10000.0
0.00.608.788 I llama_init_from_model: freq_scale    = 1
0.00.608.789 I ggml_metal_init: allocating
0.00.608.810 I ggml_metal_init: found device: Apple M4
0.00.608.819 I ggml_metal_init: picking default device: Apple M4
0.00.610.273 I ggml_metal_init: using embedded metal library
0.00.616.409 I ggml_metal_init: GPU name:   Apple M4
0.00.616.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.415 I ggml_metal_init: simdgroup reduction   = true
0.00.616.415 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.415 I ggml_metal_init: has residency sets    = true
0.00.616.416 I ggml_metal_init: has bfloat            = true
0.00.616.416 I ggml_metal_init: use bfloat            = true
0.00.616.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.908 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.529 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.687.535 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.687.602 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.545 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.691.547 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.691.547 I llama_init_from_model: graph nodes  = 967
0.00.691.548 I llama_init_from_model: graph splits = 2
0.00.691.552 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.691.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.691.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.862 I main: llama threadpool init, n_threads = 4
0.00.755.911 I 
0.00.755.932 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.932 I 
0.00.756.103 I sampler seed: 1234
0.00.756.107 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.118 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.119 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.119 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.601.542 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49408.49 tokens per second)
0.01.601.543 I llama_perf_context_print:        load time =     743.95 ms
0.01.601.543 I llama_perf_context_print: prompt eval time =      51.64 ms /     7 tokens (    7.38 ms per token,   135.55 tokens per second)
0.01.601.544 I llama_perf_context_print:        eval time =     791.29 ms /    63 runs   (   12.56 ms per token,    79.62 tokens per second)
0.01.601.544 I llama_perf_context_print:       total time =     846.37 ms /    70 tokens
0.01.601.822 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.108s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.008.923 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.995 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.000 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.002 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.004 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.005 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.005 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.911 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.942 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.839 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.840 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.841 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.842 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.842 I llama_model_loader: - type  f32:  194 tensors
0.00.024.843 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.843 I print_info: file format = GGUF V3 (latest)
0.00.024.844 I print_info: file type   = Q6_K
0.00.024.845 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.232 I load: special tokens cache size = 25
0.00.039.540 I load: token to piece cache size = 0.2984 MB
0.00.039.544 I print_info: arch             = gptneox
0.00.039.544 I print_info: vocab_only       = 0
0.00.039.544 I print_info: n_ctx_train      = 2048
0.00.039.544 I print_info: n_embd           = 2048
0.00.039.544 I print_info: n_layer          = 24
0.00.039.549 I print_info: n_head           = 16
0.00.039.550 I print_info: n_head_kv        = 16
0.00.039.550 I print_info: n_rot            = 32
0.00.039.550 I print_info: n_swa            = 0
0.00.039.550 I print_info: n_embd_head_k    = 128
0.00.039.550 I print_info: n_embd_head_v    = 128
0.00.039.551 I print_info: n_gqa            = 1
0.00.039.552 I print_info: n_embd_k_gqa     = 2048
0.00.039.552 I print_info: n_embd_v_gqa     = 2048
0.00.039.553 I print_info: f_norm_eps       = 1.0e-05
0.00.039.554 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.554 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.554 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.554 I print_info: f_logit_scale    = 0.0e+00
0.00.039.555 I print_info: n_ff             = 8192
0.00.039.555 I print_info: n_expert         = 0
0.00.039.555 I print_info: n_expert_used    = 0
0.00.039.555 I print_info: causal attn      = 1
0.00.039.555 I print_info: pooling type     = 0
0.00.039.555 I print_info: rope type        = 2
0.00.039.556 I print_info: rope scaling     = linear
0.00.039.556 I print_info: freq_base_train  = 10000.0
0.00.039.556 I print_info: freq_scale_train = 1
0.00.039.556 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.557 I print_info: rope_finetuned   = unknown
0.00.039.557 I print_info: ssm_d_conv       = 0
0.00.039.557 I print_info: ssm_d_inner      = 0
0.00.039.557 I print_info: ssm_d_state      = 0
0.00.039.557 I print_info: ssm_dt_rank      = 0
0.00.039.557 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.557 I print_info: model type       = 1.4B
0.00.039.558 I print_info: model params     = 1.41 B
0.00.039.558 I print_info: general.name     = 1.4B
0.00.039.558 I print_info: vocab type       = BPE
0.00.039.559 I print_info: n_vocab          = 50304
0.00.039.559 I print_info: n_merges         = 50009
0.00.039.559 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.559 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.559 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.559 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.560 I print_info: LF token         = 187 'Ċ'
0.00.039.560 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.560 I print_info: max token length = 1024
0.00.039.561 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.726 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.730 I load_tensors: offloading output layer to GPU
0.00.669.730 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.748 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.669.748 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.670.710 I llama_init_from_model: n_seq_max     = 1
0.00.670.714 I llama_init_from_model: n_ctx         = 2048
0.00.670.715 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.715 I llama_init_from_model: n_batch       = 2048
0.00.670.715 I llama_init_from_model: n_ubatch      = 512
0.00.670.715 I llama_init_from_model: flash_attn    = 0
0.00.670.717 I llama_init_from_model: freq_base     = 10000.0
0.00.670.717 I llama_init_from_model: freq_scale    = 1
0.00.670.718 I ggml_metal_init: allocating
0.00.670.759 I ggml_metal_init: found device: Apple M4
0.00.670.770 I ggml_metal_init: picking default device: Apple M4
0.00.671.935 I ggml_metal_init: using embedded metal library
0.00.676.171 I ggml_metal_init: GPU name:   Apple M4
0.00.676.175 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.676.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.676.176 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.676.177 I ggml_metal_init: simdgroup reduction   = true
0.00.676.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.676.177 I ggml_metal_init: has residency sets    = true
0.00.676.178 I ggml_metal_init: has bfloat            = true
0.00.676.178 I ggml_metal_init: use bfloat            = true
0.00.676.179 I ggml_metal_init: hasUnifiedMemory      = true
0.00.676.181 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.690.380 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.721.649 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.721.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.726.482 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.726.484 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.726.484 I llama_init_from_model: graph nodes  = 967
0.00.726.484 I llama_init_from_model: graph splits = 2
0.00.726.490 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.726.615 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.726.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.588 I main: llama threadpool init, n_threads = 4
0.00.789.630 I 
0.00.789.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.656 I 
0.00.789.829 I sampler seed: 1234
0.00.789.833 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.874 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.877 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.877 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.661.101 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52014.65 tokens per second)
0.01.661.102 I llama_perf_context_print:        load time =     779.97 ms
0.01.661.102 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.58 tokens per second)
0.01.661.103 I llama_perf_context_print:        eval time =     814.00 ms /    63 runs   (   12.92 ms per token,    77.40 tokens per second)
0.01.661.103 I llama_perf_context_print:       total time =     872.20 ms /    70 tokens
0.01.661.333 I ggml_metal_free: deallocating

real	0m1.681s
user	0m0.105s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.795 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.708 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.791 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.796 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.798 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.799 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.799 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.801 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.802 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.803 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.254 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.208 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.502 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.503 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.503 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.504 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.504 I llama_model_loader: - type  f32:  194 tensors
0.00.057.505 I llama_model_loader: - type  f16:   98 tensors
0.00.057.506 I print_info: file format = GGUF V3 (latest)
0.00.057.506 I print_info: file type   = all F32 (guessed)
0.00.057.508 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.972 I load: special tokens cache size = 25
0.00.078.311 I load: token to piece cache size = 0.2984 MB
0.00.078.314 I print_info: arch             = gptneox
0.00.078.314 I print_info: vocab_only       = 0
0.00.078.314 I print_info: n_ctx_train      = 2048
0.00.078.314 I print_info: n_embd           = 2048
0.00.078.315 I print_info: n_layer          = 24
0.00.078.317 I print_info: n_head           = 16
0.00.078.318 I print_info: n_head_kv        = 16
0.00.078.319 I print_info: n_rot            = 32
0.00.078.319 I print_info: n_swa            = 0
0.00.078.322 I print_info: n_embd_head_k    = 128
0.00.078.322 I print_info: n_embd_head_v    = 128
0.00.078.322 I print_info: n_gqa            = 1
0.00.078.323 I print_info: n_embd_k_gqa     = 2048
0.00.078.324 I print_info: n_embd_v_gqa     = 2048
0.00.078.325 I print_info: f_norm_eps       = 1.0e-05
0.00.078.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.325 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.325 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.325 I print_info: f_logit_scale    = 0.0e+00
0.00.078.326 I print_info: n_ff             = 8192
0.00.078.326 I print_info: n_expert         = 0
0.00.078.326 I print_info: n_expert_used    = 0
0.00.078.327 I print_info: causal attn      = 1
0.00.078.327 I print_info: pooling type     = 0
0.00.078.327 I print_info: rope type        = 2
0.00.078.327 I print_info: rope scaling     = linear
0.00.078.328 I print_info: freq_base_train  = 10000.0
0.00.078.328 I print_info: freq_scale_train = 1
0.00.078.328 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.328 I print_info: rope_finetuned   = unknown
0.00.078.328 I print_info: ssm_d_conv       = 0
0.00.078.329 I print_info: ssm_d_inner      = 0
0.00.078.329 I print_info: ssm_d_state      = 0
0.00.078.329 I print_info: ssm_dt_rank      = 0
0.00.078.329 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.329 I print_info: model type       = 1.4B
0.00.078.331 I print_info: model params     = 1.41 B
0.00.078.332 I print_info: general.name     = 1.4B
0.00.078.332 I print_info: vocab type       = BPE
0.00.078.332 I print_info: n_vocab          = 50304
0.00.078.332 I print_info: n_merges         = 50009
0.00.078.333 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.333 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.333 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.333 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.334 I print_info: LF token         = 187 'Ċ'
0.00.078.334 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.334 I print_info: max token length = 1024
0.00.078.335 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.147.843 I load_tensors: offloading 24 repeating layers to GPU
0.01.147.847 I load_tensors: offloading output layer to GPU
0.01.147.848 I load_tensors: offloaded 25/25 layers to GPU
0.01.147.873 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.147.875 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.149.063 I llama_init_from_model: n_seq_max     = 1
0.01.149.065 I llama_init_from_model: n_ctx         = 128
0.01.149.065 I llama_init_from_model: n_ctx_per_seq = 128
0.01.149.065 I llama_init_from_model: n_batch       = 128
0.01.149.066 I llama_init_from_model: n_ubatch      = 128
0.01.149.066 I llama_init_from_model: flash_attn    = 0
0.01.149.067 I llama_init_from_model: freq_base     = 10000.0
0.01.149.067 I llama_init_from_model: freq_scale    = 1
0.01.149.067 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.149.069 I ggml_metal_init: allocating
0.01.149.141 I ggml_metal_init: found device: Apple M4
0.01.149.149 I ggml_metal_init: picking default device: Apple M4
0.01.150.245 I ggml_metal_init: using embedded metal library
0.01.154.112 I ggml_metal_init: GPU name:   Apple M4
0.01.154.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.154.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.154.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.154.116 I ggml_metal_init: simdgroup reduction   = true
0.01.154.116 I ggml_metal_init: simdgroup matrix mul. = true
0.01.154.116 I ggml_metal_init: has residency sets    = true
0.01.154.116 I ggml_metal_init: has bfloat            = true
0.01.154.116 I ggml_metal_init: use bfloat            = true
0.01.154.117 I ggml_metal_init: hasUnifiedMemory      = true
0.01.154.118 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.164.698 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.166.439 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.166.442 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.166.455 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.168.087 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.168.088 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.168.089 I llama_init_from_model: graph nodes  = 967
0.01.168.089 I llama_init_from_model: graph splits = 2
0.01.168.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.168.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.203.325 I 
0.01.203.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.203.387 I perplexity: tokenizing the input ..
0.01.208.427 I perplexity: tokenization took 5.038 ms
0.01.208.449 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.326.969 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.328.329 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.328.344 I llama_perf_context_print:        load time =    1177.60 ms
0.01.328.345 I llama_perf_context_print: prompt eval time =     118.22 ms /   128 tokens (    0.92 ms per token,  1082.75 tokens per second)
0.01.328.345 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.328.346 I llama_perf_context_print:       total time =     125.02 ms /   129 tokens
0.01.328.733 I ggml_metal_free: deallocating

real	0m1.517s
user	0m0.098s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.308 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.339 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.339 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.339 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.340 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.341 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.341 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.341 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.343 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.346 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.347 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.348 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.953 I llama_model_loader: - type  f32:  194 tensors
0.00.024.954 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.954 I print_info: file format = GGUF V3 (latest)
0.00.024.955 I print_info: file type   = Q8_0
0.00.024.956 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.032.854 I load: special tokens cache size = 25
0.00.039.192 I load: token to piece cache size = 0.2984 MB
0.00.039.196 I print_info: arch             = gptneox
0.00.039.196 I print_info: vocab_only       = 0
0.00.039.196 I print_info: n_ctx_train      = 2048
0.00.039.197 I print_info: n_embd           = 2048
0.00.039.197 I print_info: n_layer          = 24
0.00.039.202 I print_info: n_head           = 16
0.00.039.203 I print_info: n_head_kv        = 16
0.00.039.203 I print_info: n_rot            = 32
0.00.039.203 I print_info: n_swa            = 0
0.00.039.204 I print_info: n_embd_head_k    = 128
0.00.039.204 I print_info: n_embd_head_v    = 128
0.00.039.204 I print_info: n_gqa            = 1
0.00.039.205 I print_info: n_embd_k_gqa     = 2048
0.00.039.206 I print_info: n_embd_v_gqa     = 2048
0.00.039.207 I print_info: f_norm_eps       = 1.0e-05
0.00.039.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.207 I print_info: f_logit_scale    = 0.0e+00
0.00.039.210 I print_info: n_ff             = 8192
0.00.039.210 I print_info: n_expert         = 0
0.00.039.211 I print_info: n_expert_used    = 0
0.00.039.211 I print_info: causal attn      = 1
0.00.039.211 I print_info: pooling type     = 0
0.00.039.211 I print_info: rope type        = 2
0.00.039.211 I print_info: rope scaling     = linear
0.00.039.212 I print_info: freq_base_train  = 10000.0
0.00.039.212 I print_info: freq_scale_train = 1
0.00.039.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.212 I print_info: rope_finetuned   = unknown
0.00.039.212 I print_info: ssm_d_conv       = 0
0.00.039.212 I print_info: ssm_d_inner      = 0
0.00.039.213 I print_info: ssm_d_state      = 0
0.00.039.213 I print_info: ssm_dt_rank      = 0
0.00.039.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.213 I print_info: model type       = 1.4B
0.00.039.214 I print_info: model params     = 1.41 B
0.00.039.214 I print_info: general.name     = 1.4B
0.00.039.214 I print_info: vocab type       = BPE
0.00.039.214 I print_info: n_vocab          = 50304
0.00.039.214 I print_info: n_merges         = 50009
0.00.039.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.215 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.216 I print_info: LF token         = 187 'Ċ'
0.00.039.216 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.217 I print_info: max token length = 1024
0.00.039.217 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.895.963 I load_tensors: offloading 24 repeating layers to GPU
0.00.895.970 I load_tensors: offloading output layer to GPU
0.00.895.971 I load_tensors: offloaded 25/25 layers to GPU
0.00.896.001 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.896.003 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.897.625 I llama_init_from_model: n_seq_max     = 1
0.00.897.626 I llama_init_from_model: n_ctx         = 128
0.00.897.627 I llama_init_from_model: n_ctx_per_seq = 128
0.00.897.628 I llama_init_from_model: n_batch       = 128
0.00.897.628 I llama_init_from_model: n_ubatch      = 128
0.00.897.628 I llama_init_from_model: flash_attn    = 0
0.00.897.629 I llama_init_from_model: freq_base     = 10000.0
0.00.897.630 I llama_init_from_model: freq_scale    = 1
0.00.897.630 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.897.632 I ggml_metal_init: allocating
0.00.897.728 I ggml_metal_init: found device: Apple M4
0.00.897.740 I ggml_metal_init: picking default device: Apple M4
0.00.899.216 I ggml_metal_init: using embedded metal library
0.00.904.608 I ggml_metal_init: GPU name:   Apple M4
0.00.904.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.904.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.904.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.904.614 I ggml_metal_init: simdgroup reduction   = true
0.00.904.614 I ggml_metal_init: simdgroup matrix mul. = true
0.00.904.614 I ggml_metal_init: has residency sets    = true
0.00.904.614 I ggml_metal_init: has bfloat            = true
0.00.904.615 I ggml_metal_init: use bfloat            = true
0.00.904.616 I ggml_metal_init: hasUnifiedMemory      = true
0.00.904.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.919.852 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.923.244 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.923.247 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.923.279 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.926.263 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.926.265 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.926.265 I llama_init_from_model: graph nodes  = 967
0.00.926.265 I llama_init_from_model: graph splits = 2
0.00.926.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.926.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.950.589 I 
0.00.950.647 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.950.663 I perplexity: tokenizing the input ..
0.00.956.997 I perplexity: tokenization took 6.332 ms
0.00.957.014 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.094.889 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.096.308 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.096.322 I llama_perf_context_print:        load time =     941.27 ms
0.01.096.323 I llama_perf_context_print: prompt eval time =     137.33 ms /   128 tokens (    1.07 ms per token,   932.08 tokens per second)
0.01.096.323 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.096.324 I llama_perf_context_print:       total time =     145.73 ms /   129 tokens
0.01.096.697 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.075s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.200 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.501 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.511 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.512 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.516 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.083 I llama_model_loader: - type  f32:  194 tensors
0.00.026.084 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.084 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.085 I print_info: file format = GGUF V3 (latest)
0.00.026.085 I print_info: file type   = Q4_0
0.00.026.091 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.357 I load: special tokens cache size = 25
0.00.040.661 I load: token to piece cache size = 0.2984 MB
0.00.040.665 I print_info: arch             = gptneox
0.00.040.666 I print_info: vocab_only       = 0
0.00.040.666 I print_info: n_ctx_train      = 2048
0.00.040.666 I print_info: n_embd           = 2048
0.00.040.666 I print_info: n_layer          = 24
0.00.040.670 I print_info: n_head           = 16
0.00.040.671 I print_info: n_head_kv        = 16
0.00.040.671 I print_info: n_rot            = 32
0.00.040.672 I print_info: n_swa            = 0
0.00.040.672 I print_info: n_embd_head_k    = 128
0.00.040.673 I print_info: n_embd_head_v    = 128
0.00.040.674 I print_info: n_gqa            = 1
0.00.040.675 I print_info: n_embd_k_gqa     = 2048
0.00.040.676 I print_info: n_embd_v_gqa     = 2048
0.00.040.676 I print_info: f_norm_eps       = 1.0e-05
0.00.040.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.676 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.677 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.677 I print_info: f_logit_scale    = 0.0e+00
0.00.040.677 I print_info: n_ff             = 8192
0.00.040.678 I print_info: n_expert         = 0
0.00.040.678 I print_info: n_expert_used    = 0
0.00.040.678 I print_info: causal attn      = 1
0.00.040.680 I print_info: pooling type     = 0
0.00.040.680 I print_info: rope type        = 2
0.00.040.680 I print_info: rope scaling     = linear
0.00.040.681 I print_info: freq_base_train  = 10000.0
0.00.040.681 I print_info: freq_scale_train = 1
0.00.040.682 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.682 I print_info: rope_finetuned   = unknown
0.00.040.682 I print_info: ssm_d_conv       = 0
0.00.040.682 I print_info: ssm_d_inner      = 0
0.00.040.683 I print_info: ssm_d_state      = 0
0.00.040.683 I print_info: ssm_dt_rank      = 0
0.00.040.683 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.704 I print_info: model type       = 1.4B
0.00.040.708 I print_info: model params     = 1.41 B
0.00.040.709 I print_info: general.name     = 1.4B
0.00.040.709 I print_info: vocab type       = BPE
0.00.040.710 I print_info: n_vocab          = 50304
0.00.040.710 I print_info: n_merges         = 50009
0.00.040.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.713 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.713 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.713 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.714 I print_info: LF token         = 187 'Ċ'
0.00.040.716 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.716 I print_info: max token length = 1024
0.00.040.717 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.634 I load_tensors: offloading output layer to GPU
0.00.614.635 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.667 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.614.668 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.615.749 I llama_init_from_model: n_seq_max     = 1
0.00.615.752 I llama_init_from_model: n_ctx         = 128
0.00.615.753 I llama_init_from_model: n_ctx_per_seq = 128
0.00.615.753 I llama_init_from_model: n_batch       = 128
0.00.615.753 I llama_init_from_model: n_ubatch      = 128
0.00.615.754 I llama_init_from_model: flash_attn    = 0
0.00.615.756 I llama_init_from_model: freq_base     = 10000.0
0.00.615.756 I llama_init_from_model: freq_scale    = 1
0.00.615.757 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.615.759 I ggml_metal_init: allocating
0.00.615.861 I ggml_metal_init: found device: Apple M4
0.00.615.875 I ggml_metal_init: picking default device: Apple M4
0.00.617.749 I ggml_metal_init: using embedded metal library
0.00.623.249 I ggml_metal_init: GPU name:   Apple M4
0.00.623.254 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.255 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.256 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.257 I ggml_metal_init: simdgroup reduction   = true
0.00.623.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.257 I ggml_metal_init: has residency sets    = true
0.00.623.258 I ggml_metal_init: has bfloat            = true
0.00.623.258 I ggml_metal_init: use bfloat            = true
0.00.623.259 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.537 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.105 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.646.109 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.649.698 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.649.700 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.649.701 I llama_init_from_model: graph nodes  = 967
0.00.649.701 I llama_init_from_model: graph splits = 2
0.00.649.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.649.704 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.033 I 
0.00.680.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.130 I perplexity: tokenizing the input ..
0.00.687.294 I perplexity: tokenization took 7.16 ms
0.00.687.316 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.958 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.825.299 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.825.311 I llama_perf_context_print:        load time =     669.82 ms
0.00.825.312 I llama_perf_context_print: prompt eval time =     135.71 ms /   128 tokens (    1.06 ms per token,   943.21 tokens per second)
0.00.825.313 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.313 I llama_perf_context_print:       total time =     145.28 ms /   129 tokens
0.00.825.664 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.815 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.112 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.127 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.129 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.130 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.130 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.132 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.132 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.132 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.847 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.848 I llama_model_loader: - type  f32:  194 tensors
0.00.024.848 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.849 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.849 I print_info: file format = GGUF V3 (latest)
0.00.024.854 I print_info: file type   = Q4_1
0.00.024.855 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.763 I load: special tokens cache size = 25
0.00.038.895 I load: token to piece cache size = 0.2984 MB
0.00.038.898 I print_info: arch             = gptneox
0.00.038.898 I print_info: vocab_only       = 0
0.00.038.899 I print_info: n_ctx_train      = 2048
0.00.038.899 I print_info: n_embd           = 2048
0.00.038.899 I print_info: n_layer          = 24
0.00.038.902 I print_info: n_head           = 16
0.00.038.903 I print_info: n_head_kv        = 16
0.00.038.903 I print_info: n_rot            = 32
0.00.038.904 I print_info: n_swa            = 0
0.00.038.904 I print_info: n_embd_head_k    = 128
0.00.038.904 I print_info: n_embd_head_v    = 128
0.00.038.905 I print_info: n_gqa            = 1
0.00.038.905 I print_info: n_embd_k_gqa     = 2048
0.00.038.908 I print_info: n_embd_v_gqa     = 2048
0.00.038.909 I print_info: f_norm_eps       = 1.0e-05
0.00.038.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.910 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.910 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.910 I print_info: f_logit_scale    = 0.0e+00
0.00.038.911 I print_info: n_ff             = 8192
0.00.038.911 I print_info: n_expert         = 0
0.00.038.911 I print_info: n_expert_used    = 0
0.00.038.911 I print_info: causal attn      = 1
0.00.038.911 I print_info: pooling type     = 0
0.00.038.911 I print_info: rope type        = 2
0.00.038.912 I print_info: rope scaling     = linear
0.00.038.913 I print_info: freq_base_train  = 10000.0
0.00.038.914 I print_info: freq_scale_train = 1
0.00.038.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.914 I print_info: rope_finetuned   = unknown
0.00.038.914 I print_info: ssm_d_conv       = 0
0.00.038.914 I print_info: ssm_d_inner      = 0
0.00.038.914 I print_info: ssm_d_state      = 0
0.00.038.915 I print_info: ssm_dt_rank      = 0
0.00.038.915 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.915 I print_info: model type       = 1.4B
0.00.038.915 I print_info: model params     = 1.41 B
0.00.038.915 I print_info: general.name     = 1.4B
0.00.038.916 I print_info: vocab type       = BPE
0.00.038.916 I print_info: n_vocab          = 50304
0.00.038.916 I print_info: n_merges         = 50009
0.00.038.917 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.917 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.917 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.917 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.917 I print_info: LF token         = 187 'Ċ'
0.00.038.918 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.918 I print_info: max token length = 1024
0.00.038.918 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.656 I load_tensors: offloading output layer to GPU
0.00.664.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.689 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.664.690 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.666.502 I llama_init_from_model: n_seq_max     = 1
0.00.666.505 I llama_init_from_model: n_ctx         = 128
0.00.666.505 I llama_init_from_model: n_ctx_per_seq = 128
0.00.666.506 I llama_init_from_model: n_batch       = 128
0.00.666.506 I llama_init_from_model: n_ubatch      = 128
0.00.666.507 I llama_init_from_model: flash_attn    = 0
0.00.666.509 I llama_init_from_model: freq_base     = 10000.0
0.00.666.510 I llama_init_from_model: freq_scale    = 1
0.00.666.510 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.666.512 I ggml_metal_init: allocating
0.00.666.591 I ggml_metal_init: found device: Apple M4
0.00.666.604 I ggml_metal_init: picking default device: Apple M4
0.00.668.465 I ggml_metal_init: using embedded metal library
0.00.675.151 I ggml_metal_init: GPU name:   Apple M4
0.00.675.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.157 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.157 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.158 I ggml_metal_init: simdgroup reduction   = true
0.00.675.158 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.159 I ggml_metal_init: has residency sets    = true
0.00.675.159 I ggml_metal_init: has bfloat            = true
0.00.675.159 I ggml_metal_init: use bfloat            = true
0.00.675.160 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.161 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.693.274 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.739 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.696.743 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.696.768 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.053 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.700.054 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.700.055 I llama_init_from_model: graph nodes  = 967
0.00.700.055 I llama_init_from_model: graph splits = 2
0.00.700.059 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.700.059 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.924 I 
0.00.727.009 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.031 I perplexity: tokenizing the input ..
0.00.734.372 I perplexity: tokenization took 7.338 ms
0.00.734.397 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.209 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.869.560 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.869.577 I llama_perf_context_print:        load time =     718.10 ms
0.00.869.578 I llama_perf_context_print: prompt eval time =     133.04 ms /   128 tokens (    1.04 ms per token,   962.15 tokens per second)
0.00.869.578 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.869.579 I llama_perf_context_print:       total time =     142.66 ms /   129 tokens
0.00.869.968 I ggml_metal_free: deallocating

real	0m0.884s
user	0m0.080s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.377 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.383 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.385 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.386 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.386 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.386 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.387 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.388 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.388 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.388 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.389 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.144 I llama_model_loader: - type  f32:  194 tensors
0.00.025.144 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.145 I print_info: file format = GGUF V3 (latest)
0.00.025.146 I print_info: file type   = Q5_0
0.00.025.147 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.661 I load: special tokens cache size = 25
0.00.039.985 I load: token to piece cache size = 0.2984 MB
0.00.039.990 I print_info: arch             = gptneox
0.00.039.990 I print_info: vocab_only       = 0
0.00.039.990 I print_info: n_ctx_train      = 2048
0.00.039.991 I print_info: n_embd           = 2048
0.00.039.991 I print_info: n_layer          = 24
0.00.039.995 I print_info: n_head           = 16
0.00.039.996 I print_info: n_head_kv        = 16
0.00.039.996 I print_info: n_rot            = 32
0.00.039.996 I print_info: n_swa            = 0
0.00.039.996 I print_info: n_embd_head_k    = 128
0.00.039.996 I print_info: n_embd_head_v    = 128
0.00.039.997 I print_info: n_gqa            = 1
0.00.039.998 I print_info: n_embd_k_gqa     = 2048
0.00.039.999 I print_info: n_embd_v_gqa     = 2048
0.00.039.999 I print_info: f_norm_eps       = 1.0e-05
0.00.039.999 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.000 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.000 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.003 I print_info: f_logit_scale    = 0.0e+00
0.00.040.004 I print_info: n_ff             = 8192
0.00.040.004 I print_info: n_expert         = 0
0.00.040.004 I print_info: n_expert_used    = 0
0.00.040.006 I print_info: causal attn      = 1
0.00.040.006 I print_info: pooling type     = 0
0.00.040.006 I print_info: rope type        = 2
0.00.040.006 I print_info: rope scaling     = linear
0.00.040.007 I print_info: freq_base_train  = 10000.0
0.00.040.007 I print_info: freq_scale_train = 1
0.00.040.007 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.007 I print_info: rope_finetuned   = unknown
0.00.040.008 I print_info: ssm_d_conv       = 0
0.00.040.008 I print_info: ssm_d_inner      = 0
0.00.040.008 I print_info: ssm_d_state      = 0
0.00.040.008 I print_info: ssm_dt_rank      = 0
0.00.040.008 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.008 I print_info: model type       = 1.4B
0.00.040.009 I print_info: model params     = 1.41 B
0.00.040.009 I print_info: general.name     = 1.4B
0.00.040.010 I print_info: vocab type       = BPE
0.00.040.011 I print_info: n_vocab          = 50304
0.00.040.011 I print_info: n_merges         = 50009
0.00.040.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.012 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.012 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.012 I print_info: LF token         = 187 'Ċ'
0.00.040.012 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.013 I print_info: max token length = 1024
0.00.040.013 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.694.124 I load_tensors: offloading 24 repeating layers to GPU
0.00.694.137 I load_tensors: offloading output layer to GPU
0.00.694.139 I load_tensors: offloaded 25/25 layers to GPU
0.00.694.165 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.694.167 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.695.656 I llama_init_from_model: n_seq_max     = 1
0.00.695.661 I llama_init_from_model: n_ctx         = 128
0.00.695.662 I llama_init_from_model: n_ctx_per_seq = 128
0.00.695.662 I llama_init_from_model: n_batch       = 128
0.00.695.663 I llama_init_from_model: n_ubatch      = 128
0.00.695.663 I llama_init_from_model: flash_attn    = 0
0.00.695.664 I llama_init_from_model: freq_base     = 10000.0
0.00.695.665 I llama_init_from_model: freq_scale    = 1
0.00.695.665 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.695.671 I ggml_metal_init: allocating
0.00.695.728 I ggml_metal_init: found device: Apple M4
0.00.695.740 I ggml_metal_init: picking default device: Apple M4
0.00.697.441 I ggml_metal_init: using embedded metal library
0.00.704.242 I ggml_metal_init: GPU name:   Apple M4
0.00.704.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.704.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.704.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.704.250 I ggml_metal_init: simdgroup reduction   = true
0.00.704.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.704.251 I ggml_metal_init: has residency sets    = true
0.00.704.251 I ggml_metal_init: has bfloat            = true
0.00.704.251 I ggml_metal_init: use bfloat            = true
0.00.704.252 I ggml_metal_init: hasUnifiedMemory      = true
0.00.704.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.722.736 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.726.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.726.299 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.726.336 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.767 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.729.769 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.729.769 I llama_init_from_model: graph nodes  = 967
0.00.729.770 I llama_init_from_model: graph splits = 2
0.00.729.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.729.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.614 I 
0.00.761.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.707 I perplexity: tokenizing the input ..
0.00.768.983 I perplexity: tokenization took 7.274 ms
0.00.769.002 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.912.040 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.913.374 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.913.387 I llama_perf_context_print:        load time =     752.61 ms
0.00.913.388 I llama_perf_context_print: prompt eval time =     142.09 ms /   128 tokens (    1.11 ms per token,   900.87 tokens per second)
0.00.913.389 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.913.391 I llama_perf_context_print:       total time =     151.78 ms /   129 tokens
0.00.913.855 I ggml_metal_free: deallocating

real	0m0.928s
user	0m0.081s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.938 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.052 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.061 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.062 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.486 I llama_model_loader: - type  f32:  194 tensors
0.00.025.486 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.487 I print_info: file format = GGUF V3 (latest)
0.00.025.487 I print_info: file type   = Q5_1
0.00.025.489 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.561 I load: special tokens cache size = 25
0.00.039.520 I load: token to piece cache size = 0.2984 MB
0.00.039.523 I print_info: arch             = gptneox
0.00.039.523 I print_info: vocab_only       = 0
0.00.039.523 I print_info: n_ctx_train      = 2048
0.00.039.523 I print_info: n_embd           = 2048
0.00.039.524 I print_info: n_layer          = 24
0.00.039.527 I print_info: n_head           = 16
0.00.039.530 I print_info: n_head_kv        = 16
0.00.039.531 I print_info: n_rot            = 32
0.00.039.531 I print_info: n_swa            = 0
0.00.039.531 I print_info: n_embd_head_k    = 128
0.00.039.531 I print_info: n_embd_head_v    = 128
0.00.039.532 I print_info: n_gqa            = 1
0.00.039.533 I print_info: n_embd_k_gqa     = 2048
0.00.039.533 I print_info: n_embd_v_gqa     = 2048
0.00.039.534 I print_info: f_norm_eps       = 1.0e-05
0.00.039.534 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.535 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.535 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.535 I print_info: f_logit_scale    = 0.0e+00
0.00.039.536 I print_info: n_ff             = 8192
0.00.039.536 I print_info: n_expert         = 0
0.00.039.536 I print_info: n_expert_used    = 0
0.00.039.537 I print_info: causal attn      = 1
0.00.039.537 I print_info: pooling type     = 0
0.00.039.537 I print_info: rope type        = 2
0.00.039.537 I print_info: rope scaling     = linear
0.00.039.538 I print_info: freq_base_train  = 10000.0
0.00.039.538 I print_info: freq_scale_train = 1
0.00.039.538 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.538 I print_info: rope_finetuned   = unknown
0.00.039.542 I print_info: ssm_d_conv       = 0
0.00.039.542 I print_info: ssm_d_inner      = 0
0.00.039.543 I print_info: ssm_d_state      = 0
0.00.039.543 I print_info: ssm_dt_rank      = 0
0.00.039.543 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.543 I print_info: model type       = 1.4B
0.00.039.545 I print_info: model params     = 1.41 B
0.00.039.545 I print_info: general.name     = 1.4B
0.00.039.545 I print_info: vocab type       = BPE
0.00.039.545 I print_info: n_vocab          = 50304
0.00.039.546 I print_info: n_merges         = 50009
0.00.039.546 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.546 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.547 I print_info: LF token         = 187 'Ċ'
0.00.039.547 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.547 I print_info: max token length = 1024
0.00.039.548 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.749 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.766 I load_tensors: offloading output layer to GPU
0.00.599.767 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.798 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.599.801 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.601.344 I llama_init_from_model: n_seq_max     = 1
0.00.601.346 I llama_init_from_model: n_ctx         = 128
0.00.601.347 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.347 I llama_init_from_model: n_batch       = 128
0.00.601.348 I llama_init_from_model: n_ubatch      = 128
0.00.601.348 I llama_init_from_model: flash_attn    = 0
0.00.601.351 I llama_init_from_model: freq_base     = 10000.0
0.00.601.351 I llama_init_from_model: freq_scale    = 1
0.00.601.352 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.354 I ggml_metal_init: allocating
0.00.601.427 I ggml_metal_init: found device: Apple M4
0.00.601.441 I ggml_metal_init: picking default device: Apple M4
0.00.603.248 I ggml_metal_init: using embedded metal library
0.00.609.818 I ggml_metal_init: GPU name:   Apple M4
0.00.609.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.824 I ggml_metal_init: simdgroup reduction   = true
0.00.609.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.824 I ggml_metal_init: has residency sets    = true
0.00.609.824 I ggml_metal_init: has bfloat            = true
0.00.609.825 I ggml_metal_init: use bfloat            = true
0.00.609.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.726 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.272 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.276 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.301 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.438 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.439 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.440 I llama_init_from_model: graph nodes  = 967
0.00.633.440 I llama_init_from_model: graph splits = 2
0.00.633.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.836 I 
0.00.663.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.932 I perplexity: tokenizing the input ..
0.00.671.191 I perplexity: tokenization took 7.255 ms
0.00.671.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.302 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.650 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.670 I llama_perf_context_print:        load time =     653.89 ms
0.00.821.671 I llama_perf_context_print: prompt eval time =     148.19 ms /   128 tokens (    1.16 ms per token,   863.76 tokens per second)
0.00.821.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.672 I llama_perf_context_print:       total time =     157.84 ms /   129 tokens
0.00.822.042 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.079s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.005 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.519 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.555 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.329 I llama_model_loader: - type  f32:  194 tensors
0.00.024.330 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.330 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.330 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.331 I print_info: file format = GGUF V3 (latest)
0.00.024.331 I print_info: file type   = Q2_K - Medium
0.00.024.332 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.079 I load: special tokens cache size = 25
0.00.038.057 I load: token to piece cache size = 0.2984 MB
0.00.038.060 I print_info: arch             = gptneox
0.00.038.061 I print_info: vocab_only       = 0
0.00.038.061 I print_info: n_ctx_train      = 2048
0.00.038.061 I print_info: n_embd           = 2048
0.00.038.061 I print_info: n_layer          = 24
0.00.038.065 I print_info: n_head           = 16
0.00.038.066 I print_info: n_head_kv        = 16
0.00.038.066 I print_info: n_rot            = 32
0.00.038.066 I print_info: n_swa            = 0
0.00.038.066 I print_info: n_embd_head_k    = 128
0.00.038.066 I print_info: n_embd_head_v    = 128
0.00.038.067 I print_info: n_gqa            = 1
0.00.038.068 I print_info: n_embd_k_gqa     = 2048
0.00.038.071 I print_info: n_embd_v_gqa     = 2048
0.00.038.072 I print_info: f_norm_eps       = 1.0e-05
0.00.038.072 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.072 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.073 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.073 I print_info: f_logit_scale    = 0.0e+00
0.00.038.073 I print_info: n_ff             = 8192
0.00.038.074 I print_info: n_expert         = 0
0.00.038.074 I print_info: n_expert_used    = 0
0.00.038.074 I print_info: causal attn      = 1
0.00.038.074 I print_info: pooling type     = 0
0.00.038.074 I print_info: rope type        = 2
0.00.038.075 I print_info: rope scaling     = linear
0.00.038.075 I print_info: freq_base_train  = 10000.0
0.00.038.075 I print_info: freq_scale_train = 1
0.00.038.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.076 I print_info: rope_finetuned   = unknown
0.00.038.076 I print_info: ssm_d_conv       = 0
0.00.038.076 I print_info: ssm_d_inner      = 0
0.00.038.077 I print_info: ssm_d_state      = 0
0.00.038.078 I print_info: ssm_dt_rank      = 0
0.00.038.078 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.078 I print_info: model type       = 1.4B
0.00.038.078 I print_info: model params     = 1.41 B
0.00.038.078 I print_info: general.name     = 1.4B
0.00.038.079 I print_info: vocab type       = BPE
0.00.038.079 I print_info: n_vocab          = 50304
0.00.038.079 I print_info: n_merges         = 50009
0.00.038.080 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.080 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.080 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.080 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.083 I print_info: LF token         = 187 'Ċ'
0.00.038.083 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.083 I print_info: max token length = 1024
0.00.038.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.342.305 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.318 I load_tensors: offloading output layer to GPU
0.00.342.318 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.348 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.350 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.343.946 I llama_init_from_model: n_seq_max     = 1
0.00.343.949 I llama_init_from_model: n_ctx         = 128
0.00.343.949 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.950 I llama_init_from_model: n_batch       = 128
0.00.343.950 I llama_init_from_model: n_ubatch      = 128
0.00.343.950 I llama_init_from_model: flash_attn    = 0
0.00.343.953 I llama_init_from_model: freq_base     = 10000.0
0.00.343.953 I llama_init_from_model: freq_scale    = 1
0.00.343.954 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.956 I ggml_metal_init: allocating
0.00.344.017 I ggml_metal_init: found device: Apple M4
0.00.344.031 I ggml_metal_init: picking default device: Apple M4
0.00.345.817 I ggml_metal_init: using embedded metal library
0.00.351.333 I ggml_metal_init: GPU name:   Apple M4
0.00.351.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.351 I ggml_metal_init: simdgroup reduction   = true
0.00.351.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.352 I ggml_metal_init: has residency sets    = true
0.00.351.352 I ggml_metal_init: has bfloat            = true
0.00.351.352 I ggml_metal_init: use bfloat            = true
0.00.351.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.011 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.375.626 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.375.636 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.375.699 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.379.076 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.379.078 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.379.079 I llama_init_from_model: graph nodes  = 967
0.00.379.079 I llama_init_from_model: graph splits = 2
0.00.379.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.379.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.662 I 
0.00.410.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.410.768 I perplexity: tokenizing the input ..
0.00.416.510 I perplexity: tokenization took 5.741 ms
0.00.416.522 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.236 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.552.587 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.552.604 I llama_perf_context_print:        load time =     401.65 ms
0.00.552.604 I llama_perf_context_print: prompt eval time =     134.48 ms /   128 tokens (    1.05 ms per token,   951.81 tokens per second)
0.00.552.605 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.552.605 I llama_perf_context_print:       total time =     141.94 ms /   129 tokens
0.00.552.986 I ggml_metal_free: deallocating

real	0m0.567s
user	0m0.078s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.777 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.828 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.840 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.841 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.841 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.845 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.441 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.442 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.442 I llama_model_loader: - type  f32:  194 tensors
0.00.024.443 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.443 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.443 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.443 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.444 I print_info: file format = GGUF V3 (latest)
0.00.024.444 I print_info: file type   = Q3_K - Medium
0.00.024.445 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.360 I load: special tokens cache size = 25
0.00.038.345 I load: token to piece cache size = 0.2984 MB
0.00.038.348 I print_info: arch             = gptneox
0.00.038.348 I print_info: vocab_only       = 0
0.00.038.348 I print_info: n_ctx_train      = 2048
0.00.038.349 I print_info: n_embd           = 2048
0.00.038.349 I print_info: n_layer          = 24
0.00.038.353 I print_info: n_head           = 16
0.00.038.354 I print_info: n_head_kv        = 16
0.00.038.354 I print_info: n_rot            = 32
0.00.038.354 I print_info: n_swa            = 0
0.00.038.354 I print_info: n_embd_head_k    = 128
0.00.038.355 I print_info: n_embd_head_v    = 128
0.00.038.355 I print_info: n_gqa            = 1
0.00.038.356 I print_info: n_embd_k_gqa     = 2048
0.00.038.357 I print_info: n_embd_v_gqa     = 2048
0.00.038.357 I print_info: f_norm_eps       = 1.0e-05
0.00.038.357 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.358 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.358 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.358 I print_info: f_logit_scale    = 0.0e+00
0.00.038.359 I print_info: n_ff             = 8192
0.00.038.359 I print_info: n_expert         = 0
0.00.038.359 I print_info: n_expert_used    = 0
0.00.038.359 I print_info: causal attn      = 1
0.00.038.360 I print_info: pooling type     = 0
0.00.038.360 I print_info: rope type        = 2
0.00.038.360 I print_info: rope scaling     = linear
0.00.038.360 I print_info: freq_base_train  = 10000.0
0.00.038.361 I print_info: freq_scale_train = 1
0.00.038.361 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.361 I print_info: rope_finetuned   = unknown
0.00.038.361 I print_info: ssm_d_conv       = 0
0.00.038.361 I print_info: ssm_d_inner      = 0
0.00.038.363 I print_info: ssm_d_state      = 0
0.00.038.363 I print_info: ssm_dt_rank      = 0
0.00.038.363 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.363 I print_info: model type       = 1.4B
0.00.038.364 I print_info: model params     = 1.41 B
0.00.038.364 I print_info: general.name     = 1.4B
0.00.038.364 I print_info: vocab type       = BPE
0.00.038.365 I print_info: n_vocab          = 50304
0.00.038.365 I print_info: n_merges         = 50009
0.00.038.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: LF token         = 187 'Ċ'
0.00.038.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: max token length = 1024
0.00.038.367 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.007 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.014 I load_tensors: offloading output layer to GPU
0.00.457.015 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.041 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.457.042 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.458.597 I llama_init_from_model: n_seq_max     = 1
0.00.458.599 I llama_init_from_model: n_ctx         = 128
0.00.458.600 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.600 I llama_init_from_model: n_batch       = 128
0.00.458.601 I llama_init_from_model: n_ubatch      = 128
0.00.458.601 I llama_init_from_model: flash_attn    = 0
0.00.458.602 I llama_init_from_model: freq_base     = 10000.0
0.00.458.603 I llama_init_from_model: freq_scale    = 1
0.00.458.603 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.605 I ggml_metal_init: allocating
0.00.458.657 I ggml_metal_init: found device: Apple M4
0.00.458.670 I ggml_metal_init: picking default device: Apple M4
0.00.460.727 I ggml_metal_init: using embedded metal library
0.00.466.551 I ggml_metal_init: GPU name:   Apple M4
0.00.466.559 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.561 I ggml_metal_init: simdgroup reduction   = true
0.00.466.562 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.562 I ggml_metal_init: has residency sets    = true
0.00.466.563 I ggml_metal_init: has bfloat            = true
0.00.466.564 I ggml_metal_init: use bfloat            = true
0.00.466.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.571 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.486.585 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.490.250 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.490.256 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.490.290 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.493.472 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.493.474 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.493.474 I llama_init_from_model: graph nodes  = 967
0.00.493.475 I llama_init_from_model: graph splits = 2
0.00.493.478 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.493.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.374 I 
0.00.524.454 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.472 I perplexity: tokenizing the input ..
0.00.529.842 I perplexity: tokenization took 5.368 ms
0.00.529.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.000 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.664.325 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.664.341 I llama_perf_context_print:        load time =     515.59 ms
0.00.664.342 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   963.01 tokens per second)
0.00.664.343 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.664.343 I llama_perf_context_print:       total time =     139.97 ms /   129 tokens
0.00.664.700 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.077s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.842 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.849 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.852 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.853 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.854 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.855 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.855 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.856 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.858 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.858 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.639 I llama_model_loader: - type  f32:  194 tensors
0.00.025.639 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.640 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.640 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.641 I print_info: file format = GGUF V3 (latest)
0.00.025.641 I print_info: file type   = Q4_K - Medium
0.00.025.642 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.762 I load: special tokens cache size = 25
0.00.039.766 I load: token to piece cache size = 0.2984 MB
0.00.039.769 I print_info: arch             = gptneox
0.00.039.769 I print_info: vocab_only       = 0
0.00.039.769 I print_info: n_ctx_train      = 2048
0.00.039.769 I print_info: n_embd           = 2048
0.00.039.769 I print_info: n_layer          = 24
0.00.039.773 I print_info: n_head           = 16
0.00.039.774 I print_info: n_head_kv        = 16
0.00.039.774 I print_info: n_rot            = 32
0.00.039.774 I print_info: n_swa            = 0
0.00.039.774 I print_info: n_embd_head_k    = 128
0.00.039.775 I print_info: n_embd_head_v    = 128
0.00.039.775 I print_info: n_gqa            = 1
0.00.039.776 I print_info: n_embd_k_gqa     = 2048
0.00.039.777 I print_info: n_embd_v_gqa     = 2048
0.00.039.777 I print_info: f_norm_eps       = 1.0e-05
0.00.039.778 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.778 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.778 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.778 I print_info: f_logit_scale    = 0.0e+00
0.00.039.779 I print_info: n_ff             = 8192
0.00.039.779 I print_info: n_expert         = 0
0.00.039.779 I print_info: n_expert_used    = 0
0.00.039.779 I print_info: causal attn      = 1
0.00.039.781 I print_info: pooling type     = 0
0.00.039.781 I print_info: rope type        = 2
0.00.039.781 I print_info: rope scaling     = linear
0.00.039.782 I print_info: freq_base_train  = 10000.0
0.00.039.782 I print_info: freq_scale_train = 1
0.00.039.782 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.782 I print_info: rope_finetuned   = unknown
0.00.039.783 I print_info: ssm_d_conv       = 0
0.00.039.783 I print_info: ssm_d_inner      = 0
0.00.039.783 I print_info: ssm_d_state      = 0
0.00.039.784 I print_info: ssm_dt_rank      = 0
0.00.039.786 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.786 I print_info: model type       = 1.4B
0.00.039.786 I print_info: model params     = 1.41 B
0.00.039.786 I print_info: general.name     = 1.4B
0.00.039.787 I print_info: vocab type       = BPE
0.00.039.787 I print_info: n_vocab          = 50304
0.00.039.787 I print_info: n_merges         = 50009
0.00.039.787 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: LF token         = 187 'Ċ'
0.00.039.788 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.789 I print_info: max token length = 1024
0.00.039.792 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.525.645 I load_tensors: offloading 24 repeating layers to GPU
0.00.525.658 I load_tensors: offloading output layer to GPU
0.00.525.659 I load_tensors: offloaded 25/25 layers to GPU
0.00.525.688 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.525.689 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.527.289 I llama_init_from_model: n_seq_max     = 1
0.00.527.295 I llama_init_from_model: n_ctx         = 128
0.00.527.296 I llama_init_from_model: n_ctx_per_seq = 128
0.00.527.296 I llama_init_from_model: n_batch       = 128
0.00.527.297 I llama_init_from_model: n_ubatch      = 128
0.00.527.297 I llama_init_from_model: flash_attn    = 0
0.00.527.299 I llama_init_from_model: freq_base     = 10000.0
0.00.527.300 I llama_init_from_model: freq_scale    = 1
0.00.527.300 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.527.303 I ggml_metal_init: allocating
0.00.527.395 I ggml_metal_init: found device: Apple M4
0.00.527.409 I ggml_metal_init: picking default device: Apple M4
0.00.529.227 I ggml_metal_init: using embedded metal library
0.00.535.852 I ggml_metal_init: GPU name:   Apple M4
0.00.535.856 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.857 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.858 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.859 I ggml_metal_init: simdgroup reduction   = true
0.00.535.859 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.859 I ggml_metal_init: has residency sets    = true
0.00.535.859 I ggml_metal_init: has bfloat            = true
0.00.535.860 I ggml_metal_init: use bfloat            = true
0.00.535.860 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.114 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.606 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.556.610 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.556.640 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.559.787 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.559.789 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.559.789 I llama_init_from_model: graph nodes  = 967
0.00.559.790 I llama_init_from_model: graph splits = 2
0.00.559.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.559.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.804 I 
0.00.590.884 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.904 I perplexity: tokenizing the input ..
0.00.598.109 I perplexity: tokenization took 7.201 ms
0.00.598.135 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.191 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.744.514 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.744.528 I llama_perf_context_print:        load time =     580.95 ms
0.00.744.529 I llama_perf_context_print: prompt eval time =     144.11 ms /   128 tokens (    1.13 ms per token,   888.19 tokens per second)
0.00.744.530 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.530 I llama_perf_context_print:       total time =     153.73 ms /   129 tokens
0.00.744.884 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.893 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.682 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.684 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.222 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.222 I llama_model_loader: - type  f32:  194 tensors
0.00.024.222 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.223 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.223 I print_info: file format = GGUF V3 (latest)
0.00.024.224 I print_info: file type   = Q5_K - Medium
0.00.024.225 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.303 I load: special tokens cache size = 25
0.00.038.364 I load: token to piece cache size = 0.2984 MB
0.00.038.367 I print_info: arch             = gptneox
0.00.038.367 I print_info: vocab_only       = 0
0.00.038.367 I print_info: n_ctx_train      = 2048
0.00.038.368 I print_info: n_embd           = 2048
0.00.038.368 I print_info: n_layer          = 24
0.00.038.371 I print_info: n_head           = 16
0.00.038.372 I print_info: n_head_kv        = 16
0.00.038.372 I print_info: n_rot            = 32
0.00.038.372 I print_info: n_swa            = 0
0.00.038.373 I print_info: n_embd_head_k    = 128
0.00.038.373 I print_info: n_embd_head_v    = 128
0.00.038.373 I print_info: n_gqa            = 1
0.00.038.374 I print_info: n_embd_k_gqa     = 2048
0.00.038.375 I print_info: n_embd_v_gqa     = 2048
0.00.038.376 I print_info: f_norm_eps       = 1.0e-05
0.00.038.376 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.376 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.376 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.377 I print_info: f_logit_scale    = 0.0e+00
0.00.038.377 I print_info: n_ff             = 8192
0.00.038.378 I print_info: n_expert         = 0
0.00.038.378 I print_info: n_expert_used    = 0
0.00.038.378 I print_info: causal attn      = 1
0.00.038.378 I print_info: pooling type     = 0
0.00.038.378 I print_info: rope type        = 2
0.00.038.379 I print_info: rope scaling     = linear
0.00.038.379 I print_info: freq_base_train  = 10000.0
0.00.038.379 I print_info: freq_scale_train = 1
0.00.038.382 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.382 I print_info: rope_finetuned   = unknown
0.00.038.382 I print_info: ssm_d_conv       = 0
0.00.038.383 I print_info: ssm_d_inner      = 0
0.00.038.383 I print_info: ssm_d_state      = 0
0.00.038.383 I print_info: ssm_dt_rank      = 0
0.00.038.383 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.383 I print_info: model type       = 1.4B
0.00.038.384 I print_info: model params     = 1.41 B
0.00.038.384 I print_info: general.name     = 1.4B
0.00.038.384 I print_info: vocab type       = BPE
0.00.038.384 I print_info: n_vocab          = 50304
0.00.038.385 I print_info: n_merges         = 50009
0.00.038.389 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.389 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.389 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.390 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.390 I print_info: LF token         = 187 'Ċ'
0.00.038.390 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.390 I print_info: max token length = 1024
0.00.038.390 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.688 I load_tensors: offloading output layer to GPU
0.00.600.689 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.720 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.721 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.602.426 I llama_init_from_model: n_seq_max     = 1
0.00.602.429 I llama_init_from_model: n_ctx         = 128
0.00.602.429 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.430 I llama_init_from_model: n_batch       = 128
0.00.602.430 I llama_init_from_model: n_ubatch      = 128
0.00.602.431 I llama_init_from_model: flash_attn    = 0
0.00.602.432 I llama_init_from_model: freq_base     = 10000.0
0.00.602.433 I llama_init_from_model: freq_scale    = 1
0.00.602.433 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.436 I ggml_metal_init: allocating
0.00.602.454 I ggml_metal_init: found device: Apple M4
0.00.602.468 I ggml_metal_init: picking default device: Apple M4
0.00.603.864 I ggml_metal_init: using embedded metal library
0.00.610.219 I ggml_metal_init: GPU name:   Apple M4
0.00.610.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.224 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.225 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.225 I ggml_metal_init: simdgroup reduction   = true
0.00.610.225 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.226 I ggml_metal_init: has residency sets    = true
0.00.610.226 I ggml_metal_init: has bfloat            = true
0.00.610.226 I ggml_metal_init: use bfloat            = true
0.00.610.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.602 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.226 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.230 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.277 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.410 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.412 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.412 I llama_init_from_model: graph nodes  = 967
0.00.634.413 I llama_init_from_model: graph splits = 2
0.00.634.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.397 I 
0.00.669.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.507 I perplexity: tokenizing the input ..
0.00.675.508 I perplexity: tokenization took 5.997 ms
0.00.675.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.219 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.817.633 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.817.653 I llama_perf_context_print:        load time =     660.50 ms
0.00.817.654 I llama_perf_context_print: prompt eval time =     139.78 ms /   128 tokens (    1.09 ms per token,   915.72 tokens per second)
0.00.817.655 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.655 I llama_perf_context_print:       total time =     148.26 ms /   129 tokens
0.00.817.979 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.077s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.093 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.747 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.748 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.506 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.508 I llama_model_loader: - type  f32:  194 tensors
0.00.024.509 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.509 I print_info: file format = GGUF V3 (latest)
0.00.024.510 I print_info: file type   = Q6_K
0.00.024.515 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.519 I load: special tokens cache size = 25
0.00.038.689 I load: token to piece cache size = 0.2984 MB
0.00.038.691 I print_info: arch             = gptneox
0.00.038.692 I print_info: vocab_only       = 0
0.00.038.692 I print_info: n_ctx_train      = 2048
0.00.038.692 I print_info: n_embd           = 2048
0.00.038.692 I print_info: n_layer          = 24
0.00.038.695 I print_info: n_head           = 16
0.00.038.696 I print_info: n_head_kv        = 16
0.00.038.697 I print_info: n_rot            = 32
0.00.038.697 I print_info: n_swa            = 0
0.00.038.697 I print_info: n_embd_head_k    = 128
0.00.038.697 I print_info: n_embd_head_v    = 128
0.00.038.698 I print_info: n_gqa            = 1
0.00.038.699 I print_info: n_embd_k_gqa     = 2048
0.00.038.699 I print_info: n_embd_v_gqa     = 2048
0.00.038.700 I print_info: f_norm_eps       = 1.0e-05
0.00.038.700 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.701 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.701 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.701 I print_info: f_logit_scale    = 0.0e+00
0.00.038.702 I print_info: n_ff             = 8192
0.00.038.702 I print_info: n_expert         = 0
0.00.038.702 I print_info: n_expert_used    = 0
0.00.038.702 I print_info: causal attn      = 1
0.00.038.702 I print_info: pooling type     = 0
0.00.038.702 I print_info: rope type        = 2
0.00.038.703 I print_info: rope scaling     = linear
0.00.038.703 I print_info: freq_base_train  = 10000.0
0.00.038.703 I print_info: freq_scale_train = 1
0.00.038.704 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.704 I print_info: rope_finetuned   = unknown
0.00.038.704 I print_info: ssm_d_conv       = 0
0.00.038.704 I print_info: ssm_d_inner      = 0
0.00.038.704 I print_info: ssm_d_state      = 0
0.00.038.704 I print_info: ssm_dt_rank      = 0
0.00.038.705 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.705 I print_info: model type       = 1.4B
0.00.038.705 I print_info: model params     = 1.41 B
0.00.038.705 I print_info: general.name     = 1.4B
0.00.038.706 I print_info: vocab type       = BPE
0.00.038.706 I print_info: n_vocab          = 50304
0.00.038.706 I print_info: n_merges         = 50009
0.00.038.706 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: LF token         = 187 'Ċ'
0.00.038.708 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: max token length = 1024
0.00.038.708 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.401.413 I load_tensors: offloading 24 repeating layers to GPU
0.00.401.421 I load_tensors: offloading output layer to GPU
0.00.401.422 I load_tensors: offloaded 25/25 layers to GPU
0.00.401.455 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.401.456 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.403.023 I llama_init_from_model: n_seq_max     = 1
0.00.403.027 I llama_init_from_model: n_ctx         = 128
0.00.403.027 I llama_init_from_model: n_ctx_per_seq = 128
0.00.403.028 I llama_init_from_model: n_batch       = 128
0.00.403.028 I llama_init_from_model: n_ubatch      = 128
0.00.403.029 I llama_init_from_model: flash_attn    = 0
0.00.403.030 I llama_init_from_model: freq_base     = 10000.0
0.00.403.031 I llama_init_from_model: freq_scale    = 1
0.00.403.031 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.403.034 I ggml_metal_init: allocating
0.00.403.061 I ggml_metal_init: found device: Apple M4
0.00.403.074 I ggml_metal_init: picking default device: Apple M4
0.00.404.396 I ggml_metal_init: using embedded metal library
0.00.410.723 I ggml_metal_init: GPU name:   Apple M4
0.00.410.728 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.410.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.410.729 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.410.730 I ggml_metal_init: simdgroup reduction   = true
0.00.410.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.410.730 I ggml_metal_init: has residency sets    = true
0.00.410.731 I ggml_metal_init: has bfloat            = true
0.00.410.731 I ggml_metal_init: use bfloat            = true
0.00.410.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.410.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.427.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.430.752 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.430.758 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.430.789 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.045 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.434.046 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.434.047 I llama_init_from_model: graph nodes  = 967
0.00.434.047 I llama_init_from_model: graph splits = 2
0.00.434.050 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.434.050 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.473.241 I 
0.00.473.328 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.473.347 I perplexity: tokenizing the input ..
0.00.479.878 I perplexity: tokenization took 6.528 ms
0.00.479.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.818 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.621.161 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.621.179 I llama_perf_context_print:        load time =     464.14 ms
0.00.621.180 I llama_perf_context_print: prompt eval time =     139.38 ms /   128 tokens (    1.09 ms per token,   918.37 tokens per second)
0.00.621.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.181 I llama_perf_context_print:       total time =     147.94 ms /   129 tokens
0.00.621.538 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.078s
sys	0m0.115s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.291 I build: 4685 (81732619) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.154 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.285 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.293 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.300 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.304 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.310 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.835 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.799 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.799 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.800 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.800 I llama_model_loader: - type  f32:  194 tensors
0.00.055.801 I llama_model_loader: - type  f16:   98 tensors
0.00.055.802 I print_info: file format = GGUF V3 (latest)
0.00.055.803 I print_info: file type   = all F32 (guessed)
0.00.055.804 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.103 I load: special tokens cache size = 25
0.00.076.272 I load: token to piece cache size = 0.2984 MB
0.00.076.276 I print_info: arch             = gptneox
0.00.076.276 I print_info: vocab_only       = 0
0.00.076.276 I print_info: n_ctx_train      = 2048
0.00.076.276 I print_info: n_embd           = 2048
0.00.076.276 I print_info: n_layer          = 24
0.00.076.280 I print_info: n_head           = 16
0.00.076.281 I print_info: n_head_kv        = 16
0.00.076.281 I print_info: n_rot            = 32
0.00.076.281 I print_info: n_swa            = 0
0.00.076.281 I print_info: n_embd_head_k    = 128
0.00.076.281 I print_info: n_embd_head_v    = 128
0.00.076.282 I print_info: n_gqa            = 1
0.00.076.283 I print_info: n_embd_k_gqa     = 2048
0.00.076.284 I print_info: n_embd_v_gqa     = 2048
0.00.076.284 I print_info: f_norm_eps       = 1.0e-05
0.00.076.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.285 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.285 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.285 I print_info: f_logit_scale    = 0.0e+00
0.00.076.286 I print_info: n_ff             = 8192
0.00.076.286 I print_info: n_expert         = 0
0.00.076.286 I print_info: n_expert_used    = 0
0.00.076.286 I print_info: causal attn      = 1
0.00.076.287 I print_info: pooling type     = 0
0.00.076.288 I print_info: rope type        = 2
0.00.076.288 I print_info: rope scaling     = linear
0.00.076.288 I print_info: freq_base_train  = 10000.0
0.00.076.289 I print_info: freq_scale_train = 1
0.00.076.289 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.289 I print_info: rope_finetuned   = unknown
0.00.076.289 I print_info: ssm_d_conv       = 0
0.00.076.289 I print_info: ssm_d_inner      = 0
0.00.076.289 I print_info: ssm_d_state      = 0
0.00.076.290 I print_info: ssm_dt_rank      = 0
0.00.076.290 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.290 I print_info: model type       = 1.4B
0.00.076.290 I print_info: model params     = 1.41 B
0.00.076.291 I print_info: general.name     = 1.4B
0.00.076.291 I print_info: vocab type       = BPE
0.00.076.291 I print_info: n_vocab          = 50304
0.00.076.291 I print_info: n_merges         = 50009
0.00.076.292 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.292 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.292 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.292 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.292 I print_info: LF token         = 187 'Ċ'
0.00.076.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.293 I print_info: max token length = 1024
0.00.076.293 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.351.360 I load_tensors: offloading 24 repeating layers to GPU
0.01.351.365 I load_tensors: offloading output layer to GPU
0.01.351.366 I load_tensors: offloaded 25/25 layers to GPU
0.01.351.397 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.351.399 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.352.212 I llama_init_from_model: n_seq_max     = 1
0.01.352.213 I llama_init_from_model: n_ctx         = 128
0.01.352.213 I llama_init_from_model: n_ctx_per_seq = 128
0.01.352.214 I llama_init_from_model: n_batch       = 128
0.01.352.214 I llama_init_from_model: n_ubatch      = 128
0.01.352.214 I llama_init_from_model: flash_attn    = 0
0.01.352.215 I llama_init_from_model: freq_base     = 10000.0
0.01.352.215 I llama_init_from_model: freq_scale    = 1
0.01.352.216 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.352.220 I ggml_metal_init: allocating
0.01.352.298 I ggml_metal_init: found device: Apple M4
0.01.352.305 I ggml_metal_init: picking default device: Apple M4
0.01.353.488 I ggml_metal_init: using embedded metal library
0.01.357.347 I ggml_metal_init: GPU name:   Apple M4
0.01.357.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.357.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.357.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.357.350 I ggml_metal_init: simdgroup reduction   = true
0.01.357.351 I ggml_metal_init: simdgroup matrix mul. = true
0.01.357.351 I ggml_metal_init: has residency sets    = true
0.01.357.351 I ggml_metal_init: has bfloat            = true
0.01.357.351 I ggml_metal_init: use bfloat            = true
0.01.357.352 I ggml_metal_init: hasUnifiedMemory      = true
0.01.357.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.367.835 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.369.497 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.369.502 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.369.517 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.371.109 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.371.110 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.371.110 I llama_init_from_model: graph nodes  = 967
0.01.371.110 I llama_init_from_model: graph splits = 2
0.01.371.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.371.112 I 
0.01.371.147 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.371.149 I compute_imatrix: tokenizing the input ..
0.01.375.061 I compute_imatrix: tokenization took 3.911 ms
0.01.375.062 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.640.480 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.643.239 I llama_perf_context_print:        load time =    1616.32 ms
0.01.643.240 I llama_perf_context_print: prompt eval time =     263.68 ms /   128 tokens (    2.06 ms per token,   485.44 tokens per second)
0.01.643.240 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.643.241 I llama_perf_context_print:       total time =    1619.07 ms /   129 tokens
0.01.643.767 I ggml_metal_free: deallocating

real	0m1.836s
user	0m0.126s
sys	0m0.258s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4685 (81732619)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1396070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1396077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x139607da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x139608350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x139608900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x139608eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x139609460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x139609a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x139609fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13960a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13960a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13960aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13960b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13960c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13960c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13960d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13960d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13960df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13960e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13960edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13960f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13960fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x139610350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x139610bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x139611310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1396115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x139611be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x139612850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139612d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x139613050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1396134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1396137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x139614040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x139614580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139614840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139614ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x139615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x139615620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x139615ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x139615f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x139616400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1396168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x139616d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1396171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1396174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x139617ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1396180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1396189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x139618ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x139619600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x139619c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13961a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13961a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13961ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13961b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13961bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13961bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13961c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13961c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13961d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13961d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13961d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13961dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13961e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13961e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13961ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13961eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13961f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13961f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13961fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x139620130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1396205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x139620a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x139620fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x139621510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139621a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x139621fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139622500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x139622a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139622fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1396234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x139623a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x139623f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1396244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x139624a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x139624f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1396254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x139625a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x139625f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1396264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x139626a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x139626f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1396274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x139627a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x139627f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1396284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1396289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1396186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x139628e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x139629610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x139629b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13962a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13962a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13962ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13962b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13962b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13962bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13962c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13962c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13962cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13962d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13962d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13962db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13962dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13962e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13962e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13962eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13962f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13962f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13962fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x139630020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a304c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a305070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a3054e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a305c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a3060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a308200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a308670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a308ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a308f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a3093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a309830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a309ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a30a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a30a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a30a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a30ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a30b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a30b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a30bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a30c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a30c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a30c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a30cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a30d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a30d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a30dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a30df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a30e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a30e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a30ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a30f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a30f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a30f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a30fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a3102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a310720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a310b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a311000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a311470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a3118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a311d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a3121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a312630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a312aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a312f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a313380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a3137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a313c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a3140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a314540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a3149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a314e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a315290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a315700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a315b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a315fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a316450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a3168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a316d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a3171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a317610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a317a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a317ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a318360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a3187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a318c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a3190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a319520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a319990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a319e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a31a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a31a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a31ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a31afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a31b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a31b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a31bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a31c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a31c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a31ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a31ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a31d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a31d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a31dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a31e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a31e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a31eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a31f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a31f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a3200d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a320390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a320950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a320f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a3214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a321a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a322050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a322610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a322bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a323190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a323750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a323d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a3242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a324890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a324e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a325410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a3259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a325f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a326550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a326b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a3270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a327690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a327c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a328210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a3287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a328d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a329350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a329910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a329ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a32a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a32aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a32b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a32b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a32bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a32c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a32c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a32ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a32d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a32d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a32de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a32e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a32e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a32ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a32f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a32fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a330090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a330650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a330c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a3311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a331790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a331d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a332310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a3328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a332e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a333450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a333a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a333fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a334590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a334a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a335490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a335990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a335e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a336390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a336890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a336d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a337290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a337790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a337c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a338190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a338690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a338b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a339090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a339aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a33a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a33a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a33b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a33b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a33bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a33bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a33c380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.703.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106304dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106305240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1063056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106305b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106305f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106306400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106306870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106306ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106307150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1063075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106307a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106308120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106308c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1063093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106309c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10630a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10630aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10630b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10630b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10630bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10630c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10630cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10630d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10630dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10630e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10630e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10630e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10630ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10630f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10630f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10630fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10630ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106310430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1063106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106310b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106310fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106311440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1063118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106311d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106312190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106312600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106312a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106312ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106313350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1063137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106313c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1063140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106314510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106314980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106314df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106315260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1063156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106315b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106315fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106316420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106316890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106316e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106317300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106317770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106317be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106318050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1063184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106318930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106318da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106319210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106319680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106319af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106319f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10631a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10631a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10631acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10631b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10631b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10631ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10631be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10631c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10631c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10631cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10631d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10631d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10631d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10631dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10631e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10631e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10631ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10631ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10631f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10631f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10631fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106320100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106320570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1063209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106320e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1063212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106321730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106321ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106322010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106322480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1063228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106322d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1063231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106323640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106323ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106323f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106324390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106324800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106324c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1063250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106325550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1063259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106325e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1063262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106326710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106326b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106326ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106327460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1063278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106327d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1063281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106328620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106328a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106328f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106329370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1063297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106329c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10632a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10632a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10632a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10632ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10632b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10632b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10632bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10632bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10632c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10632c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10632cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10632d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10632d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10632da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10632dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10632e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10632e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10632ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10632f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10632f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10632f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10632fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106330260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1063306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106330b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106330fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106331420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106331890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106331d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106332170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1063325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106332a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106332ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106333330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1063337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106333c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106334080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1063344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106334960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106334dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106335240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106335e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106336130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1063363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106336860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106336cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106337140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1063375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106337a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106337e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106338300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106338770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106338be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106339050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1063394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106339930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106339da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10633a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10633a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10633aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10633af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10633b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10633b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10633bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10633c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10633c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10633ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10633ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10633d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10633d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10633dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10633e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10633e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10633e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10633ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10633f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10633f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10633fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1063400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106340540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1063409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106340e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106341290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1063417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106341cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106342830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106342af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1063430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106343670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106343c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1063441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1063447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106344d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106345330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1063458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106345eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106346470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106346a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106346ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1063475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106347b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106348130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1063486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106348cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106349270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106349df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10634a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10634a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10634af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10634b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10634bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10634c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10634c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10634cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10634d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10634d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10634dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10634e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10634e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10634ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10634f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10634f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10634ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106350570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106350b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1063510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1063516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106351c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106352230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1063527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106352db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106353370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106353930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106353ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1063544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106354a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106355030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1063555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106355bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106356170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106356730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106356cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1063571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1063576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106357bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1063580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1063585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106358af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106358ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1063594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1063599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106359ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10635a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10635a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10635adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10635b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10635b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10635c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10635c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10635d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10635d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10635da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10635e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10635e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10635eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a5052b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a505720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a505b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a506000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a506470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a5068e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a506d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a5071c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a507630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a507aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a507f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a508630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a509150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a509900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a50a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a50a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a50af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a50b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a50bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a50c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a50cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a50d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a50da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a50e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a50e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a50eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a50ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a50f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a50f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a50fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a50ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a5104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a510940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a510c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a511070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a5114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a511950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a511dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a512230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a5126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a512b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a512f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a5133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a513860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a513cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a514140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a5145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a514a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a514e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a515300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a515770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a515be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a516050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a5164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a516930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a516da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a517310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a517810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a517c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a5180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a518560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a5189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a518e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a5192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a519720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a519b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a51a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a51a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a51a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a51ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a51b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a51b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a51baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a51bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a51c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a51c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a51cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a51d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a51d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a51d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a51de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a51e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a51e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a51eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a51efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a51f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a51f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a51fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a5201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a520610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a520a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a520ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a521360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a5217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a521c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a5220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a522520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a522990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a522e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a523270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a5236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a523b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a523fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a524850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a524b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a524f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a5253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a525860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a525cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a526140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a5265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a526a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a526e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a527300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a527770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a527be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a528050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a5284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a528930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a528da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a529210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a529680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a529af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a529f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a52a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a52a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a52acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a52b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a52b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a52ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a52be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a52c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a52c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a52cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a52d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a52d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a52d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a52dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a52e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a52e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a52ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a52ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a52f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a52f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a52fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a530100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a530570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a5309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a530e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a5312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a531730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a531ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a532010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a532480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a5328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a532d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a5331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a533640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a533ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a533f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a534390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a534800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a534c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a5350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a535550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a5359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a535e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a5362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a536710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a536b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a536ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a537460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a5378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a537d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a5381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a538620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a538a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a538f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a539370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a5397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a539c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a53a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a53a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a53a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a53ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a53b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a53b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a53bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a53bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a53c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a53c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a53cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a53d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a53d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a53da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a53dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a53e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a53e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a53ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a53f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a53f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a53f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a53fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a540260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a5406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a540b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a540fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a541420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a541890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a541d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a542880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a542b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a542e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a543270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a5436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a543b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a543fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a544430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a5448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a544d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a545180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a5455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a545a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a545ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a546340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a5467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a546c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a547090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a547500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a547970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a547de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a548250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a5486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a548b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a548fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a549410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a549880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a549cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a54a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a54a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a54aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a54aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a54b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a54b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a54bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a54c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a54c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a54c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a54cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a54d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a54d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a54db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a54df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a54e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a54e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a54ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a54f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a54f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a54fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a54fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a550300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a550770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a550be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a551050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a5514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a551930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a551da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a552210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a552680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a552af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a552f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a5533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a553840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a553cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a554120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a554590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a554a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a554e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a5552e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a555750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a555bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a556030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a5564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a556f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a557630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a557d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a558470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a558730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a558ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a5591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a5597b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.742s
user	0m0.247s
sys	0m0.254s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4685 (81732619)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114e07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114e07d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114e08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114e088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114e08e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114e09420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114e099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114e09f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114e0a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114e0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114e0af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x114e0b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114e0bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114e0c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114e0cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x114e0d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x114e0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x114e0e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x114e0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x114e0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x114e0fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114e101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114e108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x114e11160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114e11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x114e11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x114e12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114e12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114e13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x114e135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114e13a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114e13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114e145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114e14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114e14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114e15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114e156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114e15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114e16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114e164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114e16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114e16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114e172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114e17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114e17a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114e18020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x114e18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114e18f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114e19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114e19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x114e1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114e1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x114e1ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x114e1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114e1bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x114e1c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114e1c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114e1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114e1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114e1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x114e1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114e1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x114e1e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x114e1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x114e1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x114e1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x114e1f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114e1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x114e1fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x114e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x114e206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x114e20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x114e20fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x114e21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114e21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x114e21fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114e22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114e22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114e22fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x114e23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x114e23a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114e23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114e24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114e24a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114e24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114e254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x114e25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114e25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114e264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114e26a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114e26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114e274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114e27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114e27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114e284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114e28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114e28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114e18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114e293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114e29b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114e2a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114e2a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x114e2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114e2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114e2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x114e2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114e2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114e2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x114e2cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114e2d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114e2d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114e2db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114e2e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x114e2e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114e2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114e2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114e2f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114e2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114e300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114e30590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114e30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114e30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114e31370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114e31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114e31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114e32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114e325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114e32a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x114e32f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114e333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114e33870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114e33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114e341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114e34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114e34af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114e34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114e35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114e358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114e35d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114e36210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114e366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x114e36b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114e36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114e37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x114e37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114e37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x114e38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114e38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114e38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114e39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114e394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x114e39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114e39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114e3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x114e3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x114e3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114e3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x114e3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114e3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114e3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114e3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114e3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x114e3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114e3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114e3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114e3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114e3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114e3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114e3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x114e3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114e3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114e3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114e3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114e3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114e403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114e40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x114e40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114e411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114e41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114e41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114e41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114e42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114e428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114e42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114e43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114e436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114e43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114e44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114e444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114e44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114e44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114e45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114e457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114e45d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114e46280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114e467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114e46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114e470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114e476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114e47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x114e48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114e48c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114e49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114e49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114e4a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114e4a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114e4a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114e4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114e4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114e4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114e4c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114e4c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114e4caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114e4d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x114e4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114e4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114e4e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114e4e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114e4ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114e4f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114e4f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114e4fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114e50010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114e50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114e50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114e51000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114e51550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114e51aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114e51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114e52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114e52a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114e52fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114e53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114e53a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114e53fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114e54520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114e54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114e54fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114e55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114e55a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114e55fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114e56500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114e56a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114e56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114e574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114e57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114e57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114e584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114e58a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114e58f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114e594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114e59a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114e59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x114e5a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114e5aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114e5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114e5b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114e5ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114e5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114e5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114e5c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114e5cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114e5d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114e5d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114e5df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114e5e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114e5e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114e5ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114e5f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114e5f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114e5faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114e5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114e60430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114e608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114e60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114e61210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114e616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114e61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114e61ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114e62490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114e629e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114e63100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114e63820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114e63f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114e64660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114e64920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114e65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114e653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114e659e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.113.118 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.123 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124f0a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124f0ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124f0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124f0b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124f0b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124f0bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124f0c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124f0c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124f0ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124f0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124f0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124f0dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124f0e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124f0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124f0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124f0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124f103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124f10af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124f11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124f119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124f12100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124f12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124f12f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124f13660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124f13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124f14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124f14300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124f14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124f14be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124f15050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124f154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124f159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124f15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124f16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124f16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124f16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124f16e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124f172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124f17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124f17bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124f18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124f184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124f18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124f18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124f191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124f19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124f19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124f19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124f1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124f1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124f1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124f1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124f1b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124f1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124f1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124f1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124f1c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124f1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124f1d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124f1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124f1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124f1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124f1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124f1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124f1ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124f1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124f1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124f1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124f1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124f20270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124f206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124f20b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124f20fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124f21430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124f218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124f21d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124f22180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124f225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124f22a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124f22ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124f23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124f237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124f23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124f24090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124f24500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124f24970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124f24de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124f25250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124f256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124f25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124f25fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124f26410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124f26880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124f26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124f27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124f275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124f27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124f27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124f28320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124f28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124f28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124f29070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124f294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124f29950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124f29dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124f2a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124f2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124f2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124f2af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124f2b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124f2b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124f2bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124f2c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124f2c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124f2ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124f2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124f2d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124f2d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124f2dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124f2e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124f2e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124f2e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124f2eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124f2f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124f2f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124f2faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124f2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124f303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124f30840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124f30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124f31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124f31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124f31a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124f31e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124f322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124f32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124f32bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124f33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124f334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124f33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124f33d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124f341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124f34660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124f34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124f34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124f353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124f35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124f35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124f36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124f36570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124f369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124f36e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124f372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124f37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124f37ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124f38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124f38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124f388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124f38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124f391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124f39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124f39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124f39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124f3a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124f3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124f3ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124f3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124f3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124f3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124f3c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124f3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124f3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124f3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124f3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124f3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124f3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124f3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124f3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124f3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124f3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124f3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124f3f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124f3fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124f400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124f40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124f40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124f40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124f41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124f416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124f41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124f41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124f42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124f428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124f42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124f43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124f435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124f43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124f43ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124f44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124f447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124f44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124f45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124f455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124f45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124f45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124f463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124f46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124f46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124f471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124f476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124f48260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124f48520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124f48ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124f490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124f49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124f49c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124f4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124f4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124f4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124f4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124f4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124f4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124f4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124f4ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124f4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124f4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124f4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124f4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124f4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124f4eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124f4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124f4f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124f4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124f503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124f50960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124f50f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124f514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124f51aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124f52060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124f52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124f52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124f531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124f53760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124f53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124f542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124f548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124f54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124f55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124f559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124f55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124f56560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124f56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124f570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124f576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124f57c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124f58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124f587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124f58da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124f59360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124f59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124f59ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124f5a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124f5aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124f5b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124f5b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124f5bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124f5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124f5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124f5cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124f5d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124f5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124f5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124f5e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124f5e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124f5ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124f5ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124f5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124f5f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124f5fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124f60320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124f60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124f60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124f61220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124f61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124f62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124f62a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124f63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124f63450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124f63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124f63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124f64510 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x114e65690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x114e47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x114e46d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x114e47970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x114e1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x114e1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x114e1ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x114e494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x114e11e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x114e188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x114e19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x114e19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x114e17cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x114e19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x114e10e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x114e1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x114e29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x114e64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x114e13fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x114e142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x114e49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x114e47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x114e12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x114e126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x114e12990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x114e65e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x114e66100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x114e663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x114e66680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x114e66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x114e66c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x114e66ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x114e67180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x114e67440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x114e67700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x114e679c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x114e67c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x114e67f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x114e68200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x114e684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x114e68780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x114e68a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x114e68d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x114e68fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x114e69280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x114e69540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x114e69800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x114e69ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x114e69d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x114e6a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x114e6a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x114e6a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x114e6a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x114e6ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x114e6ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x114e6b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x114e6b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x114e6b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x114e6b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x114e6bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x114e6be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x114e6c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x114e6c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x114e6c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x114e6c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x114e6cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x114e6cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x114e6d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x114e6d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x114e6d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x114e6da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x114e6dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x114e6df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x114e6e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114e6e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x114e6e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114e6ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114e6ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114e6f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x114e6f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x114e6f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114e6f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114e6fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114e6fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114e70080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114e70340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x114e70600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114e708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x114e70b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114e70e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114e71100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114e713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114e71680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x114e71940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114e71c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114e71ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x114e72180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x114e72440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x114e72700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x114e729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x114e72c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x114e72f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x114e73200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x114e734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x114e73780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x114e73a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x114e73d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x114e73fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x114e74280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x114e74540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x114e74800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x114e74ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x114e74d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x114e75040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114e75300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114e755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114e75880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114e75b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114e75e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114e760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114e76380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114e76640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114e76900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114e76bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114e76e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114e77140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114e77400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114e776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114e77980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x114e77c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114e77f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114e781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114e78480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114e78740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114e78a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114e78cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114e78f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114e79240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114e79500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114e797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114e79a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114e79d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x114e7a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114e7a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114e7a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x114e7a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x114e7ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x114e7adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x114e7b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x114e7b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x114e7b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x114e7b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x114e7bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x114e7be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x114e7c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x114e7c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x114e7c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x114e7c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x114e7cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x114e7cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x114e7d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x114e7d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x114e7d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x114e7d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x114e7dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114e7df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114e7e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114e7e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114e7e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114e7ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x114e7ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114e7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114e7f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114e7f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114e7f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114e7fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114e7fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x114e80040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114e80300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114e805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114e80880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114e80b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114e80e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114e810c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114e81380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114e81640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114e81900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114e81bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114e81e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114e82140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114e82400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114e826c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114e82980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114e82c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x114e82f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114e831c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x114e83480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x114e83740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x114e83a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x114e83cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x114e83f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x114e84240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x114e84500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x114e847c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x114e84a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x114e84d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x114e85000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x114e852c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x114e856c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x114e85b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x114e86310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x114e865d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x114e86890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x114e86d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114e87170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114e875e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x114e87a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114e87ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114e88330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x114e887a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114e88c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114e89080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x114e894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114e89960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114e89dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x114e8a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x114e8a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114e8ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114e8af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x114e8b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114e8b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114e8bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x114e8c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114e8c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114e8ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x114e8cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114e8d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114e8d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x114e8dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114e8e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114e8e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x114e8e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114e8edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x114e8f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x114e8f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x114e8fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x114e8ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x114e903e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x114e90850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x114e90cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x114e91130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x114e915a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x114e91a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x114e91e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x114e922f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x114e92760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x114e92bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x114e93040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x114e934b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x114e93920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x114e93d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x114e94200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114e94670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114e94ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x114e94f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114e953c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114e95830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114e95ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x114e96110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114e96580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114e969f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114e96e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114e972d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114e97740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114e97bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114e98020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x114e98490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114e98900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x114e98d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114e991e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114e99650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x114e99ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114e99f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114e9a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114e9b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114e9b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x114e9bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114e9c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114e9c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114e9cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114e9d280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.231s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
