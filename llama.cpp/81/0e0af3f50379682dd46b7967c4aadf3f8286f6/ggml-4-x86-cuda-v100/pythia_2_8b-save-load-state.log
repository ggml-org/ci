+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 10 -c 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.295 I build: 4912 (810e0af3f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.257.856 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.273.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.273.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.273.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.273.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.273.962 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.273.962 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.273.963 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.273.967 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.273.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.273.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.273.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.273.971 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.273.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.273.974 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.273.989 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.273.990 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.273.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.280.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.282.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.289.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.289.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.289.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.289.420 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.289.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.289.421 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.289.424 I llama_model_loader: - type  f32:  258 tensors
0.00.289.424 I llama_model_loader: - type q4_0:  129 tensors
0.00.289.425 I llama_model_loader: - type q6_K:    1 tensors
0.00.289.427 I print_info: file format = GGUF V3 (latest)
0.00.289.428 I print_info: file type   = Q4_0
0.00.289.430 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.337.476 I load: special tokens cache size = 25
0.00.359.793 I load: token to piece cache size = 0.2984 MB
0.00.359.813 I print_info: arch             = gptneox
0.00.359.813 I print_info: vocab_only       = 0
0.00.359.814 I print_info: n_ctx_train      = 2048
0.00.359.814 I print_info: n_embd           = 2560
0.00.359.815 I print_info: n_layer          = 32
0.00.359.834 I print_info: n_head           = 32
0.00.359.836 I print_info: n_head_kv        = 32
0.00.359.836 I print_info: n_rot            = 20
0.00.359.837 I print_info: n_swa            = 0
0.00.359.838 I print_info: n_swa_pattern    = 1
0.00.359.839 I print_info: n_embd_head_k    = 80
0.00.359.840 I print_info: n_embd_head_v    = 80
0.00.359.843 I print_info: n_gqa            = 1
0.00.359.845 I print_info: n_embd_k_gqa     = 2560
0.00.359.847 I print_info: n_embd_v_gqa     = 2560
0.00.359.849 I print_info: f_norm_eps       = 1.0e-05
0.00.359.850 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.359.850 I print_info: f_clamp_kqv      = 0.0e+00
0.00.359.851 I print_info: f_max_alibi_bias = 0.0e+00
0.00.359.852 I print_info: f_logit_scale    = 0.0e+00
0.00.359.855 I print_info: f_attn_scale     = 0.0e+00
0.00.359.857 I print_info: n_ff             = 10240
0.00.359.857 I print_info: n_expert         = 0
0.00.359.858 I print_info: n_expert_used    = 0
0.00.359.858 I print_info: causal attn      = 1
0.00.359.858 I print_info: pooling type     = 0
0.00.359.859 I print_info: rope type        = 2
0.00.359.859 I print_info: rope scaling     = linear
0.00.359.861 I print_info: freq_base_train  = 10000.0
0.00.359.862 I print_info: freq_scale_train = 1
0.00.359.862 I print_info: n_ctx_orig_yarn  = 2048
0.00.359.863 I print_info: rope_finetuned   = unknown
0.00.359.863 I print_info: ssm_d_conv       = 0
0.00.359.863 I print_info: ssm_d_inner      = 0
0.00.359.864 I print_info: ssm_d_state      = 0
0.00.359.865 I print_info: ssm_dt_rank      = 0
0.00.359.867 I print_info: ssm_dt_b_c_rms   = 0
0.00.359.867 I print_info: model type       = 2.8B
0.00.359.868 I print_info: model params     = 2.78 B
0.00.359.869 I print_info: general.name     = 2.8B
0.00.359.872 I print_info: vocab type       = BPE
0.00.359.873 I print_info: n_vocab          = 50304
0.00.359.873 I print_info: n_merges         = 50009
0.00.359.874 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.359.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.359.875 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.359.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.359.876 I print_info: LF token         = 187 'Ċ'
0.00.359.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.359.878 I print_info: max token length = 1024
0.00.359.879 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.788 I load_tensors: offloading 10 repeating layers to GPU
0.00.457.800 I load_tensors: offloaded 10/33 layers to GPU
0.00.457.809 I load_tensors:        CUDA0 model buffer size =   423.14 MiB
0.00.457.811 I load_tensors:  CPU_AARCH64 model buffer size =   928.12 MiB
0.00.457.812 I load_tensors:   CPU_Mapped model buffer size =  1086.70 MiB
...........................................................................................
0.01.062.238 I llama_context: constructing llama_context
0.01.062.244 I llama_context: n_seq_max     = 1
0.01.062.244 I llama_context: n_ctx         = 2048
0.01.062.245 I llama_context: n_ctx_per_seq = 2048
0.01.062.245 I llama_context: n_batch       = 2048
0.01.062.246 I llama_context: n_ubatch      = 512
0.01.062.246 I llama_context: causal_attn   = 1
0.01.062.247 I llama_context: flash_attn    = 0
0.01.062.252 I llama_context: freq_base     = 10000.0
0.01.062.253 I llama_context: freq_scale    = 1
0.01.062.348 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.062.360 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.063.102 I init:      CUDA0 KV buffer size =   200.00 MiB
0.01.200.342 I init:        CPU KV buffer size =   440.00 MiB
0.01.200.379 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.228.756 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.01.228.770 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.01.228.770 I llama_context: graph nodes  = 1287
0.01.228.771 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
0.01.228.780 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.228.780 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.02.374.013 I llama_context: constructing llama_context
0.02.374.075 I llama_context: n_seq_max     = 1
0.02.374.087 I llama_context: n_ctx         = 2048
0.02.374.098 I llama_context: n_ctx_per_seq = 2048
0.02.374.108 I llama_context: n_batch       = 2048
0.02.374.119 I llama_context: n_ubatch      = 512
0.02.374.133 I llama_context: causal_attn   = 1
0.02.374.145 I llama_context: flash_attn    = 0
0.02.374.164 I llama_context: freq_base     = 10000.0
0.02.374.179 I llama_context: freq_scale    = 1
0.02.374.263 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.374.300 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.375.258 I init:      CUDA0 KV buffer size =   200.00 MiB
0.02.511.738 I init:        CPU KV buffer size =   440.00 MiB
0.02.511.760 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.540.261 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.02.540.274 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.02.540.275 I llama_context: graph nodes  = 1287
0.02.540.276 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.03.435.868 I llama_context: constructing llama_context
0.03.435.900 I llama_context: n_seq_max     = 1
0.03.435.901 I llama_context: n_ctx         = 2048
0.03.435.901 I llama_context: n_ctx_per_seq = 2048
0.03.435.901 I llama_context: n_batch       = 2048
0.03.435.902 I llama_context: n_ubatch      = 512
0.03.435.903 I llama_context: causal_attn   = 1
0.03.435.903 I llama_context: flash_attn    = 0
0.03.435.909 I llama_context: freq_base     = 10000.0
0.03.435.910 I llama_context: freq_scale    = 1
0.03.435.970 I llama_context:        CPU  output buffer size =     0.19 MiB
0.03.435.974 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.03.436.694 I init:      CUDA0 KV buffer size =   200.00 MiB
0.03.573.483 I init:        CPU KV buffer size =   440.00 MiB
0.03.573.511 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.03.602.232 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.03.602.245 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.03.602.246 I llama_context: graph nodes  = 1287
0.03.602.247 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox
            Gigot the wall from the wall,
            Scraped


second run: The quick brown fox
            Gigot the wall from the wall,
            Scraped


single seq run: The quick brown fox
            Gigot the wall from the wall,
            Scraped

real	0m5.300s
user	0m13.080s
sys	0m1.423s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 10 -c 0 -fa
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.292 I build: 4912 (810e0af3f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.253.960 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.277.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.277.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.277.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.277.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.277.300 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.277.301 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.277.302 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.277.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.277.308 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.277.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.277.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.277.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.277.314 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.277.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.277.332 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.277.333 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.277.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.284.195 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.285.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.292.915 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.292.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.292.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.292.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.292.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.292.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.292.929 I llama_model_loader: - type  f32:  258 tensors
0.00.292.929 I llama_model_loader: - type q4_0:  129 tensors
0.00.292.930 I llama_model_loader: - type q6_K:    1 tensors
0.00.292.932 I print_info: file format = GGUF V3 (latest)
0.00.292.933 I print_info: file type   = Q4_0
0.00.292.935 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.340.047 I load: special tokens cache size = 25
0.00.362.281 I load: token to piece cache size = 0.2984 MB
0.00.362.306 I print_info: arch             = gptneox
0.00.362.307 I print_info: vocab_only       = 0
0.00.362.308 I print_info: n_ctx_train      = 2048
0.00.362.308 I print_info: n_embd           = 2560
0.00.362.308 I print_info: n_layer          = 32
0.00.362.323 I print_info: n_head           = 32
0.00.362.329 I print_info: n_head_kv        = 32
0.00.362.329 I print_info: n_rot            = 20
0.00.362.330 I print_info: n_swa            = 0
0.00.362.330 I print_info: n_swa_pattern    = 1
0.00.362.331 I print_info: n_embd_head_k    = 80
0.00.362.331 I print_info: n_embd_head_v    = 80
0.00.362.334 I print_info: n_gqa            = 1
0.00.362.336 I print_info: n_embd_k_gqa     = 2560
0.00.362.338 I print_info: n_embd_v_gqa     = 2560
0.00.362.340 I print_info: f_norm_eps       = 1.0e-05
0.00.362.342 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.362.343 I print_info: f_clamp_kqv      = 0.0e+00
0.00.362.344 I print_info: f_max_alibi_bias = 0.0e+00
0.00.362.345 I print_info: f_logit_scale    = 0.0e+00
0.00.362.346 I print_info: f_attn_scale     = 0.0e+00
0.00.362.347 I print_info: n_ff             = 10240
0.00.362.348 I print_info: n_expert         = 0
0.00.362.349 I print_info: n_expert_used    = 0
0.00.362.350 I print_info: causal attn      = 1
0.00.362.350 I print_info: pooling type     = 0
0.00.362.351 I print_info: rope type        = 2
0.00.362.352 I print_info: rope scaling     = linear
0.00.362.354 I print_info: freq_base_train  = 10000.0
0.00.362.355 I print_info: freq_scale_train = 1
0.00.362.356 I print_info: n_ctx_orig_yarn  = 2048
0.00.362.357 I print_info: rope_finetuned   = unknown
0.00.362.358 I print_info: ssm_d_conv       = 0
0.00.362.358 I print_info: ssm_d_inner      = 0
0.00.362.358 I print_info: ssm_d_state      = 0
0.00.362.359 I print_info: ssm_dt_rank      = 0
0.00.362.359 I print_info: ssm_dt_b_c_rms   = 0
0.00.362.360 I print_info: model type       = 2.8B
0.00.362.361 I print_info: model params     = 2.78 B
0.00.362.361 I print_info: general.name     = 2.8B
0.00.362.364 I print_info: vocab type       = BPE
0.00.362.364 I print_info: n_vocab          = 50304
0.00.362.365 I print_info: n_merges         = 50009
0.00.362.366 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.362.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.362.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.362.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.362.371 I print_info: LF token         = 187 'Ċ'
0.00.362.372 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.362.372 I print_info: max token length = 1024
0.00.362.373 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.918 I load_tensors: offloading 10 repeating layers to GPU
0.00.455.926 I load_tensors: offloaded 10/33 layers to GPU
0.00.455.935 I load_tensors:        CUDA0 model buffer size =   423.14 MiB
0.00.455.937 I load_tensors:  CPU_AARCH64 model buffer size =   928.12 MiB
0.00.455.939 I load_tensors:   CPU_Mapped model buffer size =  1086.70 MiB
...........................................................................................
0.01.072.540 I llama_context: constructing llama_context
0.01.072.547 I llama_context: n_seq_max     = 1
0.01.072.547 I llama_context: n_ctx         = 2048
0.01.072.548 I llama_context: n_ctx_per_seq = 2048
0.01.072.548 I llama_context: n_batch       = 2048
0.01.072.549 I llama_context: n_ubatch      = 512
0.01.072.550 I llama_context: causal_attn   = 1
0.01.072.550 I llama_context: flash_attn    = 1
0.01.072.556 I llama_context: freq_base     = 10000.0
0.01.072.557 I llama_context: freq_scale    = 1
0.01.072.654 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.072.666 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.073.418 I init:      CUDA0 KV buffer size =   200.00 MiB
0.01.211.749 I init:        CPU KV buffer size =   440.00 MiB
0.01.211.783 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.240.304 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.01.240.313 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.01.240.315 I llama_context: graph nodes  = 1160
0.01.240.315 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
0.01.240.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.240.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.02.058.452 I llama_context: constructing llama_context
0.02.058.484 I llama_context: n_seq_max     = 1
0.02.058.484 I llama_context: n_ctx         = 2048
0.02.058.485 I llama_context: n_ctx_per_seq = 2048
0.02.058.485 I llama_context: n_batch       = 2048
0.02.058.486 I llama_context: n_ubatch      = 512
0.02.058.487 I llama_context: causal_attn   = 1
0.02.058.487 I llama_context: flash_attn    = 1
0.02.058.495 I llama_context: freq_base     = 10000.0
0.02.058.497 I llama_context: freq_scale    = 1
0.02.058.604 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.058.615 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.059.446 I init:      CUDA0 KV buffer size =   200.00 MiB
0.02.193.619 I init:        CPU KV buffer size =   440.00 MiB
0.02.193.647 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.222.422 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.02.222.434 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.02.222.434 I llama_context: graph nodes  = 1160
0.02.222.435 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.02.907.386 I llama_context: constructing llama_context
0.02.907.406 I llama_context: n_seq_max     = 1
0.02.907.406 I llama_context: n_ctx         = 2048
0.02.907.407 I llama_context: n_ctx_per_seq = 2048
0.02.907.407 I llama_context: n_batch       = 2048
0.02.907.408 I llama_context: n_ubatch      = 512
0.02.907.409 I llama_context: causal_attn   = 1
0.02.907.409 I llama_context: flash_attn    = 1
0.02.907.415 I llama_context: freq_base     = 10000.0
0.02.907.416 I llama_context: freq_scale    = 1
0.02.907.475 I llama_context:        CPU  output buffer size =     0.19 MiB
0.02.907.485 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.908.279 I init:      CUDA0 KV buffer size =   200.00 MiB
0.03.045.782 I init:        CPU KV buffer size =   440.00 MiB
0.03.045.805 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.03.074.003 I llama_context:      CUDA0 compute buffer size =   203.99 MiB
0.03.074.015 I llama_context:  CUDA_Host compute buffer size =    29.01 MiB
0.03.074.017 I llama_context: graph nodes  = 1160
0.03.074.017 I llama_context: graph splits = 313 (with bs=512), 5 (with bs=1)
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the


second run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the


single seq run: The quick brown fox jumps over the fence -- but the fourth fence is a dead-in-the

real	0m4.188s
user	0m11.412s
sys	0m1.336s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 99 -c 0
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.297 I build: 4912 (810e0af3f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.257.527 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.273.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.273.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.273.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.273.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.273.473 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.273.474 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.273.475 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.273.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.273.481 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.273.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.273.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.273.484 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.273.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.273.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.273.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.273.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.273.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.280.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.282.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.288.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.288.930 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.288.931 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.288.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.288.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.288.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.288.936 I llama_model_loader: - type  f32:  258 tensors
0.00.288.936 I llama_model_loader: - type q4_0:  129 tensors
0.00.288.937 I llama_model_loader: - type q6_K:    1 tensors
0.00.288.940 I print_info: file format = GGUF V3 (latest)
0.00.288.940 I print_info: file type   = Q4_0
0.00.288.943 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.333.601 I load: special tokens cache size = 25
0.00.355.767 I load: token to piece cache size = 0.2984 MB
0.00.355.784 I print_info: arch             = gptneox
0.00.355.785 I print_info: vocab_only       = 0
0.00.355.785 I print_info: n_ctx_train      = 2048
0.00.355.786 I print_info: n_embd           = 2560
0.00.355.786 I print_info: n_layer          = 32
0.00.355.801 I print_info: n_head           = 32
0.00.355.803 I print_info: n_head_kv        = 32
0.00.355.804 I print_info: n_rot            = 20
0.00.355.805 I print_info: n_swa            = 0
0.00.355.806 I print_info: n_swa_pattern    = 1
0.00.355.807 I print_info: n_embd_head_k    = 80
0.00.355.807 I print_info: n_embd_head_v    = 80
0.00.355.810 I print_info: n_gqa            = 1
0.00.355.812 I print_info: n_embd_k_gqa     = 2560
0.00.355.814 I print_info: n_embd_v_gqa     = 2560
0.00.355.816 I print_info: f_norm_eps       = 1.0e-05
0.00.355.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.355.817 I print_info: f_clamp_kqv      = 0.0e+00
0.00.355.818 I print_info: f_max_alibi_bias = 0.0e+00
0.00.355.819 I print_info: f_logit_scale    = 0.0e+00
0.00.355.819 I print_info: f_attn_scale     = 0.0e+00
0.00.355.820 I print_info: n_ff             = 10240
0.00.355.821 I print_info: n_expert         = 0
0.00.355.822 I print_info: n_expert_used    = 0
0.00.355.822 I print_info: causal attn      = 1
0.00.355.822 I print_info: pooling type     = 0
0.00.355.823 I print_info: rope type        = 2
0.00.355.823 I print_info: rope scaling     = linear
0.00.355.825 I print_info: freq_base_train  = 10000.0
0.00.355.827 I print_info: freq_scale_train = 1
0.00.355.828 I print_info: n_ctx_orig_yarn  = 2048
0.00.355.828 I print_info: rope_finetuned   = unknown
0.00.355.829 I print_info: ssm_d_conv       = 0
0.00.355.830 I print_info: ssm_d_inner      = 0
0.00.355.831 I print_info: ssm_d_state      = 0
0.00.355.831 I print_info: ssm_dt_rank      = 0
0.00.355.831 I print_info: ssm_dt_b_c_rms   = 0
0.00.355.832 I print_info: model type       = 2.8B
0.00.355.833 I print_info: model params     = 2.78 B
0.00.355.833 I print_info: general.name     = 2.8B
0.00.355.836 I print_info: vocab type       = BPE
0.00.355.837 I print_info: n_vocab          = 50304
0.00.355.838 I print_info: n_merges         = 50009
0.00.355.838 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.355.839 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.355.839 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.355.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.355.840 I print_info: LF token         = 187 'Ċ'
0.00.355.841 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.355.842 I print_info: max token length = 1024
0.00.355.843 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.791 I load_tensors: offloading 32 repeating layers to GPU
0.00.446.801 I load_tensors: offloading output layer to GPU
0.00.446.802 I load_tensors: offloaded 33/33 layers to GPU
0.00.446.811 I load_tensors:        CUDA0 model buffer size =  1454.83 MiB
0.00.446.813 I load_tensors:   CPU_Mapped model buffer size =    69.08 MiB
...........................................................................................
0.00.695.494 I llama_context: constructing llama_context
0.00.695.501 I llama_context: n_seq_max     = 1
0.00.695.502 I llama_context: n_ctx         = 2048
0.00.695.502 I llama_context: n_ctx_per_seq = 2048
0.00.695.503 I llama_context: n_batch       = 2048
0.00.695.503 I llama_context: n_ubatch      = 512
0.00.695.504 I llama_context: causal_attn   = 1
0.00.695.506 I llama_context: flash_attn    = 0
0.00.695.512 I llama_context: freq_base     = 10000.0
0.00.695.513 I llama_context: freq_scale    = 1
0.00.696.874 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.696.890 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.698.039 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.698.052 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.714.811 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.00.714.821 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.714.822 I llama_context: graph nodes  = 1287
0.00.714.823 I llama_context: graph splits = 2
0.00.714.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.844 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.01.665.007 I llama_context: constructing llama_context
0.01.665.016 I llama_context: n_seq_max     = 1
0.01.665.017 I llama_context: n_ctx         = 2048
0.01.665.018 I llama_context: n_ctx_per_seq = 2048
0.01.665.018 I llama_context: n_batch       = 2048
0.01.665.019 I llama_context: n_ubatch      = 512
0.01.665.019 I llama_context: causal_attn   = 1
0.01.665.019 I llama_context: flash_attn    = 0
0.01.665.025 I llama_context: freq_base     = 10000.0
0.01.665.026 I llama_context: freq_scale    = 1
0.01.665.093 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.01.665.102 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.668.202 I init:      CUDA0 KV buffer size =   640.00 MiB
0.01.668.213 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.685.428 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.01.685.438 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.01.685.439 I llama_context: graph nodes  = 1287
0.01.685.439 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.02.367.070 I llama_context: constructing llama_context
0.02.367.080 I llama_context: n_seq_max     = 1
0.02.367.081 I llama_context: n_ctx         = 2048
0.02.367.082 I llama_context: n_ctx_per_seq = 2048
0.02.367.082 I llama_context: n_batch       = 2048
0.02.367.083 I llama_context: n_ubatch      = 512
0.02.367.083 I llama_context: causal_attn   = 1
0.02.367.084 I llama_context: flash_attn    = 0
0.02.367.090 I llama_context: freq_base     = 10000.0
0.02.367.091 I llama_context: freq_scale    = 1
0.02.367.165 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.02.367.174 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.02.370.347 I init:      CUDA0 KV buffer size =   640.00 MiB
0.02.370.357 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.02.387.453 I llama_context:      CUDA0 compute buffer size =   162.00 MiB
0.02.387.464 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.02.387.465 I llama_context: graph nodes  = 1287
0.02.387.465 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st


second run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st


single seq run: The quick brown fox
     Lives, in the fox-hole, on the kitchen-st

real	0m4.542s
user	0m3.843s
sys	0m0.696s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla V100-PCIE-16GB, compute capability 7.0, VMM: yes
0.00.000.297 I build: 4912 (810e0af3f) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu
0.00.253.549 I llama_model_load_from_file_impl: using device CUDA0 (Tesla V100-PCIE-16GB) - 15841 MiB free
0.00.269.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 388 tensors from ../models-mnt/pythia/2.8B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.269.606 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.269.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.269.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.269.618 I llama_model_loader: - kv   2:                               general.name str              = 2.8B
0.00.269.620 I llama_model_loader: - kv   3:                           general.finetune str              = 2.8B
0.00.269.620 I llama_model_loader: - kv   4:                         general.size_label str              = 2.8B
0.00.269.624 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.269.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2560
0.00.269.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 32
0.00.269.628 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 10240
0.00.269.629 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 20
0.00.269.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 32
0.00.269.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.269.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.269.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.269.641 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.277.528 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.279.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.286.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.286.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.286.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.286.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.286.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.286.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.286.069 I llama_model_loader: - type  f32:  258 tensors
0.00.286.070 I llama_model_loader: - type q4_0:  129 tensors
0.00.286.071 I llama_model_loader: - type q6_K:    1 tensors
0.00.286.074 I print_info: file format = GGUF V3 (latest)
0.00.286.076 I print_info: file type   = Q4_0
0.00.286.080 I print_info: file size   = 1.49 GiB (4.61 BPW) 
0.00.331.729 I load: special tokens cache size = 25
0.00.354.237 I load: token to piece cache size = 0.2984 MB
0.00.354.259 I print_info: arch             = gptneox
0.00.354.260 I print_info: vocab_only       = 0
0.00.354.261 I print_info: n_ctx_train      = 2048
0.00.354.262 I print_info: n_embd           = 2560
0.00.354.262 I print_info: n_layer          = 32
0.00.354.275 I print_info: n_head           = 32
0.00.354.277 I print_info: n_head_kv        = 32
0.00.354.277 I print_info: n_rot            = 20
0.00.354.278 I print_info: n_swa            = 0
0.00.354.279 I print_info: n_swa_pattern    = 1
0.00.354.280 I print_info: n_embd_head_k    = 80
0.00.354.281 I print_info: n_embd_head_v    = 80
0.00.354.284 I print_info: n_gqa            = 1
0.00.354.286 I print_info: n_embd_k_gqa     = 2560
0.00.354.288 I print_info: n_embd_v_gqa     = 2560
0.00.354.289 I print_info: f_norm_eps       = 1.0e-05
0.00.354.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.354.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.354.291 I print_info: f_max_alibi_bias = 0.0e+00
0.00.354.292 I print_info: f_logit_scale    = 0.0e+00
0.00.354.292 I print_info: f_attn_scale     = 0.0e+00
0.00.354.294 I print_info: n_ff             = 10240
0.00.354.294 I print_info: n_expert         = 0
0.00.354.295 I print_info: n_expert_used    = 0
0.00.354.295 I print_info: causal attn      = 1
0.00.354.297 I print_info: pooling type     = 0
0.00.354.297 I print_info: rope type        = 2
0.00.354.297 I print_info: rope scaling     = linear
0.00.354.299 I print_info: freq_base_train  = 10000.0
0.00.354.300 I print_info: freq_scale_train = 1
0.00.354.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.354.301 I print_info: rope_finetuned   = unknown
0.00.354.301 I print_info: ssm_d_conv       = 0
0.00.354.302 I print_info: ssm_d_inner      = 0
0.00.354.302 I print_info: ssm_d_state      = 0
0.00.354.302 I print_info: ssm_dt_rank      = 0
0.00.354.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.354.305 I print_info: model type       = 2.8B
0.00.354.306 I print_info: model params     = 2.78 B
0.00.354.307 I print_info: general.name     = 2.8B
0.00.354.311 I print_info: vocab type       = BPE
0.00.354.312 I print_info: n_vocab          = 50304
0.00.354.312 I print_info: n_merges         = 50009
0.00.354.313 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.354.313 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.354.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.354.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.354.320 I print_info: LF token         = 187 'Ċ'
0.00.354.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.354.322 I print_info: max token length = 1024
0.00.354.324 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.445.359 I load_tensors: offloading 32 repeating layers to GPU
0.00.445.372 I load_tensors: offloading output layer to GPU
0.00.445.373 I load_tensors: offloaded 33/33 layers to GPU
0.00.445.382 I load_tensors:        CUDA0 model buffer size =  1454.83 MiB
0.00.445.383 I load_tensors:   CPU_Mapped model buffer size =    69.08 MiB
...........................................................................................
0.00.699.360 I llama_context: constructing llama_context
0.00.699.367 I llama_context: n_seq_max     = 1
0.00.699.368 I llama_context: n_ctx         = 2048
0.00.699.369 I llama_context: n_ctx_per_seq = 2048
0.00.699.369 I llama_context: n_batch       = 2048
0.00.699.370 I llama_context: n_ubatch      = 512
0.00.699.371 I llama_context: causal_attn   = 1
0.00.699.371 I llama_context: flash_attn    = 1
0.00.699.376 I llama_context: freq_base     = 10000.0
0.00.699.378 I llama_context: freq_scale    = 1
0.00.700.767 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.700.785 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.701.919 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.701.932 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.718.806 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.00.718.817 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.718.818 I llama_context: graph nodes  = 1160
0.00.718.819 I llama_context: graph splits = 2
0.00.718.827 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 1512799 out of a maximum of 1512799 bytes
0.00.921.958 I llama_context: constructing llama_context
0.00.921.967 I llama_context: n_seq_max     = 1
0.00.921.967 I llama_context: n_ctx         = 2048
0.00.921.968 I llama_context: n_ctx_per_seq = 2048
0.00.921.968 I llama_context: n_batch       = 2048
0.00.921.969 I llama_context: n_ubatch      = 512
0.00.921.969 I llama_context: causal_attn   = 1
0.00.921.970 I llama_context: flash_attn    = 1
0.00.921.975 I llama_context: freq_base     = 10000.0
0.00.921.976 I llama_context: freq_scale    = 1
0.00.922.046 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.00.922.056 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.00.924.863 I init:      CUDA0 KV buffer size =   640.00 MiB
0.00.924.874 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.00.941.295 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.00.941.303 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.00.941.304 I llama_context: graph nodes  = 1160
0.00.941.305 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
0.01.103.642 I llama_context: constructing llama_context
0.01.103.652 I llama_context: n_seq_max     = 1
0.01.103.652 I llama_context: n_ctx         = 2048
0.01.103.653 I llama_context: n_ctx_per_seq = 2048
0.01.103.654 I llama_context: n_batch       = 2048
0.01.103.654 I llama_context: n_ubatch      = 512
0.01.103.655 I llama_context: causal_attn   = 1
0.01.103.655 I llama_context: flash_attn    = 1
0.01.103.660 I llama_context: freq_base     = 10000.0
0.01.103.661 I llama_context: freq_scale    = 1
0.01.103.734 I llama_context:  CUDA_Host  output buffer size =     0.19 MiB
0.01.103.744 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
0.01.106.888 I init:      CUDA0 KV buffer size =   640.00 MiB
0.01.106.899 I llama_context: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
0.01.123.472 I llama_context:      CUDA0 compute buffer size =   103.25 MiB
0.01.123.483 I llama_context:  CUDA_Host compute buffer size =     9.01 MiB
0.01.123.484 I llama_context: graph nodes  = 1160
0.01.123.484 I llama_context: graph splits = 2
main : deserialized state from 1512799 out of a maximum of 1512799 bytes
main : seq 0 copied, 1311532 bytes
main : kv cache cleared
main : seq 1 restored, 1311532 bytes

main : success

first run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see


second run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see


single seq run: The quick brown fox jumped over the fence", "The children are playing in the garden. I see

real	0m1.551s
user	0m0.874s
sys	0m0.665s
