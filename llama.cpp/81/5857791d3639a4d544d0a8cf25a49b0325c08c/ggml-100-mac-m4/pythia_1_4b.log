Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.667s
user	0m0.909s
sys	0m1.250s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking C executable ../bin/test-c
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Built target llava_static
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Built target llava_shared
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Built target test-sampling
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-grammar-parser
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Built target test-log
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Built target test-chat-template
[ 60%] Built target test-gguf
[ 60%] Built target test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Linking CXX executable ../bin/test-rope
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Built target test-quantize-fns
[ 69%] Built target test-quantize-perf
[ 69%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Built target test-rope
[ 69%] Built target llama-embedding
[ 69%] Built target llama-eval-callback
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Built target llama-gbnf-validator
[ 75%] Built target llama-gguf-split
[ 75%] Linking CXX executable ../../bin/llama-imatrix
[ 75%] Linking CXX executable ../../bin/llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-gritlm
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-imatrix
[ 79%] Built target llama-infill
[ 79%] Built target llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-lookup-merge
[ 79%] Generating loading.html.hpp
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-lookup-create
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Generating index.html.gz.hpp
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 81%] Built target llama-cli
[ 81%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Built target llama-lookup-stats
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Built target llama-passkey
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 88%] Built target llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-run
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-speculative-simple
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-tts
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.052s
user	0m6.211s
sys	0m9.680s

main: quantize time =  3327.88 ms
main:    total time =  3327.88 ms

main: quantize time =  1979.35 ms
main:    total time =  1979.35 ms

main: quantize time =  3419.14 ms
main:    total time =  3419.14 ms

main: quantize time =  2664.41 ms
main:    total time =  2664.41 ms

main: quantize time =  3020.40 ms
main:    total time =  3020.40 ms

main: quantize time =  4782.07 ms
main:    total time =  4782.07 ms

main: quantize time =  5688.63 ms
main:    total time =  5688.63 ms

main: quantize time =  6742.61 ms
main:    total time =  6742.61 ms

main: quantize time =  6125.98 ms
main:    total time =  6125.98 ms

main: quantize time =  4398.81 ms
main:    total time =  4398.81 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.143 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.314 I main: llama backend init
0.00.000.321 I main: load the model and apply lora adapter, if any
0.00.031.513 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.046.508 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.537 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.537 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.541 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.547 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.548 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.451 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.452 I llama_model_loader: - type  f32:  194 tensors
0.00.062.453 I llama_model_loader: - type  f16:   98 tensors
0.00.062.454 I print_info: file format = GGUF V3 (latest)
0.00.062.455 I print_info: file type   = all F32 (guessed)
0.00.062.457 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.074.754 I load: special tokens cache size = 25
0.00.082.226 I load: token to piece cache size = 0.2984 MB
0.00.082.229 I print_info: arch             = gptneox
0.00.082.229 I print_info: vocab_only       = 0
0.00.082.229 I print_info: n_ctx_train      = 2048
0.00.082.230 I print_info: n_embd           = 2048
0.00.082.230 I print_info: n_layer          = 24
0.00.082.233 I print_info: n_head           = 16
0.00.082.234 I print_info: n_head_kv        = 16
0.00.082.234 I print_info: n_rot            = 32
0.00.082.234 I print_info: n_swa            = 0
0.00.082.235 I print_info: n_embd_head_k    = 128
0.00.082.235 I print_info: n_embd_head_v    = 128
0.00.082.235 I print_info: n_gqa            = 1
0.00.082.236 I print_info: n_embd_k_gqa     = 2048
0.00.082.237 I print_info: n_embd_v_gqa     = 2048
0.00.082.237 I print_info: f_norm_eps       = 1.0e-05
0.00.082.238 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.238 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.238 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.238 I print_info: f_logit_scale    = 0.0e+00
0.00.082.239 I print_info: n_ff             = 8192
0.00.082.239 I print_info: n_expert         = 0
0.00.082.239 I print_info: n_expert_used    = 0
0.00.082.239 I print_info: causal attn      = 1
0.00.082.239 I print_info: pooling type     = 0
0.00.082.240 I print_info: rope type        = 2
0.00.082.240 I print_info: rope scaling     = linear
0.00.082.245 I print_info: freq_base_train  = 10000.0
0.00.082.246 I print_info: freq_scale_train = 1
0.00.082.247 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.247 I print_info: rope_finetuned   = unknown
0.00.082.247 I print_info: ssm_d_conv       = 0
0.00.082.247 I print_info: ssm_d_inner      = 0
0.00.082.247 I print_info: ssm_d_state      = 0
0.00.082.247 I print_info: ssm_dt_rank      = 0
0.00.082.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.248 I print_info: model type       = 1.4B
0.00.082.248 I print_info: model params     = 1.41 B
0.00.082.248 I print_info: general.name     = 1.4B
0.00.082.249 I print_info: vocab type       = BPE
0.00.082.249 I print_info: n_vocab          = 50304
0.00.082.252 I print_info: n_merges         = 50009
0.00.082.253 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.253 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.253 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.253 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.253 I print_info: LF token         = 128 'Ä'
0.00.082.254 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.254 I print_info: max token length = 1024
0.00.118.092 I load_tensors: offloading 24 repeating layers to GPU
0.00.118.096 I load_tensors: offloading output layer to GPU
0.00.118.096 I load_tensors: offloaded 25/25 layers to GPU
0.00.118.122 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.118.123 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.118.533 I llama_init_from_model: n_seq_max     = 1
0.00.118.534 I llama_init_from_model: n_ctx         = 2048
0.00.118.534 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.118.536 I llama_init_from_model: n_batch       = 2048
0.00.118.536 I llama_init_from_model: n_ubatch      = 512
0.00.118.536 I llama_init_from_model: flash_attn    = 0
0.00.118.537 I llama_init_from_model: freq_base     = 10000.0
0.00.118.537 I llama_init_from_model: freq_scale    = 1
0.00.118.539 I ggml_metal_init: allocating
0.00.118.579 I ggml_metal_init: found device: Apple M4
0.00.118.584 I ggml_metal_init: picking default device: Apple M4
0.00.119.176 I ggml_metal_init: using embedded metal library
0.00.130.665 I ggml_metal_init: GPU name:   Apple M4
0.00.130.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.130.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.130.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.130.667 I ggml_metal_init: simdgroup reduction   = true
0.00.130.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.130.668 I ggml_metal_init: has residency sets    = true
0.00.130.668 I ggml_metal_init: has bfloat            = true
0.00.130.668 I ggml_metal_init: use bfloat            = true
0.00.130.669 I ggml_metal_init: hasUnifiedMemory      = true
0.00.130.670 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.153.563 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.181.827 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.181.835 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.181.859 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.186.390 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.186.393 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.186.393 I llama_init_from_model: graph nodes  = 967
0.00.186.393 I llama_init_from_model: graph splits = 2
0.00.186.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.186.528 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.186.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.258 I main: llama threadpool init, n_threads = 4
0.00.252.300 I 
0.00.252.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.335 I 
0.00.252.518 I sampler seed: 1234
0.00.252.522 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.252.547 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.252.549 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.252.549 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.080.424 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.02.080.425 I llama_perf_context_print:        load time =     219.70 ms
0.02.080.426 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.02.080.426 I llama_perf_context_print:        eval time =    1781.24 ms /    63 runs   (   28.27 ms per token,    35.37 tokens per second)
0.02.080.427 I llama_perf_context_print:       total time =    1829.20 ms /    70 tokens
0.02.080.647 I ggml_metal_free: deallocating

real	0m2.442s
user	0m0.127s
sys	0m0.130s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.768 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.768 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.721 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.792 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.719 I llama_model_loader: - type  f32:  194 tensors
0.00.033.719 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.720 I print_info: file format = GGUF V3 (latest)
0.00.033.721 I print_info: file type   = Q8_0
0.00.033.722 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.361 I load: special tokens cache size = 25
0.00.049.486 I load: token to piece cache size = 0.2984 MB
0.00.049.491 I print_info: arch             = gptneox
0.00.049.491 I print_info: vocab_only       = 0
0.00.049.491 I print_info: n_ctx_train      = 2048
0.00.049.492 I print_info: n_embd           = 2048
0.00.049.492 I print_info: n_layer          = 24
0.00.049.498 I print_info: n_head           = 16
0.00.049.499 I print_info: n_head_kv        = 16
0.00.049.499 I print_info: n_rot            = 32
0.00.049.499 I print_info: n_swa            = 0
0.00.049.499 I print_info: n_embd_head_k    = 128
0.00.049.501 I print_info: n_embd_head_v    = 128
0.00.049.502 I print_info: n_gqa            = 1
0.00.049.503 I print_info: n_embd_k_gqa     = 2048
0.00.049.504 I print_info: n_embd_v_gqa     = 2048
0.00.049.506 I print_info: f_norm_eps       = 1.0e-05
0.00.049.507 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.507 I print_info: f_logit_scale    = 0.0e+00
0.00.049.509 I print_info: n_ff             = 8192
0.00.049.509 I print_info: n_expert         = 0
0.00.049.509 I print_info: n_expert_used    = 0
0.00.049.509 I print_info: causal attn      = 1
0.00.049.511 I print_info: pooling type     = 0
0.00.049.511 I print_info: rope type        = 2
0.00.049.511 I print_info: rope scaling     = linear
0.00.049.511 I print_info: freq_base_train  = 10000.0
0.00.049.512 I print_info: freq_scale_train = 1
0.00.049.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.512 I print_info: rope_finetuned   = unknown
0.00.049.512 I print_info: ssm_d_conv       = 0
0.00.049.512 I print_info: ssm_d_inner      = 0
0.00.049.513 I print_info: ssm_d_state      = 0
0.00.049.513 I print_info: ssm_dt_rank      = 0
0.00.049.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.514 I print_info: model type       = 1.4B
0.00.049.514 I print_info: model params     = 1.41 B
0.00.049.514 I print_info: general.name     = 1.4B
0.00.049.515 I print_info: vocab type       = BPE
0.00.049.515 I print_info: n_vocab          = 50304
0.00.049.515 I print_info: n_merges         = 50009
0.00.049.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.516 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.516 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.516 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.516 I print_info: LF token         = 128 'Ä'
0.00.049.516 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.517 I print_info: max token length = 1024
0.01.117.302 I load_tensors: offloading 24 repeating layers to GPU
0.01.117.306 I load_tensors: offloading output layer to GPU
0.01.117.306 I load_tensors: offloaded 25/25 layers to GPU
0.01.117.330 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.117.331 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.118.012 I llama_init_from_model: n_seq_max     = 1
0.01.118.014 I llama_init_from_model: n_ctx         = 2048
0.01.118.014 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.118.015 I llama_init_from_model: n_batch       = 2048
0.01.118.015 I llama_init_from_model: n_ubatch      = 512
0.01.118.015 I llama_init_from_model: flash_attn    = 0
0.01.118.016 I llama_init_from_model: freq_base     = 10000.0
0.01.118.016 I llama_init_from_model: freq_scale    = 1
0.01.118.017 I ggml_metal_init: allocating
0.01.118.054 I ggml_metal_init: found device: Apple M4
0.01.118.064 I ggml_metal_init: picking default device: Apple M4
0.01.119.233 I ggml_metal_init: using embedded metal library
0.01.124.075 I ggml_metal_init: GPU name:   Apple M4
0.01.124.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.124.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.124.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.124.080 I ggml_metal_init: simdgroup reduction   = true
0.01.124.080 I ggml_metal_init: simdgroup matrix mul. = true
0.01.124.080 I ggml_metal_init: has residency sets    = true
0.01.124.080 I ggml_metal_init: has bfloat            = true
0.01.124.080 I ggml_metal_init: use bfloat            = true
0.01.124.081 I ggml_metal_init: hasUnifiedMemory      = true
0.01.124.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.137.720 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.174.537 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.174.544 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.174.566 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.178.835 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.178.836 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.178.837 I llama_init_from_model: graph nodes  = 967
0.01.178.837 I llama_init_from_model: graph splits = 2
0.01.178.842 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.178.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.178.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.235.818 I main: llama threadpool init, n_threads = 4
0.01.235.867 I 
0.01.235.890 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.235.891 I 
0.01.236.043 I sampler seed: 1234
0.01.236.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.236.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.236.059 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.236.059 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.338.621 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48005.41 tokens per second)
0.02.338.622 I llama_perf_context_print:        load time =    1225.00 ms
0.02.338.623 I llama_perf_context_print: prompt eval time =      48.90 ms /     7 tokens (    6.99 ms per token,   143.15 tokens per second)
0.02.338.623 I llama_perf_context_print:        eval time =    1051.14 ms /    63 runs   (   16.68 ms per token,    59.93 tokens per second)
0.02.338.624 I llama_perf_context_print:       total time =    1103.69 ms /    70 tokens
0.02.338.942 I ggml_metal_free: deallocating

real	0m2.356s
user	0m0.108s
sys	0m0.268s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.016.119 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.714 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.715 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.637 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.729 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.616 I llama_model_loader: - type  f32:  194 tensors
0.00.040.616 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.616 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.617 I print_info: file format = GGUF V3 (latest)
0.00.040.617 I print_info: file type   = Q4_0
0.00.040.618 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.048.899 I load: special tokens cache size = 25
0.00.054.973 I load: token to piece cache size = 0.2984 MB
0.00.054.977 I print_info: arch             = gptneox
0.00.054.977 I print_info: vocab_only       = 0
0.00.054.977 I print_info: n_ctx_train      = 2048
0.00.054.977 I print_info: n_embd           = 2048
0.00.054.977 I print_info: n_layer          = 24
0.00.054.982 I print_info: n_head           = 16
0.00.054.983 I print_info: n_head_kv        = 16
0.00.054.983 I print_info: n_rot            = 32
0.00.054.983 I print_info: n_swa            = 0
0.00.054.984 I print_info: n_embd_head_k    = 128
0.00.054.984 I print_info: n_embd_head_v    = 128
0.00.054.985 I print_info: n_gqa            = 1
0.00.054.985 I print_info: n_embd_k_gqa     = 2048
0.00.054.986 I print_info: n_embd_v_gqa     = 2048
0.00.054.986 I print_info: f_norm_eps       = 1.0e-05
0.00.054.987 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.987 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.987 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.987 I print_info: f_logit_scale    = 0.0e+00
0.00.054.988 I print_info: n_ff             = 8192
0.00.054.988 I print_info: n_expert         = 0
0.00.054.988 I print_info: n_expert_used    = 0
0.00.054.988 I print_info: causal attn      = 1
0.00.054.988 I print_info: pooling type     = 0
0.00.054.990 I print_info: rope type        = 2
0.00.054.992 I print_info: rope scaling     = linear
0.00.054.993 I print_info: freq_base_train  = 10000.0
0.00.054.993 I print_info: freq_scale_train = 1
0.00.054.993 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.993 I print_info: rope_finetuned   = unknown
0.00.054.994 I print_info: ssm_d_conv       = 0
0.00.054.994 I print_info: ssm_d_inner      = 0
0.00.054.994 I print_info: ssm_d_state      = 0
0.00.054.994 I print_info: ssm_dt_rank      = 0
0.00.054.994 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.994 I print_info: model type       = 1.4B
0.00.054.995 I print_info: model params     = 1.41 B
0.00.054.995 I print_info: general.name     = 1.4B
0.00.054.995 I print_info: vocab type       = BPE
0.00.054.996 I print_info: n_vocab          = 50304
0.00.054.996 I print_info: n_merges         = 50009
0.00.054.996 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.996 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.996 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.997 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.998 I print_info: LF token         = 128 'Ä'
0.00.054.998 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.998 I print_info: max token length = 1024
0.00.710.077 I load_tensors: offloading 24 repeating layers to GPU
0.00.710.093 I load_tensors: offloading output layer to GPU
0.00.710.094 I load_tensors: offloaded 25/25 layers to GPU
0.00.710.132 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.710.133 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.711.134 I llama_init_from_model: n_seq_max     = 1
0.00.711.140 I llama_init_from_model: n_ctx         = 2048
0.00.711.140 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.711.141 I llama_init_from_model: n_batch       = 2048
0.00.711.141 I llama_init_from_model: n_ubatch      = 512
0.00.711.141 I llama_init_from_model: flash_attn    = 0
0.00.711.143 I llama_init_from_model: freq_base     = 10000.0
0.00.711.144 I llama_init_from_model: freq_scale    = 1
0.00.711.156 I ggml_metal_init: allocating
0.00.711.238 I ggml_metal_init: found device: Apple M4
0.00.711.253 I ggml_metal_init: picking default device: Apple M4
0.00.713.053 I ggml_metal_init: using embedded metal library
0.00.718.858 I ggml_metal_init: GPU name:   Apple M4
0.00.718.867 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.718.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.718.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.718.869 I ggml_metal_init: simdgroup reduction   = true
0.00.718.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.718.870 I ggml_metal_init: has residency sets    = true
0.00.718.870 I ggml_metal_init: has bfloat            = true
0.00.718.871 I ggml_metal_init: use bfloat            = true
0.00.718.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.718.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.738.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.672 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.796.682 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.796.710 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.155 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.158 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.158 I llama_init_from_model: graph nodes  = 967
0.00.801.159 I llama_init_from_model: graph splits = 2
0.00.801.164 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.289 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.289 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.854.204 I main: llama threadpool init, n_threads = 4
0.00.854.254 I 
0.00.854.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.854.281 I 
0.00.854.429 I sampler seed: 1234
0.00.854.434 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.854.445 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.854.445 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.854.445 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.531.430 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.531.431 I llama_perf_context_print:        load time =     837.20 ms
0.01.531.431 I llama_perf_context_print: prompt eval time =      39.50 ms /     7 tokens (    5.64 ms per token,   177.23 tokens per second)
0.01.531.432 I llama_perf_context_print:        eval time =     634.58 ms /    63 runs   (   10.07 ms per token,    99.28 tokens per second)
0.01.531.432 I llama_perf_context_print:       total time =     678.11 ms /    70 tokens
0.01.531.658 I ggml_metal_free: deallocating

real	0m1.562s
user	0m0.111s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.804 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.805 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.807 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.824 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.933 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.026 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.027 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.029 I llama_model_loader: - type  f32:  194 tensors
0.00.036.029 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.030 I print_info: file format = GGUF V3 (latest)
0.00.036.030 I print_info: file type   = Q4_1
0.00.036.031 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.770 I load: special tokens cache size = 25
0.00.052.283 I load: token to piece cache size = 0.2984 MB
0.00.052.286 I print_info: arch             = gptneox
0.00.052.286 I print_info: vocab_only       = 0
0.00.052.286 I print_info: n_ctx_train      = 2048
0.00.052.286 I print_info: n_embd           = 2048
0.00.052.287 I print_info: n_layer          = 24
0.00.052.290 I print_info: n_head           = 16
0.00.052.290 I print_info: n_head_kv        = 16
0.00.052.291 I print_info: n_rot            = 32
0.00.052.291 I print_info: n_swa            = 0
0.00.052.291 I print_info: n_embd_head_k    = 128
0.00.052.291 I print_info: n_embd_head_v    = 128
0.00.052.292 I print_info: n_gqa            = 1
0.00.052.293 I print_info: n_embd_k_gqa     = 2048
0.00.052.294 I print_info: n_embd_v_gqa     = 2048
0.00.052.294 I print_info: f_norm_eps       = 1.0e-05
0.00.052.294 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.295 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.295 I print_info: f_logit_scale    = 0.0e+00
0.00.052.296 I print_info: n_ff             = 8192
0.00.052.296 I print_info: n_expert         = 0
0.00.052.296 I print_info: n_expert_used    = 0
0.00.052.296 I print_info: causal attn      = 1
0.00.052.296 I print_info: pooling type     = 0
0.00.052.298 I print_info: rope type        = 2
0.00.052.298 I print_info: rope scaling     = linear
0.00.052.299 I print_info: freq_base_train  = 10000.0
0.00.052.299 I print_info: freq_scale_train = 1
0.00.052.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.300 I print_info: rope_finetuned   = unknown
0.00.052.300 I print_info: ssm_d_conv       = 0
0.00.052.301 I print_info: ssm_d_inner      = 0
0.00.052.301 I print_info: ssm_d_state      = 0
0.00.052.301 I print_info: ssm_dt_rank      = 0
0.00.052.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.301 I print_info: model type       = 1.4B
0.00.052.301 I print_info: model params     = 1.41 B
0.00.052.302 I print_info: general.name     = 1.4B
0.00.052.302 I print_info: vocab type       = BPE
0.00.052.302 I print_info: n_vocab          = 50304
0.00.052.302 I print_info: n_merges         = 50009
0.00.052.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.303 I print_info: LF token         = 128 'Ä'
0.00.052.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.305 I print_info: max token length = 1024
0.00.631.995 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.004 I load_tensors: offloading output layer to GPU
0.00.632.005 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.038 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.632.040 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.632.936 I llama_init_from_model: n_seq_max     = 1
0.00.632.939 I llama_init_from_model: n_ctx         = 2048
0.00.632.939 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.632.940 I llama_init_from_model: n_batch       = 2048
0.00.632.940 I llama_init_from_model: n_ubatch      = 512
0.00.632.941 I llama_init_from_model: flash_attn    = 0
0.00.632.943 I llama_init_from_model: freq_base     = 10000.0
0.00.632.943 I llama_init_from_model: freq_scale    = 1
0.00.632.945 I ggml_metal_init: allocating
0.00.633.017 I ggml_metal_init: found device: Apple M4
0.00.633.032 I ggml_metal_init: picking default device: Apple M4
0.00.634.793 I ggml_metal_init: using embedded metal library
0.00.641.423 I ggml_metal_init: GPU name:   Apple M4
0.00.641.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.429 I ggml_metal_init: simdgroup reduction   = true
0.00.641.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.430 I ggml_metal_init: has residency sets    = true
0.00.641.430 I ggml_metal_init: has bfloat            = true
0.00.641.431 I ggml_metal_init: use bfloat            = true
0.00.641.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.433 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.776 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.238 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.244 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.620 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.621 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.622 I llama_init_from_model: graph nodes  = 967
0.00.720.622 I llama_init_from_model: graph splits = 2
0.00.720.627 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.771 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.404 I main: llama threadpool init, n_threads = 4
0.00.779.450 I 
0.00.779.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.475 I 
0.00.779.631 I sampler seed: 1234
0.00.779.636 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.656 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.657 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.507.429 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.507.430 I llama_perf_context_print:        load time =     769.64 ms
0.01.507.431 I llama_perf_context_print: prompt eval time =      49.09 ms /     7 tokens (    7.01 ms per token,   142.60 tokens per second)
0.01.507.431 I llama_perf_context_print:        eval time =     675.87 ms /    63 runs   (   10.73 ms per token,    93.21 tokens per second)
0.01.507.432 I llama_perf_context_print:       total time =     728.92 ms /    70 tokens
0.01.507.713 I ggml_metal_free: deallocating

real	0m1.524s
user	0m0.112s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.641 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.470 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.471 I llama_model_loader: - type  f32:  194 tensors
0.00.024.471 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.472 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.472 I print_info: file format = GGUF V3 (latest)
0.00.024.473 I print_info: file type   = Q5_0
0.00.024.474 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.432 I load: special tokens cache size = 25
0.00.038.322 I load: token to piece cache size = 0.2984 MB
0.00.038.325 I print_info: arch             = gptneox
0.00.038.325 I print_info: vocab_only       = 0
0.00.038.325 I print_info: n_ctx_train      = 2048
0.00.038.326 I print_info: n_embd           = 2048
0.00.038.326 I print_info: n_layer          = 24
0.00.038.328 I print_info: n_head           = 16
0.00.038.329 I print_info: n_head_kv        = 16
0.00.038.329 I print_info: n_rot            = 32
0.00.038.330 I print_info: n_swa            = 0
0.00.038.330 I print_info: n_embd_head_k    = 128
0.00.038.330 I print_info: n_embd_head_v    = 128
0.00.038.331 I print_info: n_gqa            = 1
0.00.038.332 I print_info: n_embd_k_gqa     = 2048
0.00.038.332 I print_info: n_embd_v_gqa     = 2048
0.00.038.333 I print_info: f_norm_eps       = 1.0e-05
0.00.038.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.334 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.334 I print_info: f_logit_scale    = 0.0e+00
0.00.038.335 I print_info: n_ff             = 8192
0.00.038.335 I print_info: n_expert         = 0
0.00.038.335 I print_info: n_expert_used    = 0
0.00.038.335 I print_info: causal attn      = 1
0.00.038.335 I print_info: pooling type     = 0
0.00.038.337 I print_info: rope type        = 2
0.00.038.339 I print_info: rope scaling     = linear
0.00.038.339 I print_info: freq_base_train  = 10000.0
0.00.038.340 I print_info: freq_scale_train = 1
0.00.038.340 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.340 I print_info: rope_finetuned   = unknown
0.00.038.340 I print_info: ssm_d_conv       = 0
0.00.038.340 I print_info: ssm_d_inner      = 0
0.00.038.341 I print_info: ssm_d_state      = 0
0.00.038.341 I print_info: ssm_dt_rank      = 0
0.00.038.341 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.341 I print_info: model type       = 1.4B
0.00.038.341 I print_info: model params     = 1.41 B
0.00.038.341 I print_info: general.name     = 1.4B
0.00.038.342 I print_info: vocab type       = BPE
0.00.038.342 I print_info: n_vocab          = 50304
0.00.038.342 I print_info: n_merges         = 50009
0.00.038.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.347 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.348 I print_info: LF token         = 128 'Ä'
0.00.038.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.359 I print_info: max token length = 1024
0.00.641.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.517 I load_tensors: offloading output layer to GPU
0.00.641.518 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.555 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.641.560 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.642.870 I llama_init_from_model: n_seq_max     = 1
0.00.642.875 I llama_init_from_model: n_ctx         = 2048
0.00.642.875 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.642.876 I llama_init_from_model: n_batch       = 2048
0.00.642.876 I llama_init_from_model: n_ubatch      = 512
0.00.642.877 I llama_init_from_model: flash_attn    = 0
0.00.642.879 I llama_init_from_model: freq_base     = 10000.0
0.00.642.879 I llama_init_from_model: freq_scale    = 1
0.00.642.887 I ggml_metal_init: allocating
0.00.642.970 I ggml_metal_init: found device: Apple M4
0.00.642.984 I ggml_metal_init: picking default device: Apple M4
0.00.644.790 I ggml_metal_init: using embedded metal library
0.00.651.247 I ggml_metal_init: GPU name:   Apple M4
0.00.651.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.254 I ggml_metal_init: simdgroup reduction   = true
0.00.651.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.254 I ggml_metal_init: has residency sets    = true
0.00.651.255 I ggml_metal_init: has bfloat            = true
0.00.651.255 I ggml_metal_init: use bfloat            = true
0.00.651.256 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.257 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.345 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.725.514 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.545 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.039 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.730.042 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.730.042 I llama_init_from_model: graph nodes  = 967
0.00.730.042 I llama_init_from_model: graph splits = 2
0.00.730.046 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.730.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.175 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.506 I main: llama threadpool init, n_threads = 4
0.00.790.551 I 
0.00.790.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.575 I 
0.00.790.747 I sampler seed: 1234
0.00.790.751 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.773 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.773 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.773 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.577.262 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.577.263 I llama_perf_context_print:        load time =     780.72 ms
0.01.577.264 I llama_perf_context_print: prompt eval time =      50.80 ms /     7 tokens (    7.26 ms per token,   137.78 tokens per second)
0.01.577.264 I llama_perf_context_print:        eval time =     732.78 ms /    63 runs   (   11.63 ms per token,    85.97 tokens per second)
0.01.577.265 I llama_perf_context_print:       total time =     787.65 ms /    70 tokens
0.01.577.544 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.601 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.360 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.361 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.215 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.943 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.944 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.945 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.946 I llama_model_loader: - type  f32:  194 tensors
0.00.026.946 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.947 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.947 I print_info: file format = GGUF V3 (latest)
0.00.026.948 I print_info: file type   = Q5_1
0.00.026.948 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.126 I load: special tokens cache size = 25
0.00.041.036 I load: token to piece cache size = 0.2984 MB
0.00.041.040 I print_info: arch             = gptneox
0.00.041.040 I print_info: vocab_only       = 0
0.00.041.041 I print_info: n_ctx_train      = 2048
0.00.041.041 I print_info: n_embd           = 2048
0.00.041.041 I print_info: n_layer          = 24
0.00.041.048 I print_info: n_head           = 16
0.00.041.049 I print_info: n_head_kv        = 16
0.00.041.049 I print_info: n_rot            = 32
0.00.041.049 I print_info: n_swa            = 0
0.00.041.050 I print_info: n_embd_head_k    = 128
0.00.041.050 I print_info: n_embd_head_v    = 128
0.00.041.050 I print_info: n_gqa            = 1
0.00.041.051 I print_info: n_embd_k_gqa     = 2048
0.00.041.052 I print_info: n_embd_v_gqa     = 2048
0.00.041.052 I print_info: f_norm_eps       = 1.0e-05
0.00.041.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.053 I print_info: f_logit_scale    = 0.0e+00
0.00.041.054 I print_info: n_ff             = 8192
0.00.041.054 I print_info: n_expert         = 0
0.00.041.054 I print_info: n_expert_used    = 0
0.00.041.055 I print_info: causal attn      = 1
0.00.041.055 I print_info: pooling type     = 0
0.00.041.055 I print_info: rope type        = 2
0.00.041.055 I print_info: rope scaling     = linear
0.00.041.055 I print_info: freq_base_train  = 10000.0
0.00.041.057 I print_info: freq_scale_train = 1
0.00.041.057 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.058 I print_info: rope_finetuned   = unknown
0.00.041.058 I print_info: ssm_d_conv       = 0
0.00.041.058 I print_info: ssm_d_inner      = 0
0.00.041.058 I print_info: ssm_d_state      = 0
0.00.041.058 I print_info: ssm_dt_rank      = 0
0.00.041.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.059 I print_info: model type       = 1.4B
0.00.041.059 I print_info: model params     = 1.41 B
0.00.041.059 I print_info: general.name     = 1.4B
0.00.041.060 I print_info: vocab type       = BPE
0.00.041.069 I print_info: n_vocab          = 50304
0.00.041.072 I print_info: n_merges         = 50009
0.00.041.074 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.074 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.074 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.075 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.076 I print_info: LF token         = 128 'Ä'
0.00.041.077 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.077 I print_info: max token length = 1024
0.00.725.945 I load_tensors: offloading 24 repeating layers to GPU
0.00.725.956 I load_tensors: offloading output layer to GPU
0.00.725.957 I load_tensors: offloaded 25/25 layers to GPU
0.00.725.990 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.725.991 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.727.376 I llama_init_from_model: n_seq_max     = 1
0.00.727.380 I llama_init_from_model: n_ctx         = 2048
0.00.727.381 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.727.381 I llama_init_from_model: n_batch       = 2048
0.00.727.382 I llama_init_from_model: n_ubatch      = 512
0.00.727.382 I llama_init_from_model: flash_attn    = 0
0.00.727.384 I llama_init_from_model: freq_base     = 10000.0
0.00.727.384 I llama_init_from_model: freq_scale    = 1
0.00.727.390 I ggml_metal_init: allocating
0.00.727.439 I ggml_metal_init: found device: Apple M4
0.00.727.460 I ggml_metal_init: picking default device: Apple M4
0.00.729.185 I ggml_metal_init: using embedded metal library
0.00.735.627 I ggml_metal_init: GPU name:   Apple M4
0.00.735.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.735.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.735.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.735.632 I ggml_metal_init: simdgroup reduction   = true
0.00.735.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.735.633 I ggml_metal_init: has residency sets    = true
0.00.735.633 I ggml_metal_init: has bfloat            = true
0.00.735.633 I ggml_metal_init: use bfloat            = true
0.00.735.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.735.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.753.216 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.810.235 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.810.241 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.810.265 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.814.566 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.814.568 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.814.568 I llama_init_from_model: graph nodes  = 967
0.00.814.569 I llama_init_from_model: graph splits = 2
0.00.814.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.814.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.814.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.681 I main: llama threadpool init, n_threads = 4
0.00.875.724 I 
0.00.875.748 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.749 I 
0.00.875.931 I sampler seed: 1234
0.00.875.935 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.947 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.947 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.713.748 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.713.749 I llama_perf_context_print:        load time =     863.19 ms
0.01.713.750 I llama_perf_context_print: prompt eval time =      49.88 ms /     7 tokens (    7.13 ms per token,   140.35 tokens per second)
0.01.713.750 I llama_perf_context_print:        eval time =     785.09 ms /    63 runs   (   12.46 ms per token,    80.25 tokens per second)
0.01.713.751 I llama_perf_context_print:       total time =     838.95 ms /    70 tokens
0.01.714.003 I ggml_metal_free: deallocating

real	0m1.732s
user	0m0.110s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.141 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.999 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.006 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.713 I llama_model_loader: - type  f32:  194 tensors
0.00.024.713 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.713 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.713 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.714 I print_info: file format = GGUF V3 (latest)
0.00.024.715 I print_info: file type   = Q2_K - Medium
0.00.024.715 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.045 I load: special tokens cache size = 25
0.00.039.068 I load: token to piece cache size = 0.2984 MB
0.00.039.071 I print_info: arch             = gptneox
0.00.039.071 I print_info: vocab_only       = 0
0.00.039.071 I print_info: n_ctx_train      = 2048
0.00.039.071 I print_info: n_embd           = 2048
0.00.039.071 I print_info: n_layer          = 24
0.00.039.074 I print_info: n_head           = 16
0.00.039.075 I print_info: n_head_kv        = 16
0.00.039.075 I print_info: n_rot            = 32
0.00.039.075 I print_info: n_swa            = 0
0.00.039.078 I print_info: n_embd_head_k    = 128
0.00.039.078 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.079 I print_info: n_embd_k_gqa     = 2048
0.00.039.084 I print_info: n_embd_v_gqa     = 2048
0.00.039.084 I print_info: f_norm_eps       = 1.0e-05
0.00.039.085 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.085 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.085 I print_info: f_logit_scale    = 0.0e+00
0.00.039.086 I print_info: n_ff             = 8192
0.00.039.086 I print_info: n_expert         = 0
0.00.039.086 I print_info: n_expert_used    = 0
0.00.039.087 I print_info: causal attn      = 1
0.00.039.087 I print_info: pooling type     = 0
0.00.039.087 I print_info: rope type        = 2
0.00.039.087 I print_info: rope scaling     = linear
0.00.039.088 I print_info: freq_base_train  = 10000.0
0.00.039.088 I print_info: freq_scale_train = 1
0.00.039.088 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.088 I print_info: rope_finetuned   = unknown
0.00.039.088 I print_info: ssm_d_conv       = 0
0.00.039.088 I print_info: ssm_d_inner      = 0
0.00.039.089 I print_info: ssm_d_state      = 0
0.00.039.089 I print_info: ssm_dt_rank      = 0
0.00.039.089 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.089 I print_info: model type       = 1.4B
0.00.039.090 I print_info: model params     = 1.41 B
0.00.039.090 I print_info: general.name     = 1.4B
0.00.039.090 I print_info: vocab type       = BPE
0.00.039.091 I print_info: n_vocab          = 50304
0.00.039.091 I print_info: n_merges         = 50009
0.00.039.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: LF token         = 128 'Ä'
0.00.039.093 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.093 I print_info: max token length = 1024
0.00.414.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.414.260 I load_tensors: offloading output layer to GPU
0.00.414.261 I load_tensors: offloaded 25/25 layers to GPU
0.00.414.299 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.414.300 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.415.694 I llama_init_from_model: n_seq_max     = 1
0.00.415.699 I llama_init_from_model: n_ctx         = 2048
0.00.415.699 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.415.700 I llama_init_from_model: n_batch       = 2048
0.00.415.700 I llama_init_from_model: n_ubatch      = 512
0.00.415.700 I llama_init_from_model: flash_attn    = 0
0.00.415.702 I llama_init_from_model: freq_base     = 10000.0
0.00.415.703 I llama_init_from_model: freq_scale    = 1
0.00.415.705 I ggml_metal_init: allocating
0.00.415.828 I ggml_metal_init: found device: Apple M4
0.00.415.843 I ggml_metal_init: picking default device: Apple M4
0.00.417.693 I ggml_metal_init: using embedded metal library
0.00.423.421 I ggml_metal_init: GPU name:   Apple M4
0.00.423.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.423.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.423.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.423.434 I ggml_metal_init: simdgroup reduction   = true
0.00.423.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.423.435 I ggml_metal_init: has residency sets    = true
0.00.423.435 I ggml_metal_init: has bfloat            = true
0.00.423.435 I ggml_metal_init: use bfloat            = true
0.00.423.439 I ggml_metal_init: hasUnifiedMemory      = true
0.00.423.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.444.661 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.494.402 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.494.411 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.494.434 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.499.222 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.499.224 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.499.224 I llama_init_from_model: graph nodes  = 967
0.00.499.224 I llama_init_from_model: graph splits = 2
0.00.499.231 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.499.358 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.499.358 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.662 I main: llama threadpool init, n_threads = 4
0.00.561.706 I 
0.00.561.727 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.561.728 I 
0.00.561.907 I sampler seed: 1234
0.00.561.911 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.561.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.561.952 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.561.955 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.246.848 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.246.848 I llama_perf_context_print:        load time =     551.63 ms
0.01.246.849 I llama_perf_context_print: prompt eval time =      44.42 ms /     7 tokens (    6.35 ms per token,   157.58 tokens per second)
0.01.246.850 I llama_perf_context_print:        eval time =     637.54 ms /    63 runs   (   10.12 ms per token,    98.82 tokens per second)
0.01.246.850 I llama_perf_context_print:       total time =     686.07 ms /    70 tokens
0.01.247.055 I ggml_metal_free: deallocating

real	0m1.264s
user	0m0.114s
sys	0m0.164s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.777 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.432 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.437 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.439 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.439 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.440 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.440 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.442 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.442 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.443 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.444 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.447 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.136 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.137 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.138 I llama_model_loader: - type  f32:  194 tensors
0.00.024.138 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.139 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.139 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.139 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.140 I print_info: file format = GGUF V3 (latest)
0.00.024.140 I print_info: file type   = Q3_K - Medium
0.00.024.141 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.266 I load: special tokens cache size = 25
0.00.038.177 I load: token to piece cache size = 0.2984 MB
0.00.038.180 I print_info: arch             = gptneox
0.00.038.180 I print_info: vocab_only       = 0
0.00.038.180 I print_info: n_ctx_train      = 2048
0.00.038.180 I print_info: n_embd           = 2048
0.00.038.181 I print_info: n_layer          = 24
0.00.038.183 I print_info: n_head           = 16
0.00.038.184 I print_info: n_head_kv        = 16
0.00.038.184 I print_info: n_rot            = 32
0.00.038.184 I print_info: n_swa            = 0
0.00.038.185 I print_info: n_embd_head_k    = 128
0.00.038.185 I print_info: n_embd_head_v    = 128
0.00.038.186 I print_info: n_gqa            = 1
0.00.038.186 I print_info: n_embd_k_gqa     = 2048
0.00.038.187 I print_info: n_embd_v_gqa     = 2048
0.00.038.188 I print_info: f_norm_eps       = 1.0e-05
0.00.038.188 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.188 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.188 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.188 I print_info: f_logit_scale    = 0.0e+00
0.00.038.191 I print_info: n_ff             = 8192
0.00.038.191 I print_info: n_expert         = 0
0.00.038.191 I print_info: n_expert_used    = 0
0.00.038.191 I print_info: causal attn      = 1
0.00.038.192 I print_info: pooling type     = 0
0.00.038.192 I print_info: rope type        = 2
0.00.038.192 I print_info: rope scaling     = linear
0.00.038.192 I print_info: freq_base_train  = 10000.0
0.00.038.193 I print_info: freq_scale_train = 1
0.00.038.193 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.193 I print_info: rope_finetuned   = unknown
0.00.038.194 I print_info: ssm_d_conv       = 0
0.00.038.195 I print_info: ssm_d_inner      = 0
0.00.038.195 I print_info: ssm_d_state      = 0
0.00.038.195 I print_info: ssm_dt_rank      = 0
0.00.038.195 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.195 I print_info: model type       = 1.4B
0.00.038.196 I print_info: model params     = 1.41 B
0.00.038.196 I print_info: general.name     = 1.4B
0.00.038.196 I print_info: vocab type       = BPE
0.00.038.196 I print_info: n_vocab          = 50304
0.00.038.197 I print_info: n_merges         = 50009
0.00.038.198 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.198 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.198 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.199 I print_info: LF token         = 128 'Ä'
0.00.038.199 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.199 I print_info: max token length = 1024
0.00.484.330 I load_tensors: offloading 24 repeating layers to GPU
0.00.484.341 I load_tensors: offloading output layer to GPU
0.00.484.342 I load_tensors: offloaded 25/25 layers to GPU
0.00.484.378 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.484.379 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.485.977 I llama_init_from_model: n_seq_max     = 1
0.00.485.981 I llama_init_from_model: n_ctx         = 2048
0.00.485.981 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.485.982 I llama_init_from_model: n_batch       = 2048
0.00.485.982 I llama_init_from_model: n_ubatch      = 512
0.00.485.982 I llama_init_from_model: flash_attn    = 0
0.00.485.987 I llama_init_from_model: freq_base     = 10000.0
0.00.485.996 I llama_init_from_model: freq_scale    = 1
0.00.485.999 I ggml_metal_init: allocating
0.00.486.102 I ggml_metal_init: found device: Apple M4
0.00.486.116 I ggml_metal_init: picking default device: Apple M4
0.00.487.946 I ggml_metal_init: using embedded metal library
0.00.493.481 I ggml_metal_init: GPU name:   Apple M4
0.00.493.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.493.487 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.493.488 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.493.489 I ggml_metal_init: simdgroup reduction   = true
0.00.493.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.493.490 I ggml_metal_init: has residency sets    = true
0.00.493.490 I ggml_metal_init: has bfloat            = true
0.00.493.490 I ggml_metal_init: use bfloat            = true
0.00.493.491 I ggml_metal_init: hasUnifiedMemory      = true
0.00.493.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.513.348 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.568.835 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.568.845 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.568.879 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.572.780 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.572.782 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.572.782 I llama_init_from_model: graph nodes  = 967
0.00.572.782 I llama_init_from_model: graph splits = 2
0.00.572.792 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.572.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.572.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.794 I main: llama threadpool init, n_threads = 4
0.00.629.840 I 
0.00.629.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.866 I 
0.00.630.021 I sampler seed: 1234
0.00.630.026 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.052 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.052 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.377.887 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.377.888 I llama_perf_context_print:        load time =     620.12 ms
0.01.377.889 I llama_perf_context_print: prompt eval time =      50.08 ms /     7 tokens (    7.15 ms per token,   139.78 tokens per second)
0.01.377.889 I llama_perf_context_print:        eval time =     694.72 ms /    63 runs   (   11.03 ms per token,    90.68 tokens per second)
0.01.377.890 I llama_perf_context_print:       total time =     748.99 ms /    70 tokens
0.01.378.115 I ggml_metal_free: deallocating

real	0m1.397s
user	0m0.110s
sys	0m0.201s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.997 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.592 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.595 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.595 I llama_model_loader: - type  f32:  194 tensors
0.00.024.595 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.596 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.596 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.596 I print_info: file format = GGUF V3 (latest)
0.00.024.597 I print_info: file type   = Q4_K - Medium
0.00.024.597 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.540 I load: special tokens cache size = 25
0.00.038.491 I load: token to piece cache size = 0.2984 MB
0.00.038.493 I print_info: arch             = gptneox
0.00.038.494 I print_info: vocab_only       = 0
0.00.038.494 I print_info: n_ctx_train      = 2048
0.00.038.494 I print_info: n_embd           = 2048
0.00.038.494 I print_info: n_layer          = 24
0.00.038.498 I print_info: n_head           = 16
0.00.038.498 I print_info: n_head_kv        = 16
0.00.038.499 I print_info: n_rot            = 32
0.00.038.499 I print_info: n_swa            = 0
0.00.038.499 I print_info: n_embd_head_k    = 128
0.00.038.499 I print_info: n_embd_head_v    = 128
0.00.038.500 I print_info: n_gqa            = 1
0.00.038.501 I print_info: n_embd_k_gqa     = 2048
0.00.038.501 I print_info: n_embd_v_gqa     = 2048
0.00.038.502 I print_info: f_norm_eps       = 1.0e-05
0.00.038.502 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.503 I print_info: f_logit_scale    = 0.0e+00
0.00.038.504 I print_info: n_ff             = 8192
0.00.038.504 I print_info: n_expert         = 0
0.00.038.504 I print_info: n_expert_used    = 0
0.00.038.504 I print_info: causal attn      = 1
0.00.038.504 I print_info: pooling type     = 0
0.00.038.504 I print_info: rope type        = 2
0.00.038.504 I print_info: rope scaling     = linear
0.00.038.505 I print_info: freq_base_train  = 10000.0
0.00.038.505 I print_info: freq_scale_train = 1
0.00.038.505 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.506 I print_info: rope_finetuned   = unknown
0.00.038.506 I print_info: ssm_d_conv       = 0
0.00.038.506 I print_info: ssm_d_inner      = 0
0.00.038.508 I print_info: ssm_d_state      = 0
0.00.038.509 I print_info: ssm_dt_rank      = 0
0.00.038.509 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.509 I print_info: model type       = 1.4B
0.00.038.509 I print_info: model params     = 1.41 B
0.00.038.509 I print_info: general.name     = 1.4B
0.00.038.510 I print_info: vocab type       = BPE
0.00.038.510 I print_info: n_vocab          = 50304
0.00.038.510 I print_info: n_merges         = 50009
0.00.038.511 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.511 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.511 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.511 I print_info: LF token         = 128 'Ä'
0.00.038.516 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.516 I print_info: max token length = 1024
0.00.527.321 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.333 I load_tensors: offloading output layer to GPU
0.00.527.334 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.365 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.367 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.528.709 I llama_init_from_model: n_seq_max     = 1
0.00.528.717 I llama_init_from_model: n_ctx         = 2048
0.00.528.717 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.528.718 I llama_init_from_model: n_batch       = 2048
0.00.528.718 I llama_init_from_model: n_ubatch      = 512
0.00.528.719 I llama_init_from_model: flash_attn    = 0
0.00.528.719 I llama_init_from_model: freq_base     = 10000.0
0.00.528.720 I llama_init_from_model: freq_scale    = 1
0.00.528.723 I ggml_metal_init: allocating
0.00.528.791 I ggml_metal_init: found device: Apple M4
0.00.528.805 I ggml_metal_init: picking default device: Apple M4
0.00.530.504 I ggml_metal_init: using embedded metal library
0.00.536.188 I ggml_metal_init: GPU name:   Apple M4
0.00.536.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.204 I ggml_metal_init: simdgroup reduction   = true
0.00.536.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.205 I ggml_metal_init: has residency sets    = true
0.00.536.205 I ggml_metal_init: has bfloat            = true
0.00.536.205 I ggml_metal_init: use bfloat            = true
0.00.536.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.688 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.608.496 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.608.502 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.608.537 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.058 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.614.060 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.614.060 I llama_init_from_model: graph nodes  = 967
0.00.614.061 I llama_init_from_model: graph splits = 2
0.00.614.065 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.614.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.614.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.451 I main: llama threadpool init, n_threads = 4
0.00.672.494 I 
0.00.672.518 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.519 I 
0.00.672.700 I sampler seed: 1234
0.00.672.704 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.672.742 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.672.745 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.672.745 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.428.714 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47523.43 tokens per second)
0.01.428.715 I llama_perf_context_print:        load time =     662.00 ms
0.01.428.716 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.51 tokens per second)
0.01.428.717 I llama_perf_context_print:        eval time =     706.08 ms /    63 runs   (   11.21 ms per token,    89.22 tokens per second)
0.01.428.717 I llama_perf_context_print:       total time =     757.21 ms /    70 tokens
0.01.428.961 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.113s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.887 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.410 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.415 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.818 I llama_model_loader: - type  f32:  194 tensors
0.00.025.818 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.818 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.819 I print_info: file format = GGUF V3 (latest)
0.00.025.819 I print_info: file type   = Q5_K - Medium
0.00.025.820 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.028 I load: special tokens cache size = 25
0.00.040.075 I load: token to piece cache size = 0.2984 MB
0.00.040.078 I print_info: arch             = gptneox
0.00.040.078 I print_info: vocab_only       = 0
0.00.040.078 I print_info: n_ctx_train      = 2048
0.00.040.078 I print_info: n_embd           = 2048
0.00.040.078 I print_info: n_layer          = 24
0.00.040.081 I print_info: n_head           = 16
0.00.040.082 I print_info: n_head_kv        = 16
0.00.040.082 I print_info: n_rot            = 32
0.00.040.082 I print_info: n_swa            = 0
0.00.040.083 I print_info: n_embd_head_k    = 128
0.00.040.083 I print_info: n_embd_head_v    = 128
0.00.040.084 I print_info: n_gqa            = 1
0.00.040.084 I print_info: n_embd_k_gqa     = 2048
0.00.040.085 I print_info: n_embd_v_gqa     = 2048
0.00.040.086 I print_info: f_norm_eps       = 1.0e-05
0.00.040.086 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.086 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.086 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.086 I print_info: f_logit_scale    = 0.0e+00
0.00.040.087 I print_info: n_ff             = 8192
0.00.040.089 I print_info: n_expert         = 0
0.00.040.089 I print_info: n_expert_used    = 0
0.00.040.089 I print_info: causal attn      = 1
0.00.040.089 I print_info: pooling type     = 0
0.00.040.089 I print_info: rope type        = 2
0.00.040.089 I print_info: rope scaling     = linear
0.00.040.090 I print_info: freq_base_train  = 10000.0
0.00.040.090 I print_info: freq_scale_train = 1
0.00.040.090 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.091 I print_info: rope_finetuned   = unknown
0.00.040.091 I print_info: ssm_d_conv       = 0
0.00.040.091 I print_info: ssm_d_inner      = 0
0.00.040.091 I print_info: ssm_d_state      = 0
0.00.040.091 I print_info: ssm_dt_rank      = 0
0.00.040.091 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.092 I print_info: model type       = 1.4B
0.00.040.092 I print_info: model params     = 1.41 B
0.00.040.092 I print_info: general.name     = 1.4B
0.00.040.093 I print_info: vocab type       = BPE
0.00.040.093 I print_info: n_vocab          = 50304
0.00.040.093 I print_info: n_merges         = 50009
0.00.040.093 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.094 I print_info: LF token         = 128 'Ä'
0.00.040.095 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.095 I print_info: max token length = 1024
0.00.654.252 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.263 I load_tensors: offloading output layer to GPU
0.00.654.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.290 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.654.291 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.655.712 I llama_init_from_model: n_seq_max     = 1
0.00.655.719 I llama_init_from_model: n_ctx         = 2048
0.00.655.720 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.720 I llama_init_from_model: n_batch       = 2048
0.00.655.721 I llama_init_from_model: n_ubatch      = 512
0.00.655.721 I llama_init_from_model: flash_attn    = 0
0.00.655.722 I llama_init_from_model: freq_base     = 10000.0
0.00.655.722 I llama_init_from_model: freq_scale    = 1
0.00.655.728 I ggml_metal_init: allocating
0.00.655.790 I ggml_metal_init: found device: Apple M4
0.00.655.804 I ggml_metal_init: picking default device: Apple M4
0.00.657.524 I ggml_metal_init: using embedded metal library
0.00.664.033 I ggml_metal_init: GPU name:   Apple M4
0.00.664.037 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.038 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.039 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.039 I ggml_metal_init: simdgroup reduction   = true
0.00.664.040 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.040 I ggml_metal_init: has residency sets    = true
0.00.664.040 I ggml_metal_init: has bfloat            = true
0.00.664.040 I ggml_metal_init: use bfloat            = true
0.00.664.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.586 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.511 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.517 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.538 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.035 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.037 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.037 I llama_init_from_model: graph nodes  = 967
0.00.748.038 I llama_init_from_model: graph splits = 2
0.00.748.047 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.180 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.559 I main: llama threadpool init, n_threads = 4
0.00.811.603 I 
0.00.811.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.635 I 
0.00.811.819 I sampler seed: 1234
0.00.811.823 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.867 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.870 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.870 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.656.519 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.656.520 I llama_perf_context_print:        load time =     801.77 ms
0.01.656.521 I llama_perf_context_print: prompt eval time =      51.71 ms /     7 tokens (    7.39 ms per token,   135.37 tokens per second)
0.01.656.521 I llama_perf_context_print:        eval time =     790.02 ms /    63 runs   (   12.54 ms per token,    79.74 tokens per second)
0.01.656.522 I llama_perf_context_print:       total time =     845.86 ms /    70 tokens
0.01.656.823 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.110s
sys	0m0.231s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.788 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.476 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.476 I llama_model_loader: - type  f32:  194 tensors
0.00.024.477 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.477 I print_info: file format = GGUF V3 (latest)
0.00.024.478 I print_info: file type   = Q6_K
0.00.024.478 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.648 I load: special tokens cache size = 25
0.00.038.530 I load: token to piece cache size = 0.2984 MB
0.00.038.533 I print_info: arch             = gptneox
0.00.038.533 I print_info: vocab_only       = 0
0.00.038.534 I print_info: n_ctx_train      = 2048
0.00.038.534 I print_info: n_embd           = 2048
0.00.038.534 I print_info: n_layer          = 24
0.00.038.537 I print_info: n_head           = 16
0.00.038.538 I print_info: n_head_kv        = 16
0.00.038.538 I print_info: n_rot            = 32
0.00.038.540 I print_info: n_swa            = 0
0.00.038.540 I print_info: n_embd_head_k    = 128
0.00.038.540 I print_info: n_embd_head_v    = 128
0.00.038.541 I print_info: n_gqa            = 1
0.00.038.542 I print_info: n_embd_k_gqa     = 2048
0.00.038.543 I print_info: n_embd_v_gqa     = 2048
0.00.038.543 I print_info: f_norm_eps       = 1.0e-05
0.00.038.544 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.544 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.544 I print_info: f_logit_scale    = 0.0e+00
0.00.038.545 I print_info: n_ff             = 8192
0.00.038.545 I print_info: n_expert         = 0
0.00.038.545 I print_info: n_expert_used    = 0
0.00.038.545 I print_info: causal attn      = 1
0.00.038.545 I print_info: pooling type     = 0
0.00.038.545 I print_info: rope type        = 2
0.00.038.547 I print_info: rope scaling     = linear
0.00.038.548 I print_info: freq_base_train  = 10000.0
0.00.038.549 I print_info: freq_scale_train = 1
0.00.038.549 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.549 I print_info: rope_finetuned   = unknown
0.00.038.549 I print_info: ssm_d_conv       = 0
0.00.038.549 I print_info: ssm_d_inner      = 0
0.00.038.549 I print_info: ssm_d_state      = 0
0.00.038.549 I print_info: ssm_dt_rank      = 0
0.00.038.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.554 I print_info: model type       = 1.4B
0.00.038.555 I print_info: model params     = 1.41 B
0.00.038.555 I print_info: general.name     = 1.4B
0.00.038.555 I print_info: vocab type       = BPE
0.00.038.556 I print_info: n_vocab          = 50304
0.00.038.557 I print_info: n_merges         = 50009
0.00.038.557 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.558 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.558 I print_info: LF token         = 128 'Ä'
0.00.038.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.558 I print_info: max token length = 1024
0.00.658.838 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.845 I load_tensors: offloading output layer to GPU
0.00.658.846 I load_tensors: offloaded 25/25 layers to GPU
0.00.658.885 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.658.888 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.660.208 I llama_init_from_model: n_seq_max     = 1
0.00.660.213 I llama_init_from_model: n_ctx         = 2048
0.00.660.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.214 I llama_init_from_model: n_batch       = 2048
0.00.660.214 I llama_init_from_model: n_ubatch      = 512
0.00.660.215 I llama_init_from_model: flash_attn    = 0
0.00.660.216 I llama_init_from_model: freq_base     = 10000.0
0.00.660.216 I llama_init_from_model: freq_scale    = 1
0.00.660.219 I ggml_metal_init: allocating
0.00.660.287 I ggml_metal_init: found device: Apple M4
0.00.660.300 I ggml_metal_init: picking default device: Apple M4
0.00.661.944 I ggml_metal_init: using embedded metal library
0.00.667.939 I ggml_metal_init: GPU name:   Apple M4
0.00.667.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.947 I ggml_metal_init: simdgroup reduction   = true
0.00.667.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.948 I ggml_metal_init: has residency sets    = true
0.00.667.948 I ggml_metal_init: has bfloat            = true
0.00.667.949 I ggml_metal_init: use bfloat            = true
0.00.667.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.959 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.111 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.197 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.221 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.695 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.697 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.698 I llama_init_from_model: graph nodes  = 967
0.00.750.698 I llama_init_from_model: graph splits = 2
0.00.750.704 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.830 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.830 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.818.882 I main: llama threadpool init, n_threads = 4
0.00.818.927 I 
0.00.818.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.818.952 I 
0.00.819.127 I sampler seed: 1234
0.00.819.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.819.152 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.819.152 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.819.152 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.690.458 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.690.458 I llama_perf_context_print:        load time =     809.10 ms
0.01.690.460 I llama_perf_context_print: prompt eval time =      54.07 ms /     7 tokens (    7.72 ms per token,   129.46 tokens per second)
0.01.690.461 I llama_perf_context_print:        eval time =     814.32 ms /    63 runs   (   12.93 ms per token,    77.36 tokens per second)
0.01.690.461 I llama_perf_context_print:       total time =     872.48 ms /    70 tokens
0.01.690.719 I ggml_metal_free: deallocating

real	0m1.708s
user	0m0.110s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.558 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.407 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.420 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.421 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.423 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.264 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.268 I llama_model_loader: - type  f32:  194 tensors
0.00.056.269 I llama_model_loader: - type  f16:   98 tensors
0.00.056.269 I print_info: file format = GGUF V3 (latest)
0.00.056.270 I print_info: file type   = all F32 (guessed)
0.00.056.271 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.406 I load: special tokens cache size = 25
0.00.075.867 I load: token to piece cache size = 0.2984 MB
0.00.075.870 I print_info: arch             = gptneox
0.00.075.871 I print_info: vocab_only       = 0
0.00.075.871 I print_info: n_ctx_train      = 2048
0.00.075.871 I print_info: n_embd           = 2048
0.00.075.871 I print_info: n_layer          = 24
0.00.075.874 I print_info: n_head           = 16
0.00.075.875 I print_info: n_head_kv        = 16
0.00.075.875 I print_info: n_rot            = 32
0.00.075.876 I print_info: n_swa            = 0
0.00.075.876 I print_info: n_embd_head_k    = 128
0.00.075.876 I print_info: n_embd_head_v    = 128
0.00.075.877 I print_info: n_gqa            = 1
0.00.075.878 I print_info: n_embd_k_gqa     = 2048
0.00.075.878 I print_info: n_embd_v_gqa     = 2048
0.00.075.879 I print_info: f_norm_eps       = 1.0e-05
0.00.075.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.879 I print_info: f_logit_scale    = 0.0e+00
0.00.075.880 I print_info: n_ff             = 8192
0.00.075.880 I print_info: n_expert         = 0
0.00.075.880 I print_info: n_expert_used    = 0
0.00.075.881 I print_info: causal attn      = 1
0.00.075.881 I print_info: pooling type     = 0
0.00.075.881 I print_info: rope type        = 2
0.00.075.881 I print_info: rope scaling     = linear
0.00.075.883 I print_info: freq_base_train  = 10000.0
0.00.075.883 I print_info: freq_scale_train = 1
0.00.075.884 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.884 I print_info: rope_finetuned   = unknown
0.00.075.884 I print_info: ssm_d_conv       = 0
0.00.075.884 I print_info: ssm_d_inner      = 0
0.00.075.884 I print_info: ssm_d_state      = 0
0.00.075.884 I print_info: ssm_dt_rank      = 0
0.00.075.884 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.885 I print_info: model type       = 1.4B
0.00.075.885 I print_info: model params     = 1.41 B
0.00.075.885 I print_info: general.name     = 1.4B
0.00.075.886 I print_info: vocab type       = BPE
0.00.075.886 I print_info: n_vocab          = 50304
0.00.075.886 I print_info: n_merges         = 50009
0.00.075.886 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.886 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.887 I print_info: LF token         = 128 'Ä'
0.00.075.888 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.888 I print_info: max token length = 1024
0.01.411.091 I load_tensors: offloading 24 repeating layers to GPU
0.01.411.095 I load_tensors: offloading output layer to GPU
0.01.411.096 I load_tensors: offloaded 25/25 layers to GPU
0.01.411.122 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.411.123 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.411.857 I llama_init_from_model: n_seq_max     = 1
0.01.411.858 I llama_init_from_model: n_ctx         = 128
0.01.411.858 I llama_init_from_model: n_ctx_per_seq = 128
0.01.411.858 I llama_init_from_model: n_batch       = 128
0.01.411.858 I llama_init_from_model: n_ubatch      = 128
0.01.411.859 I llama_init_from_model: flash_attn    = 0
0.01.411.859 I llama_init_from_model: freq_base     = 10000.0
0.01.411.859 I llama_init_from_model: freq_scale    = 1
0.01.411.860 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.411.863 I ggml_metal_init: allocating
0.01.411.909 I ggml_metal_init: found device: Apple M4
0.01.411.917 I ggml_metal_init: picking default device: Apple M4
0.01.412.917 I ggml_metal_init: using embedded metal library
0.01.416.810 I ggml_metal_init: GPU name:   Apple M4
0.01.416.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.416.813 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.416.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.416.814 I ggml_metal_init: simdgroup reduction   = true
0.01.416.814 I ggml_metal_init: simdgroup matrix mul. = true
0.01.416.814 I ggml_metal_init: has residency sets    = true
0.01.416.814 I ggml_metal_init: has bfloat            = true
0.01.416.815 I ggml_metal_init: use bfloat            = true
0.01.416.815 I ggml_metal_init: hasUnifiedMemory      = true
0.01.416.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.427.608 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.429.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.429.341 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.429.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.431.065 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.431.066 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.431.067 I llama_init_from_model: graph nodes  = 967
0.01.431.067 I llama_init_from_model: graph splits = 2
0.01.431.068 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.431.068 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.466.257 I 
0.01.466.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.466.317 I perplexity: tokenizing the input ..
0.01.471.315 I perplexity: tokenization took 4.996 ms
0.01.471.335 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.589.859 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.591.201 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.591.213 I llama_perf_context_print:        load time =    1442.44 ms
0.01.591.214 I llama_perf_context_print: prompt eval time =     118.22 ms /   128 tokens (    0.92 ms per token,  1082.75 tokens per second)
0.01.591.215 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.591.216 I llama_perf_context_print:       total time =     124.96 ms /   129 tokens
0.01.591.600 I ggml_metal_free: deallocating

real	0m1.804s
user	0m0.098s
sys	0m0.259s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.087 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.278 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.065 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.067 I llama_model_loader: - type  f32:  194 tensors
0.00.025.067 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.068 I print_info: file format = GGUF V3 (latest)
0.00.025.068 I print_info: file type   = Q8_0
0.00.025.069 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.121 I load: special tokens cache size = 25
0.00.039.581 I load: token to piece cache size = 0.2984 MB
0.00.039.585 I print_info: arch             = gptneox
0.00.039.585 I print_info: vocab_only       = 0
0.00.039.586 I print_info: n_ctx_train      = 2048
0.00.039.586 I print_info: n_embd           = 2048
0.00.039.586 I print_info: n_layer          = 24
0.00.039.590 I print_info: n_head           = 16
0.00.039.591 I print_info: n_head_kv        = 16
0.00.039.591 I print_info: n_rot            = 32
0.00.039.592 I print_info: n_swa            = 0
0.00.039.592 I print_info: n_embd_head_k    = 128
0.00.039.592 I print_info: n_embd_head_v    = 128
0.00.039.593 I print_info: n_gqa            = 1
0.00.039.594 I print_info: n_embd_k_gqa     = 2048
0.00.039.594 I print_info: n_embd_v_gqa     = 2048
0.00.039.598 I print_info: f_norm_eps       = 1.0e-05
0.00.039.598 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.599 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.599 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.599 I print_info: f_logit_scale    = 0.0e+00
0.00.039.600 I print_info: n_ff             = 8192
0.00.039.600 I print_info: n_expert         = 0
0.00.039.600 I print_info: n_expert_used    = 0
0.00.039.600 I print_info: causal attn      = 1
0.00.039.601 I print_info: pooling type     = 0
0.00.039.601 I print_info: rope type        = 2
0.00.039.601 I print_info: rope scaling     = linear
0.00.039.602 I print_info: freq_base_train  = 10000.0
0.00.039.602 I print_info: freq_scale_train = 1
0.00.039.602 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.607 I print_info: rope_finetuned   = unknown
0.00.039.607 I print_info: ssm_d_conv       = 0
0.00.039.608 I print_info: ssm_d_inner      = 0
0.00.039.608 I print_info: ssm_d_state      = 0
0.00.039.608 I print_info: ssm_dt_rank      = 0
0.00.039.608 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.608 I print_info: model type       = 1.4B
0.00.039.609 I print_info: model params     = 1.41 B
0.00.039.609 I print_info: general.name     = 1.4B
0.00.039.610 I print_info: vocab type       = BPE
0.00.039.610 I print_info: n_vocab          = 50304
0.00.039.610 I print_info: n_merges         = 50009
0.00.039.610 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: LF token         = 128 'Ä'
0.00.039.612 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: max token length = 1024
0.00.777.729 I load_tensors: offloading 24 repeating layers to GPU
0.00.777.733 I load_tensors: offloading output layer to GPU
0.00.777.734 I load_tensors: offloaded 25/25 layers to GPU
0.00.777.763 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.777.765 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.778.997 I llama_init_from_model: n_seq_max     = 1
0.00.778.999 I llama_init_from_model: n_ctx         = 128
0.00.778.999 I llama_init_from_model: n_ctx_per_seq = 128
0.00.778.999 I llama_init_from_model: n_batch       = 128
0.00.779.004 I llama_init_from_model: n_ubatch      = 128
0.00.779.006 I llama_init_from_model: flash_attn    = 0
0.00.779.007 I llama_init_from_model: freq_base     = 10000.0
0.00.779.007 I llama_init_from_model: freq_scale    = 1
0.00.779.008 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.779.009 I ggml_metal_init: allocating
0.00.779.086 I ggml_metal_init: found device: Apple M4
0.00.779.095 I ggml_metal_init: picking default device: Apple M4
0.00.780.303 I ggml_metal_init: using embedded metal library
0.00.785.473 I ggml_metal_init: GPU name:   Apple M4
0.00.785.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.785.477 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.785.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.785.478 I ggml_metal_init: simdgroup reduction   = true
0.00.785.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.785.478 I ggml_metal_init: has residency sets    = true
0.00.785.479 I ggml_metal_init: has bfloat            = true
0.00.785.479 I ggml_metal_init: use bfloat            = true
0.00.785.480 I ggml_metal_init: hasUnifiedMemory      = true
0.00.785.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.800.434 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.803.780 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.803.783 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.803.811 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.806.924 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.806.925 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.806.926 I llama_init_from_model: graph nodes  = 967
0.00.806.926 I llama_init_from_model: graph splits = 2
0.00.806.929 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.806.929 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.167 I 
0.00.834.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.261 I perplexity: tokenizing the input ..
0.00.841.205 I perplexity: tokenization took 6.942 ms
0.00.841.223 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.978.775 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.980.205 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.980.226 I llama_perf_context_print:        load time =     825.07 ms
0.00.980.227 I llama_perf_context_print: prompt eval time =     136.64 ms /   128 tokens (    1.07 ms per token,   936.76 tokens per second)
0.00.980.228 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.980.228 I llama_perf_context_print:       total time =     146.06 ms /   129 tokens
0.00.980.600 I ggml_metal_free: deallocating

real	0m0.996s
user	0m0.076s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.889 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.893 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.895 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.896 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.538 I llama_model_loader: - type  f32:  194 tensors
0.00.026.539 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.539 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.540 I print_info: file format = GGUF V3 (latest)
0.00.026.540 I print_info: file type   = Q4_0
0.00.026.541 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.727 I load: special tokens cache size = 25
0.00.040.664 I load: token to piece cache size = 0.2984 MB
0.00.040.667 I print_info: arch             = gptneox
0.00.040.667 I print_info: vocab_only       = 0
0.00.040.667 I print_info: n_ctx_train      = 2048
0.00.040.667 I print_info: n_embd           = 2048
0.00.040.668 I print_info: n_layer          = 24
0.00.040.670 I print_info: n_head           = 16
0.00.040.671 I print_info: n_head_kv        = 16
0.00.040.672 I print_info: n_rot            = 32
0.00.040.672 I print_info: n_swa            = 0
0.00.040.672 I print_info: n_embd_head_k    = 128
0.00.040.672 I print_info: n_embd_head_v    = 128
0.00.040.673 I print_info: n_gqa            = 1
0.00.040.674 I print_info: n_embd_k_gqa     = 2048
0.00.040.674 I print_info: n_embd_v_gqa     = 2048
0.00.040.675 I print_info: f_norm_eps       = 1.0e-05
0.00.040.675 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.675 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.676 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.676 I print_info: f_logit_scale    = 0.0e+00
0.00.040.677 I print_info: n_ff             = 8192
0.00.040.677 I print_info: n_expert         = 0
0.00.040.677 I print_info: n_expert_used    = 0
0.00.040.677 I print_info: causal attn      = 1
0.00.040.677 I print_info: pooling type     = 0
0.00.040.677 I print_info: rope type        = 2
0.00.040.678 I print_info: rope scaling     = linear
0.00.040.680 I print_info: freq_base_train  = 10000.0
0.00.040.680 I print_info: freq_scale_train = 1
0.00.040.681 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.681 I print_info: rope_finetuned   = unknown
0.00.040.681 I print_info: ssm_d_conv       = 0
0.00.040.681 I print_info: ssm_d_inner      = 0
0.00.040.681 I print_info: ssm_d_state      = 0
0.00.040.681 I print_info: ssm_dt_rank      = 0
0.00.040.682 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.682 I print_info: model type       = 1.4B
0.00.040.682 I print_info: model params     = 1.41 B
0.00.040.682 I print_info: general.name     = 1.4B
0.00.040.683 I print_info: vocab type       = BPE
0.00.040.683 I print_info: n_vocab          = 50304
0.00.040.683 I print_info: n_merges         = 50009
0.00.040.687 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.688 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.690 I print_info: LF token         = 128 'Ä'
0.00.040.690 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.690 I print_info: max token length = 1024
0.00.552.341 I load_tensors: offloading 24 repeating layers to GPU
0.00.552.357 I load_tensors: offloading output layer to GPU
0.00.552.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.552.391 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.552.392 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.553.941 I llama_init_from_model: n_seq_max     = 1
0.00.553.946 I llama_init_from_model: n_ctx         = 128
0.00.553.947 I llama_init_from_model: n_ctx_per_seq = 128
0.00.553.951 I llama_init_from_model: n_batch       = 128
0.00.553.952 I llama_init_from_model: n_ubatch      = 128
0.00.553.952 I llama_init_from_model: flash_attn    = 0
0.00.553.955 I llama_init_from_model: freq_base     = 10000.0
0.00.553.955 I llama_init_from_model: freq_scale    = 1
0.00.553.956 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.553.958 I ggml_metal_init: allocating
0.00.554.037 I ggml_metal_init: found device: Apple M4
0.00.554.051 I ggml_metal_init: picking default device: Apple M4
0.00.555.857 I ggml_metal_init: using embedded metal library
0.00.562.482 I ggml_metal_init: GPU name:   Apple M4
0.00.562.487 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.562.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.562.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.562.490 I ggml_metal_init: simdgroup reduction   = true
0.00.562.490 I ggml_metal_init: simdgroup matrix mul. = true
0.00.562.491 I ggml_metal_init: has residency sets    = true
0.00.562.491 I ggml_metal_init: has bfloat            = true
0.00.562.491 I ggml_metal_init: use bfloat            = true
0.00.562.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.562.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.580.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.584.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.584.310 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.584.337 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.587.563 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.587.565 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.587.565 I llama_init_from_model: graph nodes  = 967
0.00.587.566 I llama_init_from_model: graph splits = 2
0.00.587.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.587.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.278 I 
0.00.615.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.393 I perplexity: tokenizing the input ..
0.00.622.645 I perplexity: tokenization took 7.248 ms
0.00.622.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.862 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.216 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.230 I llama_perf_context_print:        load time =     604.45 ms
0.00.761.231 I llama_perf_context_print: prompt eval time =     136.24 ms /   128 tokens (    1.06 ms per token,   939.55 tokens per second)
0.00.761.232 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.232 I llama_perf_context_print:       total time =     145.96 ms /   129 tokens
0.00.761.620 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.096 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.102 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.103 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.105 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.841 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.842 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.843 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.844 I llama_model_loader: - type  f32:  194 tensors
0.00.024.844 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.844 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.845 I print_info: file format = GGUF V3 (latest)
0.00.024.845 I print_info: file type   = Q4_1
0.00.024.846 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.781 I load: special tokens cache size = 25
0.00.038.720 I load: token to piece cache size = 0.2984 MB
0.00.038.723 I print_info: arch             = gptneox
0.00.038.724 I print_info: vocab_only       = 0
0.00.038.724 I print_info: n_ctx_train      = 2048
0.00.038.724 I print_info: n_embd           = 2048
0.00.038.724 I print_info: n_layer          = 24
0.00.038.727 I print_info: n_head           = 16
0.00.038.728 I print_info: n_head_kv        = 16
0.00.038.728 I print_info: n_rot            = 32
0.00.038.728 I print_info: n_swa            = 0
0.00.038.729 I print_info: n_embd_head_k    = 128
0.00.038.729 I print_info: n_embd_head_v    = 128
0.00.038.730 I print_info: n_gqa            = 1
0.00.038.731 I print_info: n_embd_k_gqa     = 2048
0.00.038.731 I print_info: n_embd_v_gqa     = 2048
0.00.038.732 I print_info: f_norm_eps       = 1.0e-05
0.00.038.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.732 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.733 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.733 I print_info: f_logit_scale    = 0.0e+00
0.00.038.734 I print_info: n_ff             = 8192
0.00.038.734 I print_info: n_expert         = 0
0.00.038.734 I print_info: n_expert_used    = 0
0.00.038.734 I print_info: causal attn      = 1
0.00.038.734 I print_info: pooling type     = 0
0.00.038.734 I print_info: rope type        = 2
0.00.038.735 I print_info: rope scaling     = linear
0.00.038.737 I print_info: freq_base_train  = 10000.0
0.00.038.738 I print_info: freq_scale_train = 1
0.00.038.738 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.738 I print_info: rope_finetuned   = unknown
0.00.038.738 I print_info: ssm_d_conv       = 0
0.00.038.738 I print_info: ssm_d_inner      = 0
0.00.038.739 I print_info: ssm_d_state      = 0
0.00.038.739 I print_info: ssm_dt_rank      = 0
0.00.038.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.739 I print_info: model type       = 1.4B
0.00.038.739 I print_info: model params     = 1.41 B
0.00.038.740 I print_info: general.name     = 1.4B
0.00.038.740 I print_info: vocab type       = BPE
0.00.038.740 I print_info: n_vocab          = 50304
0.00.038.740 I print_info: n_merges         = 50009
0.00.038.741 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.741 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: LF token         = 128 'Ä'
0.00.038.749 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: max token length = 1024
0.00.586.017 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.033 I load_tensors: offloading output layer to GPU
0.00.586.034 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.067 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.586.068 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.587.478 I llama_init_from_model: n_seq_max     = 1
0.00.587.484 I llama_init_from_model: n_ctx         = 128
0.00.587.484 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.485 I llama_init_from_model: n_batch       = 128
0.00.587.486 I llama_init_from_model: n_ubatch      = 128
0.00.587.486 I llama_init_from_model: flash_attn    = 0
0.00.587.488 I llama_init_from_model: freq_base     = 10000.0
0.00.587.489 I llama_init_from_model: freq_scale    = 1
0.00.587.489 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.498 I ggml_metal_init: allocating
0.00.587.585 I ggml_metal_init: found device: Apple M4
0.00.587.599 I ggml_metal_init: picking default device: Apple M4
0.00.589.321 I ggml_metal_init: using embedded metal library
0.00.595.660 I ggml_metal_init: GPU name:   Apple M4
0.00.595.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.668 I ggml_metal_init: simdgroup reduction   = true
0.00.595.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.669 I ggml_metal_init: has residency sets    = true
0.00.595.669 I ggml_metal_init: has bfloat            = true
0.00.595.669 I ggml_metal_init: use bfloat            = true
0.00.595.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.296 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.859 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.862 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.333 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.335 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.336 I llama_init_from_model: graph nodes  = 967
0.00.621.336 I llama_init_from_model: graph splits = 2
0.00.621.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.761 I 
0.00.644.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.863 I perplexity: tokenizing the input ..
0.00.652.041 I perplexity: tokenization took 7.174 ms
0.00.652.061 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.055 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.777.353 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.777.368 I llama_perf_context_print:        load time =     635.93 ms
0.00.777.369 I llama_perf_context_print: prompt eval time =     123.13 ms /   128 tokens (    0.96 ms per token,  1039.54 tokens per second)
0.00.777.369 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.370 I llama_perf_context_print:       total time =     132.61 ms /   129 tokens
0.00.777.756 I ggml_metal_free: deallocating

real	0m0.792s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.845 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.789 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.790 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.491 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.492 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.493 I llama_model_loader: - type  f32:  194 tensors
0.00.024.493 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.493 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.494 I print_info: file format = GGUF V3 (latest)
0.00.024.494 I print_info: file type   = Q5_0
0.00.024.496 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.399 I load: special tokens cache size = 25
0.00.038.427 I load: token to piece cache size = 0.2984 MB
0.00.038.430 I print_info: arch             = gptneox
0.00.038.430 I print_info: vocab_only       = 0
0.00.038.430 I print_info: n_ctx_train      = 2048
0.00.038.430 I print_info: n_embd           = 2048
0.00.038.430 I print_info: n_layer          = 24
0.00.038.434 I print_info: n_head           = 16
0.00.038.436 I print_info: n_head_kv        = 16
0.00.038.436 I print_info: n_rot            = 32
0.00.038.436 I print_info: n_swa            = 0
0.00.038.437 I print_info: n_embd_head_k    = 128
0.00.038.437 I print_info: n_embd_head_v    = 128
0.00.038.438 I print_info: n_gqa            = 1
0.00.038.438 I print_info: n_embd_k_gqa     = 2048
0.00.038.439 I print_info: n_embd_v_gqa     = 2048
0.00.038.440 I print_info: f_norm_eps       = 1.0e-05
0.00.038.440 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.440 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.441 I print_info: f_logit_scale    = 0.0e+00
0.00.038.441 I print_info: n_ff             = 8192
0.00.038.441 I print_info: n_expert         = 0
0.00.038.441 I print_info: n_expert_used    = 0
0.00.038.442 I print_info: causal attn      = 1
0.00.038.442 I print_info: pooling type     = 0
0.00.038.442 I print_info: rope type        = 2
0.00.038.442 I print_info: rope scaling     = linear
0.00.038.442 I print_info: freq_base_train  = 10000.0
0.00.038.443 I print_info: freq_scale_train = 1
0.00.038.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.443 I print_info: rope_finetuned   = unknown
0.00.038.443 I print_info: ssm_d_conv       = 0
0.00.038.443 I print_info: ssm_d_inner      = 0
0.00.038.444 I print_info: ssm_d_state      = 0
0.00.038.444 I print_info: ssm_dt_rank      = 0
0.00.038.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.446 I print_info: model type       = 1.4B
0.00.038.447 I print_info: model params     = 1.41 B
0.00.038.447 I print_info: general.name     = 1.4B
0.00.038.447 I print_info: vocab type       = BPE
0.00.038.447 I print_info: n_vocab          = 50304
0.00.038.448 I print_info: n_merges         = 50009
0.00.038.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.449 I print_info: LF token         = 128 'Ä'
0.00.038.449 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.449 I print_info: max token length = 1024
0.00.651.452 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.467 I load_tensors: offloading output layer to GPU
0.00.651.467 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.500 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.651.502 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.653.090 I llama_init_from_model: n_seq_max     = 1
0.00.653.094 I llama_init_from_model: n_ctx         = 128
0.00.653.094 I llama_init_from_model: n_ctx_per_seq = 128
0.00.653.095 I llama_init_from_model: n_batch       = 128
0.00.653.096 I llama_init_from_model: n_ubatch      = 128
0.00.653.096 I llama_init_from_model: flash_attn    = 0
0.00.653.099 I llama_init_from_model: freq_base     = 10000.0
0.00.653.099 I llama_init_from_model: freq_scale    = 1
0.00.653.100 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.653.106 I ggml_metal_init: allocating
0.00.653.220 I ggml_metal_init: found device: Apple M4
0.00.653.234 I ggml_metal_init: picking default device: Apple M4
0.00.654.768 I ggml_metal_init: using embedded metal library
0.00.661.184 I ggml_metal_init: GPU name:   Apple M4
0.00.661.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.190 I ggml_metal_init: simdgroup reduction   = true
0.00.661.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.191 I ggml_metal_init: has residency sets    = true
0.00.661.191 I ggml_metal_init: has bfloat            = true
0.00.661.191 I ggml_metal_init: use bfloat            = true
0.00.661.192 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.934 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.367 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.682.371 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.682.397 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.872 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.685.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.685.874 I llama_init_from_model: graph nodes  = 967
0.00.685.874 I llama_init_from_model: graph splits = 2
0.00.685.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.685.878 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.812 I 
0.00.716.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.917 I perplexity: tokenizing the input ..
0.00.724.253 I perplexity: tokenization took 7.334 ms
0.00.724.271 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.656 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.869.985 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.869.999 I llama_perf_context_print:        load time =     707.96 ms
0.00.870.000 I llama_perf_context_print: prompt eval time =     143.98 ms /   128 tokens (    1.12 ms per token,   888.99 tokens per second)
0.00.870.000 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.001 I llama_perf_context_print:       total time =     153.19 ms /   129 tokens
0.00.870.356 I ggml_metal_free: deallocating

real	0m0.884s
user	0m0.079s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.323 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.021 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.022 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.024 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.024 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.026 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.026 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.867 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.716 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.719 I llama_model_loader: - type  f32:  194 tensors
0.00.025.719 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.720 I print_info: file format = GGUF V3 (latest)
0.00.025.721 I print_info: file type   = Q5_1
0.00.025.722 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.921 I load: special tokens cache size = 25
0.00.039.857 I load: token to piece cache size = 0.2984 MB
0.00.039.860 I print_info: arch             = gptneox
0.00.039.860 I print_info: vocab_only       = 0
0.00.039.861 I print_info: n_ctx_train      = 2048
0.00.039.861 I print_info: n_embd           = 2048
0.00.039.861 I print_info: n_layer          = 24
0.00.039.864 I print_info: n_head           = 16
0.00.039.865 I print_info: n_head_kv        = 16
0.00.039.865 I print_info: n_rot            = 32
0.00.039.865 I print_info: n_swa            = 0
0.00.039.865 I print_info: n_embd_head_k    = 128
0.00.039.865 I print_info: n_embd_head_v    = 128
0.00.039.866 I print_info: n_gqa            = 1
0.00.039.867 I print_info: n_embd_k_gqa     = 2048
0.00.039.868 I print_info: n_embd_v_gqa     = 2048
0.00.039.868 I print_info: f_norm_eps       = 1.0e-05
0.00.039.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.870 I print_info: f_logit_scale    = 0.0e+00
0.00.039.871 I print_info: n_ff             = 8192
0.00.039.871 I print_info: n_expert         = 0
0.00.039.872 I print_info: n_expert_used    = 0
0.00.039.872 I print_info: causal attn      = 1
0.00.039.872 I print_info: pooling type     = 0
0.00.039.872 I print_info: rope type        = 2
0.00.039.872 I print_info: rope scaling     = linear
0.00.039.873 I print_info: freq_base_train  = 10000.0
0.00.039.873 I print_info: freq_scale_train = 1
0.00.039.873 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.873 I print_info: rope_finetuned   = unknown
0.00.039.874 I print_info: ssm_d_conv       = 0
0.00.039.874 I print_info: ssm_d_inner      = 0
0.00.039.874 I print_info: ssm_d_state      = 0
0.00.039.876 I print_info: ssm_dt_rank      = 0
0.00.039.876 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.876 I print_info: model type       = 1.4B
0.00.039.877 I print_info: model params     = 1.41 B
0.00.039.877 I print_info: general.name     = 1.4B
0.00.039.877 I print_info: vocab type       = BPE
0.00.039.877 I print_info: n_vocab          = 50304
0.00.039.878 I print_info: n_merges         = 50009
0.00.039.878 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.878 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.878 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.878 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.879 I print_info: LF token         = 128 'Ä'
0.00.039.879 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.880 I print_info: max token length = 1024
0.00.686.410 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.423 I load_tensors: offloading output layer to GPU
0.00.686.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.456 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.686.463 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.687.953 I llama_init_from_model: n_seq_max     = 1
0.00.687.956 I llama_init_from_model: n_ctx         = 128
0.00.687.957 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.957 I llama_init_from_model: n_batch       = 128
0.00.687.958 I llama_init_from_model: n_ubatch      = 128
0.00.687.958 I llama_init_from_model: flash_attn    = 0
0.00.687.959 I llama_init_from_model: freq_base     = 10000.0
0.00.687.960 I llama_init_from_model: freq_scale    = 1
0.00.687.961 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.962 I ggml_metal_init: allocating
0.00.687.987 I ggml_metal_init: found device: Apple M4
0.00.688.001 I ggml_metal_init: picking default device: Apple M4
0.00.689.333 I ggml_metal_init: using embedded metal library
0.00.695.534 I ggml_metal_init: GPU name:   Apple M4
0.00.695.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.541 I ggml_metal_init: simdgroup reduction   = true
0.00.695.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.541 I ggml_metal_init: has residency sets    = true
0.00.695.541 I ggml_metal_init: has bfloat            = true
0.00.695.542 I ggml_metal_init: use bfloat            = true
0.00.695.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.059 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.402 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.715.412 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.715.448 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.697 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.718.699 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.718.700 I llama_init_from_model: graph nodes  = 967
0.00.718.700 I llama_init_from_model: graph splits = 2
0.00.718.703 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.718.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.155 I 
0.00.748.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.302 I perplexity: tokenizing the input ..
0.00.755.690 I perplexity: tokenization took 7.384 ms
0.00.755.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.901.089 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.902.426 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.902.444 I llama_perf_context_print:        load time =     737.82 ms
0.00.902.445 I llama_perf_context_print: prompt eval time =     144.51 ms /   128 tokens (    1.13 ms per token,   885.78 tokens per second)
0.00.902.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.902.446 I llama_perf_context_print:       total time =     154.30 ms /   129 tokens
0.00.902.859 I ggml_metal_free: deallocating

real	0m0.918s
user	0m0.079s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.198 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.200 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.205 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.205 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.206 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.206 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.207 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.207 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.209 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.023 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.917 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.919 I llama_model_loader: - type  f32:  194 tensors
0.00.024.919 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.920 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.920 I print_info: file format = GGUF V3 (latest)
0.00.024.921 I print_info: file type   = Q2_K - Medium
0.00.024.922 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.145 I load: special tokens cache size = 25
0.00.039.074 I load: token to piece cache size = 0.2984 MB
0.00.039.076 I print_info: arch             = gptneox
0.00.039.077 I print_info: vocab_only       = 0
0.00.039.077 I print_info: n_ctx_train      = 2048
0.00.039.077 I print_info: n_embd           = 2048
0.00.039.077 I print_info: n_layer          = 24
0.00.039.080 I print_info: n_head           = 16
0.00.039.081 I print_info: n_head_kv        = 16
0.00.039.081 I print_info: n_rot            = 32
0.00.039.081 I print_info: n_swa            = 0
0.00.039.081 I print_info: n_embd_head_k    = 128
0.00.039.083 I print_info: n_embd_head_v    = 128
0.00.039.084 I print_info: n_gqa            = 1
0.00.039.085 I print_info: n_embd_k_gqa     = 2048
0.00.039.086 I print_info: n_embd_v_gqa     = 2048
0.00.039.086 I print_info: f_norm_eps       = 1.0e-05
0.00.039.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.087 I print_info: f_logit_scale    = 0.0e+00
0.00.039.088 I print_info: n_ff             = 8192
0.00.039.088 I print_info: n_expert         = 0
0.00.039.088 I print_info: n_expert_used    = 0
0.00.039.088 I print_info: causal attn      = 1
0.00.039.088 I print_info: pooling type     = 0
0.00.039.088 I print_info: rope type        = 2
0.00.039.089 I print_info: rope scaling     = linear
0.00.039.089 I print_info: freq_base_train  = 10000.0
0.00.039.090 I print_info: freq_scale_train = 1
0.00.039.090 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.090 I print_info: rope_finetuned   = unknown
0.00.039.090 I print_info: ssm_d_conv       = 0
0.00.039.090 I print_info: ssm_d_inner      = 0
0.00.039.092 I print_info: ssm_d_state      = 0
0.00.039.092 I print_info: ssm_dt_rank      = 0
0.00.039.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.092 I print_info: model type       = 1.4B
0.00.039.093 I print_info: model params     = 1.41 B
0.00.039.093 I print_info: general.name     = 1.4B
0.00.039.093 I print_info: vocab type       = BPE
0.00.039.095 I print_info: n_vocab          = 50304
0.00.039.095 I print_info: n_merges         = 50009
0.00.039.095 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.096 I print_info: LF token         = 128 'Ä'
0.00.039.096 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.097 I print_info: max token length = 1024
0.00.401.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.401.152 I load_tensors: offloading output layer to GPU
0.00.401.153 I load_tensors: offloaded 25/25 layers to GPU
0.00.401.191 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.401.193 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.402.585 I llama_init_from_model: n_seq_max     = 1
0.00.402.592 I llama_init_from_model: n_ctx         = 128
0.00.402.592 I llama_init_from_model: n_ctx_per_seq = 128
0.00.402.593 I llama_init_from_model: n_batch       = 128
0.00.402.593 I llama_init_from_model: n_ubatch      = 128
0.00.402.593 I llama_init_from_model: flash_attn    = 0
0.00.402.596 I llama_init_from_model: freq_base     = 10000.0
0.00.402.596 I llama_init_from_model: freq_scale    = 1
0.00.402.597 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.402.602 I ggml_metal_init: allocating
0.00.402.681 I ggml_metal_init: found device: Apple M4
0.00.402.694 I ggml_metal_init: picking default device: Apple M4
0.00.404.439 I ggml_metal_init: using embedded metal library
0.00.410.257 I ggml_metal_init: GPU name:   Apple M4
0.00.410.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.410.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.410.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.410.275 I ggml_metal_init: simdgroup reduction   = true
0.00.410.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.410.275 I ggml_metal_init: has residency sets    = true
0.00.410.276 I ggml_metal_init: has bfloat            = true
0.00.410.276 I ggml_metal_init: use bfloat            = true
0.00.410.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.410.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.432.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.436.103 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.436.108 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.436.136 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.439.488 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.439.490 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.439.490 I llama_init_from_model: graph nodes  = 967
0.00.439.491 I llama_init_from_model: graph splits = 2
0.00.439.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.439.494 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.465.740 I 
0.00.465.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.465.817 I perplexity: tokenizing the input ..
0.00.472.715 I perplexity: tokenization took 6.892 ms
0.00.472.747 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.605.143 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.606.554 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.606.570 I llama_perf_context_print:        load time =     456.55 ms
0.00.606.571 I llama_perf_context_print: prompt eval time =     131.61 ms /   128 tokens (    1.03 ms per token,   972.56 tokens per second)
0.00.606.572 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.606.572 I llama_perf_context_print:       total time =     140.83 ms /   129 tokens
0.00.606.942 I ggml_metal_free: deallocating

real	0m0.622s
user	0m0.081s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.545 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.550 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.552 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.556 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.421 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.429 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.218 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.219 I llama_model_loader: - type  f32:  194 tensors
0.00.024.220 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.220 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.224 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.224 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.225 I print_info: file format = GGUF V3 (latest)
0.00.024.226 I print_info: file type   = Q3_K - Medium
0.00.024.226 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.437 I load: special tokens cache size = 25
0.00.038.346 I load: token to piece cache size = 0.2984 MB
0.00.038.349 I print_info: arch             = gptneox
0.00.038.349 I print_info: vocab_only       = 0
0.00.038.349 I print_info: n_ctx_train      = 2048
0.00.038.349 I print_info: n_embd           = 2048
0.00.038.350 I print_info: n_layer          = 24
0.00.038.352 I print_info: n_head           = 16
0.00.038.353 I print_info: n_head_kv        = 16
0.00.038.353 I print_info: n_rot            = 32
0.00.038.355 I print_info: n_swa            = 0
0.00.038.356 I print_info: n_embd_head_k    = 128
0.00.038.356 I print_info: n_embd_head_v    = 128
0.00.038.357 I print_info: n_gqa            = 1
0.00.038.357 I print_info: n_embd_k_gqa     = 2048
0.00.038.364 I print_info: n_embd_v_gqa     = 2048
0.00.038.366 I print_info: f_norm_eps       = 1.0e-05
0.00.038.366 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.366 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.367 I print_info: f_logit_scale    = 0.0e+00
0.00.038.369 I print_info: n_ff             = 8192
0.00.038.369 I print_info: n_expert         = 0
0.00.038.369 I print_info: n_expert_used    = 0
0.00.038.369 I print_info: causal attn      = 1
0.00.038.370 I print_info: pooling type     = 0
0.00.038.370 I print_info: rope type        = 2
0.00.038.370 I print_info: rope scaling     = linear
0.00.038.370 I print_info: freq_base_train  = 10000.0
0.00.038.371 I print_info: freq_scale_train = 1
0.00.038.371 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.371 I print_info: rope_finetuned   = unknown
0.00.038.371 I print_info: ssm_d_conv       = 0
0.00.038.371 I print_info: ssm_d_inner      = 0
0.00.038.371 I print_info: ssm_d_state      = 0
0.00.038.372 I print_info: ssm_dt_rank      = 0
0.00.038.372 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.372 I print_info: model type       = 1.4B
0.00.038.372 I print_info: model params     = 1.41 B
0.00.038.372 I print_info: general.name     = 1.4B
0.00.038.373 I print_info: vocab type       = BPE
0.00.038.373 I print_info: n_vocab          = 50304
0.00.038.373 I print_info: n_merges         = 50009
0.00.038.374 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.374 I print_info: LF token         = 128 'Ä'
0.00.038.375 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.375 I print_info: max token length = 1024
0.00.473.167 I load_tensors: offloading 24 repeating layers to GPU
0.00.473.178 I load_tensors: offloading output layer to GPU
0.00.473.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.473.212 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.473.215 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.474.550 I llama_init_from_model: n_seq_max     = 1
0.00.474.560 I llama_init_from_model: n_ctx         = 128
0.00.474.563 I llama_init_from_model: n_ctx_per_seq = 128
0.00.474.563 I llama_init_from_model: n_batch       = 128
0.00.474.564 I llama_init_from_model: n_ubatch      = 128
0.00.474.566 I llama_init_from_model: flash_attn    = 0
0.00.474.568 I llama_init_from_model: freq_base     = 10000.0
0.00.474.569 I llama_init_from_model: freq_scale    = 1
0.00.474.569 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.474.571 I ggml_metal_init: allocating
0.00.474.645 I ggml_metal_init: found device: Apple M4
0.00.474.657 I ggml_metal_init: picking default device: Apple M4
0.00.476.391 I ggml_metal_init: using embedded metal library
0.00.481.869 I ggml_metal_init: GPU name:   Apple M4
0.00.481.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.481.879 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.481.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.481.880 I ggml_metal_init: simdgroup reduction   = true
0.00.481.880 I ggml_metal_init: simdgroup matrix mul. = true
0.00.481.880 I ggml_metal_init: has residency sets    = true
0.00.481.881 I ggml_metal_init: has bfloat            = true
0.00.481.881 I ggml_metal_init: use bfloat            = true
0.00.481.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.481.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.503.315 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.506.999 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.507.004 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.507.040 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.510.558 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.510.560 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.510.560 I llama_init_from_model: graph nodes  = 967
0.00.510.561 I llama_init_from_model: graph splits = 2
0.00.510.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.510.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.185 I 
0.00.540.257 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.275 I perplexity: tokenizing the input ..
0.00.546.037 I perplexity: tokenization took 5.76 ms
0.00.546.046 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.690.772 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.692.187 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.692.199 I llama_perf_context_print:        load time =     531.32 ms
0.00.692.199 I llama_perf_context_print: prompt eval time =     144.50 ms /   128 tokens (    1.13 ms per token,   885.82 tokens per second)
0.00.692.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.200 I llama_perf_context_print:       total time =     152.02 ms /   129 tokens
0.00.692.559 I ggml_metal_free: deallocating

real	0m0.706s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.032 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.010 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.012 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.014 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.016 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.016 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.017 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.335 I llama_model_loader: - type  f32:  194 tensors
0.00.026.335 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.335 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.336 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.336 I print_info: file format = GGUF V3 (latest)
0.00.026.337 I print_info: file type   = Q4_K - Medium
0.00.026.340 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.722 I load: special tokens cache size = 25
0.00.040.784 I load: token to piece cache size = 0.2984 MB
0.00.040.790 I print_info: arch             = gptneox
0.00.040.790 I print_info: vocab_only       = 0
0.00.040.791 I print_info: n_ctx_train      = 2048
0.00.040.791 I print_info: n_embd           = 2048
0.00.040.791 I print_info: n_layer          = 24
0.00.040.795 I print_info: n_head           = 16
0.00.040.796 I print_info: n_head_kv        = 16
0.00.040.796 I print_info: n_rot            = 32
0.00.040.796 I print_info: n_swa            = 0
0.00.040.796 I print_info: n_embd_head_k    = 128
0.00.040.797 I print_info: n_embd_head_v    = 128
0.00.040.797 I print_info: n_gqa            = 1
0.00.040.798 I print_info: n_embd_k_gqa     = 2048
0.00.040.799 I print_info: n_embd_v_gqa     = 2048
0.00.040.800 I print_info: f_norm_eps       = 1.0e-05
0.00.040.800 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.801 I print_info: f_logit_scale    = 0.0e+00
0.00.040.801 I print_info: n_ff             = 8192
0.00.040.802 I print_info: n_expert         = 0
0.00.040.802 I print_info: n_expert_used    = 0
0.00.040.802 I print_info: causal attn      = 1
0.00.040.802 I print_info: pooling type     = 0
0.00.040.802 I print_info: rope type        = 2
0.00.040.802 I print_info: rope scaling     = linear
0.00.040.803 I print_info: freq_base_train  = 10000.0
0.00.040.806 I print_info: freq_scale_train = 1
0.00.040.806 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.806 I print_info: rope_finetuned   = unknown
0.00.040.806 I print_info: ssm_d_conv       = 0
0.00.040.806 I print_info: ssm_d_inner      = 0
0.00.040.806 I print_info: ssm_d_state      = 0
0.00.040.806 I print_info: ssm_dt_rank      = 0
0.00.040.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.807 I print_info: model type       = 1.4B
0.00.040.807 I print_info: model params     = 1.41 B
0.00.040.808 I print_info: general.name     = 1.4B
0.00.040.808 I print_info: vocab type       = BPE
0.00.040.808 I print_info: n_vocab          = 50304
0.00.040.808 I print_info: n_merges         = 50009
0.00.040.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: LF token         = 128 'Ä'
0.00.040.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: max token length = 1024
0.00.533.745 I load_tensors: offloading 24 repeating layers to GPU
0.00.533.748 I load_tensors: offloading output layer to GPU
0.00.533.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.533.765 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.533.766 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.534.448 I llama_init_from_model: n_seq_max     = 1
0.00.534.451 I llama_init_from_model: n_ctx         = 128
0.00.534.452 I llama_init_from_model: n_ctx_per_seq = 128
0.00.534.452 I llama_init_from_model: n_batch       = 128
0.00.534.452 I llama_init_from_model: n_ubatch      = 128
0.00.534.453 I llama_init_from_model: flash_attn    = 0
0.00.534.453 I llama_init_from_model: freq_base     = 10000.0
0.00.534.454 I llama_init_from_model: freq_scale    = 1
0.00.534.455 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.456 I ggml_metal_init: allocating
0.00.534.503 I ggml_metal_init: found device: Apple M4
0.00.534.514 I ggml_metal_init: picking default device: Apple M4
0.00.535.545 I ggml_metal_init: using embedded metal library
0.00.543.270 I ggml_metal_init: GPU name:   Apple M4
0.00.543.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.278 I ggml_metal_init: simdgroup reduction   = true
0.00.543.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.279 I ggml_metal_init: has residency sets    = true
0.00.543.280 I ggml_metal_init: has bfloat            = true
0.00.543.280 I ggml_metal_init: use bfloat            = true
0.00.543.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.309 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.562.966 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.562.969 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.562.984 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.564.533 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.564.534 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.564.534 I llama_init_from_model: graph nodes  = 967
0.00.564.534 I llama_init_from_model: graph splits = 2
0.00.564.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.564.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.587 I 
0.00.591.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.591.631 I perplexity: tokenizing the input ..
0.00.595.525 I perplexity: tokenization took 3.892 ms
0.00.595.537 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.898 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.741.516 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.741.530 I llama_perf_context_print:        load time =     581.55 ms
0.00.741.531 I llama_perf_context_print: prompt eval time =     144.13 ms /   128 tokens (    1.13 ms per token,   888.06 tokens per second)
0.00.741.532 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.533 I llama_perf_context_print:       total time =     149.94 ms /   129 tokens
0.00.741.901 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.071s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.253 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.599 I llama_model_loader: - type  f32:  194 tensors
0.00.025.601 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.602 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.602 I print_info: file format = GGUF V3 (latest)
0.00.025.603 I print_info: file type   = Q5_K - Medium
0.00.025.605 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.168 I load: special tokens cache size = 25
0.00.040.232 I load: token to piece cache size = 0.2984 MB
0.00.040.237 I print_info: arch             = gptneox
0.00.040.237 I print_info: vocab_only       = 0
0.00.040.238 I print_info: n_ctx_train      = 2048
0.00.040.239 I print_info: n_embd           = 2048
0.00.040.239 I print_info: n_layer          = 24
0.00.040.244 I print_info: n_head           = 16
0.00.040.244 I print_info: n_head_kv        = 16
0.00.040.245 I print_info: n_rot            = 32
0.00.040.245 I print_info: n_swa            = 0
0.00.040.245 I print_info: n_embd_head_k    = 128
0.00.040.245 I print_info: n_embd_head_v    = 128
0.00.040.246 I print_info: n_gqa            = 1
0.00.040.246 I print_info: n_embd_k_gqa     = 2048
0.00.040.247 I print_info: n_embd_v_gqa     = 2048
0.00.040.248 I print_info: f_norm_eps       = 1.0e-05
0.00.040.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.249 I print_info: f_logit_scale    = 0.0e+00
0.00.040.249 I print_info: n_ff             = 8192
0.00.040.249 I print_info: n_expert         = 0
0.00.040.250 I print_info: n_expert_used    = 0
0.00.040.250 I print_info: causal attn      = 1
0.00.040.250 I print_info: pooling type     = 0
0.00.040.250 I print_info: rope type        = 2
0.00.040.250 I print_info: rope scaling     = linear
0.00.040.251 I print_info: freq_base_train  = 10000.0
0.00.040.251 I print_info: freq_scale_train = 1
0.00.040.251 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.251 I print_info: rope_finetuned   = unknown
0.00.040.251 I print_info: ssm_d_conv       = 0
0.00.040.251 I print_info: ssm_d_inner      = 0
0.00.040.252 I print_info: ssm_d_state      = 0
0.00.040.252 I print_info: ssm_dt_rank      = 0
0.00.040.252 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.252 I print_info: model type       = 1.4B
0.00.040.252 I print_info: model params     = 1.41 B
0.00.040.252 I print_info: general.name     = 1.4B
0.00.040.253 I print_info: vocab type       = BPE
0.00.040.253 I print_info: n_vocab          = 50304
0.00.040.253 I print_info: n_merges         = 50009
0.00.040.254 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.254 I print_info: LF token         = 128 'Ä'
0.00.040.254 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.255 I print_info: max token length = 1024
0.00.607.941 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.953 I load_tensors: offloading output layer to GPU
0.00.607.953 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.988 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.607.990 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.609.295 I llama_init_from_model: n_seq_max     = 1
0.00.609.300 I llama_init_from_model: n_ctx         = 128
0.00.609.301 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.301 I llama_init_from_model: n_batch       = 128
0.00.609.301 I llama_init_from_model: n_ubatch      = 128
0.00.609.301 I llama_init_from_model: flash_attn    = 0
0.00.609.303 I llama_init_from_model: freq_base     = 10000.0
0.00.609.304 I llama_init_from_model: freq_scale    = 1
0.00.609.305 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.307 I ggml_metal_init: allocating
0.00.609.379 I ggml_metal_init: found device: Apple M4
0.00.609.392 I ggml_metal_init: picking default device: Apple M4
0.00.610.961 I ggml_metal_init: using embedded metal library
0.00.617.391 I ggml_metal_init: GPU name:   Apple M4
0.00.617.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.617.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.617.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.617.397 I ggml_metal_init: simdgroup reduction   = true
0.00.617.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.617.398 I ggml_metal_init: has residency sets    = true
0.00.617.398 I ggml_metal_init: has bfloat            = true
0.00.617.398 I ggml_metal_init: use bfloat            = true
0.00.617.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.617.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.634.800 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.638.292 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.638.296 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.638.329 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.641.743 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.641.745 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.641.746 I llama_init_from_model: graph nodes  = 967
0.00.641.746 I llama_init_from_model: graph splits = 2
0.00.641.749 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.641.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.973 I 
0.00.677.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.677.076 I perplexity: tokenizing the input ..
0.00.684.011 I perplexity: tokenization took 6.933 ms
0.00.684.027 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.940 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.825.281 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.825.297 I llama_perf_context_print:        load time =     667.71 ms
0.00.825.298 I llama_perf_context_print: prompt eval time =     139.68 ms /   128 tokens (    1.09 ms per token,   916.37 tokens per second)
0.00.825.298 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.825.299 I llama_perf_context_print:       total time =     148.33 ms /   129 tokens
0.00.825.715 I ggml_metal_free: deallocating

real	0m0.840s
user	0m0.079s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.088 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.898 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.904 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.905 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.905 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.905 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.906 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.907 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.909 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.911 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.911 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.813 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.644 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.644 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.645 I llama_model_loader: - type  f32:  194 tensors
0.00.024.645 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.646 I print_info: file format = GGUF V3 (latest)
0.00.024.646 I print_info: file type   = Q6_K
0.00.024.649 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.843 I load: special tokens cache size = 25
0.00.038.751 I load: token to piece cache size = 0.2984 MB
0.00.038.753 I print_info: arch             = gptneox
0.00.038.754 I print_info: vocab_only       = 0
0.00.038.754 I print_info: n_ctx_train      = 2048
0.00.038.754 I print_info: n_embd           = 2048
0.00.038.754 I print_info: n_layer          = 24
0.00.038.757 I print_info: n_head           = 16
0.00.038.758 I print_info: n_head_kv        = 16
0.00.038.758 I print_info: n_rot            = 32
0.00.038.758 I print_info: n_swa            = 0
0.00.038.758 I print_info: n_embd_head_k    = 128
0.00.038.758 I print_info: n_embd_head_v    = 128
0.00.038.759 I print_info: n_gqa            = 1
0.00.038.760 I print_info: n_embd_k_gqa     = 2048
0.00.038.761 I print_info: n_embd_v_gqa     = 2048
0.00.038.761 I print_info: f_norm_eps       = 1.0e-05
0.00.038.762 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.762 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.762 I print_info: f_logit_scale    = 0.0e+00
0.00.038.763 I print_info: n_ff             = 8192
0.00.038.763 I print_info: n_expert         = 0
0.00.038.763 I print_info: n_expert_used    = 0
0.00.038.764 I print_info: causal attn      = 1
0.00.038.764 I print_info: pooling type     = 0
0.00.038.764 I print_info: rope type        = 2
0.00.038.764 I print_info: rope scaling     = linear
0.00.038.764 I print_info: freq_base_train  = 10000.0
0.00.038.765 I print_info: freq_scale_train = 1
0.00.038.765 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.765 I print_info: rope_finetuned   = unknown
0.00.038.765 I print_info: ssm_d_conv       = 0
0.00.038.765 I print_info: ssm_d_inner      = 0
0.00.038.766 I print_info: ssm_d_state      = 0
0.00.038.766 I print_info: ssm_dt_rank      = 0
0.00.038.766 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.766 I print_info: model type       = 1.4B
0.00.038.766 I print_info: model params     = 1.41 B
0.00.038.767 I print_info: general.name     = 1.4B
0.00.038.767 I print_info: vocab type       = BPE
0.00.038.768 I print_info: n_vocab          = 50304
0.00.038.768 I print_info: n_merges         = 50009
0.00.038.768 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.768 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.768 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: LF token         = 128 'Ä'
0.00.038.769 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.769 I print_info: max token length = 1024
0.00.586.861 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.868 I load_tensors: offloading output layer to GPU
0.00.586.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.893 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.586.894 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.587.986 I llama_init_from_model: n_seq_max     = 1
0.00.587.989 I llama_init_from_model: n_ctx         = 128
0.00.587.989 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.990 I llama_init_from_model: n_batch       = 128
0.00.587.990 I llama_init_from_model: n_ubatch      = 128
0.00.587.991 I llama_init_from_model: flash_attn    = 0
0.00.587.991 I llama_init_from_model: freq_base     = 10000.0
0.00.587.992 I llama_init_from_model: freq_scale    = 1
0.00.587.993 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.994 I ggml_metal_init: allocating
0.00.588.016 I ggml_metal_init: found device: Apple M4
0.00.588.025 I ggml_metal_init: picking default device: Apple M4
0.00.589.404 I ggml_metal_init: using embedded metal library
0.00.595.299 I ggml_metal_init: GPU name:   Apple M4
0.00.595.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.305 I ggml_metal_init: simdgroup reduction   = true
0.00.595.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.305 I ggml_metal_init: has residency sets    = true
0.00.595.306 I ggml_metal_init: has bfloat            = true
0.00.595.306 I ggml_metal_init: use bfloat            = true
0.00.595.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.404 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.013 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.615.017 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.044 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.149 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.618.151 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.618.151 I llama_init_from_model: graph nodes  = 967
0.00.618.152 I llama_init_from_model: graph splits = 2
0.00.618.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.930 I 
0.00.654.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.038 I perplexity: tokenizing the input ..
0.00.659.755 I perplexity: tokenization took 5.715 ms
0.00.659.770 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.402 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.800.847 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.800.860 I llama_perf_context_print:        load time =     644.83 ms
0.00.800.861 I llama_perf_context_print: prompt eval time =     139.40 ms /   128 tokens (    1.09 ms per token,   918.22 tokens per second)
0.00.800.862 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.862 I llama_perf_context_print:       total time =     146.94 ms /   129 tokens
0.00.801.218 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.075s
sys	0m0.132s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4584 (81585779) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.144 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.929 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.941 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.946 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.947 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.952 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.264 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.368 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.370 I llama_model_loader: - type  f32:  194 tensors
0.00.056.371 I llama_model_loader: - type  f16:   98 tensors
0.00.056.372 I print_info: file format = GGUF V3 (latest)
0.00.056.373 I print_info: file type   = all F32 (guessed)
0.00.056.374 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.123 I load: special tokens cache size = 25
0.00.076.109 I load: token to piece cache size = 0.2984 MB
0.00.076.112 I print_info: arch             = gptneox
0.00.076.112 I print_info: vocab_only       = 0
0.00.076.113 I print_info: n_ctx_train      = 2048
0.00.076.113 I print_info: n_embd           = 2048
0.00.076.113 I print_info: n_layer          = 24
0.00.076.116 I print_info: n_head           = 16
0.00.076.117 I print_info: n_head_kv        = 16
0.00.076.118 I print_info: n_rot            = 32
0.00.076.118 I print_info: n_swa            = 0
0.00.076.118 I print_info: n_embd_head_k    = 128
0.00.076.118 I print_info: n_embd_head_v    = 128
0.00.076.119 I print_info: n_gqa            = 1
0.00.076.120 I print_info: n_embd_k_gqa     = 2048
0.00.076.121 I print_info: n_embd_v_gqa     = 2048
0.00.076.121 I print_info: f_norm_eps       = 1.0e-05
0.00.076.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.122 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.122 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.122 I print_info: f_logit_scale    = 0.0e+00
0.00.076.123 I print_info: n_ff             = 8192
0.00.076.123 I print_info: n_expert         = 0
0.00.076.123 I print_info: n_expert_used    = 0
0.00.076.125 I print_info: causal attn      = 1
0.00.076.125 I print_info: pooling type     = 0
0.00.076.125 I print_info: rope type        = 2
0.00.076.125 I print_info: rope scaling     = linear
0.00.076.126 I print_info: freq_base_train  = 10000.0
0.00.076.128 I print_info: freq_scale_train = 1
0.00.076.128 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.129 I print_info: rope_finetuned   = unknown
0.00.076.129 I print_info: ssm_d_conv       = 0
0.00.076.129 I print_info: ssm_d_inner      = 0
0.00.076.129 I print_info: ssm_d_state      = 0
0.00.076.129 I print_info: ssm_dt_rank      = 0
0.00.076.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.130 I print_info: model type       = 1.4B
0.00.076.130 I print_info: model params     = 1.41 B
0.00.076.130 I print_info: general.name     = 1.4B
0.00.076.131 I print_info: vocab type       = BPE
0.00.076.131 I print_info: n_vocab          = 50304
0.00.076.132 I print_info: n_merges         = 50009
0.00.076.132 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: LF token         = 128 'Ä'
0.00.076.135 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.135 I print_info: max token length = 1024
0.01.264.956 I load_tensors: offloading 24 repeating layers to GPU
0.01.264.961 I load_tensors: offloading output layer to GPU
0.01.264.961 I load_tensors: offloaded 25/25 layers to GPU
0.01.264.993 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.264.994 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.265.937 I llama_init_from_model: n_seq_max     = 1
0.01.265.938 I llama_init_from_model: n_ctx         = 128
0.01.265.938 I llama_init_from_model: n_ctx_per_seq = 128
0.01.265.938 I llama_init_from_model: n_batch       = 128
0.01.265.938 I llama_init_from_model: n_ubatch      = 128
0.01.265.938 I llama_init_from_model: flash_attn    = 0
0.01.265.939 I llama_init_from_model: freq_base     = 10000.0
0.01.265.939 I llama_init_from_model: freq_scale    = 1
0.01.265.939 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.265.940 I ggml_metal_init: allocating
0.01.266.001 I ggml_metal_init: found device: Apple M4
0.01.266.008 I ggml_metal_init: picking default device: Apple M4
0.01.267.084 I ggml_metal_init: using embedded metal library
0.01.270.932 I ggml_metal_init: GPU name:   Apple M4
0.01.270.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.270.935 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.270.936 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.270.936 I ggml_metal_init: simdgroup reduction   = true
0.01.270.936 I ggml_metal_init: simdgroup matrix mul. = true
0.01.270.936 I ggml_metal_init: has residency sets    = true
0.01.270.937 I ggml_metal_init: has bfloat            = true
0.01.270.937 I ggml_metal_init: use bfloat            = true
0.01.270.937 I ggml_metal_init: hasUnifiedMemory      = true
0.01.270.939 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.281.204 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.282.885 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.282.887 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.282.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.284.604 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.284.605 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.284.605 I llama_init_from_model: graph nodes  = 967
0.01.284.606 I llama_init_from_model: graph splits = 2
0.01.284.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.284.608 I 
0.01.284.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.284.646 I compute_imatrix: tokenizing the input ..
0.01.288.662 I compute_imatrix: tokenization took 4.015 ms
0.01.288.664 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.555.759 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.558.132 I llama_perf_context_print:        load time =    1531.61 ms
0.01.558.134 I llama_perf_context_print: prompt eval time =     265.35 ms /   128 tokens (    2.07 ms per token,   482.39 tokens per second)
0.01.558.136 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.558.136 I llama_perf_context_print:       total time =    1533.98 ms /   129 tokens
0.01.558.630 I ggml_metal_free: deallocating

real	0m1.781s
user	0m0.126s
sys	0m0.247s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4584 (81585779)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121a06250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121a068c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121a06d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121a099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121a09e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121a0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121a0a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121a0ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121a0b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121a0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121a0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121a0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121a0cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121a0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121a0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121a0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121a0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121a0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121a0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121a101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121a10900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121a11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121a11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121a11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121a12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121a129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121a12fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121a13c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121a14180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121a14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121a148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121a14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121a15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121a15970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121a15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121a160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121a16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121a16a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121a16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121a17350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121a177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121a17c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121a18130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121a185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121a18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121a18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121a194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121a19dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121a1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121a1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121a1b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121a1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121a1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121a1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121a1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121a1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121a1d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121a1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121a1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121a1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121a1e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121a1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121a1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121a1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121a1f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121a1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121a202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121a20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121a20be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121a21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121a21520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121a219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121a21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121a223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121a22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121a22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121a233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121a238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121a23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121a248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121a24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121a25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121a258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121a25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121a26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121a268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121a26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121a27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121a278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121a27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121a28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121a288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121a28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121a29340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121a29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121a29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121a19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121a2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121a2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121a2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121a2b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121a2b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121a2bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121a2c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121a2c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121a2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121a2d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121a2d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121a2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121a2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121a2e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121a2ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121a2f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121a2f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121a2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121a30190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121a30630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121a30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121a30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121a31410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121a318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121a31d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121a321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121a32690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121a32b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121a32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121a33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121a33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121a33db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121a34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121a346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121a34b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121a35030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121a354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121a35970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121a35e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121a362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121a36750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121a36bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121a37090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121a37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121a379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121a37e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121a38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121a387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121a38c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121a390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121a39590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121a39a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121a39ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121a3a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121a3a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121a3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121a3b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121a3b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121a3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121a3bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121a3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121a3c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121a3cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121a3d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121a3d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121a3daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121a3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121a3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121a3e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121a3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121a3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121a3f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121a3fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121a3fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121a40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121a40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121a40dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121a41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121a41710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121a41bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121a42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121a424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121a42990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121a42e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121a432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121a43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121a43c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121a440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121a44550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121a449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121a44e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121a457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121a45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121a46110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121a46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121a46bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121a47100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121a47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121a47910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121a47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121a48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121a48b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121a49330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121a497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121a49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121a4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121a4a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121a4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121a4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121a4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121a4bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121a4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121a4c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121a4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121a4d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121a4d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121a4dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121a4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121a4e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121a4eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121a4f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121a4f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121a4fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121a503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121a50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121a50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121a513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121a51930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121a51e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121a523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121a52920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121a52e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121a533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121a53910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121a53e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121a543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121a54900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121a54e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121a553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121a558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121a55e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121a56390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121a568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121a56e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121a57380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121a578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121a57e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121a58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121a588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121a58e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121a59360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121a598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121a59e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121a5a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121a5a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121a5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121a5b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121a5b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121a5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121a5c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121a5c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121a5cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121a5d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121a5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121a5ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121a5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121a5e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121a5edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121a5f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121a5f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121a5fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121a60030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121a604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121a60970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121a60e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121a612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121a61750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121a61bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121a62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121a62530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121a629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121a62e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121a63310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121a63860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121a63f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121a646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121a64dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121a654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121a657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121a65f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121a66250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121a66860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.688.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117304b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117304f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117305400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117305870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117305ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117306150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1173065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117306a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117306ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117307310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117307780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117307e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117308990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117309140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117309950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11730a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11730a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11730aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11730b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11730bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11730c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11730cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11730d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11730d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11730e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11730e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11730e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11730ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11730ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11730f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11730f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11730fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117310180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117310440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1173108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117310d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117311190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117311600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117311a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117311ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117312350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1173127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117312c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1173130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117313510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117313980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117313df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117314260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1173146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117314b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117314fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117315420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117315890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117315d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117316170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1173165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117316b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117317050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1173174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117317930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117317da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117318210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117318680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117318af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117318f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1173193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117319840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117319cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11731a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11731a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11731aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11731ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11731b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11731b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11731bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11731c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11731c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11731c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11731cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11731d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11731d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11731dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11731df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11731e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11731e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11731ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11731f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11731f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11731f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11731fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1173202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117320730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117320ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117321010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117321480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1173218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117321d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1173221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117322640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117322ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117322f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117323800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117323c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1173240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117324550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1173249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117324e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1173252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117325710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117325ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117326460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1173268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117326d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1173271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117327620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117327a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117327f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117328370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1173287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117328c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1173290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117329530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1173299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117329e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11732a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11732a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11732ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11732afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11732b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11732b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11732bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11732c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11732c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11732ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11732cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11732d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11732d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11732dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11732e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11732e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11732e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11732edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11732f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11732f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11732fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11732ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117330420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117330890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117330d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117331170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1173315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117331a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117331ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117332330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1173327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117333080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1173334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117333960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117333dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117334240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1173346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117334b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117334f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117335bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117335e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117336140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1173365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117336a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117336e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117337300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117337770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117337be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117338050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1173384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117338930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117338da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117339210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117339680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117339af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117339f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11733a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11733a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11733acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11733b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11733b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11733ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11733be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11733c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11733c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11733cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11733d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11733d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11733d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11733dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11733e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11733e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11733ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11733ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11733f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11733f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11733fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117340290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117340700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117340b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117340fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117341500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117341a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117342580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117342840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117342e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1173433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117343980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117343f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117344500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117344ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117345080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117345640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117345c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1173461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117346780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117346d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117347300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1173478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117347e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117348440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117348a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117348fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117349580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117349b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11734a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11734a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11734ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11734b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11734b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11734bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11734c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11734c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11734cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11734d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11734da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11734e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11734e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11734ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11734f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11734f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11734fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1173502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117350880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117350e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117351400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1173519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117351f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117352540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117352b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1173530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117353680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117353c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117354200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1173547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117354d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117355340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117355900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117355ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117356480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117356a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117356f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117357440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117357940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117357e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117358340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117358840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117358d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117359240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117359740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117359c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11735a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11735a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11735ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11735b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11735b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11735bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11735c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11735cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11735d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11735d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11735df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11735e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11735e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121c04f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121c05390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121c05800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121c05c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121c060e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121c06550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121c069c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121c06e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121c072a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121c07710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121c07b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121c08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121c08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121c09520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121c09d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121c0a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121c0ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121c0b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121c0b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121c0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121c0c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121c0cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121c0d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121c0de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121c0e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121c0e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121c0eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121c0ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121c0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121c0f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121c0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121c10190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121c10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121c108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121c10d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121c111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121c11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121c11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121c11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121c12360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121c127d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121c12c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121c130b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121c13520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121c13990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121c13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121c14270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121c146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121c14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121c14fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121c15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121c158a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121c15d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121c16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121c165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121c16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121c16fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121c174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121c17940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121c17db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121c18220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121c18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121c18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121c18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121c193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121c19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121c19cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121c1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121c1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121c1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121c1ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121c1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121c1b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121c1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11735b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11734c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11734b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117348140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117345900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117355040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117352800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117350580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11734e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117346480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117343c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117348cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117349e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11734f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11734c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117353f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117346a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117347b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11734ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117351100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117349840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11734a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11734ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117342b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11734cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11734d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1173475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117348700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117355600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117352dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1173447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11734dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1173430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117343680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117345340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117355bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11734af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117353380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117349280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11734bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11734fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117347000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1173516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117345ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1173544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117351c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11734d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117356740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117344d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117356180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117344200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117354a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11734e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117350b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117353940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117352240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11734a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11735e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117308130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117335250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117341cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117304680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11735da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11730b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11735ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11735ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11735f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11735f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11735f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11735fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11735fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11735ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117360290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117360550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117360810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117360ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117360d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117361050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117361310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1173615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117361890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117361b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117361e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1173620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117362390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117362650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117362910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117362bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117362e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117363150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117363410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1173636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117363990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117363c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117363f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1173641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117364490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117364750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117364a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117364cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117364f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117365250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117365510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1173657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117365a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117365d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117366010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1173662d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117366590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117366850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117366b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117366dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117367090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117367350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117367610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1173678d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117367b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117367e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117368110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1173683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117368690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117368950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117368c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117368ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117369190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117369450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117369710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1173699d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117369c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117369f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11736a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11736a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11736a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11736aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11736ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11736afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11736b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11736b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11736bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11736bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11736c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11736c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11736c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11736c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11736cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11736ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11736d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11736d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11736d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11736d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11736dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11736dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11736e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11736e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11736e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11736e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11736eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11736ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11736f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11736f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11736f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11736fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11736fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11736ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1173702a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117370560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117370820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117370ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117370da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117371060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117371320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1173715e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1173718a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117371b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117371e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1173720e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1173723a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117372660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117372920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117372be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117372ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117373160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117373420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1173736e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1173739a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117373c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117373f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1173741e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1173744a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117374760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117374a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117374ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117374fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117375260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117375520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1173757e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117375aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117375d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117376020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1173762e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1173765a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117376860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117376b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117376de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1173770a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117377360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117377620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1173778e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117377ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117377e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117378120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117378660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117378ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1173790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1173793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1173798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117379e20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.778s
user	0m0.282s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4584 (81585779)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15800cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15800d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15800db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15800e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15800e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15800ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15800f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15800f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15800fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1580102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1580107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158010ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1580117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158011f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158012780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158012ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1580135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158013ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158014400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158014bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1580152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158015a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158016130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1580169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1580170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1580173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1580179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158018630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158018b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158018e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1580192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158019590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158019e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15801a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15801a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15801aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15801af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15801b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15801b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15801bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15801c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15801c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15801cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15801cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15801d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15801d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15801dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15801e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15801edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15801f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15801f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158020000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158020610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158020c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158021410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1580218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158021d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158022010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158022620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158022e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1580230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158023570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158023a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158023eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158024350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1580247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158024c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158025130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1580255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158025a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158025f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1580263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158026850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158026da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1580272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158027840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158027d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1580282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1580292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158029820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158029d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15802a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15802a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15802ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15802b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15802b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15802bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15802c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15802c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15802cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15802d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15802d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15802dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15802e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15802e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15801e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15802ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15802f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15802f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15802fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1580303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158030930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158030e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1580313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158031920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158031e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1580323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158032910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1580333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158033900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158033da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158034240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1580346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158034b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158035020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1580354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158035960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1580362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158036740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158036be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158037080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158037520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1580379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158037e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158038300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1580387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158038c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1580390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158039580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158039a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158039ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15803a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15803a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15803aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15803b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15803b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15803ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15803bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15803c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15803c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15803cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15803d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15803d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15803dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15803df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15803e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15803e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15803ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15803f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15803f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15803fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15803ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158040480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158040920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158041260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158041700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158041ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158042040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1580424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158042980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158042e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1580432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158043760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158043c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1580440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158044540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1580449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158044e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158045320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1580457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158045c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158046100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1580465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158046a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158046ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158047380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158047820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158047cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158048160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158048600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158048aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158048f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1580493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158049880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158049d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15804a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15804a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15804ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15804b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15804b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15804baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15804c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15804c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15804c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15804cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15804d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15804dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15804e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15804e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15804ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15804f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15804f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15804fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1580501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158050670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158050e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158051370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1580518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158051e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158052360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1580528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158052e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158053350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1580538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158053df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158054340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158054890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158054de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158055330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158055880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158055dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158056320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158056870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158056dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158057310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158057860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158057db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158058300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158058850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158058da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1580592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158059840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158059d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15805a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15805a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15805ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15805b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15805b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15805bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15805c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15805c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15805cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15805d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15805d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15805dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15805e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15805e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15805ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15805f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15805f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15805fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158060280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1580607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158060d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158061270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1580617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158061d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158062260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1580627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158062d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158063250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1580637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158063c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1580640e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158064580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158064a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158064ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158065360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158065800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158065ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158066140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1580665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158066a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158066f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1580673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158067860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158067d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158068250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158068970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158069090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1580697b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158069ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15806a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15806a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15806ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15806b250 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158104d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1581051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158105660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158105ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158105f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1581063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158106820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158106c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158107100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158107570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1581079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1581080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158108bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1581093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158109bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15810a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15810a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15810b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15810b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15810bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15810c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15810cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15810d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15810dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15810e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15810e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15810e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15810ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15810f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15810f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15810fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15810ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1581103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1581106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158110b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158110f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1581113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158111860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158111cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158112140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1581125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158112a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158112e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158113300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158113770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158113be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158114050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1581144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158114930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158114da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158115210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158115680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158115af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158115f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1581163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158116840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158116db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1581172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158117720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158117b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158118000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158118470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1581188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158118d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1581191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158119630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158119aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158119f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15811a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15811a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15811ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15811b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15811b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15811b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15811be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15811c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15811c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15811cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15811cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15811d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15811d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15811dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15811e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15811e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15811ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15811eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15811f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15811f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15811fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1581200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158120520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158120990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158120e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158121270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1581216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158121b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158121fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158122430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1581228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158122d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158123180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1581235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158123a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158123ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158124340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1581247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158124c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158125090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158125500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158125970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158125de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158126250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1581266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158126b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158126fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158127410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158127880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158127cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158128160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1581285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158128a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158128eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158129320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158129790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158129c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15812a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15812a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15812a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15812adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15812b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15812b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15812bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15812bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15812c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15812c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15812ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15812d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15812d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15812da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15812de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15812e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15812e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15812ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15812f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15812f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15812f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15812fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158130210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158130680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158130af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158130f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1581313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158131840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158131cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158132120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158132590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158132a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158132e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1581332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158133750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158133bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158134030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1581344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158134910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158134d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1581351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158135e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1581360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1581363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158136810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158136c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1581370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158137560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1581379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158137e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1581382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158138720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158138b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158139000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158139470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1581398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158139d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15813a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15813a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15813aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15813af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15813b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15813b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15813bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15813c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15813c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15813c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15813ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15813d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15813d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15813db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15813dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15813e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15813e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15813ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15813f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15813f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15813fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158140080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1581404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158140960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158140dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158141240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158141760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158141c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1581427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158142aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158143060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158143620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158143be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1581441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158144760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158144d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1581452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1581458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158145e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158146420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1581469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158146fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158147560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158147b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1581480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1581486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158148c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158149220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1581497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158149da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15814a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15814a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15814aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15814b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15814ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15814c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15814c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15814cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15814d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15814d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15814dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15814e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15814e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15814ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15814f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15814f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15814ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158150520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158150ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1581510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158151660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158151c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1581521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1581527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158152d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158153320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1581538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158153ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158154460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158154a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158154fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1581555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158155b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158156120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1581566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158156ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1581571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1581576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158157ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1581580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1581585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158158aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158158fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1581594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1581599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158159ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15815a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15815a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15815ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15815b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15815b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15815c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15815c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15815cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15815d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15815d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15815e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15815e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15815ea90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157708470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1577088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157708d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1577091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157709630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157709f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15770a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15770a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15770ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15770b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15770b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15770c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15770cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15770d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15770da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15770e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15770e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15770ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15770f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15770fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157710580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157710ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1577113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157711ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157712060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1577124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157712940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157712db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1577132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1577137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157713ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1577147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157714d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157715230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157715730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157715c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157716130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157716630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157716b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157717030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157717530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1577179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157718280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1577186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157718b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157718fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157719440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1577198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157719d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15771a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15771a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15771ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15771b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15771b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15771bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15771c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15771c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15771cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15771d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15771d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15771da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15771df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15771e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15771e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15771ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15771f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15771f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15771fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157720030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157720580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157720ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157721020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157721570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157721ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157722560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157722ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157723000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157723550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157723aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157723ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157724540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157724a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157725a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157725fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157726520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157726a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157726fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157727510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157727a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157727fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157728500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157728a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157728fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1577294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157729a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157729f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15772a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15772aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15772af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15772b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15772ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15772bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15772c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15772ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15772cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15772d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15772d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15772dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15772e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15772e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15772eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15772efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15772f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15772f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15772fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157730240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1577306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157730b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157731020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1577314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157731960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157731e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1577322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157732740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157732be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157733080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157733520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1577339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157733e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157734300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1577347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157734c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1577350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157735580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157735ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157736360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157736800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157736ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157737140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1577375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157737a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157737f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1577383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157738860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157738d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1577391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157739640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157739ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157739f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15773a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15773a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15773ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15773b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15773b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15773bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15773bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15773c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15773c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15773cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15773d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15773d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15773dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15773e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15773e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15773e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15773ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15773f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15773f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15773fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1577400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157740540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1577409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157740e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157741320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1577417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157742100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1577425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157742a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157742ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157743380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157743820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157743cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157744160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1577446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157744c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157745150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1577456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157745960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157745f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157746580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157746b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157747380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157747820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157747ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1577480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157748700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157748ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157749830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157749cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15774a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15774a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15774af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15774b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15774b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15774bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15774c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15774c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15774cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15774d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15774d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15774def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15774e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15774e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15774eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15774f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15774f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15774fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157750420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157750970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157750ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157751410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157751960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157751eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157752400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157752950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157752ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1577533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157753940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157753e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1577543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157754930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157754e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1577553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157755920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157755e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1577563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157756910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157756e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1577573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157757900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157757e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1577583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1577588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157758e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157759390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1577598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157759e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15775a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15775a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15775ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15775b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15775b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15775be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15775c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15775c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15775ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15775d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15775d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15775dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15775e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15775e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15775e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15775ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15775f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15775f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15775fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1577600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157760580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157760a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157760ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157761360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1577618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157761fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1577626f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157762e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157763530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1577637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157763fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1577642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1577648b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.957s
user	0m0.236s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
