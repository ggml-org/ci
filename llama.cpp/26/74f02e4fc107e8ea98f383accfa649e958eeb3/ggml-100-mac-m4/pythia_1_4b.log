Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.590s
user	0m0.893s
sys	0m1.260s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target build_info
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-simple
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-tokenizer-0
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-sampling
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-arg-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-chat-template
[ 64%] Built target test-gguf
[ 64%] Built target test-backend-ops
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Built target test-quantize-perf
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-rope
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-batched
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-infill
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target llama-bench
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-cli
[ 82%] Built target llama-parallel
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-lookup-stats
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-quantize
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Built target llama-retrieval
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative
[ 92%] Built target llama-run
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tts
[ 92%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.112s
user	0m6.113s
sys	0m9.498s

main: quantize time =  5172.39 ms
main:    total time =  5172.39 ms

main: quantize time =  3152.58 ms
main:    total time =  3152.58 ms

main: quantize time =  3076.98 ms
main:    total time =  3076.98 ms

main: quantize time =  3090.89 ms
main:    total time =  3090.89 ms

main: quantize time =  2182.28 ms
main:    total time =  2182.28 ms

main: quantize time =  5268.61 ms
main:    total time =  5268.61 ms

main: quantize time =  5703.51 ms
main:    total time =  5703.51 ms

main: quantize time =  7263.88 ms
main:    total time =  7263.88 ms

main: quantize time =  5923.43 ms
main:    total time =  5923.43 ms

main: quantize time =  4638.22 ms
main:    total time =  4638.22 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.172 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.391 I main: llama backend init
0.00.000.401 I main: load the model and apply lora adapter, if any
0.00.056.900 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.069.197 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.069.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.069.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.069.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.069.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.069.216 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.069.216 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.069.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.069.220 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.069.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.069.221 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.069.222 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.069.223 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.069.224 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.069.230 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.069.230 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.069.231 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.076.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.088.593 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.088.596 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.088.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.088.597 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.088.597 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.088.598 I llama_model_loader: - type  f32:  194 tensors
0.00.088.599 I llama_model_loader: - type  f16:   98 tensors
0.00.088.600 I print_info: file format = GGUF V3 (latest)
0.00.088.604 I print_info: file type   = all F32 (guessed)
0.00.088.606 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.119.320 I load: special tokens cache size = 25
0.00.126.895 I load: token to piece cache size = 0.2984 MB
0.00.126.899 I print_info: arch             = gptneox
0.00.126.899 I print_info: vocab_only       = 0
0.00.126.899 I print_info: n_ctx_train      = 2048
0.00.126.899 I print_info: n_embd           = 2048
0.00.126.899 I print_info: n_layer          = 24
0.00.126.903 I print_info: n_head           = 16
0.00.126.904 I print_info: n_head_kv        = 16
0.00.126.904 I print_info: n_rot            = 32
0.00.126.904 I print_info: n_swa            = 0
0.00.126.904 I print_info: n_embd_head_k    = 128
0.00.126.905 I print_info: n_embd_head_v    = 128
0.00.126.906 I print_info: n_gqa            = 1
0.00.126.907 I print_info: n_embd_k_gqa     = 2048
0.00.126.907 I print_info: n_embd_v_gqa     = 2048
0.00.126.908 I print_info: f_norm_eps       = 1.0e-05
0.00.126.908 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.126.908 I print_info: f_clamp_kqv      = 0.0e+00
0.00.126.909 I print_info: f_max_alibi_bias = 0.0e+00
0.00.126.909 I print_info: f_logit_scale    = 0.0e+00
0.00.126.909 I print_info: n_ff             = 8192
0.00.126.910 I print_info: n_expert         = 0
0.00.126.910 I print_info: n_expert_used    = 0
0.00.126.910 I print_info: causal attn      = 1
0.00.126.910 I print_info: pooling type     = 0
0.00.126.912 I print_info: rope type        = 2
0.00.126.912 I print_info: rope scaling     = linear
0.00.126.912 I print_info: freq_base_train  = 10000.0
0.00.126.912 I print_info: freq_scale_train = 1
0.00.126.913 I print_info: n_ctx_orig_yarn  = 2048
0.00.126.913 I print_info: rope_finetuned   = unknown
0.00.126.913 I print_info: ssm_d_conv       = 0
0.00.126.913 I print_info: ssm_d_inner      = 0
0.00.126.913 I print_info: ssm_d_state      = 0
0.00.126.913 I print_info: ssm_dt_rank      = 0
0.00.126.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.126.914 I print_info: model type       = 1.4B
0.00.126.914 I print_info: model params     = 1.41 B
0.00.126.914 I print_info: general.name     = 1.4B
0.00.126.915 I print_info: vocab type       = BPE
0.00.126.915 I print_info: n_vocab          = 50304
0.00.126.915 I print_info: n_merges         = 50009
0.00.126.915 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.126.915 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.126.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.126.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.126.916 I print_info: LF token         = 128 'Ä'
0.00.126.921 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.126.921 I print_info: max token length = 1024
0.00.163.019 I load_tensors: offloading 24 repeating layers to GPU
0.00.163.022 I load_tensors: offloading output layer to GPU
0.00.163.023 I load_tensors: offloaded 25/25 layers to GPU
0.00.163.047 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.163.048 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.163.334 I llama_init_from_model: n_seq_max     = 1
0.00.163.335 I llama_init_from_model: n_ctx         = 2048
0.00.163.335 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.163.335 I llama_init_from_model: n_batch       = 2048
0.00.163.335 I llama_init_from_model: n_ubatch      = 512
0.00.163.336 I llama_init_from_model: flash_attn    = 0
0.00.163.336 I llama_init_from_model: freq_base     = 10000.0
0.00.163.336 I llama_init_from_model: freq_scale    = 1
0.00.163.337 I ggml_metal_init: allocating
0.00.163.356 I ggml_metal_init: found device: Apple M4
0.00.163.360 I ggml_metal_init: picking default device: Apple M4
0.00.163.945 I ggml_metal_init: using embedded metal library
0.00.172.560 I ggml_metal_init: GPU name:   Apple M4
0.00.172.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.172.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.172.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.172.563 I ggml_metal_init: simdgroup reduction   = true
0.00.172.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.172.563 I ggml_metal_init: has residency sets    = true
0.00.172.563 I ggml_metal_init: has bfloat            = true
0.00.172.564 I ggml_metal_init: use bfloat            = true
0.00.172.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.172.565 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.196.268 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.225.787 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.225.795 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.225.818 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.230.046 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.230.049 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.230.049 I llama_init_from_model: graph nodes  = 967
0.00.230.050 I llama_init_from_model: graph splits = 2
0.00.230.053 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.230.169 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.230.170 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.295.535 I main: llama threadpool init, n_threads = 4
0.00.295.577 I 
0.00.295.611 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.295.612 I 
0.00.295.677 I sampler seed: 1234
0.00.295.681 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.295.705 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.295.707 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.295.707 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.129.520 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.02.129.521 I llama_perf_context_print:        load time =     237.58 ms
0.02.129.522 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.17 tokens per second)
0.02.129.522 I llama_perf_context_print:        eval time =    1787.32 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.129.523 I llama_perf_context_print:       total time =    1835.03 ms /    70 tokens
0.02.129.741 I ggml_metal_free: deallocating

real	0m2.433s
user	0m0.150s
sys	0m0.135s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.999 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.189 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.191 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.192 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.192 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.193 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.194 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.198 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.526 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.529 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.529 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.530 I llama_model_loader: - type  f32:  194 tensors
0.00.027.530 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.531 I print_info: file format = GGUF V3 (latest)
0.00.027.532 I print_info: file type   = Q8_0
0.00.027.533 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.126 I load: special tokens cache size = 25
0.00.054.209 I load: token to piece cache size = 0.2984 MB
0.00.054.214 I print_info: arch             = gptneox
0.00.054.214 I print_info: vocab_only       = 0
0.00.054.214 I print_info: n_ctx_train      = 2048
0.00.054.214 I print_info: n_embd           = 2048
0.00.054.214 I print_info: n_layer          = 24
0.00.054.223 I print_info: n_head           = 16
0.00.054.224 I print_info: n_head_kv        = 16
0.00.054.225 I print_info: n_rot            = 32
0.00.054.225 I print_info: n_swa            = 0
0.00.054.225 I print_info: n_embd_head_k    = 128
0.00.054.225 I print_info: n_embd_head_v    = 128
0.00.054.226 I print_info: n_gqa            = 1
0.00.054.227 I print_info: n_embd_k_gqa     = 2048
0.00.054.227 I print_info: n_embd_v_gqa     = 2048
0.00.054.228 I print_info: f_norm_eps       = 1.0e-05
0.00.054.229 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.229 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.229 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.229 I print_info: f_logit_scale    = 0.0e+00
0.00.054.230 I print_info: n_ff             = 8192
0.00.054.230 I print_info: n_expert         = 0
0.00.054.231 I print_info: n_expert_used    = 0
0.00.054.231 I print_info: causal attn      = 1
0.00.054.231 I print_info: pooling type     = 0
0.00.054.234 I print_info: rope type        = 2
0.00.054.234 I print_info: rope scaling     = linear
0.00.054.234 I print_info: freq_base_train  = 10000.0
0.00.054.234 I print_info: freq_scale_train = 1
0.00.054.235 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.235 I print_info: rope_finetuned   = unknown
0.00.054.235 I print_info: ssm_d_conv       = 0
0.00.054.235 I print_info: ssm_d_inner      = 0
0.00.054.235 I print_info: ssm_d_state      = 0
0.00.054.235 I print_info: ssm_dt_rank      = 0
0.00.054.235 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.236 I print_info: model type       = 1.4B
0.00.054.236 I print_info: model params     = 1.41 B
0.00.054.236 I print_info: general.name     = 1.4B
0.00.054.237 I print_info: vocab type       = BPE
0.00.054.237 I print_info: n_vocab          = 50304
0.00.054.237 I print_info: n_merges         = 50009
0.00.054.237 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.237 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.238 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.238 I print_info: LF token         = 128 'Ä'
0.00.054.238 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.239 I print_info: max token length = 1024
0.00.906.411 I load_tensors: offloading 24 repeating layers to GPU
0.00.906.414 I load_tensors: offloading output layer to GPU
0.00.906.415 I load_tensors: offloaded 25/25 layers to GPU
0.00.906.437 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.906.440 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.907.202 I llama_init_from_model: n_seq_max     = 1
0.00.907.203 I llama_init_from_model: n_ctx         = 2048
0.00.907.204 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.907.204 I llama_init_from_model: n_batch       = 2048
0.00.907.204 I llama_init_from_model: n_ubatch      = 512
0.00.907.205 I llama_init_from_model: flash_attn    = 0
0.00.907.206 I llama_init_from_model: freq_base     = 10000.0
0.00.907.206 I llama_init_from_model: freq_scale    = 1
0.00.907.207 I ggml_metal_init: allocating
0.00.907.234 I ggml_metal_init: found device: Apple M4
0.00.907.241 I ggml_metal_init: picking default device: Apple M4
0.00.908.574 I ggml_metal_init: using embedded metal library
0.00.913.808 I ggml_metal_init: GPU name:   Apple M4
0.00.913.811 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.913.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.913.812 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.913.813 I ggml_metal_init: simdgroup reduction   = true
0.00.913.813 I ggml_metal_init: simdgroup matrix mul. = true
0.00.913.813 I ggml_metal_init: has residency sets    = true
0.00.913.814 I ggml_metal_init: has bfloat            = true
0.00.913.814 I ggml_metal_init: use bfloat            = true
0.00.913.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.913.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.928.606 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.968.542 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.968.550 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.968.574 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.973.283 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.973.286 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.973.286 I llama_init_from_model: graph nodes  = 967
0.00.973.286 I llama_init_from_model: graph splits = 2
0.00.973.293 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.973.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.973.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.030.786 I main: llama threadpool init, n_threads = 4
0.01.030.828 I 
0.01.030.850 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.030.853 I 
0.01.031.077 I sampler seed: 1234
0.01.031.081 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.031.122 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.031.125 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.031.125 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.109.391 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.02.109.391 I llama_perf_context_print:        load time =    1019.86 ms
0.02.109.392 I llama_perf_context_print: prompt eval time =      44.35 ms /     7 tokens (    6.34 ms per token,   157.83 tokens per second)
0.02.109.393 I llama_perf_context_print:        eval time =    1031.11 ms /    63 runs   (   16.37 ms per token,    61.10 tokens per second)
0.02.109.393 I llama_perf_context_print:       total time =    1079.53 ms /    70 tokens
0.02.109.675 I ggml_metal_free: deallocating

real	0m2.130s
user	0m0.121s
sys	0m0.245s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.011.140 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.947 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.947 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.954 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.872 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.858 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.859 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.859 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.860 I llama_model_loader: - type  f32:  194 tensors
0.00.027.860 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.861 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.861 I print_info: file format = GGUF V3 (latest)
0.00.027.862 I print_info: file type   = Q4_0
0.00.027.863 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.794 I load: special tokens cache size = 25
0.00.053.950 I load: token to piece cache size = 0.2984 MB
0.00.053.954 I print_info: arch             = gptneox
0.00.053.954 I print_info: vocab_only       = 0
0.00.053.954 I print_info: n_ctx_train      = 2048
0.00.053.955 I print_info: n_embd           = 2048
0.00.053.955 I print_info: n_layer          = 24
0.00.053.960 I print_info: n_head           = 16
0.00.053.963 I print_info: n_head_kv        = 16
0.00.053.963 I print_info: n_rot            = 32
0.00.053.963 I print_info: n_swa            = 0
0.00.053.963 I print_info: n_embd_head_k    = 128
0.00.053.963 I print_info: n_embd_head_v    = 128
0.00.053.964 I print_info: n_gqa            = 1
0.00.053.965 I print_info: n_embd_k_gqa     = 2048
0.00.053.966 I print_info: n_embd_v_gqa     = 2048
0.00.053.966 I print_info: f_norm_eps       = 1.0e-05
0.00.053.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.969 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.969 I print_info: f_logit_scale    = 0.0e+00
0.00.053.970 I print_info: n_ff             = 8192
0.00.053.971 I print_info: n_expert         = 0
0.00.053.971 I print_info: n_expert_used    = 0
0.00.053.971 I print_info: causal attn      = 1
0.00.053.971 I print_info: pooling type     = 0
0.00.053.971 I print_info: rope type        = 2
0.00.053.971 I print_info: rope scaling     = linear
0.00.053.972 I print_info: freq_base_train  = 10000.0
0.00.053.972 I print_info: freq_scale_train = 1
0.00.053.972 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.972 I print_info: rope_finetuned   = unknown
0.00.053.972 I print_info: ssm_d_conv       = 0
0.00.053.973 I print_info: ssm_d_inner      = 0
0.00.053.976 I print_info: ssm_d_state      = 0
0.00.053.977 I print_info: ssm_dt_rank      = 0
0.00.053.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.977 I print_info: model type       = 1.4B
0.00.053.977 I print_info: model params     = 1.41 B
0.00.053.978 I print_info: general.name     = 1.4B
0.00.053.979 I print_info: vocab type       = BPE
0.00.053.979 I print_info: n_vocab          = 50304
0.00.053.979 I print_info: n_merges         = 50009
0.00.053.979 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.980 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.980 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.980 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.980 I print_info: LF token         = 128 'Ä'
0.00.053.980 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.981 I print_info: max token length = 1024
0.00.592.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.634 I load_tensors: offloading output layer to GPU
0.00.592.635 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.657 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.592.659 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.593.466 I llama_init_from_model: n_seq_max     = 1
0.00.593.468 I llama_init_from_model: n_ctx         = 2048
0.00.593.469 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.469 I llama_init_from_model: n_batch       = 2048
0.00.593.469 I llama_init_from_model: n_ubatch      = 512
0.00.593.470 I llama_init_from_model: flash_attn    = 0
0.00.593.471 I llama_init_from_model: freq_base     = 10000.0
0.00.593.471 I llama_init_from_model: freq_scale    = 1
0.00.593.472 I ggml_metal_init: allocating
0.00.593.510 I ggml_metal_init: found device: Apple M4
0.00.593.517 I ggml_metal_init: picking default device: Apple M4
0.00.594.528 I ggml_metal_init: using embedded metal library
0.00.598.986 I ggml_metal_init: GPU name:   Apple M4
0.00.598.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.995 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.996 I ggml_metal_init: simdgroup reduction   = true
0.00.598.996 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.996 I ggml_metal_init: has residency sets    = true
0.00.598.997 I ggml_metal_init: has bfloat            = true
0.00.598.997 I ggml_metal_init: use bfloat            = true
0.00.598.998 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.644 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.644.119 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.644.124 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.644.148 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.648.820 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.648.822 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.648.823 I llama_init_from_model: graph nodes  = 967
0.00.648.823 I llama_init_from_model: graph splits = 2
0.00.648.828 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.648.944 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.648.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.378 I main: llama threadpool init, n_threads = 4
0.00.702.413 I 
0.00.702.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.433 I 
0.00.702.653 I sampler seed: 1234
0.00.702.657 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.675 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.676 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.676 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.376.074 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.376.075 I llama_perf_context_print:        load time =     690.28 ms
0.01.376.076 I llama_perf_context_print: prompt eval time =      46.62 ms /     7 tokens (    6.66 ms per token,   150.15 tokens per second)
0.01.376.076 I llama_perf_context_print:        eval time =     624.35 ms /    63 runs   (    9.91 ms per token,   100.90 tokens per second)
0.01.376.076 I llama_perf_context_print:       total time =     674.65 ms /    70 tokens
0.01.376.258 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.117s
sys	0m0.160s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.450 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.571 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.755 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.042 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.042 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.043 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.043 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.044 I llama_model_loader: - type  f32:  194 tensors
0.00.028.044 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.044 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.045 I print_info: file format = GGUF V3 (latest)
0.00.028.049 I print_info: file type   = Q4_1
0.00.028.051 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.742 I load: special tokens cache size = 25
0.00.053.710 I load: token to piece cache size = 0.2984 MB
0.00.053.716 I print_info: arch             = gptneox
0.00.053.716 I print_info: vocab_only       = 0
0.00.053.716 I print_info: n_ctx_train      = 2048
0.00.053.716 I print_info: n_embd           = 2048
0.00.053.717 I print_info: n_layer          = 24
0.00.053.721 I print_info: n_head           = 16
0.00.053.722 I print_info: n_head_kv        = 16
0.00.053.722 I print_info: n_rot            = 32
0.00.053.722 I print_info: n_swa            = 0
0.00.053.722 I print_info: n_embd_head_k    = 128
0.00.053.722 I print_info: n_embd_head_v    = 128
0.00.053.723 I print_info: n_gqa            = 1
0.00.053.723 I print_info: n_embd_k_gqa     = 2048
0.00.053.724 I print_info: n_embd_v_gqa     = 2048
0.00.053.724 I print_info: f_norm_eps       = 1.0e-05
0.00.053.725 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.725 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.725 I print_info: f_logit_scale    = 0.0e+00
0.00.053.726 I print_info: n_ff             = 8192
0.00.053.726 I print_info: n_expert         = 0
0.00.053.726 I print_info: n_expert_used    = 0
0.00.053.726 I print_info: causal attn      = 1
0.00.053.729 I print_info: pooling type     = 0
0.00.053.730 I print_info: rope type        = 2
0.00.053.730 I print_info: rope scaling     = linear
0.00.053.730 I print_info: freq_base_train  = 10000.0
0.00.053.731 I print_info: freq_scale_train = 1
0.00.053.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.731 I print_info: rope_finetuned   = unknown
0.00.053.733 I print_info: ssm_d_conv       = 0
0.00.053.733 I print_info: ssm_d_inner      = 0
0.00.053.733 I print_info: ssm_d_state      = 0
0.00.053.733 I print_info: ssm_dt_rank      = 0
0.00.053.733 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.733 I print_info: model type       = 1.4B
0.00.053.734 I print_info: model params     = 1.41 B
0.00.053.734 I print_info: general.name     = 1.4B
0.00.053.735 I print_info: vocab type       = BPE
0.00.053.735 I print_info: n_vocab          = 50304
0.00.053.735 I print_info: n_merges         = 50009
0.00.053.735 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.735 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.735 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.736 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.736 I print_info: LF token         = 128 'Ä'
0.00.053.736 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.736 I print_info: max token length = 1024
0.00.656.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.904 I load_tensors: offloading output layer to GPU
0.00.656.905 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.937 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.656.939 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.658.248 I llama_init_from_model: n_seq_max     = 1
0.00.658.253 I llama_init_from_model: n_ctx         = 2048
0.00.658.253 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.254 I llama_init_from_model: n_batch       = 2048
0.00.658.254 I llama_init_from_model: n_ubatch      = 512
0.00.658.255 I llama_init_from_model: flash_attn    = 0
0.00.658.257 I llama_init_from_model: freq_base     = 10000.0
0.00.658.257 I llama_init_from_model: freq_scale    = 1
0.00.658.261 I ggml_metal_init: allocating
0.00.658.366 I ggml_metal_init: found device: Apple M4
0.00.658.375 I ggml_metal_init: picking default device: Apple M4
0.00.660.210 I ggml_metal_init: using embedded metal library
0.00.667.163 I ggml_metal_init: GPU name:   Apple M4
0.00.667.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.173 I ggml_metal_init: simdgroup reduction   = true
0.00.667.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.174 I ggml_metal_init: has residency sets    = true
0.00.667.174 I ggml_metal_init: has bfloat            = true
0.00.667.174 I ggml_metal_init: use bfloat            = true
0.00.667.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.710 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.197 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.232 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.376 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.379 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.379 I llama_init_from_model: graph nodes  = 967
0.00.748.379 I llama_init_from_model: graph splits = 2
0.00.748.384 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.512 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.689 I main: llama threadpool init, n_threads = 4
0.00.803.734 I 
0.00.803.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.762 I 
0.00.803.959 I sampler seed: 1234
0.00.803.964 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.804.007 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.804.010 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.804.011 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.528.586 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.528.586 I llama_perf_context_print:        load time =     792.34 ms
0.01.528.587 I llama_perf_context_print: prompt eval time =      43.51 ms /     7 tokens (    6.22 ms per token,   160.88 tokens per second)
0.01.528.588 I llama_perf_context_print:        eval time =     678.29 ms /    63 runs   (   10.77 ms per token,    92.88 tokens per second)
0.01.528.588 I llama_perf_context_print:       total time =     725.80 ms /    70 tokens
0.01.528.815 I ggml_metal_free: deallocating

real	0m1.548s
user	0m0.123s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.030 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.332 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.032.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.344 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.346 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.738 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.738 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.041.740 I llama_model_loader: - type  f32:  194 tensors
0.00.041.740 I llama_model_loader: - type q5_0:   97 tensors
0.00.041.740 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.740 I print_info: file format = GGUF V3 (latest)
0.00.041.741 I print_info: file type   = Q5_0
0.00.041.742 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.063.302 I load: special tokens cache size = 25
0.00.069.652 I load: token to piece cache size = 0.2984 MB
0.00.069.655 I print_info: arch             = gptneox
0.00.069.655 I print_info: vocab_only       = 0
0.00.069.655 I print_info: n_ctx_train      = 2048
0.00.069.656 I print_info: n_embd           = 2048
0.00.069.656 I print_info: n_layer          = 24
0.00.069.659 I print_info: n_head           = 16
0.00.069.659 I print_info: n_head_kv        = 16
0.00.069.660 I print_info: n_rot            = 32
0.00.069.660 I print_info: n_swa            = 0
0.00.069.660 I print_info: n_embd_head_k    = 128
0.00.069.660 I print_info: n_embd_head_v    = 128
0.00.069.661 I print_info: n_gqa            = 1
0.00.069.661 I print_info: n_embd_k_gqa     = 2048
0.00.069.662 I print_info: n_embd_v_gqa     = 2048
0.00.069.662 I print_info: f_norm_eps       = 1.0e-05
0.00.069.663 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.663 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.663 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.663 I print_info: f_logit_scale    = 0.0e+00
0.00.069.664 I print_info: n_ff             = 8192
0.00.069.664 I print_info: n_expert         = 0
0.00.069.664 I print_info: n_expert_used    = 0
0.00.069.664 I print_info: causal attn      = 1
0.00.069.664 I print_info: pooling type     = 0
0.00.069.664 I print_info: rope type        = 2
0.00.069.666 I print_info: rope scaling     = linear
0.00.069.666 I print_info: freq_base_train  = 10000.0
0.00.069.667 I print_info: freq_scale_train = 1
0.00.069.667 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.667 I print_info: rope_finetuned   = unknown
0.00.069.667 I print_info: ssm_d_conv       = 0
0.00.069.667 I print_info: ssm_d_inner      = 0
0.00.069.667 I print_info: ssm_d_state      = 0
0.00.069.667 I print_info: ssm_dt_rank      = 0
0.00.069.668 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.668 I print_info: model type       = 1.4B
0.00.069.668 I print_info: model params     = 1.41 B
0.00.069.668 I print_info: general.name     = 1.4B
0.00.069.669 I print_info: vocab type       = BPE
0.00.069.669 I print_info: n_vocab          = 50304
0.00.069.669 I print_info: n_merges         = 50009
0.00.069.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.670 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.670 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.670 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.670 I print_info: LF token         = 128 'Ä'
0.00.069.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.671 I print_info: max token length = 1024
0.00.803.776 I load_tensors: offloading 24 repeating layers to GPU
0.00.803.791 I load_tensors: offloading output layer to GPU
0.00.803.792 I load_tensors: offloaded 25/25 layers to GPU
0.00.803.826 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.803.828 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.805.237 I llama_init_from_model: n_seq_max     = 1
0.00.805.242 I llama_init_from_model: n_ctx         = 2048
0.00.805.242 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.805.243 I llama_init_from_model: n_batch       = 2048
0.00.805.243 I llama_init_from_model: n_ubatch      = 512
0.00.805.243 I llama_init_from_model: flash_attn    = 0
0.00.805.246 I llama_init_from_model: freq_base     = 10000.0
0.00.805.246 I llama_init_from_model: freq_scale    = 1
0.00.805.248 I ggml_metal_init: allocating
0.00.805.327 I ggml_metal_init: found device: Apple M4
0.00.805.337 I ggml_metal_init: picking default device: Apple M4
0.00.807.252 I ggml_metal_init: using embedded metal library
0.00.813.706 I ggml_metal_init: GPU name:   Apple M4
0.00.813.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.813.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.813.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.813.711 I ggml_metal_init: simdgroup reduction   = true
0.00.813.712 I ggml_metal_init: simdgroup matrix mul. = true
0.00.813.712 I ggml_metal_init: has residency sets    = true
0.00.813.713 I ggml_metal_init: has bfloat            = true
0.00.813.713 I ggml_metal_init: use bfloat            = true
0.00.813.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.813.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.831.152 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.886.082 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.886.089 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.886.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.890.363 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.890.365 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.890.365 I llama_init_from_model: graph nodes  = 967
0.00.890.366 I llama_init_from_model: graph splits = 2
0.00.890.370 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.890.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.890.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.946.634 I main: llama threadpool init, n_threads = 4
0.00.946.680 I 
0.00.946.703 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.946.704 I 
0.00.946.922 I sampler seed: 1234
0.00.946.927 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.946.937 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.946.938 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.946.938 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.730.750 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54826.25 tokens per second)
0.01.730.751 I llama_perf_context_print:        load time =     936.71 ms
0.01.730.752 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.52 tokens per second)
0.01.730.752 I llama_perf_context_print:        eval time =     738.23 ms /    63 runs   (   11.72 ms per token,    85.34 tokens per second)
0.01.730.753 I llama_perf_context_print:       total time =     785.01 ms /    70 tokens
0.01.731.020 I ggml_metal_free: deallocating

real	0m1.748s
user	0m0.125s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.394 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.034 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.195 I llama_model_loader: - type  f32:  194 tensors
0.00.028.195 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.196 I print_info: file format = GGUF V3 (latest)
0.00.028.196 I print_info: file type   = Q5_1
0.00.028.197 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.047.886 I load: special tokens cache size = 25
0.00.053.933 I load: token to piece cache size = 0.2984 MB
0.00.053.936 I print_info: arch             = gptneox
0.00.053.936 I print_info: vocab_only       = 0
0.00.053.936 I print_info: n_ctx_train      = 2048
0.00.053.936 I print_info: n_embd           = 2048
0.00.053.936 I print_info: n_layer          = 24
0.00.053.939 I print_info: n_head           = 16
0.00.053.940 I print_info: n_head_kv        = 16
0.00.053.940 I print_info: n_rot            = 32
0.00.053.940 I print_info: n_swa            = 0
0.00.053.941 I print_info: n_embd_head_k    = 128
0.00.053.941 I print_info: n_embd_head_v    = 128
0.00.053.941 I print_info: n_gqa            = 1
0.00.053.942 I print_info: n_embd_k_gqa     = 2048
0.00.053.943 I print_info: n_embd_v_gqa     = 2048
0.00.053.943 I print_info: f_norm_eps       = 1.0e-05
0.00.053.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.957 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.958 I print_info: f_logit_scale    = 0.0e+00
0.00.053.963 I print_info: n_ff             = 8192
0.00.053.964 I print_info: n_expert         = 0
0.00.053.964 I print_info: n_expert_used    = 0
0.00.053.964 I print_info: causal attn      = 1
0.00.053.964 I print_info: pooling type     = 0
0.00.053.964 I print_info: rope type        = 2
0.00.053.964 I print_info: rope scaling     = linear
0.00.053.965 I print_info: freq_base_train  = 10000.0
0.00.053.965 I print_info: freq_scale_train = 1
0.00.053.965 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.967 I print_info: rope_finetuned   = unknown
0.00.053.967 I print_info: ssm_d_conv       = 0
0.00.053.967 I print_info: ssm_d_inner      = 0
0.00.053.967 I print_info: ssm_d_state      = 0
0.00.053.967 I print_info: ssm_dt_rank      = 0
0.00.053.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.968 I print_info: model type       = 1.4B
0.00.053.968 I print_info: model params     = 1.41 B
0.00.053.969 I print_info: general.name     = 1.4B
0.00.053.969 I print_info: vocab type       = BPE
0.00.053.969 I print_info: n_vocab          = 50304
0.00.053.969 I print_info: n_merges         = 50009
0.00.053.970 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.970 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.971 I print_info: LF token         = 128 'Ä'
0.00.053.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.971 I print_info: max token length = 1024
0.00.714.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.714.423 I load_tensors: offloading output layer to GPU
0.00.714.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.714.459 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.714.461 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.715.637 I llama_init_from_model: n_seq_max     = 1
0.00.715.641 I llama_init_from_model: n_ctx         = 2048
0.00.715.642 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.715.642 I llama_init_from_model: n_batch       = 2048
0.00.715.643 I llama_init_from_model: n_ubatch      = 512
0.00.715.643 I llama_init_from_model: flash_attn    = 0
0.00.715.645 I llama_init_from_model: freq_base     = 10000.0
0.00.715.646 I llama_init_from_model: freq_scale    = 1
0.00.715.648 I ggml_metal_init: allocating
0.00.715.733 I ggml_metal_init: found device: Apple M4
0.00.715.742 I ggml_metal_init: picking default device: Apple M4
0.00.717.551 I ggml_metal_init: using embedded metal library
0.00.724.228 I ggml_metal_init: GPU name:   Apple M4
0.00.724.232 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.724.232 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.724.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.724.234 I ggml_metal_init: simdgroup reduction   = true
0.00.724.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.724.234 I ggml_metal_init: has residency sets    = true
0.00.724.235 I ggml_metal_init: has bfloat            = true
0.00.724.235 I ggml_metal_init: use bfloat            = true
0.00.724.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.724.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.741.513 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.954 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.796.961 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.796.987 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.662 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.664 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.665 I llama_init_from_model: graph nodes  = 967
0.00.801.665 I llama_init_from_model: graph splits = 2
0.00.801.670 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.801 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.801 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.504 I main: llama threadpool init, n_threads = 4
0.00.860.551 I 
0.00.860.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.576 I 
0.00.860.808 I sampler seed: 1234
0.00.860.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.860.824 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.860.824 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.860.824 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.704.988 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.704.989 I llama_perf_context_print:        load time =     849.21 ms
0.01.704.990 I llama_perf_context_print: prompt eval time =      50.96 ms /     7 tokens (    7.28 ms per token,   137.37 tokens per second)
0.01.704.990 I llama_perf_context_print:        eval time =     790.33 ms /    63 runs   (   12.54 ms per token,    79.71 tokens per second)
0.01.704.991 I llama_perf_context_print:       total time =     845.39 ms /    70 tokens
0.01.705.216 I ggml_metal_free: deallocating

real	0m1.724s
user	0m0.122s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.352 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.217 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.219 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.220 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.220 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.221 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.223 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.224 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.224 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.224 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.227 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.227 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.354 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.379 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.381 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.381 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.381 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.382 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.382 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.383 I llama_model_loader: - type  f32:  194 tensors
0.00.025.383 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.383 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.383 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.384 I print_info: file format = GGUF V3 (latest)
0.00.025.384 I print_info: file type   = Q2_K - Medium
0.00.025.385 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.197 I load: special tokens cache size = 25
0.00.051.104 I load: token to piece cache size = 0.2984 MB
0.00.051.107 I print_info: arch             = gptneox
0.00.051.107 I print_info: vocab_only       = 0
0.00.051.108 I print_info: n_ctx_train      = 2048
0.00.051.108 I print_info: n_embd           = 2048
0.00.051.108 I print_info: n_layer          = 24
0.00.051.111 I print_info: n_head           = 16
0.00.051.112 I print_info: n_head_kv        = 16
0.00.051.112 I print_info: n_rot            = 32
0.00.051.112 I print_info: n_swa            = 0
0.00.051.112 I print_info: n_embd_head_k    = 128
0.00.051.112 I print_info: n_embd_head_v    = 128
0.00.051.113 I print_info: n_gqa            = 1
0.00.051.114 I print_info: n_embd_k_gqa     = 2048
0.00.051.115 I print_info: n_embd_v_gqa     = 2048
0.00.051.115 I print_info: f_norm_eps       = 1.0e-05
0.00.051.116 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.116 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.116 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.116 I print_info: f_logit_scale    = 0.0e+00
0.00.051.117 I print_info: n_ff             = 8192
0.00.051.117 I print_info: n_expert         = 0
0.00.051.117 I print_info: n_expert_used    = 0
0.00.051.117 I print_info: causal attn      = 1
0.00.051.117 I print_info: pooling type     = 0
0.00.051.117 I print_info: rope type        = 2
0.00.051.118 I print_info: rope scaling     = linear
0.00.051.118 I print_info: freq_base_train  = 10000.0
0.00.051.118 I print_info: freq_scale_train = 1
0.00.051.119 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.119 I print_info: rope_finetuned   = unknown
0.00.051.119 I print_info: ssm_d_conv       = 0
0.00.051.119 I print_info: ssm_d_inner      = 0
0.00.051.121 I print_info: ssm_d_state      = 0
0.00.051.122 I print_info: ssm_dt_rank      = 0
0.00.051.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.122 I print_info: model type       = 1.4B
0.00.051.122 I print_info: model params     = 1.41 B
0.00.051.122 I print_info: general.name     = 1.4B
0.00.051.123 I print_info: vocab type       = BPE
0.00.051.123 I print_info: n_vocab          = 50304
0.00.051.123 I print_info: n_merges         = 50009
0.00.051.124 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.124 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.124 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.124 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.124 I print_info: LF token         = 128 'Ä'
0.00.051.125 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.125 I print_info: max token length = 1024
0.00.391.948 I load_tensors: offloading 24 repeating layers to GPU
0.00.391.964 I load_tensors: offloading output layer to GPU
0.00.391.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.391.999 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.392.001 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.393.399 I llama_init_from_model: n_seq_max     = 1
0.00.393.404 I llama_init_from_model: n_ctx         = 2048
0.00.393.404 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.393.405 I llama_init_from_model: n_batch       = 2048
0.00.393.405 I llama_init_from_model: n_ubatch      = 512
0.00.393.406 I llama_init_from_model: flash_attn    = 0
0.00.393.407 I llama_init_from_model: freq_base     = 10000.0
0.00.393.408 I llama_init_from_model: freq_scale    = 1
0.00.393.410 I ggml_metal_init: allocating
0.00.393.503 I ggml_metal_init: found device: Apple M4
0.00.393.513 I ggml_metal_init: picking default device: Apple M4
0.00.395.370 I ggml_metal_init: using embedded metal library
0.00.400.960 I ggml_metal_init: GPU name:   Apple M4
0.00.400.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.400.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.400.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.400.979 I ggml_metal_init: simdgroup reduction   = true
0.00.400.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.400.979 I ggml_metal_init: has residency sets    = true
0.00.400.980 I ggml_metal_init: has bfloat            = true
0.00.400.980 I ggml_metal_init: use bfloat            = true
0.00.400.982 I ggml_metal_init: hasUnifiedMemory      = true
0.00.400.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.422.529 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.480.756 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.480.764 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.480.786 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.484.888 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.484.891 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.484.891 I llama_init_from_model: graph nodes  = 967
0.00.484.891 I llama_init_from_model: graph splits = 2
0.00.484.898 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.485.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.485.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.194 I main: llama threadpool init, n_threads = 4
0.00.547.238 I 
0.00.547.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.263 I 
0.00.547.480 I sampler seed: 1234
0.00.547.484 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.547.495 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.547.497 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.547.497 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.223.954 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.01.223.954 I llama_perf_context_print:        load time =     536.93 ms
0.01.223.955 I llama_perf_context_print: prompt eval time =      39.70 ms /     7 tokens (    5.67 ms per token,   176.34 tokens per second)
0.01.223.956 I llama_perf_context_print:        eval time =     633.95 ms /    63 runs   (   10.06 ms per token,    99.38 tokens per second)
0.01.223.956 I llama_perf_context_print:       total time =     677.67 ms /    70 tokens
0.01.224.182 I ggml_metal_free: deallocating

real	0m1.242s
user	0m0.125s
sys	0m0.173s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.958 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.712 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.712 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.619 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.620 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.620 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.621 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.621 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.622 I llama_model_loader: - type  f32:  194 tensors
0.00.025.622 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.622 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.622 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.623 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.623 I print_info: file format = GGUF V3 (latest)
0.00.025.624 I print_info: file type   = Q3_K - Medium
0.00.025.625 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.402 I load: special tokens cache size = 25
0.00.051.299 I load: token to piece cache size = 0.2984 MB
0.00.051.303 I print_info: arch             = gptneox
0.00.051.303 I print_info: vocab_only       = 0
0.00.051.303 I print_info: n_ctx_train      = 2048
0.00.051.303 I print_info: n_embd           = 2048
0.00.051.303 I print_info: n_layer          = 24
0.00.051.307 I print_info: n_head           = 16
0.00.051.307 I print_info: n_head_kv        = 16
0.00.051.307 I print_info: n_rot            = 32
0.00.051.308 I print_info: n_swa            = 0
0.00.051.308 I print_info: n_embd_head_k    = 128
0.00.051.308 I print_info: n_embd_head_v    = 128
0.00.051.309 I print_info: n_gqa            = 1
0.00.051.309 I print_info: n_embd_k_gqa     = 2048
0.00.051.310 I print_info: n_embd_v_gqa     = 2048
0.00.051.311 I print_info: f_norm_eps       = 1.0e-05
0.00.051.314 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.314 I print_info: f_logit_scale    = 0.0e+00
0.00.051.315 I print_info: n_ff             = 8192
0.00.051.315 I print_info: n_expert         = 0
0.00.051.316 I print_info: n_expert_used    = 0
0.00.051.317 I print_info: causal attn      = 1
0.00.051.319 I print_info: pooling type     = 0
0.00.051.319 I print_info: rope type        = 2
0.00.051.319 I print_info: rope scaling     = linear
0.00.051.320 I print_info: freq_base_train  = 10000.0
0.00.051.320 I print_info: freq_scale_train = 1
0.00.051.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.321 I print_info: rope_finetuned   = unknown
0.00.051.322 I print_info: ssm_d_conv       = 0
0.00.051.322 I print_info: ssm_d_inner      = 0
0.00.051.322 I print_info: ssm_d_state      = 0
0.00.051.322 I print_info: ssm_dt_rank      = 0
0.00.051.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.322 I print_info: model type       = 1.4B
0.00.051.323 I print_info: model params     = 1.41 B
0.00.051.323 I print_info: general.name     = 1.4B
0.00.051.324 I print_info: vocab type       = BPE
0.00.051.324 I print_info: n_vocab          = 50304
0.00.051.324 I print_info: n_merges         = 50009
0.00.051.324 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.325 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.325 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.325 I print_info: LF token         = 128 'Ä'
0.00.051.325 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.326 I print_info: max token length = 1024
0.00.430.760 I load_tensors: offloading 24 repeating layers to GPU
0.00.430.771 I load_tensors: offloading output layer to GPU
0.00.430.772 I load_tensors: offloaded 25/25 layers to GPU
0.00.430.805 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.430.808 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.431.978 I llama_init_from_model: n_seq_max     = 1
0.00.431.984 I llama_init_from_model: n_ctx         = 2048
0.00.431.984 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.431.985 I llama_init_from_model: n_batch       = 2048
0.00.431.985 I llama_init_from_model: n_ubatch      = 512
0.00.431.986 I llama_init_from_model: flash_attn    = 0
0.00.431.988 I llama_init_from_model: freq_base     = 10000.0
0.00.431.988 I llama_init_from_model: freq_scale    = 1
0.00.431.990 I ggml_metal_init: allocating
0.00.432.051 I ggml_metal_init: found device: Apple M4
0.00.432.060 I ggml_metal_init: picking default device: Apple M4
0.00.433.789 I ggml_metal_init: using embedded metal library
0.00.439.073 I ggml_metal_init: GPU name:   Apple M4
0.00.439.092 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.439.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.439.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.439.095 I ggml_metal_init: simdgroup reduction   = true
0.00.439.095 I ggml_metal_init: simdgroup matrix mul. = true
0.00.439.095 I ggml_metal_init: has residency sets    = true
0.00.439.096 I ggml_metal_init: has bfloat            = true
0.00.439.096 I ggml_metal_init: use bfloat            = true
0.00.439.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.439.102 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.460.578 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.519.130 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.519.137 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.519.162 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.523.483 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.523.484 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.523.485 I llama_init_from_model: graph nodes  = 967
0.00.523.485 I llama_init_from_model: graph splits = 2
0.00.523.491 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.523.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.523.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.578.529 I main: llama threadpool init, n_threads = 4
0.00.578.572 I 
0.00.578.595 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.578.595 I 
0.00.578.816 I sampler seed: 1234
0.00.578.821 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.578.845 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.578.847 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.578.847 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.314.476 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54657.43 tokens per second)
0.01.314.477 I llama_perf_context_print:        load time =     568.66 ms
0.01.314.478 I llama_perf_context_print: prompt eval time =      40.50 ms /     7 tokens (    5.79 ms per token,   172.84 tokens per second)
0.01.314.478 I llama_perf_context_print:        eval time =     692.31 ms /    63 runs   (   10.99 ms per token,    91.00 tokens per second)
0.01.314.479 I llama_perf_context_print:       total time =     736.86 ms /    70 tokens
0.01.314.705 I ggml_metal_free: deallocating

real	0m1.332s
user	0m0.125s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.011.027 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.783 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.784 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.786 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.362 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.363 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.363 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.028.364 I llama_model_loader: - type  f32:  194 tensors
0.00.028.364 I llama_model_loader: - type q4_K:   61 tensors
0.00.028.364 I llama_model_loader: - type q5_K:   24 tensors
0.00.028.364 I llama_model_loader: - type q6_K:   13 tensors
0.00.028.365 I print_info: file format = GGUF V3 (latest)
0.00.028.365 I print_info: file type   = Q4_K - Medium
0.00.028.366 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.606 I load: special tokens cache size = 25
0.00.053.740 I load: token to piece cache size = 0.2984 MB
0.00.053.743 I print_info: arch             = gptneox
0.00.053.743 I print_info: vocab_only       = 0
0.00.053.743 I print_info: n_ctx_train      = 2048
0.00.053.743 I print_info: n_embd           = 2048
0.00.053.743 I print_info: n_layer          = 24
0.00.053.746 I print_info: n_head           = 16
0.00.053.747 I print_info: n_head_kv        = 16
0.00.053.747 I print_info: n_rot            = 32
0.00.053.747 I print_info: n_swa            = 0
0.00.053.752 I print_info: n_embd_head_k    = 128
0.00.053.754 I print_info: n_embd_head_v    = 128
0.00.053.754 I print_info: n_gqa            = 1
0.00.053.755 I print_info: n_embd_k_gqa     = 2048
0.00.053.756 I print_info: n_embd_v_gqa     = 2048
0.00.053.756 I print_info: f_norm_eps       = 1.0e-05
0.00.053.761 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.761 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.762 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.762 I print_info: f_logit_scale    = 0.0e+00
0.00.053.762 I print_info: n_ff             = 8192
0.00.053.763 I print_info: n_expert         = 0
0.00.053.763 I print_info: n_expert_used    = 0
0.00.053.763 I print_info: causal attn      = 1
0.00.053.763 I print_info: pooling type     = 0
0.00.053.763 I print_info: rope type        = 2
0.00.053.764 I print_info: rope scaling     = linear
0.00.053.764 I print_info: freq_base_train  = 10000.0
0.00.053.764 I print_info: freq_scale_train = 1
0.00.053.764 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.765 I print_info: rope_finetuned   = unknown
0.00.053.765 I print_info: ssm_d_conv       = 0
0.00.053.766 I print_info: ssm_d_inner      = 0
0.00.053.766 I print_info: ssm_d_state      = 0
0.00.053.766 I print_info: ssm_dt_rank      = 0
0.00.053.766 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.766 I print_info: model type       = 1.4B
0.00.053.766 I print_info: model params     = 1.41 B
0.00.053.767 I print_info: general.name     = 1.4B
0.00.053.767 I print_info: vocab type       = BPE
0.00.053.768 I print_info: n_vocab          = 50304
0.00.053.770 I print_info: n_merges         = 50009
0.00.053.770 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.770 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.771 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.771 I print_info: LF token         = 128 'Ä'
0.00.053.771 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.771 I print_info: max token length = 1024
0.00.594.510 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.524 I load_tensors: offloading output layer to GPU
0.00.594.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.555 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.594.556 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.595.754 I llama_init_from_model: n_seq_max     = 1
0.00.595.758 I llama_init_from_model: n_ctx         = 2048
0.00.595.758 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.759 I llama_init_from_model: n_batch       = 2048
0.00.595.760 I llama_init_from_model: n_ubatch      = 512
0.00.595.760 I llama_init_from_model: flash_attn    = 0
0.00.595.761 I llama_init_from_model: freq_base     = 10000.0
0.00.595.762 I llama_init_from_model: freq_scale    = 1
0.00.595.763 I ggml_metal_init: allocating
0.00.595.791 I ggml_metal_init: found device: Apple M4
0.00.595.796 I ggml_metal_init: picking default device: Apple M4
0.00.597.265 I ggml_metal_init: using embedded metal library
0.00.603.678 I ggml_metal_init: GPU name:   Apple M4
0.00.603.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.684 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.684 I ggml_metal_init: simdgroup reduction   = true
0.00.603.685 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.685 I ggml_metal_init: has residency sets    = true
0.00.603.685 I ggml_metal_init: has bfloat            = true
0.00.603.685 I ggml_metal_init: use bfloat            = true
0.00.603.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.365 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.081 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.676.087 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.676.109 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.119 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.681.121 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.681.122 I llama_init_from_model: graph nodes  = 967
0.00.681.122 I llama_init_from_model: graph splits = 2
0.00.681.126 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.681.255 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.681.255 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.450 I main: llama threadpool init, n_threads = 4
0.00.732.492 I 
0.00.732.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.520 I 
0.00.732.692 I sampler seed: 1234
0.00.732.697 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.708 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.708 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.493.766 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50569.80 tokens per second)
0.01.493.767 I llama_perf_context_print:        load time =     720.50 ms
0.01.493.768 I llama_perf_context_print: prompt eval time =      52.24 ms /     7 tokens (    7.46 ms per token,   133.99 tokens per second)
0.01.493.768 I llama_perf_context_print:        eval time =     705.78 ms /    63 runs   (   11.20 ms per token,    89.26 tokens per second)
0.01.493.769 I llama_perf_context_print:       total time =     762.23 ms /    70 tokens
0.01.493.962 I ggml_metal_free: deallocating

real	0m1.512s
user	0m0.121s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.206 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.763 I llama_model_loader: - type  f32:  194 tensors
0.00.025.763 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.764 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.764 I print_info: file format = GGUF V3 (latest)
0.00.025.765 I print_info: file type   = Q5_K - Medium
0.00.025.765 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.932 I load: special tokens cache size = 25
0.00.050.820 I load: token to piece cache size = 0.2984 MB
0.00.050.825 I print_info: arch             = gptneox
0.00.050.826 I print_info: vocab_only       = 0
0.00.050.826 I print_info: n_ctx_train      = 2048
0.00.050.826 I print_info: n_embd           = 2048
0.00.050.826 I print_info: n_layer          = 24
0.00.050.829 I print_info: n_head           = 16
0.00.050.830 I print_info: n_head_kv        = 16
0.00.050.830 I print_info: n_rot            = 32
0.00.050.830 I print_info: n_swa            = 0
0.00.050.830 I print_info: n_embd_head_k    = 128
0.00.050.830 I print_info: n_embd_head_v    = 128
0.00.050.831 I print_info: n_gqa            = 1
0.00.050.832 I print_info: n_embd_k_gqa     = 2048
0.00.050.833 I print_info: n_embd_v_gqa     = 2048
0.00.050.833 I print_info: f_norm_eps       = 1.0e-05
0.00.050.834 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.834 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.834 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.835 I print_info: f_logit_scale    = 0.0e+00
0.00.050.836 I print_info: n_ff             = 8192
0.00.050.836 I print_info: n_expert         = 0
0.00.050.836 I print_info: n_expert_used    = 0
0.00.050.836 I print_info: causal attn      = 1
0.00.050.836 I print_info: pooling type     = 0
0.00.050.837 I print_info: rope type        = 2
0.00.050.837 I print_info: rope scaling     = linear
0.00.050.837 I print_info: freq_base_train  = 10000.0
0.00.050.838 I print_info: freq_scale_train = 1
0.00.050.838 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.838 I print_info: rope_finetuned   = unknown
0.00.050.838 I print_info: ssm_d_conv       = 0
0.00.050.838 I print_info: ssm_d_inner      = 0
0.00.050.838 I print_info: ssm_d_state      = 0
0.00.050.839 I print_info: ssm_dt_rank      = 0
0.00.050.839 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.839 I print_info: model type       = 1.4B
0.00.050.839 I print_info: model params     = 1.41 B
0.00.050.840 I print_info: general.name     = 1.4B
0.00.050.840 I print_info: vocab type       = BPE
0.00.050.840 I print_info: n_vocab          = 50304
0.00.050.841 I print_info: n_merges         = 50009
0.00.050.841 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.841 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.841 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.841 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.842 I print_info: LF token         = 128 'Ä'
0.00.050.842 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.842 I print_info: max token length = 1024
0.00.674.036 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.040 I load_tensors: offloading output layer to GPU
0.00.674.041 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.062 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.674.065 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.674.999 I llama_init_from_model: n_seq_max     = 1
0.00.675.001 I llama_init_from_model: n_ctx         = 2048
0.00.675.001 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.675.001 I llama_init_from_model: n_batch       = 2048
0.00.675.002 I llama_init_from_model: n_ubatch      = 512
0.00.675.002 I llama_init_from_model: flash_attn    = 0
0.00.675.003 I llama_init_from_model: freq_base     = 10000.0
0.00.675.003 I llama_init_from_model: freq_scale    = 1
0.00.675.004 I ggml_metal_init: allocating
0.00.675.036 I ggml_metal_init: found device: Apple M4
0.00.675.042 I ggml_metal_init: picking default device: Apple M4
0.00.676.387 I ggml_metal_init: using embedded metal library
0.00.681.762 I ggml_metal_init: GPU name:   Apple M4
0.00.681.766 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.768 I ggml_metal_init: simdgroup reduction   = true
0.00.681.768 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.769 I ggml_metal_init: has residency sets    = true
0.00.681.769 I ggml_metal_init: has bfloat            = true
0.00.681.769 I ggml_metal_init: use bfloat            = true
0.00.681.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.307 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.800 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.751.806 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.828 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.757.374 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.757.376 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.757.376 I llama_init_from_model: graph nodes  = 967
0.00.757.377 I llama_init_from_model: graph splits = 2
0.00.757.382 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.757.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.757.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.812.512 I main: llama threadpool init, n_threads = 4
0.00.812.562 I 
0.00.812.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.812.583 I 
0.00.812.753 I sampler seed: 1234
0.00.812.757 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.812.766 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.812.767 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.812.767 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.657.166 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.657.167 I llama_perf_context_print:        load time =     802.31 ms
0.01.657.168 I llama_perf_context_print: prompt eval time =      51.25 ms /     7 tokens (    7.32 ms per token,   136.57 tokens per second)
0.01.657.168 I llama_perf_context_print:        eval time =     790.31 ms /    63 runs   (   12.54 ms per token,    79.72 tokens per second)
0.01.657.169 I llama_perf_context_print:       total time =     845.65 ms /    70 tokens
0.01.657.391 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.119s
sys	0m0.247s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.629 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.634 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.640 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.641 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.646 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.646 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.651 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.995 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.070 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.028.074 I llama_model_loader: - type  f32:  194 tensors
0.00.028.074 I llama_model_loader: - type q6_K:   98 tensors
0.00.028.075 I print_info: file format = GGUF V3 (latest)
0.00.028.075 I print_info: file type   = Q6_K
0.00.028.076 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.047.859 I load: special tokens cache size = 25
0.00.053.639 I load: token to piece cache size = 0.2984 MB
0.00.053.642 I print_info: arch             = gptneox
0.00.053.642 I print_info: vocab_only       = 0
0.00.053.642 I print_info: n_ctx_train      = 2048
0.00.053.643 I print_info: n_embd           = 2048
0.00.053.643 I print_info: n_layer          = 24
0.00.053.646 I print_info: n_head           = 16
0.00.053.646 I print_info: n_head_kv        = 16
0.00.053.646 I print_info: n_rot            = 32
0.00.053.651 I print_info: n_swa            = 0
0.00.053.652 I print_info: n_embd_head_k    = 128
0.00.053.652 I print_info: n_embd_head_v    = 128
0.00.053.653 I print_info: n_gqa            = 1
0.00.053.654 I print_info: n_embd_k_gqa     = 2048
0.00.053.655 I print_info: n_embd_v_gqa     = 2048
0.00.053.656 I print_info: f_norm_eps       = 1.0e-05
0.00.053.656 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.656 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.656 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.657 I print_info: f_logit_scale    = 0.0e+00
0.00.053.657 I print_info: n_ff             = 8192
0.00.053.657 I print_info: n_expert         = 0
0.00.053.658 I print_info: n_expert_used    = 0
0.00.053.658 I print_info: causal attn      = 1
0.00.053.659 I print_info: pooling type     = 0
0.00.053.659 I print_info: rope type        = 2
0.00.053.659 I print_info: rope scaling     = linear
0.00.053.659 I print_info: freq_base_train  = 10000.0
0.00.053.660 I print_info: freq_scale_train = 1
0.00.053.662 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.662 I print_info: rope_finetuned   = unknown
0.00.053.662 I print_info: ssm_d_conv       = 0
0.00.053.662 I print_info: ssm_d_inner      = 0
0.00.053.662 I print_info: ssm_d_state      = 0
0.00.053.662 I print_info: ssm_dt_rank      = 0
0.00.053.662 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.663 I print_info: model type       = 1.4B
0.00.053.663 I print_info: model params     = 1.41 B
0.00.053.663 I print_info: general.name     = 1.4B
0.00.053.664 I print_info: vocab type       = BPE
0.00.053.664 I print_info: n_vocab          = 50304
0.00.053.664 I print_info: n_merges         = 50009
0.00.053.664 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.664 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.664 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.665 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.665 I print_info: LF token         = 128 'Ä'
0.00.053.665 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.668 I print_info: max token length = 1024
0.00.746.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.956 I load_tensors: offloading output layer to GPU
0.00.746.957 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.978 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.746.979 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.747.829 I llama_init_from_model: n_seq_max     = 1
0.00.747.831 I llama_init_from_model: n_ctx         = 2048
0.00.747.831 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.747.831 I llama_init_from_model: n_batch       = 2048
0.00.747.832 I llama_init_from_model: n_ubatch      = 512
0.00.747.832 I llama_init_from_model: flash_attn    = 0
0.00.747.833 I llama_init_from_model: freq_base     = 10000.0
0.00.747.833 I llama_init_from_model: freq_scale    = 1
0.00.747.834 I ggml_metal_init: allocating
0.00.747.845 I ggml_metal_init: found device: Apple M4
0.00.747.850 I ggml_metal_init: picking default device: Apple M4
0.00.749.158 I ggml_metal_init: using embedded metal library
0.00.754.763 I ggml_metal_init: GPU name:   Apple M4
0.00.754.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.754.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.754.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.754.769 I ggml_metal_init: simdgroup reduction   = true
0.00.754.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.754.770 I ggml_metal_init: has residency sets    = true
0.00.754.770 I ggml_metal_init: has bfloat            = true
0.00.754.770 I ggml_metal_init: use bfloat            = true
0.00.754.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.754.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.178 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.824.811 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.824.819 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.824.842 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.829.801 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.829.803 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.829.803 I llama_init_from_model: graph nodes  = 967
0.00.829.803 I llama_init_from_model: graph splits = 2
0.00.829.808 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.829.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.829.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.887.952 I main: llama threadpool init, n_threads = 4
0.00.887.998 I 
0.00.888.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.888.024 I 
0.00.888.193 I sampler seed: 1234
0.00.888.199 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.888.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.888.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.888.236 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.756.084 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48332.20 tokens per second)
0.01.756.085 I llama_perf_context_print:        load time =     876.41 ms
0.01.756.085 I llama_perf_context_print: prompt eval time =      54.61 ms /     7 tokens (    7.80 ms per token,   128.17 tokens per second)
0.01.756.086 I llama_perf_context_print:        eval time =     810.36 ms /    63 runs   (   12.86 ms per token,    77.74 tokens per second)
0.01.756.087 I llama_perf_context_print:       total time =     869.04 ms /    70 tokens
0.01.756.371 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.119s
sys	0m0.265s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.528 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.532 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.542 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.547 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.374 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.375 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.375 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.376 I llama_model_loader: - type  f32:  194 tensors
0.00.052.376 I llama_model_loader: - type  f16:   98 tensors
0.00.052.377 I print_info: file format = GGUF V3 (latest)
0.00.052.378 I print_info: file type   = all F32 (guessed)
0.00.052.379 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.078.616 I load: special tokens cache size = 25
0.00.085.010 I load: token to piece cache size = 0.2984 MB
0.00.085.013 I print_info: arch             = gptneox
0.00.085.013 I print_info: vocab_only       = 0
0.00.085.014 I print_info: n_ctx_train      = 2048
0.00.085.014 I print_info: n_embd           = 2048
0.00.085.014 I print_info: n_layer          = 24
0.00.085.017 I print_info: n_head           = 16
0.00.085.017 I print_info: n_head_kv        = 16
0.00.085.018 I print_info: n_rot            = 32
0.00.085.018 I print_info: n_swa            = 0
0.00.085.018 I print_info: n_embd_head_k    = 128
0.00.085.018 I print_info: n_embd_head_v    = 128
0.00.085.019 I print_info: n_gqa            = 1
0.00.085.020 I print_info: n_embd_k_gqa     = 2048
0.00.085.020 I print_info: n_embd_v_gqa     = 2048
0.00.085.021 I print_info: f_norm_eps       = 1.0e-05
0.00.085.021 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.021 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.021 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.023 I print_info: f_logit_scale    = 0.0e+00
0.00.085.024 I print_info: n_ff             = 8192
0.00.085.033 I print_info: n_expert         = 0
0.00.085.035 I print_info: n_expert_used    = 0
0.00.085.035 I print_info: causal attn      = 1
0.00.085.035 I print_info: pooling type     = 0
0.00.085.035 I print_info: rope type        = 2
0.00.085.035 I print_info: rope scaling     = linear
0.00.085.036 I print_info: freq_base_train  = 10000.0
0.00.085.036 I print_info: freq_scale_train = 1
0.00.085.037 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.037 I print_info: rope_finetuned   = unknown
0.00.085.037 I print_info: ssm_d_conv       = 0
0.00.085.037 I print_info: ssm_d_inner      = 0
0.00.085.038 I print_info: ssm_d_state      = 0
0.00.085.038 I print_info: ssm_dt_rank      = 0
0.00.085.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.038 I print_info: model type       = 1.4B
0.00.085.039 I print_info: model params     = 1.41 B
0.00.085.039 I print_info: general.name     = 1.4B
0.00.085.040 I print_info: vocab type       = BPE
0.00.085.040 I print_info: n_vocab          = 50304
0.00.085.040 I print_info: n_merges         = 50009
0.00.085.040 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.040 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.041 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.041 I print_info: LF token         = 128 'Ä'
0.00.085.041 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.042 I print_info: max token length = 1024
0.01.036.154 I load_tensors: offloading 24 repeating layers to GPU
0.01.036.159 I load_tensors: offloading output layer to GPU
0.01.036.160 I load_tensors: offloaded 25/25 layers to GPU
0.01.036.185 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.036.187 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.036.931 I llama_init_from_model: n_seq_max     = 1
0.01.036.933 I llama_init_from_model: n_ctx         = 128
0.01.036.933 I llama_init_from_model: n_ctx_per_seq = 128
0.01.036.933 I llama_init_from_model: n_batch       = 128
0.01.036.933 I llama_init_from_model: n_ubatch      = 128
0.01.036.934 I llama_init_from_model: flash_attn    = 0
0.01.036.934 I llama_init_from_model: freq_base     = 10000.0
0.01.036.934 I llama_init_from_model: freq_scale    = 1
0.01.036.935 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.036.937 I ggml_metal_init: allocating
0.01.036.993 I ggml_metal_init: found device: Apple M4
0.01.036.996 I ggml_metal_init: picking default device: Apple M4
0.01.037.967 I ggml_metal_init: using embedded metal library
0.01.041.184 I ggml_metal_init: GPU name:   Apple M4
0.01.041.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.041.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.041.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.041.187 I ggml_metal_init: simdgroup reduction   = true
0.01.041.187 I ggml_metal_init: simdgroup matrix mul. = true
0.01.041.187 I ggml_metal_init: has residency sets    = true
0.01.041.187 I ggml_metal_init: has bfloat            = true
0.01.041.187 I ggml_metal_init: use bfloat            = true
0.01.041.188 I ggml_metal_init: hasUnifiedMemory      = true
0.01.041.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.051.281 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.052.896 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.052.899 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.052.922 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.054.431 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.054.432 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.054.433 I llama_init_from_model: graph nodes  = 967
0.01.054.433 I llama_init_from_model: graph splits = 2
0.01.054.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.054.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.088.511 I 
0.01.088.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.088.564 I perplexity: tokenizing the input ..
0.01.096.997 I perplexity: tokenization took 8.431 ms
0.01.097.019 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.215.263 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.216.532 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.216.547 I llama_perf_context_print:        load time =    1068.62 ms
0.01.216.548 I llama_perf_context_print: prompt eval time =     117.97 ms /   128 tokens (    0.92 ms per token,  1085.00 tokens per second)
0.01.216.549 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.216.549 I llama_perf_context_print:       total time =     128.04 ms /   129 tokens
0.01.216.911 I ggml_metal_free: deallocating

real	0m1.428s
user	0m0.115s
sys	0m0.259s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.175 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.182 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.184 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.192 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.193 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.385 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.474 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.478 I llama_model_loader: - type  f32:  194 tensors
0.00.027.479 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.479 I print_info: file format = GGUF V3 (latest)
0.00.027.480 I print_info: file type   = Q8_0
0.00.027.481 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.279 I load: special tokens cache size = 25
0.00.054.918 I load: token to piece cache size = 0.2984 MB
0.00.054.921 I print_info: arch             = gptneox
0.00.054.921 I print_info: vocab_only       = 0
0.00.054.921 I print_info: n_ctx_train      = 2048
0.00.054.922 I print_info: n_embd           = 2048
0.00.054.922 I print_info: n_layer          = 24
0.00.054.926 I print_info: n_head           = 16
0.00.054.926 I print_info: n_head_kv        = 16
0.00.054.926 I print_info: n_rot            = 32
0.00.054.927 I print_info: n_swa            = 0
0.00.054.927 I print_info: n_embd_head_k    = 128
0.00.054.927 I print_info: n_embd_head_v    = 128
0.00.054.928 I print_info: n_gqa            = 1
0.00.054.928 I print_info: n_embd_k_gqa     = 2048
0.00.054.929 I print_info: n_embd_v_gqa     = 2048
0.00.054.930 I print_info: f_norm_eps       = 1.0e-05
0.00.054.930 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.931 I print_info: f_logit_scale    = 0.0e+00
0.00.054.931 I print_info: n_ff             = 8192
0.00.054.931 I print_info: n_expert         = 0
0.00.054.932 I print_info: n_expert_used    = 0
0.00.054.932 I print_info: causal attn      = 1
0.00.054.932 I print_info: pooling type     = 0
0.00.054.932 I print_info: rope type        = 2
0.00.054.932 I print_info: rope scaling     = linear
0.00.054.932 I print_info: freq_base_train  = 10000.0
0.00.054.933 I print_info: freq_scale_train = 1
0.00.054.933 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.933 I print_info: rope_finetuned   = unknown
0.00.054.933 I print_info: ssm_d_conv       = 0
0.00.054.933 I print_info: ssm_d_inner      = 0
0.00.054.934 I print_info: ssm_d_state      = 0
0.00.054.934 I print_info: ssm_dt_rank      = 0
0.00.054.934 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.934 I print_info: model type       = 1.4B
0.00.054.935 I print_info: model params     = 1.41 B
0.00.054.935 I print_info: general.name     = 1.4B
0.00.054.935 I print_info: vocab type       = BPE
0.00.054.935 I print_info: n_vocab          = 50304
0.00.054.936 I print_info: n_merges         = 50009
0.00.054.936 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.936 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.936 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.937 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.937 I print_info: LF token         = 128 'Ä'
0.00.054.939 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.939 I print_info: max token length = 1024
0.00.890.055 I load_tensors: offloading 24 repeating layers to GPU
0.00.890.060 I load_tensors: offloading output layer to GPU
0.00.890.060 I load_tensors: offloaded 25/25 layers to GPU
0.00.890.083 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.890.085 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.890.828 I llama_init_from_model: n_seq_max     = 1
0.00.890.830 I llama_init_from_model: n_ctx         = 128
0.00.890.830 I llama_init_from_model: n_ctx_per_seq = 128
0.00.890.830 I llama_init_from_model: n_batch       = 128
0.00.890.831 I llama_init_from_model: n_ubatch      = 128
0.00.890.831 I llama_init_from_model: flash_attn    = 0
0.00.890.832 I llama_init_from_model: freq_base     = 10000.0
0.00.890.832 I llama_init_from_model: freq_scale    = 1
0.00.890.833 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.890.833 I ggml_metal_init: allocating
0.00.890.871 I ggml_metal_init: found device: Apple M4
0.00.890.875 I ggml_metal_init: picking default device: Apple M4
0.00.891.980 I ggml_metal_init: using embedded metal library
0.00.896.512 I ggml_metal_init: GPU name:   Apple M4
0.00.896.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.896.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.896.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.896.517 I ggml_metal_init: simdgroup reduction   = true
0.00.896.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.896.517 I ggml_metal_init: has residency sets    = true
0.00.896.517 I ggml_metal_init: has bfloat            = true
0.00.896.518 I ggml_metal_init: use bfloat            = true
0.00.896.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.896.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.909.398 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.911.262 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.911.265 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.911.288 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.913.110 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.913.112 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.913.112 I llama_init_from_model: graph nodes  = 967
0.00.913.112 I llama_init_from_model: graph splits = 2
0.00.913.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.913.114 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.452 I 
0.00.936.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.936.489 I perplexity: tokenizing the input ..
0.00.945.264 I perplexity: tokenization took 8.773 ms
0.00.945.276 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.071.471 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.072.721 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.072.732 I llama_perf_context_print:        load time =     925.65 ms
0.01.072.733 I llama_perf_context_print: prompt eval time =     125.97 ms /   128 tokens (    0.98 ms per token,  1016.11 tokens per second)
0.01.072.734 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.072.734 I llama_perf_context_print:       total time =     136.28 ms /   129 tokens
0.01.073.105 I ggml_metal_free: deallocating

real	0m1.088s
user	0m0.088s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.414 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.415 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.415 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.417 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.418 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.418 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.418 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.479 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.602 I llama_model_loader: - type  f32:  194 tensors
0.00.027.602 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.603 I print_info: file format = GGUF V3 (latest)
0.00.027.603 I print_info: file type   = Q4_0
0.00.027.604 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.643 I load: special tokens cache size = 25
0.00.052.788 I load: token to piece cache size = 0.2984 MB
0.00.052.791 I print_info: arch             = gptneox
0.00.052.792 I print_info: vocab_only       = 0
0.00.052.792 I print_info: n_ctx_train      = 2048
0.00.052.792 I print_info: n_embd           = 2048
0.00.052.792 I print_info: n_layer          = 24
0.00.052.795 I print_info: n_head           = 16
0.00.052.796 I print_info: n_head_kv        = 16
0.00.052.796 I print_info: n_rot            = 32
0.00.052.796 I print_info: n_swa            = 0
0.00.052.797 I print_info: n_embd_head_k    = 128
0.00.052.797 I print_info: n_embd_head_v    = 128
0.00.052.798 I print_info: n_gqa            = 1
0.00.052.798 I print_info: n_embd_k_gqa     = 2048
0.00.052.799 I print_info: n_embd_v_gqa     = 2048
0.00.052.800 I print_info: f_norm_eps       = 1.0e-05
0.00.052.800 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.802 I print_info: f_logit_scale    = 0.0e+00
0.00.052.802 I print_info: n_ff             = 8192
0.00.052.803 I print_info: n_expert         = 0
0.00.052.803 I print_info: n_expert_used    = 0
0.00.052.803 I print_info: causal attn      = 1
0.00.052.803 I print_info: pooling type     = 0
0.00.052.803 I print_info: rope type        = 2
0.00.052.803 I print_info: rope scaling     = linear
0.00.052.804 I print_info: freq_base_train  = 10000.0
0.00.052.804 I print_info: freq_scale_train = 1
0.00.052.804 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.804 I print_info: rope_finetuned   = unknown
0.00.052.804 I print_info: ssm_d_conv       = 0
0.00.052.805 I print_info: ssm_d_inner      = 0
0.00.052.805 I print_info: ssm_d_state      = 0
0.00.052.807 I print_info: ssm_dt_rank      = 0
0.00.052.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.807 I print_info: model type       = 1.4B
0.00.052.807 I print_info: model params     = 1.41 B
0.00.052.807 I print_info: general.name     = 1.4B
0.00.052.808 I print_info: vocab type       = BPE
0.00.052.808 I print_info: n_vocab          = 50304
0.00.052.808 I print_info: n_merges         = 50009
0.00.052.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.809 I print_info: LF token         = 128 'Ä'
0.00.052.810 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.810 I print_info: max token length = 1024
0.00.648.279 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.289 I load_tensors: offloading output layer to GPU
0.00.648.290 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.319 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.648.320 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.649.423 I llama_init_from_model: n_seq_max     = 1
0.00.649.427 I llama_init_from_model: n_ctx         = 128
0.00.649.427 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.428 I llama_init_from_model: n_batch       = 128
0.00.649.428 I llama_init_from_model: n_ubatch      = 128
0.00.649.429 I llama_init_from_model: flash_attn    = 0
0.00.649.430 I llama_init_from_model: freq_base     = 10000.0
0.00.649.431 I llama_init_from_model: freq_scale    = 1
0.00.649.432 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.433 I ggml_metal_init: allocating
0.00.649.460 I ggml_metal_init: found device: Apple M4
0.00.649.466 I ggml_metal_init: picking default device: Apple M4
0.00.650.855 I ggml_metal_init: using embedded metal library
0.00.657.213 I ggml_metal_init: GPU name:   Apple M4
0.00.657.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.219 I ggml_metal_init: simdgroup reduction   = true
0.00.657.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.220 I ggml_metal_init: has residency sets    = true
0.00.657.220 I ggml_metal_init: has bfloat            = true
0.00.657.221 I ggml_metal_init: use bfloat            = true
0.00.657.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.287 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.620 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.677.623 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.652 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.734 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.680.736 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.680.737 I llama_init_from_model: graph nodes  = 967
0.00.680.737 I llama_init_from_model: graph splits = 2
0.00.680.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.148 I 
0.00.708.222 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.239 I perplexity: tokenizing the input ..
0.00.718.980 I perplexity: tokenization took 10.737 ms
0.00.718.994 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.224 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.849.491 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.849.511 I llama_perf_context_print:        load time =     697.18 ms
0.00.849.513 I llama_perf_context_print: prompt eval time =     128.94 ms /   128 tokens (    1.01 ms per token,   992.75 tokens per second)
0.00.849.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.514 I llama_perf_context_print:       total time =     141.37 ms /   129 tokens
0.00.849.905 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.095s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.067 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.072 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.073 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.074 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.075 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.075 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.076 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.079 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.079 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.080 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.080 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.082 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.082 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.083 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.118 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.120 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.121 I llama_model_loader: - type  f32:  194 tensors
0.00.026.122 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.123 I print_info: file format = GGUF V3 (latest)
0.00.026.123 I print_info: file type   = Q4_1
0.00.026.124 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.147 I load: special tokens cache size = 25
0.00.051.206 I load: token to piece cache size = 0.2984 MB
0.00.051.209 I print_info: arch             = gptneox
0.00.051.209 I print_info: vocab_only       = 0
0.00.051.209 I print_info: n_ctx_train      = 2048
0.00.051.210 I print_info: n_embd           = 2048
0.00.051.210 I print_info: n_layer          = 24
0.00.051.213 I print_info: n_head           = 16
0.00.051.214 I print_info: n_head_kv        = 16
0.00.051.214 I print_info: n_rot            = 32
0.00.051.214 I print_info: n_swa            = 0
0.00.051.214 I print_info: n_embd_head_k    = 128
0.00.051.214 I print_info: n_embd_head_v    = 128
0.00.051.215 I print_info: n_gqa            = 1
0.00.051.216 I print_info: n_embd_k_gqa     = 2048
0.00.051.217 I print_info: n_embd_v_gqa     = 2048
0.00.051.220 I print_info: f_norm_eps       = 1.0e-05
0.00.051.220 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.221 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.221 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.221 I print_info: f_logit_scale    = 0.0e+00
0.00.051.221 I print_info: n_ff             = 8192
0.00.051.222 I print_info: n_expert         = 0
0.00.051.222 I print_info: n_expert_used    = 0
0.00.051.222 I print_info: causal attn      = 1
0.00.051.222 I print_info: pooling type     = 0
0.00.051.222 I print_info: rope type        = 2
0.00.051.223 I print_info: rope scaling     = linear
0.00.051.223 I print_info: freq_base_train  = 10000.0
0.00.051.223 I print_info: freq_scale_train = 1
0.00.051.225 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.225 I print_info: rope_finetuned   = unknown
0.00.051.225 I print_info: ssm_d_conv       = 0
0.00.051.225 I print_info: ssm_d_inner      = 0
0.00.051.226 I print_info: ssm_d_state      = 0
0.00.051.226 I print_info: ssm_dt_rank      = 0
0.00.051.226 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.226 I print_info: model type       = 1.4B
0.00.051.227 I print_info: model params     = 1.41 B
0.00.051.227 I print_info: general.name     = 1.4B
0.00.051.227 I print_info: vocab type       = BPE
0.00.051.227 I print_info: n_vocab          = 50304
0.00.051.228 I print_info: n_merges         = 50009
0.00.051.228 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.228 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.228 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.228 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.229 I print_info: LF token         = 128 'Ä'
0.00.051.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.232 I print_info: max token length = 1024
0.00.700.887 I load_tensors: offloading 24 repeating layers to GPU
0.00.700.892 I load_tensors: offloading output layer to GPU
0.00.700.893 I load_tensors: offloaded 25/25 layers to GPU
0.00.700.917 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.700.918 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.702.042 I llama_init_from_model: n_seq_max     = 1
0.00.702.044 I llama_init_from_model: n_ctx         = 128
0.00.702.045 I llama_init_from_model: n_ctx_per_seq = 128
0.00.702.045 I llama_init_from_model: n_batch       = 128
0.00.702.045 I llama_init_from_model: n_ubatch      = 128
0.00.702.046 I llama_init_from_model: flash_attn    = 0
0.00.702.047 I llama_init_from_model: freq_base     = 10000.0
0.00.702.047 I llama_init_from_model: freq_scale    = 1
0.00.702.048 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.702.049 I ggml_metal_init: allocating
0.00.702.059 I ggml_metal_init: found device: Apple M4
0.00.702.063 I ggml_metal_init: picking default device: Apple M4
0.00.703.425 I ggml_metal_init: using embedded metal library
0.00.709.561 I ggml_metal_init: GPU name:   Apple M4
0.00.709.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.709.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.709.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.709.566 I ggml_metal_init: simdgroup reduction   = true
0.00.709.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.709.566 I ggml_metal_init: has residency sets    = true
0.00.709.566 I ggml_metal_init: has bfloat            = true
0.00.709.567 I ggml_metal_init: use bfloat            = true
0.00.709.567 I ggml_metal_init: hasUnifiedMemory      = true
0.00.709.568 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.900 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.349 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.729.352 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.729.378 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.732.514 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.732.516 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.732.516 I llama_init_from_model: graph nodes  = 967
0.00.732.517 I llama_init_from_model: graph splits = 2
0.00.732.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.732.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.967 I 
0.00.760.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.052 I perplexity: tokenizing the input ..
0.00.769.300 I perplexity: tokenization took 9.246 ms
0.00.769.313 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.891.481 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.892.737 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.892.754 I llama_perf_context_print:        load time =     750.23 ms
0.00.892.755 I llama_perf_context_print: prompt eval time =     121.94 ms /   128 tokens (    0.95 ms per token,  1049.66 tokens per second)
0.00.892.755 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.892.756 I llama_perf_context_print:       total time =     132.79 ms /   129 tokens
0.00.893.182 I ggml_metal_free: deallocating

real	0m0.908s
user	0m0.091s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.768 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.036 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.038 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.039 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.039 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.040 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.040 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.041 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.041 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.041 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.170 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.213 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.216 I llama_model_loader: - type  f32:  194 tensors
0.00.027.216 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.217 I print_info: file format = GGUF V3 (latest)
0.00.027.217 I print_info: file type   = Q5_0
0.00.027.218 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.075 I load: special tokens cache size = 25
0.00.052.069 I load: token to piece cache size = 0.2984 MB
0.00.052.072 I print_info: arch             = gptneox
0.00.052.072 I print_info: vocab_only       = 0
0.00.052.072 I print_info: n_ctx_train      = 2048
0.00.052.072 I print_info: n_embd           = 2048
0.00.052.073 I print_info: n_layer          = 24
0.00.052.076 I print_info: n_head           = 16
0.00.052.076 I print_info: n_head_kv        = 16
0.00.052.076 I print_info: n_rot            = 32
0.00.052.077 I print_info: n_swa            = 0
0.00.052.077 I print_info: n_embd_head_k    = 128
0.00.052.077 I print_info: n_embd_head_v    = 128
0.00.052.078 I print_info: n_gqa            = 1
0.00.052.078 I print_info: n_embd_k_gqa     = 2048
0.00.052.079 I print_info: n_embd_v_gqa     = 2048
0.00.052.079 I print_info: f_norm_eps       = 1.0e-05
0.00.052.080 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.080 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.080 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.080 I print_info: f_logit_scale    = 0.0e+00
0.00.052.083 I print_info: n_ff             = 8192
0.00.052.083 I print_info: n_expert         = 0
0.00.052.083 I print_info: n_expert_used    = 0
0.00.052.083 I print_info: causal attn      = 1
0.00.052.083 I print_info: pooling type     = 0
0.00.052.084 I print_info: rope type        = 2
0.00.052.084 I print_info: rope scaling     = linear
0.00.052.084 I print_info: freq_base_train  = 10000.0
0.00.052.084 I print_info: freq_scale_train = 1
0.00.052.085 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.085 I print_info: rope_finetuned   = unknown
0.00.052.087 I print_info: ssm_d_conv       = 0
0.00.052.087 I print_info: ssm_d_inner      = 0
0.00.052.087 I print_info: ssm_d_state      = 0
0.00.052.087 I print_info: ssm_dt_rank      = 0
0.00.052.087 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.088 I print_info: model type       = 1.4B
0.00.052.088 I print_info: model params     = 1.41 B
0.00.052.088 I print_info: general.name     = 1.4B
0.00.052.089 I print_info: vocab type       = BPE
0.00.052.089 I print_info: n_vocab          = 50304
0.00.052.089 I print_info: n_merges         = 50009
0.00.052.089 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.090 I print_info: LF token         = 128 'Ä'
0.00.052.091 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.091 I print_info: max token length = 1024
0.00.756.396 I load_tensors: offloading 24 repeating layers to GPU
0.00.756.400 I load_tensors: offloading output layer to GPU
0.00.756.402 I load_tensors: offloaded 25/25 layers to GPU
0.00.756.425 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.756.426 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.757.448 I llama_init_from_model: n_seq_max     = 1
0.00.757.450 I llama_init_from_model: n_ctx         = 128
0.00.757.450 I llama_init_from_model: n_ctx_per_seq = 128
0.00.757.451 I llama_init_from_model: n_batch       = 128
0.00.757.451 I llama_init_from_model: n_ubatch      = 128
0.00.757.451 I llama_init_from_model: flash_attn    = 0
0.00.757.452 I llama_init_from_model: freq_base     = 10000.0
0.00.757.453 I llama_init_from_model: freq_scale    = 1
0.00.757.453 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.757.454 I ggml_metal_init: allocating
0.00.757.463 I ggml_metal_init: found device: Apple M4
0.00.757.467 I ggml_metal_init: picking default device: Apple M4
0.00.758.694 I ggml_metal_init: using embedded metal library
0.00.764.708 I ggml_metal_init: GPU name:   Apple M4
0.00.764.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.764.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.764.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.764.715 I ggml_metal_init: simdgroup reduction   = true
0.00.764.715 I ggml_metal_init: simdgroup matrix mul. = true
0.00.764.715 I ggml_metal_init: has residency sets    = true
0.00.764.715 I ggml_metal_init: has bfloat            = true
0.00.764.716 I ggml_metal_init: use bfloat            = true
0.00.764.717 I ggml_metal_init: hasUnifiedMemory      = true
0.00.764.718 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.781.624 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.785.025 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.785.028 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.785.162 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.198 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.788.199 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.788.200 I llama_init_from_model: graph nodes  = 967
0.00.788.200 I llama_init_from_model: graph splits = 2
0.00.788.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.788.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.819.487 I 
0.00.819.574 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.819.592 I perplexity: tokenizing the input ..
0.00.831.588 I perplexity: tokenization took 11.993 ms
0.00.831.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.974.905 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.976.192 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.976.199 I llama_perf_context_print:        load time =     808.71 ms
0.00.976.200 I llama_perf_context_print: prompt eval time =     142.77 ms /   128 tokens (    1.12 ms per token,   896.56 tokens per second)
0.00.976.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.976.202 I llama_perf_context_print:       total time =     156.72 ms /   129 tokens
0.00.976.637 I ggml_metal_free: deallocating

real	0m0.993s
user	0m0.095s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.240 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.234 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.235 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.238 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.238 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.241 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.241 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.341 I llama_model_loader: - type  f32:  194 tensors
0.00.026.341 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.341 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.342 I print_info: file format = GGUF V3 (latest)
0.00.026.343 I print_info: file type   = Q5_1
0.00.026.343 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.249 I load: special tokens cache size = 25
0.00.051.254 I load: token to piece cache size = 0.2984 MB
0.00.051.257 I print_info: arch             = gptneox
0.00.051.257 I print_info: vocab_only       = 0
0.00.051.257 I print_info: n_ctx_train      = 2048
0.00.051.257 I print_info: n_embd           = 2048
0.00.051.258 I print_info: n_layer          = 24
0.00.051.262 I print_info: n_head           = 16
0.00.051.263 I print_info: n_head_kv        = 16
0.00.051.263 I print_info: n_rot            = 32
0.00.051.263 I print_info: n_swa            = 0
0.00.051.263 I print_info: n_embd_head_k    = 128
0.00.051.263 I print_info: n_embd_head_v    = 128
0.00.051.264 I print_info: n_gqa            = 1
0.00.051.265 I print_info: n_embd_k_gqa     = 2048
0.00.051.266 I print_info: n_embd_v_gqa     = 2048
0.00.051.266 I print_info: f_norm_eps       = 1.0e-05
0.00.051.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.267 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.267 I print_info: f_logit_scale    = 0.0e+00
0.00.051.268 I print_info: n_ff             = 8192
0.00.051.270 I print_info: n_expert         = 0
0.00.051.270 I print_info: n_expert_used    = 0
0.00.051.270 I print_info: causal attn      = 1
0.00.051.270 I print_info: pooling type     = 0
0.00.051.270 I print_info: rope type        = 2
0.00.051.271 I print_info: rope scaling     = linear
0.00.051.271 I print_info: freq_base_train  = 10000.0
0.00.051.271 I print_info: freq_scale_train = 1
0.00.051.271 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.272 I print_info: rope_finetuned   = unknown
0.00.051.272 I print_info: ssm_d_conv       = 0
0.00.051.272 I print_info: ssm_d_inner      = 0
0.00.051.273 I print_info: ssm_d_state      = 0
0.00.051.273 I print_info: ssm_dt_rank      = 0
0.00.051.273 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.274 I print_info: model type       = 1.4B
0.00.051.275 I print_info: model params     = 1.41 B
0.00.051.275 I print_info: general.name     = 1.4B
0.00.051.275 I print_info: vocab type       = BPE
0.00.051.276 I print_info: n_vocab          = 50304
0.00.051.276 I print_info: n_merges         = 50009
0.00.051.277 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.278 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.278 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.278 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.278 I print_info: LF token         = 128 'Ä'
0.00.051.279 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.279 I print_info: max token length = 1024
0.00.780.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.780.694 I load_tensors: offloading output layer to GPU
0.00.780.695 I load_tensors: offloaded 25/25 layers to GPU
0.00.780.734 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.780.735 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.781.495 I llama_init_from_model: n_seq_max     = 1
0.00.781.497 I llama_init_from_model: n_ctx         = 128
0.00.781.497 I llama_init_from_model: n_ctx_per_seq = 128
0.00.781.498 I llama_init_from_model: n_batch       = 128
0.00.781.498 I llama_init_from_model: n_ubatch      = 128
0.00.781.498 I llama_init_from_model: flash_attn    = 0
0.00.781.499 I llama_init_from_model: freq_base     = 10000.0
0.00.781.500 I llama_init_from_model: freq_scale    = 1
0.00.781.500 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.781.502 I ggml_metal_init: allocating
0.00.781.546 I ggml_metal_init: found device: Apple M4
0.00.781.553 I ggml_metal_init: picking default device: Apple M4
0.00.782.556 I ggml_metal_init: using embedded metal library
0.00.786.163 I ggml_metal_init: GPU name:   Apple M4
0.00.786.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.786.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.786.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.786.166 I ggml_metal_init: simdgroup reduction   = true
0.00.786.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.786.167 I ggml_metal_init: has residency sets    = true
0.00.786.167 I ggml_metal_init: has bfloat            = true
0.00.786.167 I ggml_metal_init: use bfloat            = true
0.00.786.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.786.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.796.307 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.798.025 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.798.029 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.798.045 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.799.649 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.799.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.799.651 I llama_init_from_model: graph nodes  = 967
0.00.799.651 I llama_init_from_model: graph splits = 2
0.00.799.652 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.799.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.232 I 
0.00.827.269 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.278 I perplexity: tokenizing the input ..
0.00.834.744 I perplexity: tokenization took 7.464 ms
0.00.834.760 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.969.371 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.970.625 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.970.638 I llama_perf_context_print:        load time =     816.99 ms
0.00.970.639 I llama_perf_context_print: prompt eval time =     134.38 ms /   128 tokens (    1.05 ms per token,   952.49 tokens per second)
0.00.970.640 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.970.640 I llama_perf_context_print:       total time =     143.41 ms /   129 tokens
0.00.971.054 I ggml_metal_free: deallocating

real	0m0.985s
user	0m0.081s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.374 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.198 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.209 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.213 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.174 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.292 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.289 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.291 I llama_model_loader: - type  f32:  194 tensors
0.00.026.291 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.291 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.292 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.292 I print_info: file format = GGUF V3 (latest)
0.00.026.292 I print_info: file type   = Q2_K - Medium
0.00.026.293 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.351 I load: special tokens cache size = 25
0.00.051.435 I load: token to piece cache size = 0.2984 MB
0.00.051.438 I print_info: arch             = gptneox
0.00.051.438 I print_info: vocab_only       = 0
0.00.051.439 I print_info: n_ctx_train      = 2048
0.00.051.439 I print_info: n_embd           = 2048
0.00.051.439 I print_info: n_layer          = 24
0.00.051.443 I print_info: n_head           = 16
0.00.051.444 I print_info: n_head_kv        = 16
0.00.051.444 I print_info: n_rot            = 32
0.00.051.444 I print_info: n_swa            = 0
0.00.051.445 I print_info: n_embd_head_k    = 128
0.00.051.446 I print_info: n_embd_head_v    = 128
0.00.051.446 I print_info: n_gqa            = 1
0.00.051.447 I print_info: n_embd_k_gqa     = 2048
0.00.051.448 I print_info: n_embd_v_gqa     = 2048
0.00.051.448 I print_info: f_norm_eps       = 1.0e-05
0.00.051.449 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.449 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.449 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.449 I print_info: f_logit_scale    = 0.0e+00
0.00.051.450 I print_info: n_ff             = 8192
0.00.051.450 I print_info: n_expert         = 0
0.00.051.450 I print_info: n_expert_used    = 0
0.00.051.450 I print_info: causal attn      = 1
0.00.051.450 I print_info: pooling type     = 0
0.00.051.451 I print_info: rope type        = 2
0.00.051.451 I print_info: rope scaling     = linear
0.00.051.451 I print_info: freq_base_train  = 10000.0
0.00.051.452 I print_info: freq_scale_train = 1
0.00.051.452 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.452 I print_info: rope_finetuned   = unknown
0.00.051.453 I print_info: ssm_d_conv       = 0
0.00.051.454 I print_info: ssm_d_inner      = 0
0.00.051.454 I print_info: ssm_d_state      = 0
0.00.051.454 I print_info: ssm_dt_rank      = 0
0.00.051.454 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.454 I print_info: model type       = 1.4B
0.00.051.455 I print_info: model params     = 1.41 B
0.00.051.455 I print_info: general.name     = 1.4B
0.00.051.455 I print_info: vocab type       = BPE
0.00.051.455 I print_info: n_vocab          = 50304
0.00.051.456 I print_info: n_merges         = 50009
0.00.051.456 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.456 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.456 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.456 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.457 I print_info: LF token         = 128 'Ä'
0.00.051.457 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.457 I print_info: max token length = 1024
0.00.429.174 I load_tensors: offloading 24 repeating layers to GPU
0.00.429.186 I load_tensors: offloading output layer to GPU
0.00.429.186 I load_tensors: offloaded 25/25 layers to GPU
0.00.429.218 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.429.220 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.430.596 I llama_init_from_model: n_seq_max     = 1
0.00.430.601 I llama_init_from_model: n_ctx         = 128
0.00.430.601 I llama_init_from_model: n_ctx_per_seq = 128
0.00.430.602 I llama_init_from_model: n_batch       = 128
0.00.430.603 I llama_init_from_model: n_ubatch      = 128
0.00.430.603 I llama_init_from_model: flash_attn    = 0
0.00.430.605 I llama_init_from_model: freq_base     = 10000.0
0.00.430.606 I llama_init_from_model: freq_scale    = 1
0.00.430.607 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.430.611 I ggml_metal_init: allocating
0.00.430.712 I ggml_metal_init: found device: Apple M4
0.00.430.722 I ggml_metal_init: picking default device: Apple M4
0.00.432.744 I ggml_metal_init: using embedded metal library
0.00.438.898 I ggml_metal_init: GPU name:   Apple M4
0.00.438.908 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.438.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.438.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.438.910 I ggml_metal_init: simdgroup reduction   = true
0.00.438.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.438.911 I ggml_metal_init: has residency sets    = true
0.00.438.911 I ggml_metal_init: has bfloat            = true
0.00.438.911 I ggml_metal_init: use bfloat            = true
0.00.438.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.438.915 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.459.887 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.463.438 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.463.445 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.463.474 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.466.718 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.466.720 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.466.721 I llama_init_from_model: graph nodes  = 967
0.00.466.721 I llama_init_from_model: graph splits = 2
0.00.466.724 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.466.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.589 I 
0.00.495.669 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.690 I perplexity: tokenizing the input ..
0.00.506.674 I perplexity: tokenization took 10.982 ms
0.00.506.688 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.638.145 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.639.406 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.639.421 I llama_perf_context_print:        load time =     485.21 ms
0.00.639.422 I llama_perf_context_print: prompt eval time =     131.23 ms /   128 tokens (    1.03 ms per token,   975.36 tokens per second)
0.00.639.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.639.423 I llama_perf_context_print:       total time =     143.84 ms /   129 tokens
0.00.639.810 I ggml_metal_free: deallocating

real	0m0.656s
user	0m0.098s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.960 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.968 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.973 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.974 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.116 I llama_model_loader: - type  f32:  194 tensors
0.00.026.117 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.117 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.117 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.117 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.118 I print_info: file format = GGUF V3 (latest)
0.00.026.118 I print_info: file type   = Q3_K - Medium
0.00.026.119 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.765 I load: special tokens cache size = 25
0.00.051.841 I load: token to piece cache size = 0.2984 MB
0.00.051.844 I print_info: arch             = gptneox
0.00.051.844 I print_info: vocab_only       = 0
0.00.051.844 I print_info: n_ctx_train      = 2048
0.00.051.844 I print_info: n_embd           = 2048
0.00.051.845 I print_info: n_layer          = 24
0.00.051.847 I print_info: n_head           = 16
0.00.051.848 I print_info: n_head_kv        = 16
0.00.051.848 I print_info: n_rot            = 32
0.00.051.849 I print_info: n_swa            = 0
0.00.051.850 I print_info: n_embd_head_k    = 128
0.00.051.850 I print_info: n_embd_head_v    = 128
0.00.051.851 I print_info: n_gqa            = 1
0.00.051.852 I print_info: n_embd_k_gqa     = 2048
0.00.051.852 I print_info: n_embd_v_gqa     = 2048
0.00.051.853 I print_info: f_norm_eps       = 1.0e-05
0.00.051.853 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.853 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.854 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.856 I print_info: f_logit_scale    = 0.0e+00
0.00.051.856 I print_info: n_ff             = 8192
0.00.051.856 I print_info: n_expert         = 0
0.00.051.857 I print_info: n_expert_used    = 0
0.00.051.857 I print_info: causal attn      = 1
0.00.051.857 I print_info: pooling type     = 0
0.00.051.857 I print_info: rope type        = 2
0.00.051.857 I print_info: rope scaling     = linear
0.00.051.858 I print_info: freq_base_train  = 10000.0
0.00.051.858 I print_info: freq_scale_train = 1
0.00.051.858 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.858 I print_info: rope_finetuned   = unknown
0.00.051.858 I print_info: ssm_d_conv       = 0
0.00.051.858 I print_info: ssm_d_inner      = 0
0.00.051.859 I print_info: ssm_d_state      = 0
0.00.051.859 I print_info: ssm_dt_rank      = 0
0.00.051.859 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.859 I print_info: model type       = 1.4B
0.00.051.860 I print_info: model params     = 1.41 B
0.00.051.860 I print_info: general.name     = 1.4B
0.00.051.860 I print_info: vocab type       = BPE
0.00.051.860 I print_info: n_vocab          = 50304
0.00.051.861 I print_info: n_merges         = 50009
0.00.051.861 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.861 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.861 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.861 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.862 I print_info: LF token         = 128 'Ä'
0.00.051.862 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.862 I print_info: max token length = 1024
0.00.504.361 I load_tensors: offloading 24 repeating layers to GPU
0.00.504.393 I load_tensors: offloading output layer to GPU
0.00.504.395 I load_tensors: offloaded 25/25 layers to GPU
0.00.504.419 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.504.423 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.505.531 I llama_init_from_model: n_seq_max     = 1
0.00.505.533 I llama_init_from_model: n_ctx         = 128
0.00.505.534 I llama_init_from_model: n_ctx_per_seq = 128
0.00.505.535 I llama_init_from_model: n_batch       = 128
0.00.505.535 I llama_init_from_model: n_ubatch      = 128
0.00.505.536 I llama_init_from_model: flash_attn    = 0
0.00.505.536 I llama_init_from_model: freq_base     = 10000.0
0.00.505.537 I llama_init_from_model: freq_scale    = 1
0.00.505.538 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.505.539 I ggml_metal_init: allocating
0.00.505.551 I ggml_metal_init: found device: Apple M4
0.00.505.556 I ggml_metal_init: picking default device: Apple M4
0.00.506.935 I ggml_metal_init: using embedded metal library
0.00.513.148 I ggml_metal_init: GPU name:   Apple M4
0.00.513.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.513.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.513.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.513.154 I ggml_metal_init: simdgroup reduction   = true
0.00.513.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.513.155 I ggml_metal_init: has residency sets    = true
0.00.513.155 I ggml_metal_init: has bfloat            = true
0.00.513.156 I ggml_metal_init: use bfloat            = true
0.00.513.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.513.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.529.923 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.533.342 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.533.346 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.533.372 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.511 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.536.513 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.536.513 I llama_init_from_model: graph nodes  = 967
0.00.536.514 I llama_init_from_model: graph splits = 2
0.00.536.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.536.517 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.566.276 I 
0.00.566.352 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.566.370 I perplexity: tokenizing the input ..
0.00.578.513 I perplexity: tokenization took 12.138 ms
0.00.578.540 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.325 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.722.607 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.722.625 I llama_perf_context_print:        load time =     556.62 ms
0.00.722.626 I llama_perf_context_print: prompt eval time =     141.88 ms /   128 tokens (    1.11 ms per token,   902.16 tokens per second)
0.00.722.627 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.628 I llama_perf_context_print:       total time =     156.35 ms /   129 tokens
0.00.723.026 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.096s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.218 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.165 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.111 I llama_model_loader: - type  f32:  194 tensors
0.00.027.111 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.111 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.111 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.112 I print_info: file format = GGUF V3 (latest)
0.00.027.113 I print_info: file type   = Q4_K - Medium
0.00.027.118 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.760 I load: special tokens cache size = 25
0.00.053.037 I load: token to piece cache size = 0.2984 MB
0.00.053.041 I print_info: arch             = gptneox
0.00.053.041 I print_info: vocab_only       = 0
0.00.053.041 I print_info: n_ctx_train      = 2048
0.00.053.041 I print_info: n_embd           = 2048
0.00.053.041 I print_info: n_layer          = 24
0.00.053.044 I print_info: n_head           = 16
0.00.053.045 I print_info: n_head_kv        = 16
0.00.053.045 I print_info: n_rot            = 32
0.00.053.045 I print_info: n_swa            = 0
0.00.053.045 I print_info: n_embd_head_k    = 128
0.00.053.045 I print_info: n_embd_head_v    = 128
0.00.053.046 I print_info: n_gqa            = 1
0.00.053.048 I print_info: n_embd_k_gqa     = 2048
0.00.053.049 I print_info: n_embd_v_gqa     = 2048
0.00.053.049 I print_info: f_norm_eps       = 1.0e-05
0.00.053.050 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.050 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.052 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.052 I print_info: f_logit_scale    = 0.0e+00
0.00.053.053 I print_info: n_ff             = 8192
0.00.053.053 I print_info: n_expert         = 0
0.00.053.053 I print_info: n_expert_used    = 0
0.00.053.053 I print_info: causal attn      = 1
0.00.053.053 I print_info: pooling type     = 0
0.00.053.053 I print_info: rope type        = 2
0.00.053.053 I print_info: rope scaling     = linear
0.00.053.054 I print_info: freq_base_train  = 10000.0
0.00.053.054 I print_info: freq_scale_train = 1
0.00.053.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.054 I print_info: rope_finetuned   = unknown
0.00.053.055 I print_info: ssm_d_conv       = 0
0.00.053.055 I print_info: ssm_d_inner      = 0
0.00.053.055 I print_info: ssm_d_state      = 0
0.00.053.055 I print_info: ssm_dt_rank      = 0
0.00.053.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.060 I print_info: model type       = 1.4B
0.00.053.060 I print_info: model params     = 1.41 B
0.00.053.061 I print_info: general.name     = 1.4B
0.00.053.061 I print_info: vocab type       = BPE
0.00.053.061 I print_info: n_vocab          = 50304
0.00.053.061 I print_info: n_merges         = 50009
0.00.053.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.062 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.062 I print_info: LF token         = 128 'Ä'
0.00.053.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.063 I print_info: max token length = 1024
0.00.594.181 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.186 I load_tensors: offloading output layer to GPU
0.00.594.187 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.210 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.594.212 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.595.259 I llama_init_from_model: n_seq_max     = 1
0.00.595.262 I llama_init_from_model: n_ctx         = 128
0.00.595.262 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.262 I llama_init_from_model: n_batch       = 128
0.00.595.263 I llama_init_from_model: n_ubatch      = 128
0.00.595.263 I llama_init_from_model: flash_attn    = 0
0.00.595.264 I llama_init_from_model: freq_base     = 10000.0
0.00.595.265 I llama_init_from_model: freq_scale    = 1
0.00.595.265 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.266 I ggml_metal_init: allocating
0.00.595.282 I ggml_metal_init: found device: Apple M4
0.00.595.287 I ggml_metal_init: picking default device: Apple M4
0.00.596.643 I ggml_metal_init: using embedded metal library
0.00.602.705 I ggml_metal_init: GPU name:   Apple M4
0.00.602.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.710 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.711 I ggml_metal_init: simdgroup reduction   = true
0.00.602.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.711 I ggml_metal_init: has residency sets    = true
0.00.602.712 I ggml_metal_init: has bfloat            = true
0.00.602.712 I ggml_metal_init: use bfloat            = true
0.00.602.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.714 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.605 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.005 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.009 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.038 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.970 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.625.972 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.625.972 I llama_init_from_model: graph nodes  = 967
0.00.625.973 I llama_init_from_model: graph splits = 2
0.00.625.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.112 I 
0.00.651.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.202 I perplexity: tokenizing the input ..
0.00.659.913 I perplexity: tokenization took 8.709 ms
0.00.659.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.985 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.794.237 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.794.249 I llama_perf_context_print:        load time =     639.89 ms
0.00.794.250 I llama_perf_context_print: prompt eval time =     132.83 ms /   128 tokens (    1.04 ms per token,   963.65 tokens per second)
0.00.794.250 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.251 I llama_perf_context_print:       total time =     143.14 ms /   129 tokens
0.00.794.625 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.092s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.405 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.338 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.339 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.341 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.278 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.386 I llama_model_loader: - type  f32:  194 tensors
0.00.025.387 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.387 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.387 I print_info: file format = GGUF V3 (latest)
0.00.025.388 I print_info: file type   = Q5_K - Medium
0.00.025.388 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.086 I load: special tokens cache size = 25
0.00.051.124 I load: token to piece cache size = 0.2984 MB
0.00.051.127 I print_info: arch             = gptneox
0.00.051.128 I print_info: vocab_only       = 0
0.00.051.128 I print_info: n_ctx_train      = 2048
0.00.051.128 I print_info: n_embd           = 2048
0.00.051.128 I print_info: n_layer          = 24
0.00.051.131 I print_info: n_head           = 16
0.00.051.132 I print_info: n_head_kv        = 16
0.00.051.132 I print_info: n_rot            = 32
0.00.051.132 I print_info: n_swa            = 0
0.00.051.132 I print_info: n_embd_head_k    = 128
0.00.051.137 I print_info: n_embd_head_v    = 128
0.00.051.138 I print_info: n_gqa            = 1
0.00.051.139 I print_info: n_embd_k_gqa     = 2048
0.00.051.140 I print_info: n_embd_v_gqa     = 2048
0.00.051.141 I print_info: f_norm_eps       = 1.0e-05
0.00.051.141 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.141 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.141 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.141 I print_info: f_logit_scale    = 0.0e+00
0.00.051.142 I print_info: n_ff             = 8192
0.00.051.142 I print_info: n_expert         = 0
0.00.051.142 I print_info: n_expert_used    = 0
0.00.051.143 I print_info: causal attn      = 1
0.00.051.143 I print_info: pooling type     = 0
0.00.051.143 I print_info: rope type        = 2
0.00.051.143 I print_info: rope scaling     = linear
0.00.051.143 I print_info: freq_base_train  = 10000.0
0.00.051.144 I print_info: freq_scale_train = 1
0.00.051.144 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.144 I print_info: rope_finetuned   = unknown
0.00.051.144 I print_info: ssm_d_conv       = 0
0.00.051.144 I print_info: ssm_d_inner      = 0
0.00.051.144 I print_info: ssm_d_state      = 0
0.00.051.145 I print_info: ssm_dt_rank      = 0
0.00.051.145 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.145 I print_info: model type       = 1.4B
0.00.051.146 I print_info: model params     = 1.41 B
0.00.051.146 I print_info: general.name     = 1.4B
0.00.051.147 I print_info: vocab type       = BPE
0.00.051.147 I print_info: n_vocab          = 50304
0.00.051.149 I print_info: n_merges         = 50009
0.00.051.149 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.149 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.150 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.150 I print_info: LF token         = 128 'Ä'
0.00.051.150 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.150 I print_info: max token length = 1024
0.00.674.413 I load_tensors: offloading 24 repeating layers to GPU
0.00.674.418 I load_tensors: offloading output layer to GPU
0.00.674.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.674.443 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.674.443 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.675.419 I llama_init_from_model: n_seq_max     = 1
0.00.675.421 I llama_init_from_model: n_ctx         = 128
0.00.675.421 I llama_init_from_model: n_ctx_per_seq = 128
0.00.675.421 I llama_init_from_model: n_batch       = 128
0.00.675.422 I llama_init_from_model: n_ubatch      = 128
0.00.675.422 I llama_init_from_model: flash_attn    = 0
0.00.675.423 I llama_init_from_model: freq_base     = 10000.0
0.00.675.423 I llama_init_from_model: freq_scale    = 1
0.00.675.424 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.675.425 I ggml_metal_init: allocating
0.00.675.440 I ggml_metal_init: found device: Apple M4
0.00.675.446 I ggml_metal_init: picking default device: Apple M4
0.00.676.611 I ggml_metal_init: using embedded metal library
0.00.682.102 I ggml_metal_init: GPU name:   Apple M4
0.00.682.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.107 I ggml_metal_init: simdgroup reduction   = true
0.00.682.108 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.108 I ggml_metal_init: has residency sets    = true
0.00.682.108 I ggml_metal_init: has bfloat            = true
0.00.682.108 I ggml_metal_init: use bfloat            = true
0.00.682.109 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.110 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.604 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.700.811 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.700.821 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.700.853 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.870 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.703.872 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.703.872 I llama_init_from_model: graph nodes  = 967
0.00.703.873 I llama_init_from_model: graph splits = 2
0.00.703.875 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.703.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.668 I 
0.00.738.719 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.733 I perplexity: tokenizing the input ..
0.00.747.042 I perplexity: tokenization took 8.307 ms
0.00.747.060 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.653 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.887.912 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.887.928 I llama_perf_context_print:        load time =     729.25 ms
0.00.887.929 I llama_perf_context_print: prompt eval time =     139.37 ms /   128 tokens (    1.09 ms per token,   918.43 tokens per second)
0.00.887.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.930 I llama_perf_context_print:       total time =     149.26 ms /   129 tokens
0.00.888.312 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.089s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.413 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.293 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.299 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.299 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.300 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.304 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.305 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.307 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.307 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.076 I llama_model_loader: - type  f32:  194 tensors
0.00.027.077 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.077 I print_info: file format = GGUF V3 (latest)
0.00.027.078 I print_info: file type   = Q6_K
0.00.027.078 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.889 I load: special tokens cache size = 25
0.00.051.628 I load: token to piece cache size = 0.2984 MB
0.00.051.631 I print_info: arch             = gptneox
0.00.051.631 I print_info: vocab_only       = 0
0.00.051.631 I print_info: n_ctx_train      = 2048
0.00.051.631 I print_info: n_embd           = 2048
0.00.051.632 I print_info: n_layer          = 24
0.00.051.634 I print_info: n_head           = 16
0.00.051.640 I print_info: n_head_kv        = 16
0.00.051.640 I print_info: n_rot            = 32
0.00.051.640 I print_info: n_swa            = 0
0.00.051.641 I print_info: n_embd_head_k    = 128
0.00.051.641 I print_info: n_embd_head_v    = 128
0.00.051.642 I print_info: n_gqa            = 1
0.00.051.643 I print_info: n_embd_k_gqa     = 2048
0.00.051.644 I print_info: n_embd_v_gqa     = 2048
0.00.051.645 I print_info: f_norm_eps       = 1.0e-05
0.00.051.645 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.645 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.645 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.645 I print_info: f_logit_scale    = 0.0e+00
0.00.051.646 I print_info: n_ff             = 8192
0.00.051.646 I print_info: n_expert         = 0
0.00.051.646 I print_info: n_expert_used    = 0
0.00.051.647 I print_info: causal attn      = 1
0.00.051.647 I print_info: pooling type     = 0
0.00.051.647 I print_info: rope type        = 2
0.00.051.647 I print_info: rope scaling     = linear
0.00.051.647 I print_info: freq_base_train  = 10000.0
0.00.051.648 I print_info: freq_scale_train = 1
0.00.051.648 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.648 I print_info: rope_finetuned   = unknown
0.00.051.649 I print_info: ssm_d_conv       = 0
0.00.051.649 I print_info: ssm_d_inner      = 0
0.00.051.650 I print_info: ssm_d_state      = 0
0.00.051.650 I print_info: ssm_dt_rank      = 0
0.00.051.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.651 I print_info: model type       = 1.4B
0.00.051.651 I print_info: model params     = 1.41 B
0.00.051.651 I print_info: general.name     = 1.4B
0.00.051.652 I print_info: vocab type       = BPE
0.00.051.652 I print_info: n_vocab          = 50304
0.00.051.652 I print_info: n_merges         = 50009
0.00.051.652 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.653 I print_info: LF token         = 128 'Ä'
0.00.051.656 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.656 I print_info: max token length = 1024
0.00.268.909 I load_tensors: offloading 24 repeating layers to GPU
0.00.268.914 I load_tensors: offloading output layer to GPU
0.00.268.915 I load_tensors: offloaded 25/25 layers to GPU
0.00.268.937 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.268.940 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.269.770 I llama_init_from_model: n_seq_max     = 1
0.00.269.772 I llama_init_from_model: n_ctx         = 128
0.00.269.773 I llama_init_from_model: n_ctx_per_seq = 128
0.00.269.773 I llama_init_from_model: n_batch       = 128
0.00.269.774 I llama_init_from_model: n_ubatch      = 128
0.00.269.774 I llama_init_from_model: flash_attn    = 0
0.00.269.774 I llama_init_from_model: freq_base     = 10000.0
0.00.269.775 I llama_init_from_model: freq_scale    = 1
0.00.269.775 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.269.776 I ggml_metal_init: allocating
0.00.269.813 I ggml_metal_init: found device: Apple M4
0.00.269.818 I ggml_metal_init: picking default device: Apple M4
0.00.270.967 I ggml_metal_init: using embedded metal library
0.00.275.945 I ggml_metal_init: GPU name:   Apple M4
0.00.275.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.275.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.275.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.275.950 I ggml_metal_init: simdgroup reduction   = true
0.00.275.950 I ggml_metal_init: simdgroup matrix mul. = true
0.00.275.950 I ggml_metal_init: has residency sets    = true
0.00.275.951 I ggml_metal_init: has bfloat            = true
0.00.275.951 I ggml_metal_init: use bfloat            = true
0.00.275.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.275.953 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.289.672 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.291.600 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.291.603 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.291.619 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.293.471 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.293.473 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.293.473 I llama_init_from_model: graph nodes  = 967
0.00.293.473 I llama_init_from_model: graph splits = 2
0.00.293.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.293.475 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.323.123 I 
0.00.323.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.323.164 I perplexity: tokenizing the input ..
0.00.331.083 I perplexity: tokenization took 7.918 ms
0.00.331.107 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.470.133 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.471.430 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.471.442 I llama_perf_context_print:        load time =     311.71 ms
0.00.471.443 I llama_perf_context_print: prompt eval time =     138.80 ms /   128 tokens (    1.08 ms per token,   922.17 tokens per second)
0.00.471.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.471.444 I llama_perf_context_print:       total time =     148.32 ms /   129 tokens
0.00.471.872 I ggml_metal_free: deallocating

real	0m0.488s
user	0m0.085s
sys	0m0.092s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.254 I build: 4559 (2674f02e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.403 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.479 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.487 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.488 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.488 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.489 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.490 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.490 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.491 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.491 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.492 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.628 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.629 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.629 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.630 I llama_model_loader: - type  f32:  194 tensors
0.00.050.630 I llama_model_loader: - type  f16:   98 tensors
0.00.050.631 I print_info: file format = GGUF V3 (latest)
0.00.050.632 I print_info: file type   = all F32 (guessed)
0.00.050.633 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.075.445 I load: special tokens cache size = 25
0.00.082.096 I load: token to piece cache size = 0.2984 MB
0.00.082.099 I print_info: arch             = gptneox
0.00.082.099 I print_info: vocab_only       = 0
0.00.082.099 I print_info: n_ctx_train      = 2048
0.00.082.099 I print_info: n_embd           = 2048
0.00.082.099 I print_info: n_layer          = 24
0.00.082.102 I print_info: n_head           = 16
0.00.082.103 I print_info: n_head_kv        = 16
0.00.082.103 I print_info: n_rot            = 32
0.00.082.103 I print_info: n_swa            = 0
0.00.082.103 I print_info: n_embd_head_k    = 128
0.00.082.103 I print_info: n_embd_head_v    = 128
0.00.082.104 I print_info: n_gqa            = 1
0.00.082.105 I print_info: n_embd_k_gqa     = 2048
0.00.082.105 I print_info: n_embd_v_gqa     = 2048
0.00.082.106 I print_info: f_norm_eps       = 1.0e-05
0.00.082.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.107 I print_info: f_logit_scale    = 0.0e+00
0.00.082.109 I print_info: n_ff             = 8192
0.00.082.109 I print_info: n_expert         = 0
0.00.082.109 I print_info: n_expert_used    = 0
0.00.082.109 I print_info: causal attn      = 1
0.00.082.110 I print_info: pooling type     = 0
0.00.082.110 I print_info: rope type        = 2
0.00.082.111 I print_info: rope scaling     = linear
0.00.082.112 I print_info: freq_base_train  = 10000.0
0.00.082.112 I print_info: freq_scale_train = 1
0.00.082.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.112 I print_info: rope_finetuned   = unknown
0.00.082.112 I print_info: ssm_d_conv       = 0
0.00.082.112 I print_info: ssm_d_inner      = 0
0.00.082.117 I print_info: ssm_d_state      = 0
0.00.082.117 I print_info: ssm_dt_rank      = 0
0.00.082.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.118 I print_info: model type       = 1.4B
0.00.082.118 I print_info: model params     = 1.41 B
0.00.082.118 I print_info: general.name     = 1.4B
0.00.082.119 I print_info: vocab type       = BPE
0.00.082.119 I print_info: n_vocab          = 50304
0.00.082.119 I print_info: n_merges         = 50009
0.00.082.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.121 I print_info: LF token         = 128 'Ä'
0.00.082.121 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.121 I print_info: max token length = 1024
0.01.447.806 I load_tensors: offloading 24 repeating layers to GPU
0.01.447.813 I load_tensors: offloading output layer to GPU
0.01.447.815 I load_tensors: offloaded 25/25 layers to GPU
0.01.447.839 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.447.841 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.448.516 I llama_init_from_model: n_seq_max     = 1
0.01.448.518 I llama_init_from_model: n_ctx         = 128
0.01.448.518 I llama_init_from_model: n_ctx_per_seq = 128
0.01.448.518 I llama_init_from_model: n_batch       = 128
0.01.448.518 I llama_init_from_model: n_ubatch      = 128
0.01.448.519 I llama_init_from_model: flash_attn    = 0
0.01.448.519 I llama_init_from_model: freq_base     = 10000.0
0.01.448.519 I llama_init_from_model: freq_scale    = 1
0.01.448.520 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.448.521 I ggml_metal_init: allocating
0.01.448.552 I ggml_metal_init: found device: Apple M4
0.01.448.555 I ggml_metal_init: picking default device: Apple M4
0.01.449.497 I ggml_metal_init: using embedded metal library
0.01.452.950 I ggml_metal_init: GPU name:   Apple M4
0.01.452.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.452.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.452.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.452.953 I ggml_metal_init: simdgroup reduction   = true
0.01.452.953 I ggml_metal_init: simdgroup matrix mul. = true
0.01.452.953 I ggml_metal_init: has residency sets    = true
0.01.452.953 I ggml_metal_init: has bfloat            = true
0.01.452.953 I ggml_metal_init: use bfloat            = true
0.01.452.954 I ggml_metal_init: hasUnifiedMemory      = true
0.01.452.955 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.462.694 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.464.279 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.464.281 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.464.294 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.465.773 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.465.774 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.465.775 I llama_init_from_model: graph nodes  = 967
0.01.465.775 I llama_init_from_model: graph splits = 2
0.01.465.776 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.465.777 I 
0.01.465.813 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.465.814 I compute_imatrix: tokenizing the input ..
0.01.472.624 I compute_imatrix: tokenization took 6.809 ms
0.01.472.625 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.737.414 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.739.863 I llama_perf_context_print:        load time =    1717.01 ms
0.01.739.864 I llama_perf_context_print: prompt eval time =     263.69 ms /   128 tokens (    2.06 ms per token,   485.42 tokens per second)
0.01.739.864 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.739.865 I llama_perf_context_print:       total time =    1719.45 ms /   129 tokens
0.01.740.380 I ggml_metal_free: deallocating

real	0m1.930s
user	0m0.141s
sys	0m0.334s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4559 (2674f02e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155107080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155107540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155107af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1551080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155108650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155108c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1551091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155109760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155109d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15510a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15510a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15510ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15510b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15510bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15510c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15510ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15510d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15510dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15510e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15510eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15510f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15510f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1551100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155110940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155111060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155111320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155111930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1551125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155112ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155112da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155113240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155113500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155113d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1551142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155114590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155114a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155114ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155115370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155115810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155115cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155116150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1551165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155116a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155116f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1551171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155117800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155117e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155118730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155118d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155119350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155119960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155119f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15511a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15511ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15511b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15511b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15511bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15511bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15511c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15511cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15511d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15511d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15511d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15511de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15511e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15511e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15511ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15511f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15511f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15511f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15511fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155120320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1551207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155120d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155121260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1551217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155121d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155122250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1551227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155122cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155123240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155123790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155123ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155124230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155124780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155124cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155125220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155125770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155125cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155126210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155126760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155126cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155127200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155127750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155127ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1551281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155128740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155118420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155128bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155129360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1551298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155129e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15512a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15512a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15512adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15512b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15512b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15512bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15512c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15512c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15512cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15512d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15512d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15512dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15512e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15512e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15512eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15512ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15512f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15512f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15512fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155130210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1551306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155130b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155130ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155131490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155131930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155131dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155132270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155132710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155132bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155133050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1551334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155133990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155133e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1551342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155134770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155134c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1551350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155135550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1551359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155135e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155136330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1551367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155136c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155137110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1551375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155137a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155137ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155138390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155138830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155138cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155139170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155139610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155139ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155139f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15513a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15513a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15513ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15513b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15513b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15513bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15513bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15513c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15513c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15513cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15513d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15513d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15513db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15513e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15513e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15513e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15513edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15513f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15513f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15513fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155140070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155140510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1551409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155140e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1551412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155141790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155141c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1551420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155142570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155142a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155142eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155143350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1551437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155143c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155144130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1551445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155144a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155144fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155145510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155145a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155145fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155146270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155146880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155146e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1551474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155147c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155148130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1551483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155148a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155149010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155149800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155149ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15514a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15514a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15514ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15514b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15514b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15514bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15514c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15514c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15514cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15514d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15514d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15514dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15514e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15514e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15514ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15514f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15514f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15514fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155150290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1551507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155150d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155151280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1551517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155151d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155152270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1551527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155152d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155153260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1551537b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155153d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155154250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1551547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155154cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155155240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155155790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155155ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155156230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155156780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155156cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155157220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155157770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155157cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155158210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155158760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155158cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155159200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155159750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155159ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15515a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15515a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15515ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15515b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15515b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15515bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15515c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15515c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15515cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15515d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15515d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15515dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15515e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15515e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15515e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15515ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15515f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15515f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15515fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1551600b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155160550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1551609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155160e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155161330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1551617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155161c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1551621c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1551628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155163000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155163720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155163e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155164100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1551648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155164bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1551651c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.778.974 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.778.978 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ef04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ef04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ef05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ef05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ef05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ef06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ef065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ef06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ef06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ef07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ef07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ef07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ef08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ef09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ef09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ef0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ef0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ef0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ef0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ef0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ef0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ef0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ef0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ef0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ef0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ef0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ef0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ef0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ef0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ef0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ef0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ef0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ef10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ef10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ef108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ef10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ef11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ef11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ef11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ef11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ef12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ef127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ef12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ef130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ef13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ef13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ef13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ef14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ef146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ef14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ef14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ef15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ef15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ef15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ef16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ef165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ef16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ef17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ef174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ef17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ef17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ef18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ef18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ef18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ef18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ef193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ef19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ef19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ef1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ef1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ef1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ef1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ef1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ef1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ef1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ef1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ef1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ef1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ef1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ef1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ef1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ef1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ef1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ef1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ef1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ef1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ef1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ef1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ef1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ef1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ef202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ef20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ef20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ef21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ef21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ef218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ef21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ef221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ef22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ef22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ef22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ef23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ef23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ef23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ef240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ef24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ef249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ef24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ef252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ef25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ef25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ef25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ef26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ef268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ef26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ef271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ef27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ef27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ef27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ef28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ef287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ef28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ef290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ef29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ef299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ef29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ef2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ef2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ef2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ef2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ef2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ef2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ef2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ef2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ef2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ef2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ef2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ef2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ef2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ef2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ef2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ef2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ef2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ef2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ef2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ef2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ef2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ef2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ef30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ef30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ef30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ef31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ef315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ef31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ef31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ef32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ef327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ef32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ef33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ef334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ef33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ef33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ef34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ef346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ef34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ef34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ef35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ef35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ef36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ef365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ef36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ef36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ef37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ef37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ef37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ef38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ef384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ef38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ef38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ef39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ef39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ef39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ef39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ef3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ef3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ef3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ef3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ef3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ef3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ef3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ef3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ef3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ef3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ef3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ef3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ef3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ef3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ef3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ef3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ef3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ef3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ef3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ef3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ef3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ef40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ef40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ef40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ef40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ef41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ef41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ef42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ef42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ef42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ef433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ef43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ef43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ef44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ef44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ef45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ef45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ef45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ef461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ef46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ef46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ef47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ef478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ef47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ef48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ef48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ef48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ef49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ef49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ef4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ef4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ef4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ef4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ef4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ef4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ef4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ef4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ef4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ef4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ef4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ef4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ef4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ef4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ef4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ef4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ef4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ef502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ef50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ef50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ef51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ef519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ef51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ef52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ef52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ef530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ef53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ef53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ef54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ef547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ef54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ef55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ef55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ef55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ef56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ef56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ef56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ef57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ef57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ef57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ef58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ef58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ef58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ef59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ef59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ef59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ef5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ef5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ef5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ef5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ef5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ef5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ef5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ef5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ef5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ef5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ef5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ef5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ef5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ef5b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ef4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ef4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ef48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ef45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ef55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ef52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ef50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ef4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ef46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ef43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ef48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ef49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ef4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ef4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ef53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ef47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ef51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ef4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ef4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ef475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ef55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ef447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ef430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ef45340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ef55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ef4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ef53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ef49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ef4bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ef4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ef47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ef4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ef516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ef45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ef544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ef51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ef4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ef56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ef44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ef56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ef44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ef54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ef4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ef50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ef53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ef52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ef4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ef41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ef04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ef5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ef0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ef5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ef5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ef5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ef5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ef5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ef5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ef5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ef60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ef60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ef607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ef60a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ef60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ef61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ef612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ef61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ef61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ef61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ef61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ef62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ef62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ef62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ef628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ef62b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ef62e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ef63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ef633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ef63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ef63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ef63c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ef63ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ef64190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ef64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ef64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ef649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ef64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ef64f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ef65210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ef654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ef65790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ef65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ef65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ef65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ef66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ef66550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ef66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ef66ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ef66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ef67050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ef67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ef675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ef67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ef67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ef67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ef680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ef68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ef68650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ef68910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ef68bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ef68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ef69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ef69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ef696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ef69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ef69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ef69f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ef6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ef6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ef6a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ef6aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ef6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ef6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ef6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ef6b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ef6b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ef6ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ef6bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ef6c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ef6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ef6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ef6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ef6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ef6cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ef6d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ef6d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ef6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ef6d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ef6db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ef6de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ef6e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ef6e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ef6e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ef6e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ef6ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ef6eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ef6f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ef6f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ef6f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ef6f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ef6fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ef6ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ef70210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ef704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ef70790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ef70a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ef70d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ef70fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ef71290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ef71550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ef71810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ef71ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ef71d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ef72050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ef72310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ef725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ef72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ef72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ef72e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ef730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ef73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ef73650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ef73910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ef73bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ef73e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ef74150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ef74410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ef746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ef74990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ef74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ef74f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ef751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ef75490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ef75750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ef75a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ef75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ef75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ef76250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ef76510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ef767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ef76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ef76d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ef77010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ef772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ef77590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ef77850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ef77b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ef77dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ef78090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ef78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ef78610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ef788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ef78b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ef78e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ef79110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ef793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ef79690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ef79950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ef79c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ef79ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ef7a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ef7a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ef7aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ef7ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ef7afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ef7b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ef7ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ef7bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ef7c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ef7ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ef7cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ef7d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ef7da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ef7df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ef7e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ef7ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ef7ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ef7f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ef7fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ef7ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ef804a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ef809f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ef80f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ef81490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ef819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ef81f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ef82480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ef829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ef82f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ef83470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ef839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ef83f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ef84460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ef849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ef84f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ef85450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ef859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ef85ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ef86440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ef86990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ef86ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ef87430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ef87980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ef87ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ef88420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ef88970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ef88ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ef89410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ef89960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ef89eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ef8a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ef8a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ef8aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ef8b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ef8b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ef8be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ef8c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ef8c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ef8c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ef8cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ef8d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ef8d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ef8d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ef8dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ef8e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ef8e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ef8eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ef8efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ef8f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ef8f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ef8fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ef90160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ef905d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ef90a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ef91730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ef91e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ef92570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ef92830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ef92ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ef932a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ef938b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.032s
user	0m0.298s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4559 (2674f02e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c80ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c80e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c80ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c80f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c80f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c80fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c810150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c810700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c810cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c8111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c8116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c811bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c8126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c813690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c813db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c8144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c814bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c815310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c815ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c816200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c816920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c817040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c8178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c8182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c8188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c819540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c819a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c819d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c81a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c81a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c81ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c81b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c81b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c81b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c81be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c81c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c81c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c81cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c81d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c81d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c81da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c81ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c81e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c81e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c81edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c81f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c81fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c8202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c820900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c820f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c821520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c821b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c822320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c8227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c822c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c822f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c823530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c823d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c823fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c824480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c824920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c824dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c825260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c825700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c825ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c8264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c826980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c826e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c8272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c827760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c827cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c828200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c828750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c8291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c829740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c829c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c82a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c82a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c82ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c82b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c82b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c82bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c82c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c82c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c82cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c82d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c82d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c82dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c82e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c82e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c82ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c82f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c82f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c81f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c82fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c830300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c830850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c830da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c8312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c831840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c831d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c8322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c832830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c832d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c8332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c833820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c833d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c8342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c834810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c834cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c835150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c8355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c835a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c835f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c8363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c836870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c836d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c8371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c837650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c837af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c837f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c838430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c8388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c838d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c839210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c8396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c839b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c839ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c83a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c83a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c83add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c83b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c83b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c83bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c83c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c83c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c83c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c83ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c83d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c83d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c83dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c83e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c83e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c83e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c83ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c83f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c83f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c83fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c840110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c8405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c840a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c840ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c841390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c841830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c841cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c842170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c842610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c842ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c842f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c8433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c843890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c843d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c8441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c844670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c844fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c845450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c8458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c845d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c846230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c8466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c846b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c847010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c8474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c847950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c847df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c848290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c848730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c848bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c849070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c849510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c8499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c849e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c84a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c84a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c84ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c84b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c84b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c84ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c84bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c84c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c84ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c84d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c84d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c84de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c84e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c84ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c84f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c84f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c84f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c84ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c8507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c850c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c8510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c851580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c851d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c852280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c8527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c852d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c853270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c8537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c853d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c854260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c8547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c854d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c855250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c8557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c855cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c856240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c856790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c856ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c857230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c857780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c857cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c858220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c858770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c858cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c859210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c859760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c859cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c85a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c85a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c85aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c85b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c85b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c85bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c85c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c85c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c85cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c85d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c85d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c85dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c85e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c85e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c85ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c85f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c85f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c85fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c8601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c8606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c860c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c861190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c8616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c861c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c862180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c8626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c862c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c863170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c8636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c863c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c864160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c8646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c864b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c864ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c865490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c865930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c865dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c866270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c866710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c866bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c867050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c8674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c867990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c867e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c8682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c868770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c868c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c869160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c869880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c869fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c86a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c86ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c86b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c86b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c86bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c86c160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.108.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14f204ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14f204f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14f2053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14f205830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14f205ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14f206110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14f206580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14f2069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14f206e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14f2073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14f207850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14f207ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14f2089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14f2091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14f2099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14f20a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14f20a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14f20af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14f20b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14f20be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14f20c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14f20cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14f20d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14f20da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14f20e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14f20e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14f20e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14f20eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14f20f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14f20f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14f20f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14f20fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14f210280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14f210540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14f2109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14f210e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14f211290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14f211700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14f211b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14f211fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14f212450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14f2128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14f212d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14f2131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14f213610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14f213a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14f213ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14f214360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14f2147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14f214c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14f2150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14f215520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14f215990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14f215e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14f216270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14f2166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14f216c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14f217150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14f2175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14f217a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14f217ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14f218310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14f218780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14f218bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14f219060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14f2194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14f219940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14f219db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14f21a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14f21a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14f21ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14f21af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14f21b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14f21b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14f21bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14f21c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14f21c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14f21ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14f21ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14f21d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14f21d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14f21dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14f21e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14f21e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14f21e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14f21ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14f21f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14f21f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14f21fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14f21ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14f2203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14f220830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14f220ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14f221110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14f221580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14f2219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14f221e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14f2222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14f222740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14f222bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14f223020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14f223490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14f223900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14f223d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14f2241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14f224650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14f224ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14f224f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14f2253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14f225810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14f225c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14f2260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14f226560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14f2269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14f226e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14f2272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14f227720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14f227b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14f228000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14f228470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14f2288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14f228d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14f2291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14f229630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14f229aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14f229f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14f22a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14f22a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14f22ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14f22b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14f22b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14f22b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14f22be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14f22c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14f22c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14f22cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14f22cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14f22d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14f22d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14f22dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14f22e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14f22e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14f22ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14f22eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14f22f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14f22f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14f22fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14f2300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14f230520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14f230990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14f230e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14f231270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14f2316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14f231b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14f231fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14f232430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14f2328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14f232d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14f233180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14f2335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14f233a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14f233ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14f234340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14f2347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14f234c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14f235090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14f235cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14f235f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14f236240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14f2366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14f236b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14f236f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14f237400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14f237870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14f237ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14f238150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14f2385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14f238a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14f238ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14f239310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14f239780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14f239bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14f23a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14f23a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14f23a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14f23adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14f23b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14f23b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14f23bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14f23bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14f23c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14f23c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14f23ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14f23d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14f23d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14f23da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14f23de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14f23e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14f23e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14f23ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14f23f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14f23f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14f23fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14f23ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14f240390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14f240800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14f240c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14f2410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14f241600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14f241b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14f242680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14f242940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14f242f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14f2434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14f243a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14f244040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14f244600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14f244bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14f245180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14f245740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14f245d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14f2462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14f246880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14f246e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14f247400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14f2479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14f247f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14f248540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14f248b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14f2490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14f249680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14f249c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14f24a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14f24a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14f24ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14f24b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14f24b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14f24bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14f24c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14f24ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14f24d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14f24d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14f24db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14f24e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14f24e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14f24ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14f24f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14f24f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14f24fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14f2503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14f250980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14f250f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14f251500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14f251ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14f252080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14f252640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14f252c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14f2531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14f253780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14f253d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14f254300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14f2548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14f254e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14f255440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14f255a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14f255fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14f256580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14f256b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14f257040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14f257540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14f257a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14f257f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14f258440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14f258940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14f258e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14f259340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14f259840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14f259d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14f25a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14f25a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14f25ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14f25b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14f25b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14f25c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14f25c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14f25ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14f25d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14f25d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14f25e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14f25e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14f25e930 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14be04c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14be050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14be05540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14be059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14be05e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14be06290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14be06700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14be06b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14be06fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14be07450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14be078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14be07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14be08b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14be092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14be09ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14be0a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14be0a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14be0b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14be0b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14be0be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14be0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14be0ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14be0d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14be0db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14be0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14be0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14be0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14be0ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14be0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14be0f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14be0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14be0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14be10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14be105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14be10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14be10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14be11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14be11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14be11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14be12060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14be124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14be12940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14be12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14be13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14be13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14be13b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14be13f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14be143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14be14850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14be14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14be15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14be155a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14be15a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14be15e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14be162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14be16760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14be16cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14be171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14be17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14be17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14be17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14be18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14be18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14be18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14be190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14be19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14be199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14be19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14be1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14be1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14be1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14be1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14be1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14be1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14be1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14be1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14be1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14be1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14be1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14be1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14be1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14be1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14be1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14be1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14be1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14be1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14be1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14be1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14be1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14be1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14be20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14be208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14be20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14be21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14be21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14be21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14be21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14be22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14be227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14be22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14be230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14be23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14be23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14be24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14be244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14be24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14be24db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14be25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14be25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14be25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14be25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14be263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14be26850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14be26cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14be27130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14be275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14be27a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14be27e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14be282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14be28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14be28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14be29040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14be294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14be29920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14be29d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14be2a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14be2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14be2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14be2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14be2b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14be2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14be2bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14be2c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14be2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14be2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14be2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14be2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14be2d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14be2dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14be2e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14be2e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14be2e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14be2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14be2f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14be2f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14be2fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14be2ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14be303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14be30810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14be30c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14be310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14be31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14be319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14be31e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14be322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14be32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14be32b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14be33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14be33470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14be338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14be33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14be341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14be34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14be34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14be34f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14be35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14be357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14be35c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14be360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14be36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14be369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14be36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14be37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14be37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14be37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14be37fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14be38450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14be388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14be38d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14be391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14be39610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14be39a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14be39ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14be3a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14be3a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14be3ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14be3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14be3b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14be3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14be3be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14be3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14be3c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14be3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14be3cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14be3d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14be3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14be3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14be3e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14be3e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14be3ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14be3eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14be3f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14be3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14be3fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14be40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14be40500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14be40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14be40de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14be41250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14be416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14be42240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14be42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14be427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14be42c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14be430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14be43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14be43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14be43df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14be44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14be446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14be44b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14be44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14be45420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14be45890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14be45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14be46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14be465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14be46a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14be46ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14be47330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14be477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14be47c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14be48080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14be484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14be48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14be48dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14be49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14be496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14be49b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14be49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14be4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14be4a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14be4ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14be4b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14be4b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14be4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14be4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14be4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14be4c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14be4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14be4d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14be4d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14be4d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14be4ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14be4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14be4e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14be4eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14be4ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14be4f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14be4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14be4fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14be50130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14be505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14be50a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14be50e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14be512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14be51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14be51bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14be52040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14be524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14be52920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14be52d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14be53200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14be53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14be53ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14be53f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14be543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14be54830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14be54ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14be55110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14be55580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14be559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14be55e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14be568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14be56ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14be57710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14be57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14be580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14be58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14be58b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14be59170 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.926s
user	0m0.249s
sys	0m0.146s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
