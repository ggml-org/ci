### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.61 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.25 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.24 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.81 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.83 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  189.27 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.99 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 234.54 sec*proc (28 tests)

Total Test time (real) = 234.55 sec

real	3m54.582s
user	8m19.348s
sys	0m6.804s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.21 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.20 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.43 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.46 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.19 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.58 sec*proc (28 tests)

Total Test time (real) =  52.59 sec

real	0m52.602s
user	1m15.912s
sys	0m6.038s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.067 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.118 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.836 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.846 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.847 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.848 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.848 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.849 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.850 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.851 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.851 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.852 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.855 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.856 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.856 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.857 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.857 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.858 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.858 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.372 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.376 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.377 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.377 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.378 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.379 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.380 I llama_model_loader: - type  f32:  124 tensors
0.00.030.380 I llama_model_loader: - type  f16:   73 tensors
0.00.030.381 I print_info: file format = GGUF V3 (latest)
0.00.030.382 I print_info: file type   = F16
0.00.030.384 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.033.325 I load: special tokens cache size = 5
0.00.034.524 I load: token to piece cache size = 0.2032 MB
0.00.034.529 I print_info: arch             = bert
0.00.034.530 I print_info: vocab_only       = 0
0.00.034.530 I print_info: n_ctx_train      = 512
0.00.034.530 I print_info: n_embd           = 384
0.00.034.530 I print_info: n_layer          = 12
0.00.034.535 I print_info: n_head           = 12
0.00.034.535 I print_info: n_head_kv        = 12
0.00.034.535 I print_info: n_rot            = 32
0.00.034.537 I print_info: n_swa            = 0
0.00.034.538 I print_info: n_embd_head_k    = 32
0.00.034.538 I print_info: n_embd_head_v    = 32
0.00.034.538 I print_info: n_gqa            = 1
0.00.034.539 I print_info: n_embd_k_gqa     = 384
0.00.034.540 I print_info: n_embd_v_gqa     = 384
0.00.034.540 I print_info: f_norm_eps       = 1.0e-12
0.00.034.541 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.541 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.541 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.541 I print_info: f_logit_scale    = 0.0e+00
0.00.034.542 I print_info: n_ff             = 1536
0.00.034.545 I print_info: n_expert         = 0
0.00.034.545 I print_info: n_expert_used    = 0
0.00.034.545 I print_info: causal attn      = 0
0.00.034.545 I print_info: pooling type     = 2
0.00.034.545 I print_info: rope type        = 2
0.00.034.546 I print_info: rope scaling     = linear
0.00.034.546 I print_info: freq_base_train  = 10000.0
0.00.034.546 I print_info: freq_scale_train = 1
0.00.034.547 I print_info: n_ctx_orig_yarn  = 512
0.00.034.547 I print_info: rope_finetuned   = unknown
0.00.034.547 I print_info: ssm_d_conv       = 0
0.00.034.547 I print_info: ssm_d_inner      = 0
0.00.034.547 I print_info: ssm_d_state      = 0
0.00.034.547 I print_info: ssm_dt_rank      = 0
0.00.034.548 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.548 I print_info: model type       = 33M
0.00.034.548 I print_info: model params     = 33.21 M
0.00.034.549 I print_info: general.name     = Bge Small
0.00.034.549 I print_info: vocab type       = WPM
0.00.034.549 I print_info: n_vocab          = 30522
0.00.034.549 I print_info: n_merges         = 0
0.00.034.550 I print_info: BOS token        = 101 '[CLS]'
0.00.034.550 I print_info: UNK token        = 100 '[UNK]'
0.00.034.550 I print_info: SEP token        = 102 '[SEP]'
0.00.034.550 I print_info: PAD token        = 0 '[PAD]'
0.00.034.551 I print_info: MASK token       = 103 '[MASK]'
0.00.034.551 I print_info: LF token         = 0 '[PAD]'
0.00.034.551 I print_info: max token length = 21
0.00.035.866 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.866 I load_tensors: offloading output layer to GPU
0.00.035.867 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.889 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.890 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.036.054 I llama_init_from_model: n_seq_max     = 1
0.00.036.054 I llama_init_from_model: n_ctx         = 512
0.00.036.055 I llama_init_from_model: n_ctx_per_seq = 512
0.00.036.055 I llama_init_from_model: n_batch       = 2048
0.00.036.055 I llama_init_from_model: n_ubatch      = 2048
0.00.036.055 I llama_init_from_model: flash_attn    = 0
0.00.036.055 I llama_init_from_model: freq_base     = 10000.0
0.00.036.056 I llama_init_from_model: freq_scale    = 1
0.00.036.056 I ggml_metal_init: allocating
0.00.036.059 I ggml_metal_init: found device: Apple M4
0.00.036.063 I ggml_metal_init: picking default device: Apple M4
0.00.036.602 I ggml_metal_init: using embedded metal library
0.00.039.099 I ggml_metal_init: GPU name:   Apple M4
0.00.039.101 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.102 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.102 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.102 I ggml_metal_init: simdgroup reduction   = true
0.00.039.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.102 I ggml_metal_init: has bfloat            = true
0.00.039.103 I ggml_metal_init: use bfloat            = true
0.00.039.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.049.299 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.049.786 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.049.787 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.049.789 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.050.380 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.050.381 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.050.381 I llama_init_from_model: graph nodes  = 429
0.00.050.382 I llama_init_from_model: graph splits = 2
0.00.050.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.050.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.055.463 I 
0.00.055.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.056.038 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.060.405 I llama_perf_context_print:        load time =      35.34 ms
0.00.060.406 I llama_perf_context_print: prompt eval time =       4.22 ms /     9 tokens (    0.47 ms per token,  2131.69 tokens per second)
0.00.060.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.060.407 I llama_perf_context_print:       total time =       4.94 ms /    10 tokens
0.00.060.617 I ggml_metal_free: deallocating

real	0m0.235s
user	0m0.040s
sys	0m0.024s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.278 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.281 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.283 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.284 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.285 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.285 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.286 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.286 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.287 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.287 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.287 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.290 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.290 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.291 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.291 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.291 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.292 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.452 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.041 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.042 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.042 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.042 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.043 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.043 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.043 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.044 I llama_model_loader: - type  f32:  124 tensors
0.00.014.044 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.045 I print_info: file format = GGUF V3 (latest)
0.00.014.045 I print_info: file type   = Q8_0
0.00.014.046 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.252 I load: special tokens cache size = 5
0.00.017.458 I load: token to piece cache size = 0.2032 MB
0.00.017.461 I print_info: arch             = bert
0.00.017.461 I print_info: vocab_only       = 0
0.00.017.462 I print_info: n_ctx_train      = 512
0.00.017.462 I print_info: n_embd           = 384
0.00.017.462 I print_info: n_layer          = 12
0.00.017.465 I print_info: n_head           = 12
0.00.017.466 I print_info: n_head_kv        = 12
0.00.017.466 I print_info: n_rot            = 32
0.00.017.466 I print_info: n_swa            = 0
0.00.017.466 I print_info: n_embd_head_k    = 32
0.00.017.466 I print_info: n_embd_head_v    = 32
0.00.017.467 I print_info: n_gqa            = 1
0.00.017.468 I print_info: n_embd_k_gqa     = 384
0.00.017.468 I print_info: n_embd_v_gqa     = 384
0.00.017.469 I print_info: f_norm_eps       = 1.0e-12
0.00.017.469 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.469 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.470 I print_info: f_logit_scale    = 0.0e+00
0.00.017.470 I print_info: n_ff             = 1536
0.00.017.471 I print_info: n_expert         = 0
0.00.017.471 I print_info: n_expert_used    = 0
0.00.017.471 I print_info: causal attn      = 0
0.00.017.471 I print_info: pooling type     = 2
0.00.017.471 I print_info: rope type        = 2
0.00.017.471 I print_info: rope scaling     = linear
0.00.017.472 I print_info: freq_base_train  = 10000.0
0.00.017.472 I print_info: freq_scale_train = 1
0.00.017.472 I print_info: n_ctx_orig_yarn  = 512
0.00.017.473 I print_info: rope_finetuned   = unknown
0.00.017.473 I print_info: ssm_d_conv       = 0
0.00.017.473 I print_info: ssm_d_inner      = 0
0.00.017.473 I print_info: ssm_d_state      = 0
0.00.017.473 I print_info: ssm_dt_rank      = 0
0.00.017.473 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.473 I print_info: model type       = 33M
0.00.017.474 I print_info: model params     = 33.21 M
0.00.017.474 I print_info: general.name     = Bge Small
0.00.017.475 I print_info: vocab type       = WPM
0.00.017.475 I print_info: n_vocab          = 30522
0.00.017.475 I print_info: n_merges         = 0
0.00.017.475 I print_info: BOS token        = 101 '[CLS]'
0.00.017.475 I print_info: UNK token        = 100 '[UNK]'
0.00.017.476 I print_info: SEP token        = 102 '[SEP]'
0.00.017.476 I print_info: PAD token        = 0 '[PAD]'
0.00.017.476 I print_info: MASK token       = 103 '[MASK]'
0.00.017.476 I print_info: LF token         = 0 '[PAD]'
0.00.017.476 I print_info: max token length = 21
0.00.018.702 I load_tensors: offloading 12 repeating layers to GPU
0.00.018.703 I load_tensors: offloading output layer to GPU
0.00.018.704 I load_tensors: offloaded 13/13 layers to GPU
0.00.018.712 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.018.713 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.018.858 I llama_init_from_model: n_seq_max     = 1
0.00.018.859 I llama_init_from_model: n_ctx         = 512
0.00.018.859 I llama_init_from_model: n_ctx_per_seq = 512
0.00.018.859 I llama_init_from_model: n_batch       = 2048
0.00.018.859 I llama_init_from_model: n_ubatch      = 2048
0.00.018.860 I llama_init_from_model: flash_attn    = 0
0.00.018.860 I llama_init_from_model: freq_base     = 10000.0
0.00.018.860 I llama_init_from_model: freq_scale    = 1
0.00.018.861 I ggml_metal_init: allocating
0.00.018.864 I ggml_metal_init: found device: Apple M4
0.00.018.866 I ggml_metal_init: picking default device: Apple M4
0.00.019.363 I ggml_metal_init: using embedded metal library
0.00.021.749 I ggml_metal_init: GPU name:   Apple M4
0.00.021.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.021.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.021.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.021.752 I ggml_metal_init: simdgroup reduction   = true
0.00.021.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.021.752 I ggml_metal_init: has bfloat            = true
0.00.021.753 I ggml_metal_init: use bfloat            = true
0.00.021.753 I ggml_metal_init: hasUnifiedMemory      = true
0.00.021.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.007 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.032.509 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.032.511 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.032.513 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.033.083 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.033.084 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.033.085 I llama_init_from_model: graph nodes  = 429
0.00.033.085 I llama_init_from_model: graph splits = 2
0.00.033.087 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.360 I 
0.00.037.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.037.906 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.320 I llama_perf_context_print:        load time =      28.50 ms
0.00.042.321 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2099.37 tokens per second)
0.00.042.322 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.322 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.042.519 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.209 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.711 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.718 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.726 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.727 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.728 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.728 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.730 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.731 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.731 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.732 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.732 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.735 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.736 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.737 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.737 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.611 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.612 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.612 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.612 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.613 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.613 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.613 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.614 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.614 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.615 I llama_model_loader: - type  f32:   40 tensors
0.00.049.615 I llama_model_loader: - type  f16:   30 tensors
0.00.049.616 I print_info: file format = GGUF V3 (latest)
0.00.049.616 I print_info: file type   = F16
0.00.049.618 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.065.691 W load: empty token at index 5
0.00.070.073 W load: model vocab missing newline token, using special_pad_id instead
0.00.071.360 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.389 I load: special tokens cache size = 5
0.00.331.687 I load: token to piece cache size = 1.5060 MB
0.00.331.694 I print_info: arch             = jina-bert-v2
0.00.331.695 I print_info: vocab_only       = 0
0.00.331.695 I print_info: n_ctx_train      = 8192
0.00.331.695 I print_info: n_embd           = 384
0.00.331.696 I print_info: n_layer          = 4
0.00.331.702 I print_info: n_head           = 12
0.00.331.702 I print_info: n_head_kv        = 12
0.00.331.703 I print_info: n_rot            = 32
0.00.331.703 I print_info: n_swa            = 0
0.00.331.703 I print_info: n_embd_head_k    = 32
0.00.331.703 I print_info: n_embd_head_v    = 32
0.00.331.703 I print_info: n_gqa            = 1
0.00.331.704 I print_info: n_embd_k_gqa     = 384
0.00.331.704 I print_info: n_embd_v_gqa     = 384
0.00.331.705 I print_info: f_norm_eps       = 1.0e-12
0.00.331.706 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.331.706 I print_info: f_clamp_kqv      = 0.0e+00
0.00.331.708 I print_info: f_max_alibi_bias = 8.0e+00
0.00.331.709 I print_info: f_logit_scale    = 0.0e+00
0.00.331.716 I print_info: n_ff             = 1536
0.00.331.718 I print_info: n_expert         = 0
0.00.331.718 I print_info: n_expert_used    = 0
0.00.331.718 I print_info: causal attn      = 0
0.00.331.719 I print_info: pooling type     = -1
0.00.331.720 I print_info: rope type        = -1
0.00.331.720 I print_info: rope scaling     = linear
0.00.331.720 I print_info: freq_base_train  = 10000.0
0.00.331.721 I print_info: freq_scale_train = 1
0.00.331.721 I print_info: n_ctx_orig_yarn  = 8192
0.00.331.721 I print_info: rope_finetuned   = unknown
0.00.331.721 I print_info: ssm_d_conv       = 0
0.00.331.722 I print_info: ssm_d_inner      = 0
0.00.331.722 I print_info: ssm_d_state      = 0
0.00.331.722 I print_info: ssm_dt_rank      = 0
0.00.331.722 I print_info: ssm_dt_b_c_rms   = 0
0.00.331.722 I print_info: model type       = 33M
0.00.331.722 I print_info: model params     = 32.90 M
0.00.331.723 I print_info: general.name     = Jina Bert Implementation
0.00.331.724 I print_info: vocab type       = BPE
0.00.331.724 I print_info: n_vocab          = 61056
0.00.331.724 I print_info: n_merges         = 39382
0.00.331.724 I print_info: BOS token        = 0 '<s>'
0.00.331.725 I print_info: EOS token        = 2 '</s>'
0.00.331.726 I print_info: UNK token        = 3 '<unk>'
0.00.331.727 I print_info: SEP token        = 2 '</s>'
0.00.331.727 I print_info: PAD token        = 1 '<pad>'
0.00.331.727 I print_info: MASK token       = 4 '<mask>'
0.00.331.727 I print_info: EOG token        = 2 '</s>'
0.00.331.727 I print_info: max token length = 45
0.00.333.082 I load_tensors: offloading 4 repeating layers to GPU
0.00.333.083 I load_tensors: offloading output layer to GPU
0.00.333.083 I load_tensors: offloaded 5/5 layers to GPU
0.00.333.109 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.110 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.333.418 I llama_init_from_model: n_seq_max     = 1
0.00.333.419 I llama_init_from_model: n_ctx         = 8192
0.00.333.419 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.333.420 I llama_init_from_model: n_batch       = 2048
0.00.333.420 I llama_init_from_model: n_ubatch      = 2048
0.00.333.420 I llama_init_from_model: flash_attn    = 0
0.00.333.420 I llama_init_from_model: freq_base     = 10000.0
0.00.333.420 I llama_init_from_model: freq_scale    = 1
0.00.333.421 I ggml_metal_init: allocating
0.00.333.425 I ggml_metal_init: found device: Apple M4
0.00.333.427 I ggml_metal_init: picking default device: Apple M4
0.00.334.319 I ggml_metal_init: using embedded metal library
0.00.337.086 I ggml_metal_init: GPU name:   Apple M4
0.00.337.087 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.088 I ggml_metal_init: simdgroup reduction   = true
0.00.337.088 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.088 I ggml_metal_init: has bfloat            = true
0.00.337.089 I ggml_metal_init: use bfloat            = true
0.00.337.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.346.439 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.348.738 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.348.740 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.348.742 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.276 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.277 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.277 I llama_init_from_model: graph nodes  = 154
0.00.349.278 I llama_init_from_model: graph splits = 2
0.00.349.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.032 I 
0.00.359.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.218 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.219 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.222 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.222 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.224 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.224 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.359.712 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.363.423 I llama_perf_context_print:        load time =     336.18 ms
0.00.363.424 I llama_perf_context_print: prompt eval time =       3.69 ms /    62 tokens (    0.06 ms per token, 16811.28 tokens per second)
0.00.363.425 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.363.426 I llama_perf_context_print:       total time =       4.39 ms /    63 tokens
0.00.363.687 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.338s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.100 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.240 I main: llama backend init
0.00.000.246 I main: load the model and apply lora adapter, if any
0.00.071.831 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.084.138 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.084.149 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.084.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.084.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.084.153 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.084.153 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.084.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.084.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.084.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.084.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.084.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.084.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.084.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.084.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.084.165 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.084.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.084.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.091.107 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.093.306 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.100.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.100.155 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.100.156 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.100.156 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.100.156 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.100.157 I llama_model_loader: - type  f32:  194 tensors
0.00.100.157 I llama_model_loader: - type  f16:   98 tensors
0.00.100.158 I print_info: file format = GGUF V3 (latest)
0.00.100.158 I print_info: file type   = all F32 (guessed)
0.00.100.160 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.119.975 I load: special tokens cache size = 25
0.00.125.940 I load: token to piece cache size = 0.2984 MB
0.00.125.945 I print_info: arch             = gptneox
0.00.125.945 I print_info: vocab_only       = 0
0.00.125.945 I print_info: n_ctx_train      = 2048
0.00.125.947 I print_info: n_embd           = 2048
0.00.125.947 I print_info: n_layer          = 24
0.00.125.952 I print_info: n_head           = 16
0.00.125.953 I print_info: n_head_kv        = 16
0.00.125.953 I print_info: n_rot            = 32
0.00.125.953 I print_info: n_swa            = 0
0.00.125.953 I print_info: n_embd_head_k    = 128
0.00.125.955 I print_info: n_embd_head_v    = 128
0.00.125.956 I print_info: n_gqa            = 1
0.00.125.960 I print_info: n_embd_k_gqa     = 2048
0.00.125.961 I print_info: n_embd_v_gqa     = 2048
0.00.125.961 I print_info: f_norm_eps       = 1.0e-05
0.00.125.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.125.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.125.963 I print_info: f_max_alibi_bias = 0.0e+00
0.00.125.963 I print_info: f_logit_scale    = 0.0e+00
0.00.125.964 I print_info: n_ff             = 8192
0.00.125.964 I print_info: n_expert         = 0
0.00.125.964 I print_info: n_expert_used    = 0
0.00.125.964 I print_info: causal attn      = 1
0.00.125.965 I print_info: pooling type     = 0
0.00.125.965 I print_info: rope type        = 2
0.00.125.965 I print_info: rope scaling     = linear
0.00.125.965 I print_info: freq_base_train  = 10000.0
0.00.125.965 I print_info: freq_scale_train = 1
0.00.125.966 I print_info: n_ctx_orig_yarn  = 2048
0.00.125.966 I print_info: rope_finetuned   = unknown
0.00.125.968 I print_info: ssm_d_conv       = 0
0.00.125.968 I print_info: ssm_d_inner      = 0
0.00.125.968 I print_info: ssm_d_state      = 0
0.00.125.968 I print_info: ssm_dt_rank      = 0
0.00.125.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.125.969 I print_info: model type       = 1.4B
0.00.125.969 I print_info: model params     = 1.41 B
0.00.125.969 I print_info: general.name     = 1.4B
0.00.125.970 I print_info: vocab type       = BPE
0.00.125.970 I print_info: n_vocab          = 50304
0.00.125.970 I print_info: n_merges         = 50009
0.00.125.970 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.125.970 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.125.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.125.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.125.971 I print_info: LF token         = 128 'Ä'
0.00.125.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.125.971 I print_info: max token length = 1024
0.00.128.504 I load_tensors: offloading 24 repeating layers to GPU
0.00.128.504 I load_tensors: offloading output layer to GPU
0.00.128.504 I load_tensors: offloaded 25/25 layers to GPU
0.00.128.524 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.128.525 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.128.803 I llama_init_from_model: n_seq_max     = 1
0.00.128.804 I llama_init_from_model: n_ctx         = 2048
0.00.128.804 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.128.804 I llama_init_from_model: n_batch       = 2048
0.00.128.804 I llama_init_from_model: n_ubatch      = 512
0.00.128.804 I llama_init_from_model: flash_attn    = 0
0.00.128.805 I llama_init_from_model: freq_base     = 10000.0
0.00.128.805 I llama_init_from_model: freq_scale    = 1
0.00.128.805 I ggml_metal_init: allocating
0.00.128.809 I ggml_metal_init: found device: Apple M4
0.00.128.811 I ggml_metal_init: picking default device: Apple M4
0.00.129.341 I ggml_metal_init: using embedded metal library
0.00.160.336 I ggml_metal_init: GPU name:   Apple M4
0.00.160.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.160.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.160.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.160.341 I ggml_metal_init: simdgroup reduction   = true
0.00.160.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.160.341 I ggml_metal_init: has bfloat            = true
0.00.160.341 I ggml_metal_init: use bfloat            = true
0.00.160.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.160.343 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.279.275 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.311.302 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.311.309 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.311.333 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.312.272 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.312.273 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.312.273 I llama_init_from_model: graph nodes  = 967
0.00.312.273 I llama_init_from_model: graph splits = 2
0.00.312.276 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.312.409 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.312.410 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.391.454 I main: llama threadpool init, n_threads = 4
0.00.391.488 I 
0.00.391.517 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.391.519 I 
0.00.391.578 I sampler seed: 1234
0.00.391.583 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.391.606 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.391.608 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.391.608 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.243.683 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.02.243.683 I llama_perf_context_print:        load time =     318.59 ms
0.02.243.684 I llama_perf_context_print: prompt eval time =      54.22 ms /     7 tokens (    7.75 ms per token,   129.10 tokens per second)
0.02.243.686 I llama_perf_context_print:        eval time =    1794.95 ms /    63 runs   (   28.49 ms per token,    35.10 tokens per second)
0.02.243.686 I llama_perf_context_print:       total time =    1853.26 ms /    70 tokens
0.02.243.900 I ggml_metal_free: deallocating

real	0m2.593s
user	0m0.135s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.498 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.808 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.455 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.481 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.481 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.482 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.482 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.798 I llama_model_loader: - type  f32:  194 tensors
0.00.040.799 I llama_model_loader: - type  f16:   98 tensors
0.00.040.799 I print_info: file format = GGUF V3 (latest)
0.00.040.800 I print_info: file type   = all F32 (guessed)
0.00.040.802 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.060.019 I load: special tokens cache size = 25
0.00.066.250 I load: token to piece cache size = 0.2984 MB
0.00.066.255 I print_info: arch             = gptneox
0.00.066.255 I print_info: vocab_only       = 0
0.00.066.255 I print_info: n_ctx_train      = 2048
0.00.066.255 I print_info: n_embd           = 2048
0.00.066.255 I print_info: n_layer          = 24
0.00.066.259 I print_info: n_head           = 16
0.00.066.260 I print_info: n_head_kv        = 16
0.00.066.260 I print_info: n_rot            = 32
0.00.066.261 I print_info: n_swa            = 0
0.00.066.261 I print_info: n_embd_head_k    = 128
0.00.066.263 I print_info: n_embd_head_v    = 128
0.00.066.264 I print_info: n_gqa            = 1
0.00.066.265 I print_info: n_embd_k_gqa     = 2048
0.00.066.265 I print_info: n_embd_v_gqa     = 2048
0.00.066.266 I print_info: f_norm_eps       = 1.0e-05
0.00.066.266 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.266 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.266 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.266 I print_info: f_logit_scale    = 0.0e+00
0.00.066.267 I print_info: n_ff             = 8192
0.00.066.267 I print_info: n_expert         = 0
0.00.066.267 I print_info: n_expert_used    = 0
0.00.066.267 I print_info: causal attn      = 1
0.00.066.268 I print_info: pooling type     = 0
0.00.066.268 I print_info: rope type        = 2
0.00.066.268 I print_info: rope scaling     = linear
0.00.066.269 I print_info: freq_base_train  = 10000.0
0.00.066.269 I print_info: freq_scale_train = 1
0.00.066.269 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.270 I print_info: rope_finetuned   = unknown
0.00.066.270 I print_info: ssm_d_conv       = 0
0.00.066.270 I print_info: ssm_d_inner      = 0
0.00.066.270 I print_info: ssm_d_state      = 0
0.00.066.270 I print_info: ssm_dt_rank      = 0
0.00.066.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.271 I print_info: model type       = 1.4B
0.00.066.271 I print_info: model params     = 1.41 B
0.00.066.271 I print_info: general.name     = 1.4B
0.00.066.272 I print_info: vocab type       = BPE
0.00.066.272 I print_info: n_vocab          = 50304
0.00.066.272 I print_info: n_merges         = 50009
0.00.066.272 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.273 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.273 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.274 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.274 I print_info: LF token         = 128 'Ä'
0.00.066.274 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.274 I print_info: max token length = 1024
0.00.068.596 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.596 I load_tensors: offloading output layer to GPU
0.00.068.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.611 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.068.613 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.068.884 I llama_init_from_model: n_seq_max     = 1
0.00.068.885 I llama_init_from_model: n_ctx         = 128
0.00.068.885 I llama_init_from_model: n_ctx_per_seq = 128
0.00.068.885 I llama_init_from_model: n_batch       = 128
0.00.068.885 I llama_init_from_model: n_ubatch      = 128
0.00.068.886 I llama_init_from_model: flash_attn    = 0
0.00.068.886 I llama_init_from_model: freq_base     = 10000.0
0.00.068.886 I llama_init_from_model: freq_scale    = 1
0.00.068.887 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.887 I ggml_metal_init: allocating
0.00.068.891 I ggml_metal_init: found device: Apple M4
0.00.068.894 I ggml_metal_init: picking default device: Apple M4
0.00.069.388 I ggml_metal_init: using embedded metal library
0.00.071.780 I ggml_metal_init: GPU name:   Apple M4
0.00.071.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.782 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.783 I ggml_metal_init: simdgroup reduction   = true
0.00.071.783 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.783 I ggml_metal_init: has bfloat            = true
0.00.071.783 I ggml_metal_init: use bfloat            = true
0.00.071.784 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.784 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.554 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.556 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.571 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.435 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.084.436 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.084.437 I llama_init_from_model: graph nodes  = 967
0.00.084.437 I llama_init_from_model: graph splits = 2
0.00.084.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.242.790 I 
0.01.242.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.242.884 I perplexity: tokenizing the input ..
0.01.251.181 I perplexity: tokenization took 8.293 ms
0.01.251.203 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.369.493 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.371.050 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.371.070 I llama_perf_context_print:        load time =    1224.97 ms
0.01.371.072 I llama_perf_context_print: prompt eval time =     118.03 ms /   128 tokens (    0.92 ms per token,  1084.52 tokens per second)
0.01.371.072 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.371.073 I llama_perf_context_print:       total time =     128.28 ms /   129 tokens
0.01.371.406 I ggml_metal_free: deallocating

real	0m1.637s
user	0m0.093s
sys	0m0.206s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.027.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.006 I llama_model_loader: - type  f32:  194 tensors
0.00.037.007 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.007 I print_info: file format = GGUF V3 (latest)
0.00.037.008 I print_info: file type   = Q8_0
0.00.037.009 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.384 I load: special tokens cache size = 25
0.00.064.829 I load: token to piece cache size = 0.2984 MB
0.00.064.832 I print_info: arch             = gptneox
0.00.064.833 I print_info: vocab_only       = 0
0.00.064.833 I print_info: n_ctx_train      = 2048
0.00.064.833 I print_info: n_embd           = 2048
0.00.064.833 I print_info: n_layer          = 24
0.00.064.837 I print_info: n_head           = 16
0.00.064.838 I print_info: n_head_kv        = 16
0.00.064.838 I print_info: n_rot            = 32
0.00.064.838 I print_info: n_swa            = 0
0.00.064.838 I print_info: n_embd_head_k    = 128
0.00.064.838 I print_info: n_embd_head_v    = 128
0.00.064.839 I print_info: n_gqa            = 1
0.00.064.839 I print_info: n_embd_k_gqa     = 2048
0.00.064.840 I print_info: n_embd_v_gqa     = 2048
0.00.064.840 I print_info: f_norm_eps       = 1.0e-05
0.00.064.841 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.841 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.841 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.841 I print_info: f_logit_scale    = 0.0e+00
0.00.064.842 I print_info: n_ff             = 8192
0.00.064.842 I print_info: n_expert         = 0
0.00.064.842 I print_info: n_expert_used    = 0
0.00.064.842 I print_info: causal attn      = 1
0.00.064.842 I print_info: pooling type     = 0
0.00.064.842 I print_info: rope type        = 2
0.00.064.843 I print_info: rope scaling     = linear
0.00.064.843 I print_info: freq_base_train  = 10000.0
0.00.064.843 I print_info: freq_scale_train = 1
0.00.064.843 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.844 I print_info: rope_finetuned   = unknown
0.00.064.844 I print_info: ssm_d_conv       = 0
0.00.064.844 I print_info: ssm_d_inner      = 0
0.00.064.844 I print_info: ssm_d_state      = 0
0.00.064.844 I print_info: ssm_dt_rank      = 0
0.00.064.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.844 I print_info: model type       = 1.4B
0.00.064.845 I print_info: model params     = 1.41 B
0.00.064.845 I print_info: general.name     = 1.4B
0.00.064.846 I print_info: vocab type       = BPE
0.00.064.846 I print_info: n_vocab          = 50304
0.00.064.846 I print_info: n_merges         = 50009
0.00.064.846 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.846 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.849 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.849 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.849 I print_info: LF token         = 128 'Ä'
0.00.064.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.849 I print_info: max token length = 1024
0.00.067.248 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.248 I load_tensors: offloading output layer to GPU
0.00.067.248 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.260 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.261 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.583 I llama_init_from_model: n_seq_max     = 1
0.00.067.584 I llama_init_from_model: n_ctx         = 2048
0.00.067.584 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.584 I llama_init_from_model: n_batch       = 2048
0.00.067.585 I llama_init_from_model: n_ubatch      = 512
0.00.067.585 I llama_init_from_model: flash_attn    = 0
0.00.067.585 I llama_init_from_model: freq_base     = 10000.0
0.00.067.585 I llama_init_from_model: freq_scale    = 1
0.00.067.586 I ggml_metal_init: allocating
0.00.067.588 I ggml_metal_init: found device: Apple M4
0.00.067.591 I ggml_metal_init: picking default device: Apple M4
0.00.068.211 I ggml_metal_init: using embedded metal library
0.00.071.019 I ggml_metal_init: GPU name:   Apple M4
0.00.071.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.021 I ggml_metal_init: simdgroup reduction   = true
0.00.071.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.022 I ggml_metal_init: has bfloat            = true
0.00.071.022 I ggml_metal_init: use bfloat            = true
0.00.071.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.885 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.693 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.703 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.906 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.105.908 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.105.909 I llama_init_from_model: graph nodes  = 967
0.00.105.909 I llama_init_from_model: graph splits = 2
0.00.105.913 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.226.949 I main: llama threadpool init, n_threads = 4
0.01.226.986 I 
0.01.227.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.227.010 I 
0.01.227.237 I sampler seed: 1234
0.01.227.241 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.227.279 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.227.281 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.227.281 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.315.846 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63449.51 tokens per second)
0.02.315.846 I llama_perf_context_print:        load time =    1216.20 ms
0.02.315.847 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.62 tokens per second)
0.02.315.848 I llama_perf_context_print:        eval time =    1041.96 ms /    63 runs   (   16.54 ms per token,    60.46 tokens per second)
0.02.315.851 I llama_perf_context_print:       total time =    1089.77 ms /    70 tokens
0.02.316.057 I ggml_metal_free: deallocating

real	0m2.334s
user	0m0.114s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.239 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.155 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.778 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.785 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.785 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.788 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.789 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.833 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.881 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.641 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.643 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.643 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.647 I llama_model_loader: - type  f32:  194 tensors
0.00.026.647 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.648 I print_info: file format = GGUF V3 (latest)
0.00.026.648 I print_info: file type   = Q8_0
0.00.026.649 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.045.911 I load: special tokens cache size = 25
0.00.052.153 I load: token to piece cache size = 0.2984 MB
0.00.052.157 I print_info: arch             = gptneox
0.00.052.158 I print_info: vocab_only       = 0
0.00.052.158 I print_info: n_ctx_train      = 2048
0.00.052.158 I print_info: n_embd           = 2048
0.00.052.158 I print_info: n_layer          = 24
0.00.052.163 I print_info: n_head           = 16
0.00.052.164 I print_info: n_head_kv        = 16
0.00.052.164 I print_info: n_rot            = 32
0.00.052.164 I print_info: n_swa            = 0
0.00.052.164 I print_info: n_embd_head_k    = 128
0.00.052.164 I print_info: n_embd_head_v    = 128
0.00.052.165 I print_info: n_gqa            = 1
0.00.052.166 I print_info: n_embd_k_gqa     = 2048
0.00.052.167 I print_info: n_embd_v_gqa     = 2048
0.00.052.167 I print_info: f_norm_eps       = 1.0e-05
0.00.052.168 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.168 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.168 I print_info: f_logit_scale    = 0.0e+00
0.00.052.169 I print_info: n_ff             = 8192
0.00.052.169 I print_info: n_expert         = 0
0.00.052.169 I print_info: n_expert_used    = 0
0.00.052.169 I print_info: causal attn      = 1
0.00.052.169 I print_info: pooling type     = 0
0.00.052.169 I print_info: rope type        = 2
0.00.052.170 I print_info: rope scaling     = linear
0.00.052.170 I print_info: freq_base_train  = 10000.0
0.00.052.170 I print_info: freq_scale_train = 1
0.00.052.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.171 I print_info: rope_finetuned   = unknown
0.00.052.171 I print_info: ssm_d_conv       = 0
0.00.052.171 I print_info: ssm_d_inner      = 0
0.00.052.171 I print_info: ssm_d_state      = 0
0.00.052.171 I print_info: ssm_dt_rank      = 0
0.00.052.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.171 I print_info: model type       = 1.4B
0.00.052.172 I print_info: model params     = 1.41 B
0.00.052.172 I print_info: general.name     = 1.4B
0.00.052.172 I print_info: vocab type       = BPE
0.00.052.173 I print_info: n_vocab          = 50304
0.00.052.173 I print_info: n_merges         = 50009
0.00.052.173 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.173 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.173 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.173 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.174 I print_info: LF token         = 128 'Ä'
0.00.052.174 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.174 I print_info: max token length = 1024
0.00.054.242 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.242 I load_tensors: offloading output layer to GPU
0.00.054.243 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.253 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.254 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.054.528 I llama_init_from_model: n_seq_max     = 1
0.00.054.528 I llama_init_from_model: n_ctx         = 128
0.00.054.529 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.529 I llama_init_from_model: n_batch       = 128
0.00.054.529 I llama_init_from_model: n_ubatch      = 128
0.00.054.529 I llama_init_from_model: flash_attn    = 0
0.00.054.530 I llama_init_from_model: freq_base     = 10000.0
0.00.054.530 I llama_init_from_model: freq_scale    = 1
0.00.054.530 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.531 I ggml_metal_init: allocating
0.00.054.534 I ggml_metal_init: found device: Apple M4
0.00.054.536 I ggml_metal_init: picking default device: Apple M4
0.00.055.038 I ggml_metal_init: using embedded metal library
0.00.057.411 I ggml_metal_init: GPU name:   Apple M4
0.00.057.412 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.413 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.413 I ggml_metal_init: simdgroup reduction   = true
0.00.057.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.414 I ggml_metal_init: has bfloat            = true
0.00.057.414 I ggml_metal_init: use bfloat            = true
0.00.057.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.415 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.673 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.033 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.048 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.910 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.911 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.911 I llama_init_from_model: graph nodes  = 967
0.00.069.911 I llama_init_from_model: graph splits = 2
0.00.069.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.959.545 I 
0.00.959.582 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.959.592 I perplexity: tokenizing the input ..
0.00.968.733 I perplexity: tokenization took 9.138 ms
0.00.968.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.094.156 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.095.364 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.095.377 I llama_perf_context_print:        load time =     949.38 ms
0.01.095.378 I llama_perf_context_print: prompt eval time =     125.15 ms /   128 tokens (    0.98 ms per token,  1022.81 tokens per second)
0.01.095.378 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.095.379 I llama_perf_context_print:       total time =     135.84 ms /   129 tokens
0.01.095.691 I ggml_metal_free: deallocating

real	0m1.113s
user	0m0.080s
sys	0m0.154s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.012.751 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.143 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.149 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.150 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.150 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.152 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.153 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.153 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.977 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.977 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.978 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.978 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.979 I llama_model_loader: - type  f32:  194 tensors
0.00.030.979 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.979 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.980 I print_info: file format = GGUF V3 (latest)
0.00.030.981 I print_info: file type   = Q4_0
0.00.030.982 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.049.698 I load: special tokens cache size = 25
0.00.055.782 I load: token to piece cache size = 0.2984 MB
0.00.055.785 I print_info: arch             = gptneox
0.00.055.785 I print_info: vocab_only       = 0
0.00.055.786 I print_info: n_ctx_train      = 2048
0.00.055.786 I print_info: n_embd           = 2048
0.00.055.786 I print_info: n_layer          = 24
0.00.055.792 I print_info: n_head           = 16
0.00.055.792 I print_info: n_head_kv        = 16
0.00.055.793 I print_info: n_rot            = 32
0.00.055.793 I print_info: n_swa            = 0
0.00.055.793 I print_info: n_embd_head_k    = 128
0.00.055.793 I print_info: n_embd_head_v    = 128
0.00.055.794 I print_info: n_gqa            = 1
0.00.055.795 I print_info: n_embd_k_gqa     = 2048
0.00.055.796 I print_info: n_embd_v_gqa     = 2048
0.00.055.796 I print_info: f_norm_eps       = 1.0e-05
0.00.055.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.797 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.798 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.798 I print_info: f_logit_scale    = 0.0e+00
0.00.055.799 I print_info: n_ff             = 8192
0.00.055.799 I print_info: n_expert         = 0
0.00.055.799 I print_info: n_expert_used    = 0
0.00.055.799 I print_info: causal attn      = 1
0.00.055.799 I print_info: pooling type     = 0
0.00.055.799 I print_info: rope type        = 2
0.00.055.799 I print_info: rope scaling     = linear
0.00.055.800 I print_info: freq_base_train  = 10000.0
0.00.055.800 I print_info: freq_scale_train = 1
0.00.055.800 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.801 I print_info: rope_finetuned   = unknown
0.00.055.801 I print_info: ssm_d_conv       = 0
0.00.055.801 I print_info: ssm_d_inner      = 0
0.00.055.801 I print_info: ssm_d_state      = 0
0.00.055.805 I print_info: ssm_dt_rank      = 0
0.00.055.805 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.806 I print_info: model type       = 1.4B
0.00.055.806 I print_info: model params     = 1.41 B
0.00.055.806 I print_info: general.name     = 1.4B
0.00.055.807 I print_info: vocab type       = BPE
0.00.055.807 I print_info: n_vocab          = 50304
0.00.055.807 I print_info: n_merges         = 50009
0.00.055.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.809 I print_info: LF token         = 128 'Ä'
0.00.055.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.809 I print_info: max token length = 1024
0.00.057.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.982 I load_tensors: offloading output layer to GPU
0.00.057.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.994 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.995 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.058.342 I llama_init_from_model: n_seq_max     = 1
0.00.058.342 I llama_init_from_model: n_ctx         = 2048
0.00.058.342 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.343 I llama_init_from_model: n_batch       = 2048
0.00.058.343 I llama_init_from_model: n_ubatch      = 512
0.00.058.343 I llama_init_from_model: flash_attn    = 0
0.00.058.343 I llama_init_from_model: freq_base     = 10000.0
0.00.058.343 I llama_init_from_model: freq_scale    = 1
0.00.058.344 I ggml_metal_init: allocating
0.00.058.348 I ggml_metal_init: found device: Apple M4
0.00.058.350 I ggml_metal_init: picking default device: Apple M4
0.00.058.976 I ggml_metal_init: using embedded metal library
0.00.061.525 I ggml_metal_init: GPU name:   Apple M4
0.00.061.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.527 I ggml_metal_init: simdgroup reduction   = true
0.00.061.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.527 I ggml_metal_init: has bfloat            = true
0.00.061.528 I ggml_metal_init: use bfloat            = true
0.00.061.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.004 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.876 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.883 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.096.156 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.096.158 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.096.158 I llama_init_from_model: graph nodes  = 967
0.00.096.158 I llama_init_from_model: graph splits = 2
0.00.096.163 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.303 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.303 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.168 I main: llama threadpool init, n_threads = 4
0.00.707.198 I 
0.00.707.220 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.220 I 
0.00.707.437 I sampler seed: 1234
0.00.707.442 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.453 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.453 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.391.849 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.391.850 I llama_perf_context_print:        load time =     693.57 ms
0.01.391.851 I llama_perf_context_print: prompt eval time =      43.25 ms /     7 tokens (    6.18 ms per token,   161.86 tokens per second)
0.01.391.852 I llama_perf_context_print:        eval time =     638.14 ms /    63 runs   (   10.13 ms per token,    98.72 tokens per second)
0.01.391.852 I llama_perf_context_print:       total time =     685.53 ms /    70 tokens
0.01.392.081 I ggml_metal_free: deallocating

real	0m1.410s
user	0m0.109s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.240 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.110 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.615 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.616 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.618 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.619 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.401 I llama_model_loader: - type  f32:  194 tensors
0.00.026.401 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.402 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.402 I print_info: file format = GGUF V3 (latest)
0.00.026.402 I print_info: file type   = Q4_0
0.00.026.403 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.704 I load: special tokens cache size = 25
0.00.051.819 I load: token to piece cache size = 0.2984 MB
0.00.051.822 I print_info: arch             = gptneox
0.00.051.822 I print_info: vocab_only       = 0
0.00.051.822 I print_info: n_ctx_train      = 2048
0.00.051.822 I print_info: n_embd           = 2048
0.00.051.822 I print_info: n_layer          = 24
0.00.051.825 I print_info: n_head           = 16
0.00.051.826 I print_info: n_head_kv        = 16
0.00.051.826 I print_info: n_rot            = 32
0.00.051.826 I print_info: n_swa            = 0
0.00.051.826 I print_info: n_embd_head_k    = 128
0.00.051.827 I print_info: n_embd_head_v    = 128
0.00.051.827 I print_info: n_gqa            = 1
0.00.051.828 I print_info: n_embd_k_gqa     = 2048
0.00.051.829 I print_info: n_embd_v_gqa     = 2048
0.00.051.829 I print_info: f_norm_eps       = 1.0e-05
0.00.051.830 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.830 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.830 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.830 I print_info: f_logit_scale    = 0.0e+00
0.00.051.831 I print_info: n_ff             = 8192
0.00.051.831 I print_info: n_expert         = 0
0.00.051.831 I print_info: n_expert_used    = 0
0.00.051.831 I print_info: causal attn      = 1
0.00.051.831 I print_info: pooling type     = 0
0.00.051.831 I print_info: rope type        = 2
0.00.051.831 I print_info: rope scaling     = linear
0.00.051.832 I print_info: freq_base_train  = 10000.0
0.00.051.832 I print_info: freq_scale_train = 1
0.00.051.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.834 I print_info: rope_finetuned   = unknown
0.00.051.834 I print_info: ssm_d_conv       = 0
0.00.051.834 I print_info: ssm_d_inner      = 0
0.00.051.834 I print_info: ssm_d_state      = 0
0.00.051.835 I print_info: ssm_dt_rank      = 0
0.00.051.835 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.835 I print_info: model type       = 1.4B
0.00.051.835 I print_info: model params     = 1.41 B
0.00.051.836 I print_info: general.name     = 1.4B
0.00.051.836 I print_info: vocab type       = BPE
0.00.051.839 I print_info: n_vocab          = 50304
0.00.051.839 I print_info: n_merges         = 50009
0.00.051.839 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.840 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.840 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.840 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.841 I print_info: LF token         = 128 'Ä'
0.00.051.841 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.842 I print_info: max token length = 1024
0.00.053.790 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.790 I load_tensors: offloading output layer to GPU
0.00.053.790 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.801 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.802 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.072 I llama_init_from_model: n_seq_max     = 1
0.00.054.072 I llama_init_from_model: n_ctx         = 128
0.00.054.072 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.073 I llama_init_from_model: n_batch       = 128
0.00.054.073 I llama_init_from_model: n_ubatch      = 128
0.00.054.073 I llama_init_from_model: flash_attn    = 0
0.00.054.073 I llama_init_from_model: freq_base     = 10000.0
0.00.054.074 I llama_init_from_model: freq_scale    = 1
0.00.054.074 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.074 I ggml_metal_init: allocating
0.00.054.077 I ggml_metal_init: found device: Apple M4
0.00.054.080 I ggml_metal_init: picking default device: Apple M4
0.00.054.564 I ggml_metal_init: using embedded metal library
0.00.056.953 I ggml_metal_init: GPU name:   Apple M4
0.00.056.954 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.955 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.955 I ggml_metal_init: simdgroup reduction   = true
0.00.056.956 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.956 I ggml_metal_init: has bfloat            = true
0.00.056.956 I ggml_metal_init: use bfloat            = true
0.00.056.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.092 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.352 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.355 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.372 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.260 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.261 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.261 I llama_init_from_model: graph nodes  = 967
0.00.069.262 I llama_init_from_model: graph splits = 2
0.00.069.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.452 I 
0.00.616.501 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.511 I perplexity: tokenizing the input ..
0.00.624.573 I perplexity: tokenization took 8.06 ms
0.00.624.584 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.465 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.748.619 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.748.635 I llama_perf_context_print:        load time =     606.33 ms
0.00.748.637 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.60 tokens per second)
0.00.748.637 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.638 I llama_perf_context_print:       total time =     132.19 ms /   129 tokens
0.00.749.099 I ggml_metal_free: deallocating

real	0m0.764s
user	0m0.079s
sys	0m0.091s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.167 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.168 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.030.911 I llama_model_loader: - type  f32:  194 tensors
0.00.030.911 I llama_model_loader: - type q4_1:   97 tensors
0.00.030.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.912 I print_info: file format = GGUF V3 (latest)
0.00.030.913 I print_info: file type   = Q4_1
0.00.030.913 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.049.711 I load: special tokens cache size = 25
0.00.055.458 I load: token to piece cache size = 0.2984 MB
0.00.055.460 I print_info: arch             = gptneox
0.00.055.461 I print_info: vocab_only       = 0
0.00.055.461 I print_info: n_ctx_train      = 2048
0.00.055.461 I print_info: n_embd           = 2048
0.00.055.461 I print_info: n_layer          = 24
0.00.055.464 I print_info: n_head           = 16
0.00.055.465 I print_info: n_head_kv        = 16
0.00.055.465 I print_info: n_rot            = 32
0.00.055.465 I print_info: n_swa            = 0
0.00.055.468 I print_info: n_embd_head_k    = 128
0.00.055.468 I print_info: n_embd_head_v    = 128
0.00.055.468 I print_info: n_gqa            = 1
0.00.055.469 I print_info: n_embd_k_gqa     = 2048
0.00.055.470 I print_info: n_embd_v_gqa     = 2048
0.00.055.471 I print_info: f_norm_eps       = 1.0e-05
0.00.055.471 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.471 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.471 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.471 I print_info: f_logit_scale    = 0.0e+00
0.00.055.472 I print_info: n_ff             = 8192
0.00.055.472 I print_info: n_expert         = 0
0.00.055.472 I print_info: n_expert_used    = 0
0.00.055.472 I print_info: causal attn      = 1
0.00.055.473 I print_info: pooling type     = 0
0.00.055.474 I print_info: rope type        = 2
0.00.055.476 I print_info: rope scaling     = linear
0.00.055.476 I print_info: freq_base_train  = 10000.0
0.00.055.477 I print_info: freq_scale_train = 1
0.00.055.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.477 I print_info: rope_finetuned   = unknown
0.00.055.477 I print_info: ssm_d_conv       = 0
0.00.055.477 I print_info: ssm_d_inner      = 0
0.00.055.477 I print_info: ssm_d_state      = 0
0.00.055.477 I print_info: ssm_dt_rank      = 0
0.00.055.478 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.478 I print_info: model type       = 1.4B
0.00.055.479 I print_info: model params     = 1.41 B
0.00.055.483 I print_info: general.name     = 1.4B
0.00.055.483 I print_info: vocab type       = BPE
0.00.055.484 I print_info: n_vocab          = 50304
0.00.055.484 I print_info: n_merges         = 50009
0.00.055.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.485 I print_info: LF token         = 128 'Ä'
0.00.055.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.487 I print_info: max token length = 1024
0.00.057.406 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.406 I load_tensors: offloading output layer to GPU
0.00.057.406 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.417 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.057.418 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.057.693 I llama_init_from_model: n_seq_max     = 1
0.00.057.694 I llama_init_from_model: n_ctx         = 2048
0.00.057.694 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.694 I llama_init_from_model: n_batch       = 2048
0.00.057.694 I llama_init_from_model: n_ubatch      = 512
0.00.057.694 I llama_init_from_model: flash_attn    = 0
0.00.057.695 I llama_init_from_model: freq_base     = 10000.0
0.00.057.695 I llama_init_from_model: freq_scale    = 1
0.00.057.695 I ggml_metal_init: allocating
0.00.057.698 I ggml_metal_init: found device: Apple M4
0.00.057.700 I ggml_metal_init: picking default device: Apple M4
0.00.058.205 I ggml_metal_init: using embedded metal library
0.00.060.525 I ggml_metal_init: GPU name:   Apple M4
0.00.060.526 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.527 I ggml_metal_init: simdgroup reduction   = true
0.00.060.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.527 I ggml_metal_init: has bfloat            = true
0.00.060.528 I ggml_metal_init: use bfloat            = true
0.00.060.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.152 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.131 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.140 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.169 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.090.260 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.090.261 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.090.261 I llama_init_from_model: graph nodes  = 967
0.00.090.262 I llama_init_from_model: graph splits = 2
0.00.090.265 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.404 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.634 I main: llama threadpool init, n_threads = 4
0.00.729.669 I 
0.00.729.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.692 I 
0.00.729.930 I sampler seed: 1234
0.00.729.935 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.729.946 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.729.946 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.729.946 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.455.231 I llama_perf_sampler_print:    sampling time =       1.08 ms /    71 runs   (    0.02 ms per token, 65437.79 tokens per second)
0.01.455.232 I llama_perf_context_print:        load time =     719.85 ms
0.01.455.233 I llama_perf_context_print: prompt eval time =      42.60 ms /     7 tokens (    6.09 ms per token,   164.33 tokens per second)
0.01.455.234 I llama_perf_context_print:        eval time =     679.72 ms /    63 runs   (   10.79 ms per token,    92.69 tokens per second)
0.01.455.234 I llama_perf_context_print:       total time =     726.54 ms /    70 tokens
0.01.455.418 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.109s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.561 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.537 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.543 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.543 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.546 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.547 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.547 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.550 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.550 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.551 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.549 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.575 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.583 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.584 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.585 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.586 I llama_model_loader: - type  f32:  194 tensors
0.00.025.586 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.587 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.587 I print_info: file format = GGUF V3 (latest)
0.00.025.587 I print_info: file type   = Q4_1
0.00.025.588 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.843 I load: special tokens cache size = 25
0.00.050.845 I load: token to piece cache size = 0.2984 MB
0.00.050.847 I print_info: arch             = gptneox
0.00.050.848 I print_info: vocab_only       = 0
0.00.050.848 I print_info: n_ctx_train      = 2048
0.00.050.848 I print_info: n_embd           = 2048
0.00.050.848 I print_info: n_layer          = 24
0.00.050.851 I print_info: n_head           = 16
0.00.050.852 I print_info: n_head_kv        = 16
0.00.050.852 I print_info: n_rot            = 32
0.00.050.852 I print_info: n_swa            = 0
0.00.050.852 I print_info: n_embd_head_k    = 128
0.00.050.853 I print_info: n_embd_head_v    = 128
0.00.050.853 I print_info: n_gqa            = 1
0.00.050.854 I print_info: n_embd_k_gqa     = 2048
0.00.050.855 I print_info: n_embd_v_gqa     = 2048
0.00.050.855 I print_info: f_norm_eps       = 1.0e-05
0.00.050.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.857 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.858 I print_info: f_logit_scale    = 0.0e+00
0.00.050.858 I print_info: n_ff             = 8192
0.00.050.858 I print_info: n_expert         = 0
0.00.050.858 I print_info: n_expert_used    = 0
0.00.050.859 I print_info: causal attn      = 1
0.00.050.859 I print_info: pooling type     = 0
0.00.050.859 I print_info: rope type        = 2
0.00.050.859 I print_info: rope scaling     = linear
0.00.050.861 I print_info: freq_base_train  = 10000.0
0.00.050.863 I print_info: freq_scale_train = 1
0.00.050.863 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.863 I print_info: rope_finetuned   = unknown
0.00.050.863 I print_info: ssm_d_conv       = 0
0.00.050.863 I print_info: ssm_d_inner      = 0
0.00.050.864 I print_info: ssm_d_state      = 0
0.00.050.864 I print_info: ssm_dt_rank      = 0
0.00.050.864 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.864 I print_info: model type       = 1.4B
0.00.050.865 I print_info: model params     = 1.41 B
0.00.050.869 I print_info: general.name     = 1.4B
0.00.050.869 I print_info: vocab type       = BPE
0.00.050.869 I print_info: n_vocab          = 50304
0.00.050.870 I print_info: n_merges         = 50009
0.00.050.870 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.870 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.870 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.870 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.871 I print_info: LF token         = 128 'Ä'
0.00.050.871 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.871 I print_info: max token length = 1024
0.00.052.852 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.852 I load_tensors: offloading output layer to GPU
0.00.052.852 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.863 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.864 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.146 I llama_init_from_model: n_seq_max     = 1
0.00.053.146 I llama_init_from_model: n_ctx         = 128
0.00.053.147 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.147 I llama_init_from_model: n_batch       = 128
0.00.053.147 I llama_init_from_model: n_ubatch      = 128
0.00.053.147 I llama_init_from_model: flash_attn    = 0
0.00.053.147 I llama_init_from_model: freq_base     = 10000.0
0.00.053.147 I llama_init_from_model: freq_scale    = 1
0.00.053.148 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.148 I ggml_metal_init: allocating
0.00.053.151 I ggml_metal_init: found device: Apple M4
0.00.053.153 I ggml_metal_init: picking default device: Apple M4
0.00.053.643 I ggml_metal_init: using embedded metal library
0.00.056.001 I ggml_metal_init: GPU name:   Apple M4
0.00.056.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.003 I ggml_metal_init: simdgroup reduction   = true
0.00.056.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.004 I ggml_metal_init: has bfloat            = true
0.00.056.004 I ggml_metal_init: use bfloat            = true
0.00.056.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.005 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.822 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.101 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.104 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.120 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.004 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.005 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.006 I llama_init_from_model: graph nodes  = 967
0.00.068.006 I llama_init_from_model: graph splits = 2
0.00.068.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.207 I 
0.00.685.260 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.277 I perplexity: tokenizing the input ..
0.00.693.738 I perplexity: tokenization took 8.46 ms
0.00.693.749 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.566 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.817.812 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.817.825 I llama_perf_context_print:        load time =     675.64 ms
0.00.817.826 I llama_perf_context_print: prompt eval time =     122.58 ms /   128 tokens (    0.96 ms per token,  1044.21 tokens per second)
0.00.817.827 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.827 I llama_perf_context_print:       total time =     132.62 ms /   129 tokens
0.00.818.304 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.078s
sys	0m0.100s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.342 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.746 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.748 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.753 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.753 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.754 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.018 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.019 I llama_model_loader: - type  f32:  194 tensors
0.00.026.020 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.020 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.020 I print_info: file format = GGUF V3 (latest)
0.00.026.021 I print_info: file type   = Q5_0
0.00.026.025 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.293 I load: special tokens cache size = 25
0.00.051.373 I load: token to piece cache size = 0.2984 MB
0.00.051.376 I print_info: arch             = gptneox
0.00.051.377 I print_info: vocab_only       = 0
0.00.051.377 I print_info: n_ctx_train      = 2048
0.00.051.377 I print_info: n_embd           = 2048
0.00.051.377 I print_info: n_layer          = 24
0.00.051.380 I print_info: n_head           = 16
0.00.051.381 I print_info: n_head_kv        = 16
0.00.051.381 I print_info: n_rot            = 32
0.00.051.381 I print_info: n_swa            = 0
0.00.051.381 I print_info: n_embd_head_k    = 128
0.00.051.381 I print_info: n_embd_head_v    = 128
0.00.051.382 I print_info: n_gqa            = 1
0.00.051.383 I print_info: n_embd_k_gqa     = 2048
0.00.051.384 I print_info: n_embd_v_gqa     = 2048
0.00.051.384 I print_info: f_norm_eps       = 1.0e-05
0.00.051.385 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.385 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.385 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.389 I print_info: f_logit_scale    = 0.0e+00
0.00.051.390 I print_info: n_ff             = 8192
0.00.051.390 I print_info: n_expert         = 0
0.00.051.390 I print_info: n_expert_used    = 0
0.00.051.390 I print_info: causal attn      = 1
0.00.051.390 I print_info: pooling type     = 0
0.00.051.390 I print_info: rope type        = 2
0.00.051.391 I print_info: rope scaling     = linear
0.00.051.391 I print_info: freq_base_train  = 10000.0
0.00.051.391 I print_info: freq_scale_train = 1
0.00.051.392 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.392 I print_info: rope_finetuned   = unknown
0.00.051.392 I print_info: ssm_d_conv       = 0
0.00.051.392 I print_info: ssm_d_inner      = 0
0.00.051.392 I print_info: ssm_d_state      = 0
0.00.051.392 I print_info: ssm_dt_rank      = 0
0.00.051.393 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.393 I print_info: model type       = 1.4B
0.00.051.393 I print_info: model params     = 1.41 B
0.00.051.393 I print_info: general.name     = 1.4B
0.00.051.394 I print_info: vocab type       = BPE
0.00.051.394 I print_info: n_vocab          = 50304
0.00.051.394 I print_info: n_merges         = 50009
0.00.051.395 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.395 I print_info: LF token         = 128 'Ä'
0.00.051.396 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.396 I print_info: max token length = 1024
0.00.053.383 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.383 I load_tensors: offloading output layer to GPU
0.00.053.383 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.393 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.395 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.685 I llama_init_from_model: n_seq_max     = 1
0.00.053.686 I llama_init_from_model: n_ctx         = 2048
0.00.053.686 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.686 I llama_init_from_model: n_batch       = 2048
0.00.053.686 I llama_init_from_model: n_ubatch      = 512
0.00.053.686 I llama_init_from_model: flash_attn    = 0
0.00.053.687 I llama_init_from_model: freq_base     = 10000.0
0.00.053.687 I llama_init_from_model: freq_scale    = 1
0.00.053.687 I ggml_metal_init: allocating
0.00.053.691 I ggml_metal_init: found device: Apple M4
0.00.053.693 I ggml_metal_init: picking default device: Apple M4
0.00.054.203 I ggml_metal_init: using embedded metal library
0.00.056.556 I ggml_metal_init: GPU name:   Apple M4
0.00.056.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.559 I ggml_metal_init: simdgroup reduction   = true
0.00.056.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.559 I ggml_metal_init: has bfloat            = true
0.00.056.559 I ggml_metal_init: use bfloat            = true
0.00.056.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.560 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.396 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.166 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.172 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.190 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.351 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.352 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.353 I llama_init_from_model: graph nodes  = 967
0.00.086.353 I llama_init_from_model: graph splits = 2
0.00.086.356 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.045 I main: llama threadpool init, n_threads = 4
0.00.734.079 I 
0.00.734.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.100 I 
0.00.734.328 I sampler seed: 1234
0.00.734.332 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.343 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.343 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.343 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.524.424 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.524.425 I llama_perf_context_print:        load time =     723.85 ms
0.01.524.426 I llama_perf_context_print: prompt eval time =      43.15 ms /     7 tokens (    6.16 ms per token,   162.24 tokens per second)
0.01.524.427 I llama_perf_context_print:        eval time =     744.39 ms /    63 runs   (   11.82 ms per token,    84.63 tokens per second)
0.01.524.428 I llama_perf_context_print:       total time =     791.23 ms /    70 tokens
0.01.524.696 I ggml_metal_free: deallocating

real	0m1.543s
user	0m0.112s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.513 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.368 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.369 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.370 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.370 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.373 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.377 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.303 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.305 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.306 I llama_model_loader: - type  f32:  194 tensors
0.00.026.307 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.307 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.308 I print_info: file format = GGUF V3 (latest)
0.00.026.308 I print_info: file type   = Q5_0
0.00.026.309 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.862 I load: special tokens cache size = 25
0.00.050.823 I load: token to piece cache size = 0.2984 MB
0.00.050.826 I print_info: arch             = gptneox
0.00.050.826 I print_info: vocab_only       = 0
0.00.050.826 I print_info: n_ctx_train      = 2048
0.00.050.827 I print_info: n_embd           = 2048
0.00.050.827 I print_info: n_layer          = 24
0.00.050.830 I print_info: n_head           = 16
0.00.050.831 I print_info: n_head_kv        = 16
0.00.050.831 I print_info: n_rot            = 32
0.00.050.831 I print_info: n_swa            = 0
0.00.050.831 I print_info: n_embd_head_k    = 128
0.00.050.831 I print_info: n_embd_head_v    = 128
0.00.050.832 I print_info: n_gqa            = 1
0.00.050.833 I print_info: n_embd_k_gqa     = 2048
0.00.050.833 I print_info: n_embd_v_gqa     = 2048
0.00.050.834 I print_info: f_norm_eps       = 1.0e-05
0.00.050.834 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.839 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.839 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.839 I print_info: f_logit_scale    = 0.0e+00
0.00.050.840 I print_info: n_ff             = 8192
0.00.050.840 I print_info: n_expert         = 0
0.00.050.840 I print_info: n_expert_used    = 0
0.00.050.842 I print_info: causal attn      = 1
0.00.050.843 I print_info: pooling type     = 0
0.00.050.843 I print_info: rope type        = 2
0.00.050.843 I print_info: rope scaling     = linear
0.00.050.843 I print_info: freq_base_train  = 10000.0
0.00.050.844 I print_info: freq_scale_train = 1
0.00.050.845 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.847 I print_info: rope_finetuned   = unknown
0.00.050.847 I print_info: ssm_d_conv       = 0
0.00.050.847 I print_info: ssm_d_inner      = 0
0.00.050.847 I print_info: ssm_d_state      = 0
0.00.050.847 I print_info: ssm_dt_rank      = 0
0.00.050.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.847 I print_info: model type       = 1.4B
0.00.050.848 I print_info: model params     = 1.41 B
0.00.050.848 I print_info: general.name     = 1.4B
0.00.050.848 I print_info: vocab type       = BPE
0.00.050.849 I print_info: n_vocab          = 50304
0.00.050.849 I print_info: n_merges         = 50009
0.00.050.849 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.849 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.850 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.850 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.850 I print_info: LF token         = 128 'Ä'
0.00.050.850 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.850 I print_info: max token length = 1024
0.00.052.578 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.578 I load_tensors: offloading output layer to GPU
0.00.052.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.583 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.584 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.849 I llama_init_from_model: n_seq_max     = 1
0.00.052.850 I llama_init_from_model: n_ctx         = 128
0.00.052.850 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.850 I llama_init_from_model: n_batch       = 128
0.00.052.850 I llama_init_from_model: n_ubatch      = 128
0.00.052.850 I llama_init_from_model: flash_attn    = 0
0.00.052.851 I llama_init_from_model: freq_base     = 10000.0
0.00.052.851 I llama_init_from_model: freq_scale    = 1
0.00.052.851 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.852 I ggml_metal_init: allocating
0.00.052.855 I ggml_metal_init: found device: Apple M4
0.00.052.857 I ggml_metal_init: picking default device: Apple M4
0.00.053.317 I ggml_metal_init: using embedded metal library
0.00.055.637 I ggml_metal_init: GPU name:   Apple M4
0.00.055.638 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.639 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.639 I ggml_metal_init: simdgroup reduction   = true
0.00.055.639 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.639 I ggml_metal_init: has bfloat            = true
0.00.055.640 I ggml_metal_init: use bfloat            = true
0.00.055.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.641 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.345 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.677 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.680 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.695 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.568 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.568 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.569 I llama_init_from_model: graph nodes  = 967
0.00.067.569 I llama_init_from_model: graph splits = 2
0.00.067.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.999 I 
0.00.757.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.047 I perplexity: tokenizing the input ..
0.00.764.645 I perplexity: tokenization took 7.597 ms
0.00.764.656 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.621 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.900.792 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.900.813 I llama_perf_context_print:        load time =     746.48 ms
0.00.900.814 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   950.00 tokens per second)
0.00.900.814 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.815 I llama_perf_context_print:       total time =     143.82 ms /   129 tokens
0.00.901.325 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.077s
sys	0m0.103s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.825 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.830 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.839 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.774 I llama_model_loader: - type  f32:  194 tensors
0.00.035.774 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.775 I print_info: file format = GGUF V3 (latest)
0.00.035.775 I print_info: file type   = Q5_1
0.00.035.776 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.057.852 I load: special tokens cache size = 25
0.00.065.141 I load: token to piece cache size = 0.2984 MB
0.00.065.144 I print_info: arch             = gptneox
0.00.065.144 I print_info: vocab_only       = 0
0.00.065.145 I print_info: n_ctx_train      = 2048
0.00.065.145 I print_info: n_embd           = 2048
0.00.065.145 I print_info: n_layer          = 24
0.00.065.148 I print_info: n_head           = 16
0.00.065.149 I print_info: n_head_kv        = 16
0.00.065.149 I print_info: n_rot            = 32
0.00.065.149 I print_info: n_swa            = 0
0.00.065.149 I print_info: n_embd_head_k    = 128
0.00.065.149 I print_info: n_embd_head_v    = 128
0.00.065.150 I print_info: n_gqa            = 1
0.00.065.151 I print_info: n_embd_k_gqa     = 2048
0.00.065.151 I print_info: n_embd_v_gqa     = 2048
0.00.065.152 I print_info: f_norm_eps       = 1.0e-05
0.00.065.152 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.152 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.153 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.153 I print_info: f_logit_scale    = 0.0e+00
0.00.065.153 I print_info: n_ff             = 8192
0.00.065.153 I print_info: n_expert         = 0
0.00.065.153 I print_info: n_expert_used    = 0
0.00.065.154 I print_info: causal attn      = 1
0.00.065.154 I print_info: pooling type     = 0
0.00.065.155 I print_info: rope type        = 2
0.00.065.157 I print_info: rope scaling     = linear
0.00.065.157 I print_info: freq_base_train  = 10000.0
0.00.065.158 I print_info: freq_scale_train = 1
0.00.065.158 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.158 I print_info: rope_finetuned   = unknown
0.00.065.158 I print_info: ssm_d_conv       = 0
0.00.065.159 I print_info: ssm_d_inner      = 0
0.00.065.159 I print_info: ssm_d_state      = 0
0.00.065.159 I print_info: ssm_dt_rank      = 0
0.00.065.159 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.159 I print_info: model type       = 1.4B
0.00.065.159 I print_info: model params     = 1.41 B
0.00.065.161 I print_info: general.name     = 1.4B
0.00.065.161 I print_info: vocab type       = BPE
0.00.065.161 I print_info: n_vocab          = 50304
0.00.065.162 I print_info: n_merges         = 50009
0.00.065.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.166 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.169 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.169 I print_info: LF token         = 128 'Ä'
0.00.065.169 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.170 I print_info: max token length = 1024
0.00.067.338 I load_tensors: offloading 24 repeating layers to GPU
0.00.067.338 I load_tensors: offloading output layer to GPU
0.00.067.339 I load_tensors: offloaded 25/25 layers to GPU
0.00.067.349 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.067.351 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.067.657 I llama_init_from_model: n_seq_max     = 1
0.00.067.658 I llama_init_from_model: n_ctx         = 2048
0.00.067.658 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.067.658 I llama_init_from_model: n_batch       = 2048
0.00.067.658 I llama_init_from_model: n_ubatch      = 512
0.00.067.658 I llama_init_from_model: flash_attn    = 0
0.00.067.659 I llama_init_from_model: freq_base     = 10000.0
0.00.067.659 I llama_init_from_model: freq_scale    = 1
0.00.067.660 I ggml_metal_init: allocating
0.00.067.663 I ggml_metal_init: found device: Apple M4
0.00.067.665 I ggml_metal_init: picking default device: Apple M4
0.00.068.215 I ggml_metal_init: using embedded metal library
0.00.071.027 I ggml_metal_init: GPU name:   Apple M4
0.00.071.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.029 I ggml_metal_init: simdgroup reduction   = true
0.00.071.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.030 I ggml_metal_init: has bfloat            = true
0.00.071.030 I ggml_metal_init: use bfloat            = true
0.00.071.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.474 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.481 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.501 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.591 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.103.593 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.103.593 I llama_init_from_model: graph nodes  = 967
0.00.103.593 I llama_init_from_model: graph splits = 2
0.00.103.598 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.720 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.444 I main: llama threadpool init, n_threads = 4
0.00.778.481 I 
0.00.778.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.506 I 
0.00.778.727 I sampler seed: 1234
0.00.778.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.765 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.784 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.785 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.612.336 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.612.336 I llama_perf_context_print:        load time =     768.71 ms
0.01.612.337 I llama_perf_context_print: prompt eval time =      42.29 ms /     7 tokens (    6.04 ms per token,   165.54 tokens per second)
0.01.612.339 I llama_perf_context_print:        eval time =     788.18 ms /    63 runs   (   12.51 ms per token,    79.93 tokens per second)
0.01.612.339 I llama_perf_context_print:       total time =     834.75 ms /    70 tokens
0.01.612.512 I ggml_metal_free: deallocating

real	0m1.628s
user	0m0.114s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.814 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.821 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.727 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.533 I llama_model_loader: - type  f32:  194 tensors
0.00.024.533 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.534 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.534 I print_info: file format = GGUF V3 (latest)
0.00.024.535 I print_info: file type   = Q5_1
0.00.024.535 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.011 I load: special tokens cache size = 25
0.00.049.098 I load: token to piece cache size = 0.2984 MB
0.00.049.100 I print_info: arch             = gptneox
0.00.049.101 I print_info: vocab_only       = 0
0.00.049.101 I print_info: n_ctx_train      = 2048
0.00.049.101 I print_info: n_embd           = 2048
0.00.049.101 I print_info: n_layer          = 24
0.00.049.104 I print_info: n_head           = 16
0.00.049.105 I print_info: n_head_kv        = 16
0.00.049.105 I print_info: n_rot            = 32
0.00.049.105 I print_info: n_swa            = 0
0.00.049.106 I print_info: n_embd_head_k    = 128
0.00.049.106 I print_info: n_embd_head_v    = 128
0.00.049.106 I print_info: n_gqa            = 1
0.00.049.107 I print_info: n_embd_k_gqa     = 2048
0.00.049.108 I print_info: n_embd_v_gqa     = 2048
0.00.049.108 I print_info: f_norm_eps       = 1.0e-05
0.00.049.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.109 I print_info: f_logit_scale    = 0.0e+00
0.00.049.110 I print_info: n_ff             = 8192
0.00.049.110 I print_info: n_expert         = 0
0.00.049.110 I print_info: n_expert_used    = 0
0.00.049.110 I print_info: causal attn      = 1
0.00.049.111 I print_info: pooling type     = 0
0.00.049.111 I print_info: rope type        = 2
0.00.049.111 I print_info: rope scaling     = linear
0.00.049.113 I print_info: freq_base_train  = 10000.0
0.00.049.115 I print_info: freq_scale_train = 1
0.00.049.115 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.115 I print_info: rope_finetuned   = unknown
0.00.049.116 I print_info: ssm_d_conv       = 0
0.00.049.116 I print_info: ssm_d_inner      = 0
0.00.049.116 I print_info: ssm_d_state      = 0
0.00.049.116 I print_info: ssm_dt_rank      = 0
0.00.049.116 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.116 I print_info: model type       = 1.4B
0.00.049.117 I print_info: model params     = 1.41 B
0.00.049.117 I print_info: general.name     = 1.4B
0.00.049.117 I print_info: vocab type       = BPE
0.00.049.119 I print_info: n_vocab          = 50304
0.00.049.119 I print_info: n_merges         = 50009
0.00.049.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.120 I print_info: LF token         = 128 'Ä'
0.00.049.122 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.122 I print_info: max token length = 1024
0.00.051.068 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.068 I load_tensors: offloading output layer to GPU
0.00.051.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.079 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.080 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.388 I llama_init_from_model: n_seq_max     = 1
0.00.051.388 I llama_init_from_model: n_ctx         = 128
0.00.051.388 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.388 I llama_init_from_model: n_batch       = 128
0.00.051.389 I llama_init_from_model: n_ubatch      = 128
0.00.051.389 I llama_init_from_model: flash_attn    = 0
0.00.051.389 I llama_init_from_model: freq_base     = 10000.0
0.00.051.389 I llama_init_from_model: freq_scale    = 1
0.00.051.390 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.390 I ggml_metal_init: allocating
0.00.051.393 I ggml_metal_init: found device: Apple M4
0.00.051.395 I ggml_metal_init: picking default device: Apple M4
0.00.051.866 I ggml_metal_init: using embedded metal library
0.00.054.187 I ggml_metal_init: GPU name:   Apple M4
0.00.054.188 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.189 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.189 I ggml_metal_init: simdgroup reduction   = true
0.00.054.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.190 I ggml_metal_init: has bfloat            = true
0.00.054.190 I ggml_metal_init: use bfloat            = true
0.00.054.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.698 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.918 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.932 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.866 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.867 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.867 I llama_init_from_model: graph nodes  = 967
0.00.065.867 I llama_init_from_model: graph splits = 2
0.00.065.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.372 I 
0.00.708.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.410 I perplexity: tokenizing the input ..
0.00.716.331 I perplexity: tokenization took 7.92 ms
0.00.716.342 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.503 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.852.655 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.852.680 I llama_perf_context_print:        load time =     699.61 ms
0.00.852.681 I llama_perf_context_print: prompt eval time =     134.91 ms /   128 tokens (    1.05 ms per token,   948.79 tokens per second)
0.00.852.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.684 I llama_perf_context_print:       total time =     144.31 ms /   129 tokens
0.00.853.058 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.077s
sys	0m0.124s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.642 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.560 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.561 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.567 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.522 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.707 I llama_model_loader: - type  f32:  194 tensors
0.00.027.707 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.707 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.708 I print_info: file format = GGUF V3 (latest)
0.00.027.708 I print_info: file type   = Q2_K - Medium
0.00.027.709 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.183 I load: special tokens cache size = 25
0.00.052.098 I load: token to piece cache size = 0.2984 MB
0.00.052.101 I print_info: arch             = gptneox
0.00.052.101 I print_info: vocab_only       = 0
0.00.052.101 I print_info: n_ctx_train      = 2048
0.00.052.101 I print_info: n_embd           = 2048
0.00.052.102 I print_info: n_layer          = 24
0.00.052.105 I print_info: n_head           = 16
0.00.052.105 I print_info: n_head_kv        = 16
0.00.052.106 I print_info: n_rot            = 32
0.00.052.106 I print_info: n_swa            = 0
0.00.052.106 I print_info: n_embd_head_k    = 128
0.00.052.106 I print_info: n_embd_head_v    = 128
0.00.052.107 I print_info: n_gqa            = 1
0.00.052.108 I print_info: n_embd_k_gqa     = 2048
0.00.052.108 I print_info: n_embd_v_gqa     = 2048
0.00.052.110 I print_info: f_norm_eps       = 1.0e-05
0.00.052.111 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.111 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.111 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.113 I print_info: f_logit_scale    = 0.0e+00
0.00.052.113 I print_info: n_ff             = 8192
0.00.052.113 I print_info: n_expert         = 0
0.00.052.114 I print_info: n_expert_used    = 0
0.00.052.114 I print_info: causal attn      = 1
0.00.052.114 I print_info: pooling type     = 0
0.00.052.115 I print_info: rope type        = 2
0.00.052.115 I print_info: rope scaling     = linear
0.00.052.116 I print_info: freq_base_train  = 10000.0
0.00.052.116 I print_info: freq_scale_train = 1
0.00.052.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.116 I print_info: rope_finetuned   = unknown
0.00.052.117 I print_info: ssm_d_conv       = 0
0.00.052.117 I print_info: ssm_d_inner      = 0
0.00.052.117 I print_info: ssm_d_state      = 0
0.00.052.117 I print_info: ssm_dt_rank      = 0
0.00.052.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.117 I print_info: model type       = 1.4B
0.00.052.118 I print_info: model params     = 1.41 B
0.00.052.118 I print_info: general.name     = 1.4B
0.00.052.118 I print_info: vocab type       = BPE
0.00.052.119 I print_info: n_vocab          = 50304
0.00.052.119 I print_info: n_merges         = 50009
0.00.052.119 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.119 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.120 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.120 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.120 I print_info: LF token         = 128 'Ä'
0.00.052.120 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.120 I print_info: max token length = 1024
0.00.053.931 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.931 I load_tensors: offloading output layer to GPU
0.00.053.931 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.942 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.943 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.206 I llama_init_from_model: n_seq_max     = 1
0.00.054.206 I llama_init_from_model: n_ctx         = 2048
0.00.054.207 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.207 I llama_init_from_model: n_batch       = 2048
0.00.054.207 I llama_init_from_model: n_ubatch      = 512
0.00.054.207 I llama_init_from_model: flash_attn    = 0
0.00.054.207 I llama_init_from_model: freq_base     = 10000.0
0.00.054.208 I llama_init_from_model: freq_scale    = 1
0.00.054.208 I ggml_metal_init: allocating
0.00.054.211 I ggml_metal_init: found device: Apple M4
0.00.054.212 I ggml_metal_init: picking default device: Apple M4
0.00.054.709 I ggml_metal_init: using embedded metal library
0.00.057.071 I ggml_metal_init: GPU name:   Apple M4
0.00.057.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.074 I ggml_metal_init: simdgroup reduction   = true
0.00.057.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.074 I ggml_metal_init: has bfloat            = true
0.00.057.074 I ggml_metal_init: use bfloat            = true
0.00.057.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.549 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.555 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.572 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.735 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.737 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.737 I llama_init_from_model: graph nodes  = 967
0.00.086.737 I llama_init_from_model: graph splits = 2
0.00.086.740 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.032 I main: llama threadpool init, n_threads = 4
0.00.506.069 I 
0.00.506.092 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.093 I 
0.00.506.319 I sampler seed: 1234
0.00.506.323 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.349 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.349 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.179.833 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.179.834 I llama_perf_context_print:        load time =     494.53 ms
0.01.179.835 I llama_perf_context_print: prompt eval time =      36.41 ms /     7 tokens (    5.20 ms per token,   192.23 tokens per second)
0.01.179.837 I llama_perf_context_print:        eval time =     634.12 ms /    63 runs   (   10.07 ms per token,    99.35 tokens per second)
0.01.179.837 I llama_perf_context_print:       total time =     674.66 ms /    70 tokens
0.01.180.071 I ggml_metal_free: deallocating

real	0m1.197s
user	0m0.108s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.892 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.738 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.745 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.746 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.748 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.748 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.748 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.749 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.752 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.753 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.753 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.695 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.816 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.731 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.732 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.733 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.733 I llama_model_loader: - type  f32:  194 tensors
0.00.025.734 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.734 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.734 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.735 I print_info: file format = GGUF V3 (latest)
0.00.025.735 I print_info: file type   = Q2_K - Medium
0.00.025.736 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.104 I load: special tokens cache size = 25
0.00.050.031 I load: token to piece cache size = 0.2984 MB
0.00.050.033 I print_info: arch             = gptneox
0.00.050.034 I print_info: vocab_only       = 0
0.00.050.034 I print_info: n_ctx_train      = 2048
0.00.050.034 I print_info: n_embd           = 2048
0.00.050.034 I print_info: n_layer          = 24
0.00.050.038 I print_info: n_head           = 16
0.00.050.038 I print_info: n_head_kv        = 16
0.00.050.039 I print_info: n_rot            = 32
0.00.050.039 I print_info: n_swa            = 0
0.00.050.039 I print_info: n_embd_head_k    = 128
0.00.050.039 I print_info: n_embd_head_v    = 128
0.00.050.040 I print_info: n_gqa            = 1
0.00.050.041 I print_info: n_embd_k_gqa     = 2048
0.00.050.041 I print_info: n_embd_v_gqa     = 2048
0.00.050.042 I print_info: f_norm_eps       = 1.0e-05
0.00.050.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.043 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.043 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.043 I print_info: f_logit_scale    = 0.0e+00
0.00.050.044 I print_info: n_ff             = 8192
0.00.050.044 I print_info: n_expert         = 0
0.00.050.044 I print_info: n_expert_used    = 0
0.00.050.044 I print_info: causal attn      = 1
0.00.050.044 I print_info: pooling type     = 0
0.00.050.045 I print_info: rope type        = 2
0.00.050.045 I print_info: rope scaling     = linear
0.00.050.045 I print_info: freq_base_train  = 10000.0
0.00.050.045 I print_info: freq_scale_train = 1
0.00.050.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.046 I print_info: rope_finetuned   = unknown
0.00.050.046 I print_info: ssm_d_conv       = 0
0.00.050.046 I print_info: ssm_d_inner      = 0
0.00.050.046 I print_info: ssm_d_state      = 0
0.00.050.047 I print_info: ssm_dt_rank      = 0
0.00.050.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.047 I print_info: model type       = 1.4B
0.00.050.047 I print_info: model params     = 1.41 B
0.00.050.047 I print_info: general.name     = 1.4B
0.00.050.048 I print_info: vocab type       = BPE
0.00.050.048 I print_info: n_vocab          = 50304
0.00.050.048 I print_info: n_merges         = 50009
0.00.050.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.049 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.050 I print_info: LF token         = 128 'Ä'
0.00.050.050 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.050 I print_info: max token length = 1024
0.00.051.916 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.916 I load_tensors: offloading output layer to GPU
0.00.051.916 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.927 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.928 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.229 I llama_init_from_model: n_seq_max     = 1
0.00.052.230 I llama_init_from_model: n_ctx         = 128
0.00.052.230 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.230 I llama_init_from_model: n_batch       = 128
0.00.052.230 I llama_init_from_model: n_ubatch      = 128
0.00.052.230 I llama_init_from_model: flash_attn    = 0
0.00.052.231 I llama_init_from_model: freq_base     = 10000.0
0.00.052.231 I llama_init_from_model: freq_scale    = 1
0.00.052.231 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.232 I ggml_metal_init: allocating
0.00.052.235 I ggml_metal_init: found device: Apple M4
0.00.052.237 I ggml_metal_init: picking default device: Apple M4
0.00.052.729 I ggml_metal_init: using embedded metal library
0.00.055.214 I ggml_metal_init: GPU name:   Apple M4
0.00.055.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.216 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.216 I ggml_metal_init: simdgroup reduction   = true
0.00.055.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.217 I ggml_metal_init: has bfloat            = true
0.00.055.217 I ggml_metal_init: use bfloat            = true
0.00.055.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.218 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.670 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.939 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.954 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.815 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.816 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.816 I llama_init_from_model: graph nodes  = 967
0.00.066.816 I llama_init_from_model: graph splits = 2
0.00.066.817 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.063 I 
0.00.437.109 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.118 I perplexity: tokenizing the input ..
0.00.445.205 I perplexity: tokenization took 8.085 ms
0.00.445.214 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.816 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.002 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.019 I llama_perf_context_print:        load time =     427.16 ms
0.00.579.020 I llama_perf_context_print: prompt eval time =     132.38 ms /   128 tokens (    1.03 ms per token,   966.94 tokens per second)
0.00.579.020 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.021 I llama_perf_context_print:       total time =     141.96 ms /   129 tokens
0.00.579.521 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.077s
sys	0m0.077s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.705 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.505 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.451 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.452 I llama_model_loader: - type  f32:  194 tensors
0.00.025.452 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.452 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.453 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.453 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.453 I print_info: file format = GGUF V3 (latest)
0.00.025.454 I print_info: file type   = Q3_K - Medium
0.00.025.455 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.836 I load: special tokens cache size = 25
0.00.050.970 I load: token to piece cache size = 0.2984 MB
0.00.050.972 I print_info: arch             = gptneox
0.00.050.973 I print_info: vocab_only       = 0
0.00.050.973 I print_info: n_ctx_train      = 2048
0.00.050.973 I print_info: n_embd           = 2048
0.00.050.973 I print_info: n_layer          = 24
0.00.050.976 I print_info: n_head           = 16
0.00.050.977 I print_info: n_head_kv        = 16
0.00.050.977 I print_info: n_rot            = 32
0.00.050.977 I print_info: n_swa            = 0
0.00.050.978 I print_info: n_embd_head_k    = 128
0.00.050.978 I print_info: n_embd_head_v    = 128
0.00.050.978 I print_info: n_gqa            = 1
0.00.050.979 I print_info: n_embd_k_gqa     = 2048
0.00.050.980 I print_info: n_embd_v_gqa     = 2048
0.00.050.981 I print_info: f_norm_eps       = 1.0e-05
0.00.050.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.983 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.983 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.983 I print_info: f_logit_scale    = 0.0e+00
0.00.050.984 I print_info: n_ff             = 8192
0.00.050.984 I print_info: n_expert         = 0
0.00.050.984 I print_info: n_expert_used    = 0
0.00.050.984 I print_info: causal attn      = 1
0.00.050.984 I print_info: pooling type     = 0
0.00.050.984 I print_info: rope type        = 2
0.00.050.985 I print_info: rope scaling     = linear
0.00.050.985 I print_info: freq_base_train  = 10000.0
0.00.050.985 I print_info: freq_scale_train = 1
0.00.050.986 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.986 I print_info: rope_finetuned   = unknown
0.00.050.986 I print_info: ssm_d_conv       = 0
0.00.050.986 I print_info: ssm_d_inner      = 0
0.00.050.986 I print_info: ssm_d_state      = 0
0.00.050.986 I print_info: ssm_dt_rank      = 0
0.00.050.987 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.987 I print_info: model type       = 1.4B
0.00.050.987 I print_info: model params     = 1.41 B
0.00.050.987 I print_info: general.name     = 1.4B
0.00.050.988 I print_info: vocab type       = BPE
0.00.050.988 I print_info: n_vocab          = 50304
0.00.050.988 I print_info: n_merges         = 50009
0.00.050.989 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.989 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.989 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.990 I print_info: LF token         = 128 'Ä'
0.00.050.990 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.990 I print_info: max token length = 1024
0.00.052.921 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.921 I load_tensors: offloading output layer to GPU
0.00.052.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.932 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.933 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.208 I llama_init_from_model: n_seq_max     = 1
0.00.053.209 I llama_init_from_model: n_ctx         = 2048
0.00.053.209 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.209 I llama_init_from_model: n_batch       = 2048
0.00.053.210 I llama_init_from_model: n_ubatch      = 512
0.00.053.210 I llama_init_from_model: flash_attn    = 0
0.00.053.210 I llama_init_from_model: freq_base     = 10000.0
0.00.053.210 I llama_init_from_model: freq_scale    = 1
0.00.053.211 I ggml_metal_init: allocating
0.00.053.214 I ggml_metal_init: found device: Apple M4
0.00.053.216 I ggml_metal_init: picking default device: Apple M4
0.00.053.716 I ggml_metal_init: using embedded metal library
0.00.056.081 I ggml_metal_init: GPU name:   Apple M4
0.00.056.083 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.083 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.083 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.084 I ggml_metal_init: simdgroup reduction   = true
0.00.056.084 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.084 I ggml_metal_init: has bfloat            = true
0.00.056.084 I ggml_metal_init: use bfloat            = true
0.00.056.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.969 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.208 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.217 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.241 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.199 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.200 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.200 I llama_init_from_model: graph nodes  = 967
0.00.086.201 I llama_init_from_model: graph splits = 2
0.00.086.203 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.623 I main: llama threadpool init, n_threads = 4
0.00.536.657 I 
0.00.536.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.679 I 
0.00.536.897 I sampler seed: 1234
0.00.536.901 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.536.934 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.536.934 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.536.934 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.276.786 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.276.787 I llama_perf_context_print:        load time =     527.05 ms
0.01.276.787 I llama_perf_context_print: prompt eval time =      44.08 ms /     7 tokens (    6.30 ms per token,   158.80 tokens per second)
0.01.276.788 I llama_perf_context_print:        eval time =     692.81 ms /    63 runs   (   11.00 ms per token,    90.93 tokens per second)
0.01.276.788 I llama_perf_context_print:       total time =     741.03 ms /    70 tokens
0.01.277.075 I ggml_metal_free: deallocating

real	0m1.294s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.024 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.024 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.025 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.889 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.890 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.891 I llama_model_loader: - type  f32:  194 tensors
0.00.024.891 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.892 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.892 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.892 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.893 I print_info: file format = GGUF V3 (latest)
0.00.024.893 I print_info: file type   = Q3_K - Medium
0.00.024.894 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.399 I load: special tokens cache size = 25
0.00.049.533 I load: token to piece cache size = 0.2984 MB
0.00.049.536 I print_info: arch             = gptneox
0.00.049.536 I print_info: vocab_only       = 0
0.00.049.536 I print_info: n_ctx_train      = 2048
0.00.049.537 I print_info: n_embd           = 2048
0.00.049.537 I print_info: n_layer          = 24
0.00.049.539 I print_info: n_head           = 16
0.00.049.540 I print_info: n_head_kv        = 16
0.00.049.540 I print_info: n_rot            = 32
0.00.049.541 I print_info: n_swa            = 0
0.00.049.541 I print_info: n_embd_head_k    = 128
0.00.049.541 I print_info: n_embd_head_v    = 128
0.00.049.542 I print_info: n_gqa            = 1
0.00.049.543 I print_info: n_embd_k_gqa     = 2048
0.00.049.543 I print_info: n_embd_v_gqa     = 2048
0.00.049.544 I print_info: f_norm_eps       = 1.0e-05
0.00.049.544 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.544 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.549 I print_info: f_logit_scale    = 0.0e+00
0.00.049.550 I print_info: n_ff             = 8192
0.00.049.550 I print_info: n_expert         = 0
0.00.049.550 I print_info: n_expert_used    = 0
0.00.049.551 I print_info: causal attn      = 1
0.00.049.551 I print_info: pooling type     = 0
0.00.049.553 I print_info: rope type        = 2
0.00.049.553 I print_info: rope scaling     = linear
0.00.049.553 I print_info: freq_base_train  = 10000.0
0.00.049.554 I print_info: freq_scale_train = 1
0.00.049.554 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.554 I print_info: rope_finetuned   = unknown
0.00.049.554 I print_info: ssm_d_conv       = 0
0.00.049.554 I print_info: ssm_d_inner      = 0
0.00.049.554 I print_info: ssm_d_state      = 0
0.00.049.554 I print_info: ssm_dt_rank      = 0
0.00.049.555 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.555 I print_info: model type       = 1.4B
0.00.049.555 I print_info: model params     = 1.41 B
0.00.049.558 I print_info: general.name     = 1.4B
0.00.049.559 I print_info: vocab type       = BPE
0.00.049.559 I print_info: n_vocab          = 50304
0.00.049.559 I print_info: n_merges         = 50009
0.00.049.559 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.559 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.559 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.560 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.560 I print_info: LF token         = 128 'Ä'
0.00.049.560 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.560 I print_info: max token length = 1024
0.00.051.420 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.421 I load_tensors: offloading output layer to GPU
0.00.051.421 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.432 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.433 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.715 I llama_init_from_model: n_seq_max     = 1
0.00.051.716 I llama_init_from_model: n_ctx         = 128
0.00.051.716 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.716 I llama_init_from_model: n_batch       = 128
0.00.051.716 I llama_init_from_model: n_ubatch      = 128
0.00.051.716 I llama_init_from_model: flash_attn    = 0
0.00.051.717 I llama_init_from_model: freq_base     = 10000.0
0.00.051.717 I llama_init_from_model: freq_scale    = 1
0.00.051.717 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.718 I ggml_metal_init: allocating
0.00.051.721 I ggml_metal_init: found device: Apple M4
0.00.051.722 I ggml_metal_init: picking default device: Apple M4
0.00.052.191 I ggml_metal_init: using embedded metal library
0.00.054.523 I ggml_metal_init: GPU name:   Apple M4
0.00.054.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.525 I ggml_metal_init: simdgroup reduction   = true
0.00.054.525 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.526 I ggml_metal_init: has bfloat            = true
0.00.054.526 I ggml_metal_init: use bfloat            = true
0.00.054.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.972 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.359 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.361 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.386 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.314 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.315 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.315 I llama_init_from_model: graph nodes  = 967
0.00.066.315 I llama_init_from_model: graph splits = 2
0.00.066.316 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.568 I 
0.00.467.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.667 I perplexity: tokenizing the input ..
0.00.475.887 I perplexity: tokenization took 8.218 ms
0.00.475.897 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.608.567 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.609.852 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.609.881 I llama_perf_context_print:        load time =     458.76 ms
0.00.609.882 I llama_perf_context_print: prompt eval time =     132.43 ms /   128 tokens (    1.03 ms per token,   966.53 tokens per second)
0.00.609.883 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.609.883 I llama_perf_context_print:       total time =     142.32 ms /   129 tokens
0.00.610.451 I ggml_metal_free: deallocating

real	0m0.624s
user	0m0.077s
sys	0m0.078s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.095 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.474 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.490 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.490 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.491 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.597 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.532 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.533 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.534 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.534 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.534 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.535 I llama_model_loader: - type  f32:  194 tensors
0.00.025.535 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.535 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.536 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.536 I print_info: file format = GGUF V3 (latest)
0.00.025.537 I print_info: file type   = Q4_K - Medium
0.00.025.538 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.124 I load: special tokens cache size = 25
0.00.050.324 I load: token to piece cache size = 0.2984 MB
0.00.050.327 I print_info: arch             = gptneox
0.00.050.327 I print_info: vocab_only       = 0
0.00.050.327 I print_info: n_ctx_train      = 2048
0.00.050.327 I print_info: n_embd           = 2048
0.00.050.327 I print_info: n_layer          = 24
0.00.050.331 I print_info: n_head           = 16
0.00.050.331 I print_info: n_head_kv        = 16
0.00.050.332 I print_info: n_rot            = 32
0.00.050.332 I print_info: n_swa            = 0
0.00.050.332 I print_info: n_embd_head_k    = 128
0.00.050.332 I print_info: n_embd_head_v    = 128
0.00.050.333 I print_info: n_gqa            = 1
0.00.050.334 I print_info: n_embd_k_gqa     = 2048
0.00.050.334 I print_info: n_embd_v_gqa     = 2048
0.00.050.335 I print_info: f_norm_eps       = 1.0e-05
0.00.050.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.336 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.336 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.336 I print_info: f_logit_scale    = 0.0e+00
0.00.050.337 I print_info: n_ff             = 8192
0.00.050.337 I print_info: n_expert         = 0
0.00.050.337 I print_info: n_expert_used    = 0
0.00.050.337 I print_info: causal attn      = 1
0.00.050.339 I print_info: pooling type     = 0
0.00.050.341 I print_info: rope type        = 2
0.00.050.341 I print_info: rope scaling     = linear
0.00.050.341 I print_info: freq_base_train  = 10000.0
0.00.050.342 I print_info: freq_scale_train = 1
0.00.050.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.342 I print_info: rope_finetuned   = unknown
0.00.050.343 I print_info: ssm_d_conv       = 0
0.00.050.343 I print_info: ssm_d_inner      = 0
0.00.050.343 I print_info: ssm_d_state      = 0
0.00.050.343 I print_info: ssm_dt_rank      = 0
0.00.050.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.343 I print_info: model type       = 1.4B
0.00.050.344 I print_info: model params     = 1.41 B
0.00.050.344 I print_info: general.name     = 1.4B
0.00.050.345 I print_info: vocab type       = BPE
0.00.050.345 I print_info: n_vocab          = 50304
0.00.050.345 I print_info: n_merges         = 50009
0.00.050.345 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.345 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.346 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.346 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.346 I print_info: LF token         = 128 'Ä'
0.00.050.346 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.347 I print_info: max token length = 1024
0.00.052.284 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.284 I load_tensors: offloading output layer to GPU
0.00.052.284 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.295 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.296 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.594 I llama_init_from_model: n_seq_max     = 1
0.00.052.595 I llama_init_from_model: n_ctx         = 2048
0.00.052.595 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.595 I llama_init_from_model: n_batch       = 2048
0.00.052.595 I llama_init_from_model: n_ubatch      = 512
0.00.052.596 I llama_init_from_model: flash_attn    = 0
0.00.052.596 I llama_init_from_model: freq_base     = 10000.0
0.00.052.596 I llama_init_from_model: freq_scale    = 1
0.00.052.597 I ggml_metal_init: allocating
0.00.052.599 I ggml_metal_init: found device: Apple M4
0.00.052.601 I ggml_metal_init: picking default device: Apple M4
0.00.053.099 I ggml_metal_init: using embedded metal library
0.00.055.482 I ggml_metal_init: GPU name:   Apple M4
0.00.055.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.485 I ggml_metal_init: simdgroup reduction   = true
0.00.055.485 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.485 I ggml_metal_init: has bfloat            = true
0.00.055.485 I ggml_metal_init: use bfloat            = true
0.00.055.485 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.673 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.124 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.133 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.156 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.247 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.248 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.249 I llama_init_from_model: graph nodes  = 967
0.00.086.249 I llama_init_from_model: graph splits = 2
0.00.086.252 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.380 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.381 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.388 I main: llama threadpool init, n_threads = 4
0.00.615.438 I 
0.00.615.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.459 I 
0.00.615.677 I sampler seed: 1234
0.00.615.681 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.615.722 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.615.722 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.615.723 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.366.802 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.366.803 I llama_perf_context_print:        load time =     605.41 ms
0.01.366.803 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.56 tokens per second)
0.01.366.804 I llama_perf_context_print:        eval time =     700.91 ms /    63 runs   (   11.13 ms per token,    89.88 tokens per second)
0.01.366.804 I llama_perf_context_print:       total time =     752.30 ms /    70 tokens
0.01.367.034 I ggml_metal_free: deallocating

real	0m1.384s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.720 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.891 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.892 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.897 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.901 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.915 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.025 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.074 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.076 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.077 I llama_model_loader: - type  f32:  194 tensors
0.00.025.077 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.077 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.077 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.078 I print_info: file format = GGUF V3 (latest)
0.00.025.078 I print_info: file type   = Q4_K - Medium
0.00.025.083 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.429 I load: special tokens cache size = 25
0.00.050.306 I load: token to piece cache size = 0.2984 MB
0.00.050.309 I print_info: arch             = gptneox
0.00.050.310 I print_info: vocab_only       = 0
0.00.050.310 I print_info: n_ctx_train      = 2048
0.00.050.310 I print_info: n_embd           = 2048
0.00.050.310 I print_info: n_layer          = 24
0.00.050.313 I print_info: n_head           = 16
0.00.050.314 I print_info: n_head_kv        = 16
0.00.050.314 I print_info: n_rot            = 32
0.00.050.314 I print_info: n_swa            = 0
0.00.050.314 I print_info: n_embd_head_k    = 128
0.00.050.315 I print_info: n_embd_head_v    = 128
0.00.050.315 I print_info: n_gqa            = 1
0.00.050.316 I print_info: n_embd_k_gqa     = 2048
0.00.050.317 I print_info: n_embd_v_gqa     = 2048
0.00.050.317 I print_info: f_norm_eps       = 1.0e-05
0.00.050.318 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.318 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.318 I print_info: f_logit_scale    = 0.0e+00
0.00.050.319 I print_info: n_ff             = 8192
0.00.050.319 I print_info: n_expert         = 0
0.00.050.319 I print_info: n_expert_used    = 0
0.00.050.320 I print_info: causal attn      = 1
0.00.050.320 I print_info: pooling type     = 0
0.00.050.320 I print_info: rope type        = 2
0.00.050.324 I print_info: rope scaling     = linear
0.00.050.324 I print_info: freq_base_train  = 10000.0
0.00.050.324 I print_info: freq_scale_train = 1
0.00.050.324 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.325 I print_info: rope_finetuned   = unknown
0.00.050.325 I print_info: ssm_d_conv       = 0
0.00.050.325 I print_info: ssm_d_inner      = 0
0.00.050.325 I print_info: ssm_d_state      = 0
0.00.050.325 I print_info: ssm_dt_rank      = 0
0.00.050.325 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.325 I print_info: model type       = 1.4B
0.00.050.330 I print_info: model params     = 1.41 B
0.00.050.330 I print_info: general.name     = 1.4B
0.00.050.330 I print_info: vocab type       = BPE
0.00.050.331 I print_info: n_vocab          = 50304
0.00.050.331 I print_info: n_merges         = 50009
0.00.050.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.331 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: LF token         = 128 'Ä'
0.00.050.332 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.332 I print_info: max token length = 1024
0.00.052.306 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.306 I load_tensors: offloading output layer to GPU
0.00.052.306 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.317 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.318 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.592 I llama_init_from_model: n_seq_max     = 1
0.00.052.593 I llama_init_from_model: n_ctx         = 128
0.00.052.593 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.593 I llama_init_from_model: n_batch       = 128
0.00.052.593 I llama_init_from_model: n_ubatch      = 128
0.00.052.593 I llama_init_from_model: flash_attn    = 0
0.00.052.594 I llama_init_from_model: freq_base     = 10000.0
0.00.052.594 I llama_init_from_model: freq_scale    = 1
0.00.052.594 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.595 I ggml_metal_init: allocating
0.00.052.598 I ggml_metal_init: found device: Apple M4
0.00.052.599 I ggml_metal_init: picking default device: Apple M4
0.00.053.103 I ggml_metal_init: using embedded metal library
0.00.055.438 I ggml_metal_init: GPU name:   Apple M4
0.00.055.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.440 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.440 I ggml_metal_init: simdgroup reduction   = true
0.00.055.440 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.440 I ggml_metal_init: has bfloat            = true
0.00.055.441 I ggml_metal_init: use bfloat            = true
0.00.055.441 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.243 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.510 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.514 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.528 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.465 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.466 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.466 I llama_init_from_model: graph nodes  = 967
0.00.067.466 I llama_init_from_model: graph splits = 2
0.00.067.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.693 I 
0.00.610.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.739 I perplexity: tokenizing the input ..
0.00.618.387 I perplexity: tokenization took 7.646 ms
0.00.618.398 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.866 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.754.025 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.754.042 I llama_perf_context_print:        load time =     601.97 ms
0.00.754.043 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.51 tokens per second)
0.00.754.044 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.045 I llama_perf_context_print:       total time =     143.35 ms /   129 tokens
0.00.754.520 I ggml_metal_free: deallocating

real	0m0.769s
user	0m0.078s
sys	0m0.098s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.674 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.010 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.011 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.012 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.013 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.013 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.013 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.014 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.018 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.018 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.018 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.956 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.012 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.953 I llama_model_loader: - type  f32:  194 tensors
0.00.025.953 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.953 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.954 I print_info: file format = GGUF V3 (latest)
0.00.025.954 I print_info: file type   = Q5_K - Medium
0.00.025.955 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.557 I load: special tokens cache size = 25
0.00.050.541 I load: token to piece cache size = 0.2984 MB
0.00.050.543 I print_info: arch             = gptneox
0.00.050.544 I print_info: vocab_only       = 0
0.00.050.544 I print_info: n_ctx_train      = 2048
0.00.050.544 I print_info: n_embd           = 2048
0.00.050.544 I print_info: n_layer          = 24
0.00.050.547 I print_info: n_head           = 16
0.00.050.548 I print_info: n_head_kv        = 16
0.00.050.548 I print_info: n_rot            = 32
0.00.050.548 I print_info: n_swa            = 0
0.00.050.549 I print_info: n_embd_head_k    = 128
0.00.050.549 I print_info: n_embd_head_v    = 128
0.00.050.550 I print_info: n_gqa            = 1
0.00.050.551 I print_info: n_embd_k_gqa     = 2048
0.00.050.551 I print_info: n_embd_v_gqa     = 2048
0.00.050.552 I print_info: f_norm_eps       = 1.0e-05
0.00.050.552 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.552 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.552 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.553 I print_info: f_logit_scale    = 0.0e+00
0.00.050.553 I print_info: n_ff             = 8192
0.00.050.553 I print_info: n_expert         = 0
0.00.050.554 I print_info: n_expert_used    = 0
0.00.050.554 I print_info: causal attn      = 1
0.00.050.554 I print_info: pooling type     = 0
0.00.050.555 I print_info: rope type        = 2
0.00.050.557 I print_info: rope scaling     = linear
0.00.050.557 I print_info: freq_base_train  = 10000.0
0.00.050.558 I print_info: freq_scale_train = 1
0.00.050.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.558 I print_info: rope_finetuned   = unknown
0.00.050.558 I print_info: ssm_d_conv       = 0
0.00.050.558 I print_info: ssm_d_inner      = 0
0.00.050.559 I print_info: ssm_d_state      = 0
0.00.050.559 I print_info: ssm_dt_rank      = 0
0.00.050.559 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.559 I print_info: model type       = 1.4B
0.00.050.559 I print_info: model params     = 1.41 B
0.00.050.560 I print_info: general.name     = 1.4B
0.00.050.560 I print_info: vocab type       = BPE
0.00.050.560 I print_info: n_vocab          = 50304
0.00.050.560 I print_info: n_merges         = 50009
0.00.050.561 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: LF token         = 128 'Ä'
0.00.050.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.562 I print_info: max token length = 1024
0.00.052.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.576 I load_tensors: offloading output layer to GPU
0.00.052.577 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.587 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.589 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.857 I llama_init_from_model: n_seq_max     = 1
0.00.052.857 I llama_init_from_model: n_ctx         = 2048
0.00.052.858 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.858 I llama_init_from_model: n_batch       = 2048
0.00.052.858 I llama_init_from_model: n_ubatch      = 512
0.00.052.858 I llama_init_from_model: flash_attn    = 0
0.00.052.858 I llama_init_from_model: freq_base     = 10000.0
0.00.052.859 I llama_init_from_model: freq_scale    = 1
0.00.052.859 I ggml_metal_init: allocating
0.00.052.862 I ggml_metal_init: found device: Apple M4
0.00.052.864 I ggml_metal_init: picking default device: Apple M4
0.00.053.359 I ggml_metal_init: using embedded metal library
0.00.055.715 I ggml_metal_init: GPU name:   Apple M4
0.00.055.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.718 I ggml_metal_init: simdgroup reduction   = true
0.00.055.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.718 I ggml_metal_init: has bfloat            = true
0.00.055.718 I ggml_metal_init: use bfloat            = true
0.00.055.719 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.303 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.760 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.765 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.803 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.805 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.805 I llama_init_from_model: graph nodes  = 967
0.00.085.805 I llama_init_from_model: graph splits = 2
0.00.085.808 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.278 I main: llama threadpool init, n_threads = 4
0.00.712.312 I 
0.00.712.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.363 I 
0.00.712.594 I sampler seed: 1234
0.00.712.599 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.618 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.618 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.618 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.560 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64486.83 tokens per second)
0.01.557.562 I llama_perf_context_print:        load time =     702.74 ms
0.01.557.563 I llama_perf_context_print: prompt eval time =      55.58 ms /     7 tokens (    7.94 ms per token,   125.95 tokens per second)
0.01.557.563 I llama_perf_context_print:        eval time =     786.58 ms /    63 runs   (   12.49 ms per token,    80.09 tokens per second)
0.01.557.563 I llama_perf_context_print:       total time =     846.15 ms /    70 tokens
0.01.557.790 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.928 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.934 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.936 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.937 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.939 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.940 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.940 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.941 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.941 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.945 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.804 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.549 I llama_model_loader: - type  f32:  194 tensors
0.00.025.550 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.550 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.550 I print_info: file format = GGUF V3 (latest)
0.00.025.551 I print_info: file type   = Q5_K - Medium
0.00.025.552 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.623 I load: special tokens cache size = 25
0.00.050.620 I load: token to piece cache size = 0.2984 MB
0.00.050.623 I print_info: arch             = gptneox
0.00.050.623 I print_info: vocab_only       = 0
0.00.050.623 I print_info: n_ctx_train      = 2048
0.00.050.623 I print_info: n_embd           = 2048
0.00.050.623 I print_info: n_layer          = 24
0.00.050.626 I print_info: n_head           = 16
0.00.050.627 I print_info: n_head_kv        = 16
0.00.050.627 I print_info: n_rot            = 32
0.00.050.627 I print_info: n_swa            = 0
0.00.050.628 I print_info: n_embd_head_k    = 128
0.00.050.628 I print_info: n_embd_head_v    = 128
0.00.050.628 I print_info: n_gqa            = 1
0.00.050.629 I print_info: n_embd_k_gqa     = 2048
0.00.050.630 I print_info: n_embd_v_gqa     = 2048
0.00.050.630 I print_info: f_norm_eps       = 1.0e-05
0.00.050.631 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.631 I print_info: f_logit_scale    = 0.0e+00
0.00.050.632 I print_info: n_ff             = 8192
0.00.050.632 I print_info: n_expert         = 0
0.00.050.632 I print_info: n_expert_used    = 0
0.00.050.633 I print_info: causal attn      = 1
0.00.050.633 I print_info: pooling type     = 0
0.00.050.633 I print_info: rope type        = 2
0.00.050.633 I print_info: rope scaling     = linear
0.00.050.633 I print_info: freq_base_train  = 10000.0
0.00.050.634 I print_info: freq_scale_train = 1
0.00.050.634 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.634 I print_info: rope_finetuned   = unknown
0.00.050.634 I print_info: ssm_d_conv       = 0
0.00.050.635 I print_info: ssm_d_inner      = 0
0.00.050.635 I print_info: ssm_d_state      = 0
0.00.050.635 I print_info: ssm_dt_rank      = 0
0.00.050.635 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.635 I print_info: model type       = 1.4B
0.00.050.636 I print_info: model params     = 1.41 B
0.00.050.636 I print_info: general.name     = 1.4B
0.00.050.637 I print_info: vocab type       = BPE
0.00.050.637 I print_info: n_vocab          = 50304
0.00.050.637 I print_info: n_merges         = 50009
0.00.050.637 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.637 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.638 I print_info: LF token         = 128 'Ä'
0.00.050.638 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.639 I print_info: max token length = 1024
0.00.052.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.643 I load_tensors: offloading output layer to GPU
0.00.052.643 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.654 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.655 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.949 I llama_init_from_model: n_seq_max     = 1
0.00.052.950 I llama_init_from_model: n_ctx         = 128
0.00.052.950 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.950 I llama_init_from_model: n_batch       = 128
0.00.052.950 I llama_init_from_model: n_ubatch      = 128
0.00.052.950 I llama_init_from_model: flash_attn    = 0
0.00.052.951 I llama_init_from_model: freq_base     = 10000.0
0.00.052.951 I llama_init_from_model: freq_scale    = 1
0.00.052.951 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.952 I ggml_metal_init: allocating
0.00.052.955 I ggml_metal_init: found device: Apple M4
0.00.052.956 I ggml_metal_init: picking default device: Apple M4
0.00.053.446 I ggml_metal_init: using embedded metal library
0.00.055.787 I ggml_metal_init: GPU name:   Apple M4
0.00.055.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.790 I ggml_metal_init: simdgroup reduction   = true
0.00.055.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.790 I ggml_metal_init: has bfloat            = true
0.00.055.790 I ggml_metal_init: use bfloat            = true
0.00.055.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.791 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.428 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.739 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.744 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.760 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.617 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.618 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.618 I llama_init_from_model: graph nodes  = 967
0.00.067.619 I llama_init_from_model: graph splits = 2
0.00.067.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.698 I 
0.00.635.734 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.746 I perplexity: tokenizing the input ..
0.00.643.436 I perplexity: tokenization took 7.689 ms
0.00.643.446 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.784.143 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.785.366 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.785.384 I llama_perf_context_print:        load time =     625.74 ms
0.00.785.386 I llama_perf_context_print: prompt eval time =     140.46 ms /   128 tokens (    1.10 ms per token,   911.32 tokens per second)
0.00.785.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.785.389 I llama_perf_context_print:       total time =     149.69 ms /   129 tokens
0.00.785.900 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.077s
sys	0m0.115s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.458 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.477 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.719 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.721 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.721 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.721 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.722 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.722 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.723 I llama_model_loader: - type  f32:  194 tensors
0.00.026.723 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.724 I print_info: file format = GGUF V3 (latest)
0.00.026.724 I print_info: file type   = Q6_K
0.00.026.725 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.111 I load: special tokens cache size = 25
0.00.052.160 I load: token to piece cache size = 0.2984 MB
0.00.052.163 I print_info: arch             = gptneox
0.00.052.164 I print_info: vocab_only       = 0
0.00.052.164 I print_info: n_ctx_train      = 2048
0.00.052.164 I print_info: n_embd           = 2048
0.00.052.164 I print_info: n_layer          = 24
0.00.052.167 I print_info: n_head           = 16
0.00.052.168 I print_info: n_head_kv        = 16
0.00.052.168 I print_info: n_rot            = 32
0.00.052.170 I print_info: n_swa            = 0
0.00.052.170 I print_info: n_embd_head_k    = 128
0.00.052.170 I print_info: n_embd_head_v    = 128
0.00.052.171 I print_info: n_gqa            = 1
0.00.052.172 I print_info: n_embd_k_gqa     = 2048
0.00.052.173 I print_info: n_embd_v_gqa     = 2048
0.00.052.173 I print_info: f_norm_eps       = 1.0e-05
0.00.052.174 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.174 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.174 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.174 I print_info: f_logit_scale    = 0.0e+00
0.00.052.175 I print_info: n_ff             = 8192
0.00.052.175 I print_info: n_expert         = 0
0.00.052.175 I print_info: n_expert_used    = 0
0.00.052.175 I print_info: causal attn      = 1
0.00.052.175 I print_info: pooling type     = 0
0.00.052.175 I print_info: rope type        = 2
0.00.052.180 I print_info: rope scaling     = linear
0.00.052.180 I print_info: freq_base_train  = 10000.0
0.00.052.181 I print_info: freq_scale_train = 1
0.00.052.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.181 I print_info: rope_finetuned   = unknown
0.00.052.183 I print_info: ssm_d_conv       = 0
0.00.052.183 I print_info: ssm_d_inner      = 0
0.00.052.183 I print_info: ssm_d_state      = 0
0.00.052.183 I print_info: ssm_dt_rank      = 0
0.00.052.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.183 I print_info: model type       = 1.4B
0.00.052.184 I print_info: model params     = 1.41 B
0.00.052.184 I print_info: general.name     = 1.4B
0.00.052.185 I print_info: vocab type       = BPE
0.00.052.185 I print_info: n_vocab          = 50304
0.00.052.185 I print_info: n_merges         = 50009
0.00.052.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.185 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.185 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.186 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.186 I print_info: LF token         = 128 'Ä'
0.00.052.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.186 I print_info: max token length = 1024
0.00.054.235 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.235 I load_tensors: offloading output layer to GPU
0.00.054.235 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.246 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.248 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.522 I llama_init_from_model: n_seq_max     = 1
0.00.054.523 I llama_init_from_model: n_ctx         = 2048
0.00.054.523 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.523 I llama_init_from_model: n_batch       = 2048
0.00.054.523 I llama_init_from_model: n_ubatch      = 512
0.00.054.523 I llama_init_from_model: flash_attn    = 0
0.00.054.524 I llama_init_from_model: freq_base     = 10000.0
0.00.054.524 I llama_init_from_model: freq_scale    = 1
0.00.054.525 I ggml_metal_init: allocating
0.00.054.528 I ggml_metal_init: found device: Apple M4
0.00.054.530 I ggml_metal_init: picking default device: Apple M4
0.00.055.035 I ggml_metal_init: using embedded metal library
0.00.057.400 I ggml_metal_init: GPU name:   Apple M4
0.00.057.401 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.402 I ggml_metal_init: simdgroup reduction   = true
0.00.057.402 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.402 I ggml_metal_init: has bfloat            = true
0.00.057.403 I ggml_metal_init: use bfloat            = true
0.00.057.403 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.404 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.144 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.112 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.118 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.137 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.139 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.139 I llama_init_from_model: graph nodes  = 967
0.00.088.139 I llama_init_from_model: graph splits = 2
0.00.088.142 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.270 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.472 I main: llama threadpool init, n_threads = 4
0.00.747.512 I 
0.00.747.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.545 I 
0.00.747.763 I sampler seed: 1234
0.00.747.768 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.829 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.830 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.835 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.625.125 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.625.126 I llama_perf_context_print:        load time =     736.68 ms
0.01.625.127 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.75 tokens per second)
0.01.625.128 I llama_perf_context_print:        eval time =     819.95 ms /    63 runs   (   13.02 ms per token,    76.83 tokens per second)
0.01.625.128 I llama_perf_context_print:       total time =     878.52 ms /    70 tokens
0.01.625.357 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4555 (26771a14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.829 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.835 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.836 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.836 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.838 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.841 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.842 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.842 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.843 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.766 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.769 I llama_model_loader: - type  f32:  194 tensors
0.00.024.770 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.770 I print_info: file format = GGUF V3 (latest)
0.00.024.771 I print_info: file type   = Q6_K
0.00.024.771 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.037 I load: special tokens cache size = 25
0.00.050.075 I load: token to piece cache size = 0.2984 MB
0.00.050.078 I print_info: arch             = gptneox
0.00.050.078 I print_info: vocab_only       = 0
0.00.050.078 I print_info: n_ctx_train      = 2048
0.00.050.079 I print_info: n_embd           = 2048
0.00.050.079 I print_info: n_layer          = 24
0.00.050.081 I print_info: n_head           = 16
0.00.050.082 I print_info: n_head_kv        = 16
0.00.050.082 I print_info: n_rot            = 32
0.00.050.082 I print_info: n_swa            = 0
0.00.050.082 I print_info: n_embd_head_k    = 128
0.00.050.083 I print_info: n_embd_head_v    = 128
0.00.050.083 I print_info: n_gqa            = 1
0.00.050.084 I print_info: n_embd_k_gqa     = 2048
0.00.050.086 I print_info: n_embd_v_gqa     = 2048
0.00.050.087 I print_info: f_norm_eps       = 1.0e-05
0.00.050.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.087 I print_info: f_logit_scale    = 0.0e+00
0.00.050.088 I print_info: n_ff             = 8192
0.00.050.088 I print_info: n_expert         = 0
0.00.050.088 I print_info: n_expert_used    = 0
0.00.050.088 I print_info: causal attn      = 1
0.00.050.089 I print_info: pooling type     = 0
0.00.050.089 I print_info: rope type        = 2
0.00.050.089 I print_info: rope scaling     = linear
0.00.050.089 I print_info: freq_base_train  = 10000.0
0.00.050.091 I print_info: freq_scale_train = 1
0.00.050.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.091 I print_info: rope_finetuned   = unknown
0.00.050.095 I print_info: ssm_d_conv       = 0
0.00.050.095 I print_info: ssm_d_inner      = 0
0.00.050.096 I print_info: ssm_d_state      = 0
0.00.050.096 I print_info: ssm_dt_rank      = 0
0.00.050.096 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.096 I print_info: model type       = 1.4B
0.00.050.097 I print_info: model params     = 1.41 B
0.00.050.097 I print_info: general.name     = 1.4B
0.00.050.097 I print_info: vocab type       = BPE
0.00.050.098 I print_info: n_vocab          = 50304
0.00.050.098 I print_info: n_merges         = 50009
0.00.050.098 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: LF token         = 128 'Ä'
0.00.050.099 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.100 I print_info: max token length = 1024
0.00.052.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.121 I load_tensors: offloading output layer to GPU
0.00.052.122 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.132 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.133 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.402 I llama_init_from_model: n_seq_max     = 1
0.00.052.402 I llama_init_from_model: n_ctx         = 128
0.00.052.402 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.402 I llama_init_from_model: n_batch       = 128
0.00.052.403 I llama_init_from_model: n_ubatch      = 128
0.00.052.403 I llama_init_from_model: flash_attn    = 0
0.00.052.403 I llama_init_from_model: freq_base     = 10000.0
0.00.052.403 I llama_init_from_model: freq_scale    = 1
0.00.052.404 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.404 I ggml_metal_init: allocating
0.00.052.407 I ggml_metal_init: found device: Apple M4
0.00.052.409 I ggml_metal_init: picking default device: Apple M4
0.00.052.884 I ggml_metal_init: using embedded metal library
0.00.055.188 I ggml_metal_init: GPU name:   Apple M4
0.00.055.190 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.191 I ggml_metal_init: simdgroup reduction   = true
0.00.055.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.191 I ggml_metal_init: has bfloat            = true
0.00.055.191 I ggml_metal_init: use bfloat            = true
0.00.055.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.192 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.839 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.117 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.119 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.134 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.013 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.014 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.014 I llama_init_from_model: graph nodes  = 967
0.00.067.015 I llama_init_from_model: graph splits = 2
0.00.067.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.464.631 I 
0.00.464.665 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.464.674 I perplexity: tokenizing the input ..
0.00.472.729 I perplexity: tokenization took 8.054 ms
0.00.472.740 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.979 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.614.145 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.614.161 I llama_perf_context_print:        load time =     455.70 ms
0.00.614.162 I llama_perf_context_print: prompt eval time =     140.01 ms /   128 tokens (    1.09 ms per token,   914.21 tokens per second)
0.00.614.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.163 I llama_perf_context_print:       total time =     149.53 ms /   129 tokens
0.00.614.635 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.078s
sys	0m0.091s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4555 (26771a14)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c60a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c60a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c60ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c60b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c60b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c60bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c60c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c60caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c60d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c60d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c60da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c60df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c60ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c60f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c60fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c610150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c610870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c610f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c6116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c611e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c6125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c612cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c6133e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c613c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c6143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c614660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c614c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c6158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c615e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c6160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c616580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c616840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c6170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c617610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c6178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c617d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c618210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c6186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c618b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c618ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c619490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c619930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c619dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c61a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c61a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c61ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c61b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c61ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c61c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c61c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c61cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c61d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c61d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c61ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c61e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c61eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c61f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c61f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c61f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c6200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c620380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c620820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c620cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c621160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c621aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c621f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c6223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c622880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c622d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c6231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c623660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c623b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c624050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c6245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c624af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c625040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c625590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c625ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c626030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c626580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c626ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c627020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c627570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c627ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c628010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c628560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c628ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c629000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c629550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c629aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c629ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c62a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c62aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c62afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c62b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c62ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c61b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c62bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c62c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c62cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c62d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c62d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c62dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c62e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c62e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c62ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c62f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c62f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c62fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c630110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c630660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c630bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c6314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c631990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c631e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c6322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c632770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c632c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c6330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c633550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c6339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c633e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c634330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c6347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c634c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c635110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c6355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c635a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c635ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c636830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c637170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c637610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c637ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c637f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c6383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c638890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c638d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c6391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c639670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c639b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c639fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c63a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c63a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c63ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c63b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c63b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c63bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c63c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c63c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c63c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c63cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c63d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c63d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c63dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c63e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c63e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c63e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c63ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c63f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c63f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c63fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c6400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c640570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c640a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c640eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c641350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c6417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c642130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c6425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c642a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c642f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c6433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c643850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c643cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c644190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c644630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c644ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c644f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c645410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c6458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c6461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c646690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c646b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c646fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c647470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c647910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c647db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c648300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c648850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c648da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c6492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c6495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c649bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c64a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c64a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c64afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c64b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c64b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c64bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c64c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c64cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c64cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c64d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c64d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c64e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c64e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c64eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c64f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c64f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c64fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c6500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c650600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c650b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c6510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c6515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c651b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c652090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c6525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c652b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c653080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c6535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c653b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c654070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c6545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c654b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c655060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c6555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c655b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c656050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c6565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c656af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c657040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c657590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c657ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c658030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c658580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c658ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c659020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c659570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c659ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c65a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c65a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c65aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c65b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c65b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c65baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c65bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c65c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c65ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c65cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c65d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c65da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c65dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c65e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c65ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c65efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c65f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c65fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c65ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c660500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c660a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c660ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c661390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c661830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c661cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c662170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c662610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c662ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c662f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c6633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c663890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c663d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c6641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c664670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c664b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c664fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c665500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c665c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c666340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c666a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c667180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c667440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c667c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c667ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c668500 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.146.003 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.146.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x108f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x108f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x108f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x108f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x108f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x108f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x108f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x108f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x108f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x108f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x108f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x108f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x108f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x108f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x108f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x108f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x108f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x108f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x108f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108f5b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108f4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108f4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108f48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108f45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108f55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108f52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108f4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108f46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108f43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108f48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108f49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108f4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108f4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108f53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108f47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108f51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108f4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108f4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108f475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108f55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108f447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108f430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108f45340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108f55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108f4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108f53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108f49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108f4bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108f4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108f47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108f4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108f516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108f45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108f544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108f51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108f4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108f56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108f44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108f56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108f44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108f54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108f50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108f53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108f52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108f4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108f41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108f04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108f5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108f0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108f5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108f5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108f5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108f5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108f5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108f5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108f5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108f60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108f60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108f607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108f60a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108f60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108f61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108f612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108f61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108f61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108f61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108f61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108f62090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108f62350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108f62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x108f628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x108f62b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x108f62e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x108f63110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x108f633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x108f63690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x108f63950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x108f63c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x108f63ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x108f64190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x108f64450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x108f64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x108f649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x108f64c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108f64f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x108f65210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x108f654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108f65790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108f65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108f65d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108f65fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108f66290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108f66550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108f66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108f66ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108f66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108f67050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108f67310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108f675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108f67890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108f67b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108f67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108f680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108f68390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108f68650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x108f68910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108f68bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108f68e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108f69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108f69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108f696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108f69990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108f69c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108f69f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108f6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108f6a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108f6a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108f6aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108f6acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108f6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108f6b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108f6b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108f6b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108f6ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108f6bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108f6c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108f6c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108f6c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108f6c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108f6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108f6cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108f6d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108f6d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108f6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108f6d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108f6db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108f6de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108f6e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108f6e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108f6e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108f6e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108f6ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108f6eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108f6f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108f6f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108f6f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108f6f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108f6fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108f6ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108f70210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108f704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108f70790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108f70a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108f70d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108f70fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108f71290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108f71550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108f71810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108f71ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108f71d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108f72050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108f72310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108f725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108f72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108f72b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108f72e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108f730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108f73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108f73650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108f73910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108f73bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108f73e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108f74150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108f74410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108f746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108f74990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108f74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108f74f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108f751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108f75490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x108f75750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108f75a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108f75cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108f75f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108f76250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108f76510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108f767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108f76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108f76d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108f77010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108f772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108f77590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108f77850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108f77b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108f77dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108f78090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108f78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108f78610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108f788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108f78b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108f78e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108f79110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108f793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108f79690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108f79950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108f79c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108f79ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108f7a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108f7a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108f7aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108f7ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108f7afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108f7b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108f7b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108f7b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108f7baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108f7bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108f7c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108f7ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108f7cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108f7d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108f7da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108f7dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108f7e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108f7ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108f7efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108f7f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108f7fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108f7ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108f80500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108f80a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108f80fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108f814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108f81a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108f81f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108f824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108f82a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108f82f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108f834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108f83a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108f83f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108f844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108f84a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108f84f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x108f854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108f85a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108f85f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108f864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108f869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108f86f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108f87490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108f879e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108f87f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108f88480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108f889d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108f88f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108f89470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108f899c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108f89f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108f8a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108f8a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108f8af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108f8b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108f8b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108f8bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108f8bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108f8c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108f8c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108f8cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108f8cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108f8d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108f8d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108f8dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108f8e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108f8e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108f8e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108f8ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108f8f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108f8f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108f8fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108f90000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108f90cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108f91410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108f91b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108f91df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108f92260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108f92860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108f92e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.891s
user	0m0.295s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4555 (26771a14)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c60b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c60b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c60be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c60c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c60c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c60cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c60d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c60da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c60e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c60e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c60ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c60ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c60fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c6101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c610a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c611f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c612680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c613570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c613c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c6143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c614c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c615370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c615630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c615c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c6168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c616df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c6170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c617550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c617810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c6180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c6185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c6188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c618d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c6191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c619680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c619b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c619fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c61a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c61a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c61ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c61b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c61b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c61bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c61c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c61ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c61d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c61d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c61dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c61e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c61e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c61eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c61f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c61fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c61ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c6208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c621090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c621350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c6217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c621c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c622130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c6225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c622a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c622f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c6233b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c623850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c623cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c624190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c624630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c624ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c625570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c626010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c626ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c627000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c627550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c627aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c627ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c628fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c629530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c629a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c629fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c62a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c62aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c62afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c62b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c62ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c62bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c62c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c62ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c61c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c62cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c62d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c62dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c62e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c62e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c62ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c62f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c62f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c62fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c6300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c630640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c6310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c631630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c631b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c6324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c632960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c6332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c633740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c633be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c634520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c6349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c634e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c6357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c635c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c6360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c636580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c636a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c636ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c637800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c637ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c638140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c6385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c638a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c638f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c6393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c639860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c639d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c63a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c63a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c63aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c63af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c63b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c63b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c63bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c63c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c63c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c63cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c63cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c63d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c63d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c63ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c63e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c63e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c63eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c63f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c63f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c63f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c63fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c6402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c640760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c640c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c6410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c641540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c6419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c641e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c642320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c6427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c642c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c643100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c6435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c643a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c643ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c644820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c644cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c645160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c645600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c645aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c645f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c6463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c646880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c646d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c6471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c647b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c647fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c648440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c6488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c648d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c6492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c649820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c649d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c64a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c64a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c64ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c64b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c64b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c64bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c64c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c64c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c64cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c64d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c64db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c64dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c64e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c64e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c64f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c64f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c64fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c650090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c6505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c651080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c6515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c652070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c6525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c652b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c653060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c6535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c653b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c654050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c6545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c654af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c655040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c655590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c655ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c656030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c656580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c656ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c657020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c657570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c657ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c658010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c658560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c658ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c659000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c659550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c659aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c659ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c65a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c65aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c65afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c65b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c65ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c65bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c65c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c65ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c65cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c65d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c65da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c65dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c65e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c65ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c65efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c65f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c65fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c65ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c6604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c660a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c660f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c6614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c661a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c661ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c662360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c662800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c662ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c663140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c6635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c663a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c663f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c6643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c664860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c664d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c6651a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c665640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c665ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c665f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c6664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c666bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c667310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c667a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c668150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c668410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c668c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c668ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c6694d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.093.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c669180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c64c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c64a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c64b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c61e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c61df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c620550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c64cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c6158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c61c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c61cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c61d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c61b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c61d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c6148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c620b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c62d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c6686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c617ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c617d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c64d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c64ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c615f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c6161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c616480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c669930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c669bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c669eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c66a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c66a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c66a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c66a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c66ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c66af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c66b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c66b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c66b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c66ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c66bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c66bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c66c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c66c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c66c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c66cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c66cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c66d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c66d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c66d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c66d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c66db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c66ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c66e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c66e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c66e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c66e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c66ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c66ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c66f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c66f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c66f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c66f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c66fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c66fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c6701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c670470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c670730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c6709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c670cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c670f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c671230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c6714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c6717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c671a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c671d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c671ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c6722b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c672570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c672830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c672af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c672db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c673070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c673330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c6735f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c6738b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c673b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c673e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c6740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c6743b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c674670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c674930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c674bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c674eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c675170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c675430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c6756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c6759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c675c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c675f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c6761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c6764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c676770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c676a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c676cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c676fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c677270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c677530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c6777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c677ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c677d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c678030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c6782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c6785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c678870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c678b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c678df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c6790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c679370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c679630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c6798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c679bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c679e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c67a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c67a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c67a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c67a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c67ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c67aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c67b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c67b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c67b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c67b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c67bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c67bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c67c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c67c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c67c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c67ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c67cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c67cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c67d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c67d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c67d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c67daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c67ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c67e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c67e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c67e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c67e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c67eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c67ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c67f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c67f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c67f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c67f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c67fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c67feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c680170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c680430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c6806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c6809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c680c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c680f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c6811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c6814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c681770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c681a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c681cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c681fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c682270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c682530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c6827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c682ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c682d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c683030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c6832f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c6835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c683870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c683b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c683df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c6840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c684370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c684630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c6848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c684bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c684e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c685130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c6853f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c6856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c685970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c685c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c685ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c6861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c686470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c686730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c6869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c686cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c686f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c687230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c6874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c6877b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c687a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c687d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c687ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c6882b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c688570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c688830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c688af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c688db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c689070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c689330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c689ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c689da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c68a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c68a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c68a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c68adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c68b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c68b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c68bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c68bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c68c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c68c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c68ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c68d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c68d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c68da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c68de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c68e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c68e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c68ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c68f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c68f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c68f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c68fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c690200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c690670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c690ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c690f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c6913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c691830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c691ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c692110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c692580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c6929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c692e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c6932d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c693740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c693bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c694020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c694490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c694900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c694d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c6951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c695650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c695ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c695f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c6963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c696810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c696c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c6970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c697560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c6979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c697e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c6982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c698720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c698b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c699000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c699470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c6998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c699d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c69a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c69a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c69aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c69af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c69b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c69b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c69bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c69c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c69c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c69c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c69ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c69d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c69d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c69e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c69e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c69efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c69f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c69f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c6a0180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c6a0440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c6a0a50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c70a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c70ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c70b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c70b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c70bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c70c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c70c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c70cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c70d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c70d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c70db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c70ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c70e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c70ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c70f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c70fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c7105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c710d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c711420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c7124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c712c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c713330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c713a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c714170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c714430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c714a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c715050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c715660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c715e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c7162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c7165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c716e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c717380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c717640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c717ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c717f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c718420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c7188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c718d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c719200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c7196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c719b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c719fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c71a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c71a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c71aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c71b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c71bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c71c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c71c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c71cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c71d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c71d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c71e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c71e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c71ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c71ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c71f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c71fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c71ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c720460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c720900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c720da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c721240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c7216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c721b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c722020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c7224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c722960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c722e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c7232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c723740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c723c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c7241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c724730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c724c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c7251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c725720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c725c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c7261c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c726c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c7271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c727700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c727c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c7281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c7286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c728c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c729190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c7296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c729c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c72a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c72a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c72ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c72b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c72b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c72bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c72c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c72c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c72cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c72d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c72d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c72dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c72e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c72e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c72ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c72f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c72f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c72fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c730120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c730670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c730bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c731060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c731500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c7319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c731e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c7322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c732780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c732c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c7330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c733560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c733a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c733ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c7347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c734c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c735120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c7355c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c735a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c735f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c7363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c736840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c736ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c737180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c737620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c737ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c737f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c738400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c7388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c738d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c7391e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c739b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c739fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c73a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c73a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c73ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c73b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c73b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c73bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c73c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c73c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c73c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c73ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c73d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c73d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c73dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c73e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c73e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c73e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c73ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c73f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c73f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c73fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c7400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c740580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c740a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c741360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c741800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c741ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c7425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c742a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c7433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c743860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c743d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c7441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c744640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c744ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c744f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c745420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c7458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c745d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c746200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c7466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c746b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c746fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c747480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c747920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c747dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c748310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c748860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c748db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c749300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c7495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c749bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c74a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c74a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c74afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c74b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c74b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c74bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c74c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c74cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c74cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c74d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c74d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c74e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c74e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c74eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c74f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c74f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c74fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c7500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c750610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c750b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c7510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c751600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c751b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c7520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c7525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c753090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c7535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c753b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c754080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c7545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c754b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c755070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c7555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c755b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c756060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c7565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c756b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c757050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c7575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c757af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c758040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c758590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c758ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c759030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c759580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c759ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c75a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c75a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c75aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c75b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c75b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c75bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c75c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c75c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c75caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c75cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c75d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c75da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c75dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c75e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c75ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c75efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c75f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c75fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c75ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c760510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c760a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c760f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c7613a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c761840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c761ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c762180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c762620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c762ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c762f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c763400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c7638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c763d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c7641e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c764680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c764b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c764fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c765510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c765c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c766350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c766a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c767190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c767450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c767f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c768510 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.933s
user	0m0.243s
sys	0m0.137s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.68 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
