+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/26/b5cfde6f137ff78c9ab8e94869dbf3b3632faf/ggml-1-arm64-cpu-low-perf/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/26/b5cfde6f137ff78c9ab8e94869dbf3b3632faf/ggml-1-arm64-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=ares+crypto+noprofile+dotprod+noi8mm+nosve 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m2.061s
user	0m1.307s
sys	0m0.551s
+ tee -a /home/ggml/results/llama.cpp/26/b5cfde6f137ff78c9ab8e94869dbf3b3632faf/ggml-1-arm64-cpu-low-perf/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Built target xxhash
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Built target sha256
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target sha1
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Linking CXX shared library libggml-base.so
[  7%] Built target build_info
[  7%] Built target ggml-base
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Built target llama-gguf
[ 17%] Built target llama-gguf-hash
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
In file included from /home/ggml/work/llama.cpp/src/llama-batch.cpp:1:
/home/ggml/work/llama.cpp/src/llama-batch.h:79:10: error: ‘array’ in namespace ‘std’ does not name a template type
   79 |     std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id
      |          ^~~~~
/home/ggml/work/llama.cpp/src/llama-batch.h:6:1: note: ‘std::array’ is defined in header ‘<array>’; did you forget to ‘#include <array>’?
    5 | #include <vector>
  +++ |+#include <array>
    6 | 
/home/ggml/work/llama.cpp/src/llama-batch.h:79:49: error: extra ‘;’ [-Werror=pedantic]
   79 |     std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id
      |                                                 ^
      |                                                 -
/home/ggml/work/llama.cpp/src/llama-batch.cpp: In constructor ‘llama_batch_allocr::llama_batch_allocr(llama_batch, llama_pos)’:
/home/ggml/work/llama.cpp/src/llama-batch.cpp:289:27: error: ‘seq_id_0’ was not declared in this scope; did you mean ‘seq_id’?
  289 |             n_seq_id[i] = seq_id_0.size();
      |                           ^~~~~~~~
      |                           seq_id
/home/ggml/work/llama.cpp/src/llama-batch.cpp:297:25: error: ‘seq_id_0’ was not declared in this scope; did you mean ‘seq_id’?
  297 |             seq_id[i] = seq_id_0.data();
      |                         ^~~~~~~~
      |                         seq_id
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:118: src/CMakeFiles/llama.dir/llama-batch.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-batch.h:79:10: error: ‘array’ in namespace ‘std’ does not name a template type
   79 |     std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id
      |          ^~~~~
/home/ggml/work/llama.cpp/src/llama-batch.h:6:1: note: ‘std::array’ is defined in header ‘<array>’; did you forget to ‘#include <array>’?
    5 | #include <vector>
  +++ |+#include <array>
    6 | 
/home/ggml/work/llama.cpp/src/llama-batch.h:79:49: error: extra ‘;’ [-Werror=pedantic]
   79 |     std::array<llama_seq_id, 1> seq_id_0 = { 0 }; // default sequence id
      |                                                 ^
      |                                                 -
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_data_write_buffer::write(const void*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:605:9: error: ‘memcpy’ was not declared in this scope
  605 |         memcpy(ptr, src, size);
      |         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp:4:1: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    3 | #include <stdexcept>
  +++ |+#include <cstring>
    4 | 
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_data_read_buffer::read_to(void*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:645:9: error: ‘memcpy’ was not declared in this scope
  645 |         memcpy(dst, read(size), size);
      |         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp:645:9: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m9.491s
user	0m11.447s
sys	0m1.251s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/26/b5cfde6f137ff78c9ab8e94869dbf3b3632faf/ggml-1-arm64-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
