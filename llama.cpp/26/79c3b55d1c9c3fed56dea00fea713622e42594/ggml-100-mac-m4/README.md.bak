### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.13 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.49 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.26 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.71 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.50 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.27 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.94 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.93 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.53 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.68 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.38 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.39 sec*proc (29 tests)

Total Test time (real) = 164.40 sec

real	2m44.418s
user	4m37.017s
sys	0m5.686s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.86 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.86 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.43 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.44 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.04 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.30 sec*proc (29 tests)

Total Test time (real) =  48.31 sec

real	0m48.320s
user	0m54.673s
sys	0m5.196s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.112 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.634 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.928 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.936 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.938 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.940 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.941 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.942 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.946 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.947 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.948 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.948 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.949 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.952 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.953 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.954 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.962 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.963 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.964 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.965 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.763 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.765 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.766 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.766 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.767 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.767 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.768 I llama_model_loader: - type  f32:  124 tensors
0.00.027.768 I llama_model_loader: - type  f16:   73 tensors
0.00.027.769 I print_info: file format = GGUF V3 (latest)
0.00.027.770 I print_info: file type   = F16
0.00.027.771 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.835 I load: special tokens cache size = 5
0.00.034.017 I load: token to piece cache size = 0.2032 MB
0.00.034.021 I print_info: arch             = bert
0.00.034.021 I print_info: vocab_only       = 0
0.00.034.022 I print_info: n_ctx_train      = 512
0.00.034.022 I print_info: n_embd           = 384
0.00.034.022 I print_info: n_layer          = 12
0.00.034.026 I print_info: n_head           = 12
0.00.034.027 I print_info: n_head_kv        = 12
0.00.034.027 I print_info: n_rot            = 32
0.00.034.027 I print_info: n_swa            = 0
0.00.034.027 I print_info: n_embd_head_k    = 32
0.00.034.028 I print_info: n_embd_head_v    = 32
0.00.034.028 I print_info: n_gqa            = 1
0.00.034.029 I print_info: n_embd_k_gqa     = 384
0.00.034.030 I print_info: n_embd_v_gqa     = 384
0.00.034.031 I print_info: f_norm_eps       = 1.0e-12
0.00.034.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.032 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.032 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.032 I print_info: f_logit_scale    = 0.0e+00
0.00.034.035 I print_info: n_ff             = 1536
0.00.034.036 I print_info: n_expert         = 0
0.00.034.036 I print_info: n_expert_used    = 0
0.00.034.036 I print_info: causal attn      = 0
0.00.034.036 I print_info: pooling type     = 2
0.00.034.036 I print_info: rope type        = 2
0.00.034.037 I print_info: rope scaling     = linear
0.00.034.038 I print_info: freq_base_train  = 10000.0
0.00.034.039 I print_info: freq_scale_train = 1
0.00.034.039 I print_info: n_ctx_orig_yarn  = 512
0.00.034.039 I print_info: rope_finetuned   = unknown
0.00.034.039 I print_info: ssm_d_conv       = 0
0.00.034.039 I print_info: ssm_d_inner      = 0
0.00.034.040 I print_info: ssm_d_state      = 0
0.00.034.040 I print_info: ssm_dt_rank      = 0
0.00.034.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.041 I print_info: model type       = 33M
0.00.034.041 I print_info: model params     = 33.21 M
0.00.034.041 I print_info: general.name     = Bge Small
0.00.034.042 I print_info: vocab type       = WPM
0.00.034.042 I print_info: n_vocab          = 30522
0.00.034.043 I print_info: n_merges         = 0
0.00.034.043 I print_info: BOS token        = 101 '[CLS]'
0.00.034.043 I print_info: UNK token        = 100 '[UNK]'
0.00.034.043 I print_info: SEP token        = 102 '[SEP]'
0.00.034.044 I print_info: PAD token        = 0 '[PAD]'
0.00.034.044 I print_info: MASK token       = 103 '[MASK]'
0.00.034.045 I print_info: LF token         = 0 '[PAD]'
0.00.034.045 I print_info: max token length = 21
0.00.034.046 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.331 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.333 I load_tensors: offloading output layer to GPU
0.00.037.334 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.358 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.360 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.619 I llama_init_from_model: n_seq_max     = 1
0.00.037.621 I llama_init_from_model: n_ctx         = 512
0.00.037.621 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.621 I llama_init_from_model: n_batch       = 2048
0.00.037.622 I llama_init_from_model: n_ubatch      = 2048
0.00.037.622 I llama_init_from_model: flash_attn    = 0
0.00.037.623 I llama_init_from_model: freq_base     = 10000.0
0.00.037.623 I llama_init_from_model: freq_scale    = 1
0.00.037.624 I ggml_metal_init: allocating
0.00.037.628 I ggml_metal_init: found device: Apple M4
0.00.037.633 I ggml_metal_init: picking default device: Apple M4
0.00.038.368 I ggml_metal_init: using embedded metal library
0.00.042.458 I ggml_metal_init: GPU name:   Apple M4
0.00.042.460 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.461 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.461 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.462 I ggml_metal_init: simdgroup reduction   = true
0.00.042.462 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.462 I ggml_metal_init: has residency sets    = true
0.00.042.462 I ggml_metal_init: has bfloat            = true
0.00.042.462 I ggml_metal_init: use bfloat            = true
0.00.042.463 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.464 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.360 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.020 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.022 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.043 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.159 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.161 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.161 I llama_init_from_model: graph nodes  = 429
0.00.056.161 I llama_init_from_model: graph splits = 2
0.00.056.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.780 I 
0.00.061.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.473 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.270 I llama_perf_context_print:        load time =      45.14 ms
0.00.066.272 I llama_perf_context_print: prompt eval time =       3.67 ms /     9 tokens (    0.41 ms per token,  2455.66 tokens per second)
0.00.066.273 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.273 I llama_perf_context_print:       total time =       4.49 ms /    10 tokens
0.00.066.408 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.049s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.167 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.835 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.838 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.840 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.841 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.841 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.841 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.842 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.842 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.843 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.843 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.843 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.845 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.846 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.846 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.846 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.847 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.847 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.230 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.851 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.852 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.852 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.853 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.853 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.853 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.853 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.854 I llama_model_loader: - type  f32:  124 tensors
0.00.014.854 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.855 I print_info: file format = GGUF V3 (latest)
0.00.014.855 I print_info: file type   = Q8_0
0.00.014.856 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.239 I load: special tokens cache size = 5
0.00.018.554 I load: token to piece cache size = 0.2032 MB
0.00.018.558 I print_info: arch             = bert
0.00.018.558 I print_info: vocab_only       = 0
0.00.018.558 I print_info: n_ctx_train      = 512
0.00.018.559 I print_info: n_embd           = 384
0.00.018.559 I print_info: n_layer          = 12
0.00.018.562 I print_info: n_head           = 12
0.00.018.563 I print_info: n_head_kv        = 12
0.00.018.563 I print_info: n_rot            = 32
0.00.018.563 I print_info: n_swa            = 0
0.00.018.563 I print_info: n_embd_head_k    = 32
0.00.018.563 I print_info: n_embd_head_v    = 32
0.00.018.564 I print_info: n_gqa            = 1
0.00.018.566 I print_info: n_embd_k_gqa     = 384
0.00.018.566 I print_info: n_embd_v_gqa     = 384
0.00.018.567 I print_info: f_norm_eps       = 1.0e-12
0.00.018.568 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.570 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.570 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.570 I print_info: f_logit_scale    = 0.0e+00
0.00.018.571 I print_info: n_ff             = 1536
0.00.018.571 I print_info: n_expert         = 0
0.00.018.571 I print_info: n_expert_used    = 0
0.00.018.571 I print_info: causal attn      = 0
0.00.018.571 I print_info: pooling type     = 2
0.00.018.572 I print_info: rope type        = 2
0.00.018.572 I print_info: rope scaling     = linear
0.00.018.572 I print_info: freq_base_train  = 10000.0
0.00.018.572 I print_info: freq_scale_train = 1
0.00.018.573 I print_info: n_ctx_orig_yarn  = 512
0.00.018.573 I print_info: rope_finetuned   = unknown
0.00.018.573 I print_info: ssm_d_conv       = 0
0.00.018.573 I print_info: ssm_d_inner      = 0
0.00.018.573 I print_info: ssm_d_state      = 0
0.00.018.573 I print_info: ssm_dt_rank      = 0
0.00.018.573 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.574 I print_info: model type       = 33M
0.00.018.574 I print_info: model params     = 33.21 M
0.00.018.574 I print_info: general.name     = Bge Small
0.00.018.574 I print_info: vocab type       = WPM
0.00.018.575 I print_info: n_vocab          = 30522
0.00.018.575 I print_info: n_merges         = 0
0.00.018.575 I print_info: BOS token        = 101 '[CLS]'
0.00.018.575 I print_info: UNK token        = 100 '[UNK]'
0.00.018.575 I print_info: SEP token        = 102 '[SEP]'
0.00.018.575 I print_info: PAD token        = 0 '[PAD]'
0.00.018.575 I print_info: MASK token       = 103 '[MASK]'
0.00.018.576 I print_info: LF token         = 0 '[PAD]'
0.00.018.576 I print_info: max token length = 21
0.00.018.576 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.323 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.324 I load_tensors: offloading output layer to GPU
0.00.020.324 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.331 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.331 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.511 I llama_init_from_model: n_seq_max     = 1
0.00.020.512 I llama_init_from_model: n_ctx         = 512
0.00.020.512 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.512 I llama_init_from_model: n_batch       = 2048
0.00.020.512 I llama_init_from_model: n_ubatch      = 2048
0.00.020.512 I llama_init_from_model: flash_attn    = 0
0.00.020.513 I llama_init_from_model: freq_base     = 10000.0
0.00.020.513 I llama_init_from_model: freq_scale    = 1
0.00.020.513 I ggml_metal_init: allocating
0.00.020.518 I ggml_metal_init: found device: Apple M4
0.00.020.522 I ggml_metal_init: picking default device: Apple M4
0.00.021.040 I ggml_metal_init: using embedded metal library
0.00.023.553 I ggml_metal_init: GPU name:   Apple M4
0.00.023.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.555 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.556 I ggml_metal_init: simdgroup reduction   = true
0.00.023.556 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.556 I ggml_metal_init: has residency sets    = true
0.00.023.556 I ggml_metal_init: has bfloat            = true
0.00.023.556 I ggml_metal_init: use bfloat            = true
0.00.023.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.959 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.554 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.557 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.570 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.537 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.538 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.539 I llama_init_from_model: graph nodes  = 429
0.00.035.539 I llama_init_from_model: graph splits = 2
0.00.035.540 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.112 I 
0.00.039.134 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.656 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.050 I llama_perf_context_print:        load time =      29.94 ms
0.00.043.052 I llama_perf_context_print: prompt eval time =       3.26 ms /     9 tokens (    0.36 ms per token,  2759.89 tokens per second)
0.00.043.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.053 I llama_perf_context_print:       total time =       3.94 ms /    10 tokens
0.00.043.258 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.250 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.061 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.142 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.149 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.151 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.152 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.152 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.154 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.155 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.155 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.160 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.160 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.164 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.164 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.165 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.166 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.166 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.935 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.818 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.821 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.821 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.822 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.822 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.823 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.823 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.823 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.824 I llama_model_loader: - type  f32:   40 tensors
0.00.048.824 I llama_model_loader: - type  f16:   30 tensors
0.00.048.830 I print_info: file format = GGUF V3 (latest)
0.00.048.831 I print_info: file type   = F16
0.00.048.835 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.085 W load: empty token at index 5
0.00.058.168 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.692 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.729 I load: special tokens cache size = 5
0.00.319.613 I load: token to piece cache size = 1.5060 MB
0.00.319.625 I print_info: arch             = jina-bert-v2
0.00.319.625 I print_info: vocab_only       = 0
0.00.319.626 I print_info: n_ctx_train      = 8192
0.00.319.626 I print_info: n_embd           = 384
0.00.319.626 I print_info: n_layer          = 4
0.00.319.632 I print_info: n_head           = 12
0.00.319.632 I print_info: n_head_kv        = 12
0.00.319.633 I print_info: n_rot            = 32
0.00.319.633 I print_info: n_swa            = 0
0.00.319.633 I print_info: n_embd_head_k    = 32
0.00.319.633 I print_info: n_embd_head_v    = 32
0.00.319.633 I print_info: n_gqa            = 1
0.00.319.634 I print_info: n_embd_k_gqa     = 384
0.00.319.634 I print_info: n_embd_v_gqa     = 384
0.00.319.635 I print_info: f_norm_eps       = 1.0e-12
0.00.319.635 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.319.635 I print_info: f_clamp_kqv      = 0.0e+00
0.00.319.635 I print_info: f_max_alibi_bias = 8.0e+00
0.00.319.636 I print_info: f_logit_scale    = 0.0e+00
0.00.319.636 I print_info: n_ff             = 1536
0.00.319.636 I print_info: n_expert         = 0
0.00.319.636 I print_info: n_expert_used    = 0
0.00.319.636 I print_info: causal attn      = 0
0.00.319.636 I print_info: pooling type     = -1
0.00.319.637 I print_info: rope type        = -1
0.00.319.637 I print_info: rope scaling     = linear
0.00.319.637 I print_info: freq_base_train  = 10000.0
0.00.319.637 I print_info: freq_scale_train = 1
0.00.319.637 I print_info: n_ctx_orig_yarn  = 8192
0.00.319.638 I print_info: rope_finetuned   = unknown
0.00.319.638 I print_info: ssm_d_conv       = 0
0.00.319.638 I print_info: ssm_d_inner      = 0
0.00.319.638 I print_info: ssm_d_state      = 0
0.00.319.638 I print_info: ssm_dt_rank      = 0
0.00.319.638 I print_info: ssm_dt_b_c_rms   = 0
0.00.319.639 I print_info: model type       = 33M
0.00.319.639 I print_info: model params     = 32.90 M
0.00.319.639 I print_info: general.name     = Jina Bert Implementation
0.00.319.641 I print_info: vocab type       = BPE
0.00.319.641 I print_info: n_vocab          = 61056
0.00.319.641 I print_info: n_merges         = 39382
0.00.319.641 I print_info: BOS token        = 0 '<s>'
0.00.319.641 I print_info: EOS token        = 2 '</s>'
0.00.319.645 I print_info: UNK token        = 3 '<unk>'
0.00.319.645 I print_info: SEP token        = 2 '</s>'
0.00.319.645 I print_info: PAD token        = 1 '<pad>'
0.00.319.645 I print_info: MASK token       = 4 '<mask>'
0.00.319.646 I print_info: EOG token        = 2 '</s>'
0.00.319.646 I print_info: max token length = 45
0.00.319.647 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.865 I load_tensors: offloading 4 repeating layers to GPU
0.00.321.866 I load_tensors: offloading output layer to GPU
0.00.321.866 I load_tensors: offloaded 5/5 layers to GPU
0.00.321.892 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.321.894 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.322.271 I llama_init_from_model: n_seq_max     = 1
0.00.322.272 I llama_init_from_model: n_ctx         = 8192
0.00.322.272 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.322.272 I llama_init_from_model: n_batch       = 2048
0.00.322.273 I llama_init_from_model: n_ubatch      = 2048
0.00.322.273 I llama_init_from_model: flash_attn    = 0
0.00.322.273 I llama_init_from_model: freq_base     = 10000.0
0.00.322.274 I llama_init_from_model: freq_scale    = 1
0.00.322.275 I ggml_metal_init: allocating
0.00.322.289 I ggml_metal_init: found device: Apple M4
0.00.322.296 I ggml_metal_init: picking default device: Apple M4
0.00.323.221 I ggml_metal_init: using embedded metal library
0.00.325.853 I ggml_metal_init: GPU name:   Apple M4
0.00.325.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.325.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.325.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.325.856 I ggml_metal_init: simdgroup reduction   = true
0.00.325.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.325.856 I ggml_metal_init: has residency sets    = true
0.00.325.856 I ggml_metal_init: has bfloat            = true
0.00.325.856 I ggml_metal_init: use bfloat            = true
0.00.325.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.325.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.335.638 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.338.793 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.338.798 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.338.820 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.345.093 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.345.095 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.345.095 I llama_init_from_model: graph nodes  = 154
0.00.345.095 I llama_init_from_model: graph splits = 2
0.00.345.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.345.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.352.508 I 
0.00.352.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.352.635 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.352.635 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.352.638 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.352.638 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.352.645 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.352.645 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.353.162 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.356.945 I llama_perf_context_print:        load time =     331.44 ms
0.00.356.946 I llama_perf_context_print: prompt eval time =       3.77 ms /    62 tokens (    0.06 ms per token, 16428.19 tokens per second)
0.00.356.947 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.356.947 I llama_perf_context_print:       total time =       4.44 ms /    63 tokens
0.00.357.153 I ggml_metal_free: deallocating

real	0m1.045s
user	0m0.328s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.159 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.350 I main: llama backend init
0.00.000.355 I main: load the model and apply lora adapter, if any
0.00.102.535 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.114.931 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.114.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.114.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.114.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.114.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.114.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.114.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.114.958 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.114.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.114.959 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.114.960 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.114.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.114.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.114.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.114.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.114.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.114.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.121.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.123.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.130.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.130.695 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.130.695 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.130.696 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.130.697 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.130.698 I llama_model_loader: - type  f32:  194 tensors
0.00.130.699 I llama_model_loader: - type  f16:   98 tensors
0.00.130.700 I print_info: file format = GGUF V3 (latest)
0.00.130.708 I print_info: file type   = all F32 (guessed)
0.00.130.711 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.149.272 I load: special tokens cache size = 25
0.00.159.821 I load: token to piece cache size = 0.2984 MB
0.00.159.827 I print_info: arch             = gptneox
0.00.159.827 I print_info: vocab_only       = 0
0.00.159.827 I print_info: n_ctx_train      = 2048
0.00.159.827 I print_info: n_embd           = 2048
0.00.159.828 I print_info: n_layer          = 24
0.00.159.836 I print_info: n_head           = 16
0.00.159.838 I print_info: n_head_kv        = 16
0.00.159.838 I print_info: n_rot            = 32
0.00.159.838 I print_info: n_swa            = 0
0.00.159.838 I print_info: n_embd_head_k    = 128
0.00.159.840 I print_info: n_embd_head_v    = 128
0.00.159.841 I print_info: n_gqa            = 1
0.00.159.843 I print_info: n_embd_k_gqa     = 2048
0.00.159.844 I print_info: n_embd_v_gqa     = 2048
0.00.159.845 I print_info: f_norm_eps       = 1.0e-05
0.00.159.845 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.159.846 I print_info: f_clamp_kqv      = 0.0e+00
0.00.159.846 I print_info: f_max_alibi_bias = 0.0e+00
0.00.159.848 I print_info: f_logit_scale    = 0.0e+00
0.00.159.849 I print_info: n_ff             = 8192
0.00.159.849 I print_info: n_expert         = 0
0.00.159.850 I print_info: n_expert_used    = 0
0.00.159.850 I print_info: causal attn      = 1
0.00.159.850 I print_info: pooling type     = 0
0.00.159.850 I print_info: rope type        = 2
0.00.159.850 I print_info: rope scaling     = linear
0.00.159.851 I print_info: freq_base_train  = 10000.0
0.00.159.851 I print_info: freq_scale_train = 1
0.00.159.852 I print_info: n_ctx_orig_yarn  = 2048
0.00.159.852 I print_info: rope_finetuned   = unknown
0.00.159.852 I print_info: ssm_d_conv       = 0
0.00.159.852 I print_info: ssm_d_inner      = 0
0.00.159.852 I print_info: ssm_d_state      = 0
0.00.159.853 I print_info: ssm_dt_rank      = 0
0.00.159.853 I print_info: ssm_dt_b_c_rms   = 0
0.00.159.853 I print_info: model type       = 1.4B
0.00.159.853 I print_info: model params     = 1.41 B
0.00.159.854 I print_info: general.name     = 1.4B
0.00.159.860 I print_info: vocab type       = BPE
0.00.159.860 I print_info: n_vocab          = 50304
0.00.159.860 I print_info: n_merges         = 50009
0.00.159.861 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.159.861 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.159.861 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.159.861 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.159.862 I print_info: LF token         = 187 ''
0.00.159.862 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.159.862 I print_info: max token length = 1024
0.00.159.863 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.273.055 I load_tensors: offloading 24 repeating layers to GPU
0.00.273.058 I load_tensors: offloading output layer to GPU
0.00.273.059 I load_tensors: offloaded 25/25 layers to GPU
0.00.273.084 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.273.085 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.273.733 I llama_init_from_model: n_seq_max     = 1
0.00.273.734 I llama_init_from_model: n_ctx         = 2048
0.00.273.734 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.273.734 I llama_init_from_model: n_batch       = 2048
0.00.273.734 I llama_init_from_model: n_ubatch      = 512
0.00.273.735 I llama_init_from_model: flash_attn    = 0
0.00.273.735 I llama_init_from_model: freq_base     = 10000.0
0.00.273.735 I llama_init_from_model: freq_scale    = 1
0.00.273.736 I ggml_metal_init: allocating
0.00.273.774 I ggml_metal_init: found device: Apple M4
0.00.273.779 I ggml_metal_init: picking default device: Apple M4
0.00.274.456 I ggml_metal_init: using embedded metal library
0.00.284.254 I ggml_metal_init: GPU name:   Apple M4
0.00.284.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.284.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.284.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.284.257 I ggml_metal_init: simdgroup reduction   = true
0.00.284.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.284.257 I ggml_metal_init: has residency sets    = true
0.00.284.257 I ggml_metal_init: has bfloat            = true
0.00.284.257 I ggml_metal_init: use bfloat            = true
0.00.284.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.284.259 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.312.987 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.341.917 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.341.924 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.341.967 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.345.791 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.345.794 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.345.794 I llama_init_from_model: graph nodes  = 967
0.00.345.794 I llama_init_from_model: graph splits = 2
0.00.345.802 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.345.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.345.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.413.181 I main: llama threadpool init, n_threads = 4
0.00.413.233 I 
0.00.413.263 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.413.264 I 
0.00.413.447 I sampler seed: 1234
0.00.413.452 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.413.476 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.413.477 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.413.477 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.239.346 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.02.239.347 I llama_perf_context_print:        load time =     309.77 ms
0.02.239.347 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.49 tokens per second)
0.02.239.348 I llama_perf_context_print:        eval time =    1779.38 ms /    63 runs   (   28.24 ms per token,    35.41 tokens per second)
0.02.239.349 I llama_perf_context_print:       total time =    1827.03 ms /    70 tokens
0.02.239.561 I ggml_metal_free: deallocating

real	0m2.560s
user	0m0.134s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.566 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.113 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.852 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.886 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.887 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.887 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.900 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.903 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.257 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.258 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.258 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.259 I llama_model_loader: - type  f32:  194 tensors
0.00.056.259 I llama_model_loader: - type  f16:   98 tensors
0.00.056.260 I print_info: file format = GGUF V3 (latest)
0.00.056.261 I print_info: file type   = all F32 (guessed)
0.00.056.263 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.371 I load: special tokens cache size = 25
0.00.077.554 I load: token to piece cache size = 0.2984 MB
0.00.077.558 I print_info: arch             = gptneox
0.00.077.558 I print_info: vocab_only       = 0
0.00.077.558 I print_info: n_ctx_train      = 2048
0.00.077.558 I print_info: n_embd           = 2048
0.00.077.558 I print_info: n_layer          = 24
0.00.077.562 I print_info: n_head           = 16
0.00.077.563 I print_info: n_head_kv        = 16
0.00.077.563 I print_info: n_rot            = 32
0.00.077.563 I print_info: n_swa            = 0
0.00.077.563 I print_info: n_embd_head_k    = 128
0.00.077.563 I print_info: n_embd_head_v    = 128
0.00.077.564 I print_info: n_gqa            = 1
0.00.077.565 I print_info: n_embd_k_gqa     = 2048
0.00.077.566 I print_info: n_embd_v_gqa     = 2048
0.00.077.566 I print_info: f_norm_eps       = 1.0e-05
0.00.077.567 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.567 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.567 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.567 I print_info: f_logit_scale    = 0.0e+00
0.00.077.568 I print_info: n_ff             = 8192
0.00.077.568 I print_info: n_expert         = 0
0.00.077.568 I print_info: n_expert_used    = 0
0.00.077.568 I print_info: causal attn      = 1
0.00.077.568 I print_info: pooling type     = 0
0.00.077.568 I print_info: rope type        = 2
0.00.077.569 I print_info: rope scaling     = linear
0.00.077.571 I print_info: freq_base_train  = 10000.0
0.00.077.571 I print_info: freq_scale_train = 1
0.00.077.571 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.571 I print_info: rope_finetuned   = unknown
0.00.077.572 I print_info: ssm_d_conv       = 0
0.00.077.572 I print_info: ssm_d_inner      = 0
0.00.077.572 I print_info: ssm_d_state      = 0
0.00.077.572 I print_info: ssm_dt_rank      = 0
0.00.077.572 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.572 I print_info: model type       = 1.4B
0.00.077.573 I print_info: model params     = 1.41 B
0.00.077.573 I print_info: general.name     = 1.4B
0.00.077.573 I print_info: vocab type       = BPE
0.00.077.574 I print_info: n_vocab          = 50304
0.00.077.574 I print_info: n_merges         = 50009
0.00.077.574 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.574 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.574 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.575 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.575 I print_info: LF token         = 187 ''
0.00.077.575 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.575 I print_info: max token length = 1024
0.00.077.576 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.419.015 I load_tensors: offloading 24 repeating layers to GPU
0.01.419.019 I load_tensors: offloading output layer to GPU
0.01.419.019 I load_tensors: offloaded 25/25 layers to GPU
0.01.419.046 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.419.048 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.419.775 I llama_init_from_model: n_seq_max     = 1
0.01.419.776 I llama_init_from_model: n_ctx         = 128
0.01.419.777 I llama_init_from_model: n_ctx_per_seq = 128
0.01.419.777 I llama_init_from_model: n_batch       = 128
0.01.419.777 I llama_init_from_model: n_ubatch      = 128
0.01.419.777 I llama_init_from_model: flash_attn    = 0
0.01.419.778 I llama_init_from_model: freq_base     = 10000.0
0.01.419.778 I llama_init_from_model: freq_scale    = 1
0.01.419.778 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.419.779 I ggml_metal_init: allocating
0.01.419.861 I ggml_metal_init: found device: Apple M4
0.01.419.871 I ggml_metal_init: picking default device: Apple M4
0.01.421.060 I ggml_metal_init: using embedded metal library
0.01.425.033 I ggml_metal_init: GPU name:   Apple M4
0.01.425.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.425.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.425.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.425.036 I ggml_metal_init: simdgroup reduction   = true
0.01.425.036 I ggml_metal_init: simdgroup matrix mul. = true
0.01.425.036 I ggml_metal_init: has residency sets    = true
0.01.425.036 I ggml_metal_init: has bfloat            = true
0.01.425.036 I ggml_metal_init: use bfloat            = true
0.01.425.037 I ggml_metal_init: hasUnifiedMemory      = true
0.01.425.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.436.477 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.438.281 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.438.283 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.438.308 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.440.033 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.440.034 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.440.034 I llama_init_from_model: graph nodes  = 967
0.01.440.035 I llama_init_from_model: graph splits = 2
0.01.440.036 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.440.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.476.229 I 
0.01.476.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.476.288 I perplexity: tokenizing the input ..
0.01.481.524 I perplexity: tokenization took 5.234 ms
0.01.481.545 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.613.198 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.614.544 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.614.561 I llama_perf_context_print:        load time =    1452.11 ms
0.01.614.562 I llama_perf_context_print: prompt eval time =     131.34 ms /   128 tokens (    1.03 ms per token,   974.53 tokens per second)
0.01.614.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.614.566 I llama_perf_context_print:       total time =     138.33 ms /   129 tokens
0.01.614.986 I ggml_metal_free: deallocating

real	0m1.805s
user	0m0.099s
sys	0m0.268s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.139 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.032.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.740 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.742 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.746 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.042.327 I llama_model_loader: - type  f32:  194 tensors
0.00.042.328 I llama_model_loader: - type q8_0:   98 tensors
0.00.042.328 I print_info: file format = GGUF V3 (latest)
0.00.042.331 I print_info: file type   = Q8_0
0.00.042.332 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.052.044 I load: special tokens cache size = 25
0.00.059.381 I load: token to piece cache size = 0.2984 MB
0.00.059.385 I print_info: arch             = gptneox
0.00.059.385 I print_info: vocab_only       = 0
0.00.059.386 I print_info: n_ctx_train      = 2048
0.00.059.386 I print_info: n_embd           = 2048
0.00.059.386 I print_info: n_layer          = 24
0.00.059.392 I print_info: n_head           = 16
0.00.059.392 I print_info: n_head_kv        = 16
0.00.059.394 I print_info: n_rot            = 32
0.00.059.394 I print_info: n_swa            = 0
0.00.059.394 I print_info: n_embd_head_k    = 128
0.00.059.394 I print_info: n_embd_head_v    = 128
0.00.059.395 I print_info: n_gqa            = 1
0.00.059.396 I print_info: n_embd_k_gqa     = 2048
0.00.059.397 I print_info: n_embd_v_gqa     = 2048
0.00.059.397 I print_info: f_norm_eps       = 1.0e-05
0.00.059.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.398 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.398 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.401 I print_info: f_logit_scale    = 0.0e+00
0.00.059.401 I print_info: n_ff             = 8192
0.00.059.402 I print_info: n_expert         = 0
0.00.059.402 I print_info: n_expert_used    = 0
0.00.059.402 I print_info: causal attn      = 1
0.00.059.402 I print_info: pooling type     = 0
0.00.059.402 I print_info: rope type        = 2
0.00.059.402 I print_info: rope scaling     = linear
0.00.059.403 I print_info: freq_base_train  = 10000.0
0.00.059.403 I print_info: freq_scale_train = 1
0.00.059.403 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.403 I print_info: rope_finetuned   = unknown
0.00.059.403 I print_info: ssm_d_conv       = 0
0.00.059.404 I print_info: ssm_d_inner      = 0
0.00.059.404 I print_info: ssm_d_state      = 0
0.00.059.404 I print_info: ssm_dt_rank      = 0
0.00.059.404 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.404 I print_info: model type       = 1.4B
0.00.059.404 I print_info: model params     = 1.41 B
0.00.059.405 I print_info: general.name     = 1.4B
0.00.059.406 I print_info: vocab type       = BPE
0.00.059.406 I print_info: n_vocab          = 50304
0.00.059.406 I print_info: n_merges         = 50009
0.00.059.406 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.411 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.411 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.411 I print_info: LF token         = 187 ''
0.00.059.411 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.412 I print_info: max token length = 1024
0.00.059.413 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.137.689 I load_tensors: offloading 24 repeating layers to GPU
0.01.137.694 I load_tensors: offloading output layer to GPU
0.01.137.695 I load_tensors: offloaded 25/25 layers to GPU
0.01.137.717 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.137.718 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.138.773 I llama_init_from_model: n_seq_max     = 1
0.01.138.775 I llama_init_from_model: n_ctx         = 2048
0.01.138.775 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.138.775 I llama_init_from_model: n_batch       = 2048
0.01.138.776 I llama_init_from_model: n_ubatch      = 512
0.01.138.776 I llama_init_from_model: flash_attn    = 0
0.01.138.777 I llama_init_from_model: freq_base     = 10000.0
0.01.138.778 I llama_init_from_model: freq_scale    = 1
0.01.138.779 I ggml_metal_init: allocating
0.01.138.795 I ggml_metal_init: found device: Apple M4
0.01.138.805 I ggml_metal_init: picking default device: Apple M4
0.01.140.132 I ggml_metal_init: using embedded metal library
0.01.146.042 I ggml_metal_init: GPU name:   Apple M4
0.01.146.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.146.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.146.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.146.047 I ggml_metal_init: simdgroup reduction   = true
0.01.146.047 I ggml_metal_init: simdgroup matrix mul. = true
0.01.146.048 I ggml_metal_init: has residency sets    = true
0.01.146.048 I ggml_metal_init: has bfloat            = true
0.01.146.048 I ggml_metal_init: use bfloat            = true
0.01.146.049 I ggml_metal_init: hasUnifiedMemory      = true
0.01.146.051 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.188.981 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.249.389 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.249.402 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.249.432 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.253.432 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.253.434 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.253.434 I llama_init_from_model: graph nodes  = 967
0.01.253.435 I llama_init_from_model: graph splits = 2
0.01.253.439 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.253.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.253.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.307.164 I main: llama threadpool init, n_threads = 4
0.01.307.207 I 
0.01.307.231 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.307.232 I 
0.01.307.406 I sampler seed: 1234
0.01.307.411 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.307.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.307.422 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.307.422 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.402.597 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.02.402.598 I llama_perf_context_print:        load time =    1296.30 ms
0.02.402.599 I llama_perf_context_print: prompt eval time =      48.91 ms /     7 tokens (    6.99 ms per token,   143.12 tokens per second)
0.02.402.599 I llama_perf_context_print:        eval time =    1043.30 ms /    63 runs   (   16.56 ms per token,    60.39 tokens per second)
0.02.402.600 I llama_perf_context_print:       total time =    1096.16 ms /    70 tokens
0.02.402.824 I ggml_metal_free: deallocating

real	0m2.422s
user	0m0.116s
sys	0m0.274s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.288 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.796 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.602 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.610 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.610 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.328 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.332 I llama_model_loader: - type  f32:  194 tensors
0.00.026.332 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.333 I print_info: file format = GGUF V3 (latest)
0.00.026.337 I print_info: file type   = Q8_0
0.00.026.339 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.711 I load: special tokens cache size = 25
0.00.040.883 I load: token to piece cache size = 0.2984 MB
0.00.040.888 I print_info: arch             = gptneox
0.00.040.888 I print_info: vocab_only       = 0
0.00.040.888 I print_info: n_ctx_train      = 2048
0.00.040.888 I print_info: n_embd           = 2048
0.00.040.889 I print_info: n_layer          = 24
0.00.040.893 I print_info: n_head           = 16
0.00.040.894 I print_info: n_head_kv        = 16
0.00.040.894 I print_info: n_rot            = 32
0.00.040.894 I print_info: n_swa            = 0
0.00.040.894 I print_info: n_embd_head_k    = 128
0.00.040.894 I print_info: n_embd_head_v    = 128
0.00.040.895 I print_info: n_gqa            = 1
0.00.040.896 I print_info: n_embd_k_gqa     = 2048
0.00.040.897 I print_info: n_embd_v_gqa     = 2048
0.00.040.897 I print_info: f_norm_eps       = 1.0e-05
0.00.040.900 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.901 I print_info: f_logit_scale    = 0.0e+00
0.00.040.901 I print_info: n_ff             = 8192
0.00.040.902 I print_info: n_expert         = 0
0.00.040.902 I print_info: n_expert_used    = 0
0.00.040.902 I print_info: causal attn      = 1
0.00.040.902 I print_info: pooling type     = 0
0.00.040.902 I print_info: rope type        = 2
0.00.040.902 I print_info: rope scaling     = linear
0.00.040.903 I print_info: freq_base_train  = 10000.0
0.00.040.903 I print_info: freq_scale_train = 1
0.00.040.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.903 I print_info: rope_finetuned   = unknown
0.00.040.905 I print_info: ssm_d_conv       = 0
0.00.040.905 I print_info: ssm_d_inner      = 0
0.00.040.905 I print_info: ssm_d_state      = 0
0.00.040.905 I print_info: ssm_dt_rank      = 0
0.00.040.905 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.906 I print_info: model type       = 1.4B
0.00.040.912 I print_info: model params     = 1.41 B
0.00.040.912 I print_info: general.name     = 1.4B
0.00.040.913 I print_info: vocab type       = BPE
0.00.040.913 I print_info: n_vocab          = 50304
0.00.040.913 I print_info: n_merges         = 50009
0.00.040.914 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.914 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.914 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.914 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.914 I print_info: LF token         = 187 ''
0.00.040.915 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.915 I print_info: max token length = 1024
0.00.040.916 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.899.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.899.306 I load_tensors: offloading output layer to GPU
0.00.899.306 I load_tensors: offloaded 25/25 layers to GPU
0.00.899.335 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.899.337 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.900.716 I llama_init_from_model: n_seq_max     = 1
0.00.900.718 I llama_init_from_model: n_ctx         = 128
0.00.900.719 I llama_init_from_model: n_ctx_per_seq = 128
0.00.900.719 I llama_init_from_model: n_batch       = 128
0.00.900.719 I llama_init_from_model: n_ubatch      = 128
0.00.900.719 I llama_init_from_model: flash_attn    = 0
0.00.900.720 I llama_init_from_model: freq_base     = 10000.0
0.00.900.721 I llama_init_from_model: freq_scale    = 1
0.00.900.722 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.900.723 I ggml_metal_init: allocating
0.00.900.782 I ggml_metal_init: found device: Apple M4
0.00.900.792 I ggml_metal_init: picking default device: Apple M4
0.00.902.052 I ggml_metal_init: using embedded metal library
0.00.907.540 I ggml_metal_init: GPU name:   Apple M4
0.00.907.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.907.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.907.544 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.907.545 I ggml_metal_init: simdgroup reduction   = true
0.00.907.545 I ggml_metal_init: simdgroup matrix mul. = true
0.00.907.545 I ggml_metal_init: has residency sets    = true
0.00.907.545 I ggml_metal_init: has bfloat            = true
0.00.907.546 I ggml_metal_init: use bfloat            = true
0.00.907.546 I ggml_metal_init: hasUnifiedMemory      = true
0.00.907.548 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.922.713 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.926.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.926.129 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.926.168 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.929.098 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.929.099 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.929.100 I llama_init_from_model: graph nodes  = 967
0.00.929.100 I llama_init_from_model: graph splits = 2
0.00.929.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.929.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.956.556 I 
0.00.956.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.956.653 I perplexity: tokenizing the input ..
0.00.964.128 I perplexity: tokenization took 7.47 ms
0.00.964.156 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.103.217 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.104.566 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.104.581 I llama_perf_context_print:        load time =     945.75 ms
0.01.104.582 I llama_perf_context_print: prompt eval time =     138.11 ms /   128 tokens (    1.08 ms per token,   926.81 tokens per second)
0.01.104.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.104.583 I llama_perf_context_print:       total time =     148.03 ms /   129 tokens
0.01.104.972 I ggml_metal_free: deallocating

real	0m1.122s
user	0m0.077s
sys	0m0.174s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.016.245 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.766 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.768 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.769 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.769 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.770 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.771 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.771 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.772 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.775 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.611 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.148 I llama_model_loader: - type  f32:  194 tensors
0.00.047.148 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.149 I print_info: file format = GGUF V3 (latest)
0.00.047.150 I print_info: file type   = Q4_0
0.00.047.151 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.058.397 I load: special tokens cache size = 25
0.00.066.183 I load: token to piece cache size = 0.2984 MB
0.00.066.189 I print_info: arch             = gptneox
0.00.066.190 I print_info: vocab_only       = 0
0.00.066.190 I print_info: n_ctx_train      = 2048
0.00.066.190 I print_info: n_embd           = 2048
0.00.066.191 I print_info: n_layer          = 24
0.00.066.197 I print_info: n_head           = 16
0.00.066.198 I print_info: n_head_kv        = 16
0.00.066.198 I print_info: n_rot            = 32
0.00.066.199 I print_info: n_swa            = 0
0.00.066.199 I print_info: n_embd_head_k    = 128
0.00.066.199 I print_info: n_embd_head_v    = 128
0.00.066.200 I print_info: n_gqa            = 1
0.00.066.201 I print_info: n_embd_k_gqa     = 2048
0.00.066.202 I print_info: n_embd_v_gqa     = 2048
0.00.066.203 I print_info: f_norm_eps       = 1.0e-05
0.00.066.206 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.206 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.206 I print_info: f_logit_scale    = 0.0e+00
0.00.066.207 I print_info: n_ff             = 8192
0.00.066.207 I print_info: n_expert         = 0
0.00.066.208 I print_info: n_expert_used    = 0
0.00.066.208 I print_info: causal attn      = 1
0.00.066.208 I print_info: pooling type     = 0
0.00.066.208 I print_info: rope type        = 2
0.00.066.209 I print_info: rope scaling     = linear
0.00.066.209 I print_info: freq_base_train  = 10000.0
0.00.066.210 I print_info: freq_scale_train = 1
0.00.066.210 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.210 I print_info: rope_finetuned   = unknown
0.00.066.210 I print_info: ssm_d_conv       = 0
0.00.066.210 I print_info: ssm_d_inner      = 0
0.00.066.211 I print_info: ssm_d_state      = 0
0.00.066.211 I print_info: ssm_dt_rank      = 0
0.00.066.211 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.214 I print_info: model type       = 1.4B
0.00.066.214 I print_info: model params     = 1.41 B
0.00.066.221 I print_info: general.name     = 1.4B
0.00.066.221 I print_info: vocab type       = BPE
0.00.066.222 I print_info: n_vocab          = 50304
0.00.066.224 I print_info: n_merges         = 50009
0.00.066.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.224 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.224 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.225 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.225 I print_info: LF token         = 187 ''
0.00.066.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.226 I print_info: max token length = 1024
0.00.066.227 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.677.957 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.973 I load_tensors: offloading output layer to GPU
0.00.677.974 I load_tensors: offloaded 25/25 layers to GPU
0.00.678.011 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.678.012 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.679.478 I llama_init_from_model: n_seq_max     = 1
0.00.679.481 I llama_init_from_model: n_ctx         = 2048
0.00.679.482 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.679.483 I llama_init_from_model: n_batch       = 2048
0.00.679.483 I llama_init_from_model: n_ubatch      = 512
0.00.679.484 I llama_init_from_model: flash_attn    = 0
0.00.679.486 I llama_init_from_model: freq_base     = 10000.0
0.00.679.487 I llama_init_from_model: freq_scale    = 1
0.00.679.489 I ggml_metal_init: allocating
0.00.679.614 I ggml_metal_init: found device: Apple M4
0.00.679.629 I ggml_metal_init: picking default device: Apple M4
0.00.681.524 I ggml_metal_init: using embedded metal library
0.00.688.353 I ggml_metal_init: GPU name:   Apple M4
0.00.688.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.363 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.364 I ggml_metal_init: simdgroup reduction   = true
0.00.688.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.365 I ggml_metal_init: has residency sets    = true
0.00.688.365 I ggml_metal_init: has bfloat            = true
0.00.688.365 I ggml_metal_init: use bfloat            = true
0.00.688.366 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.707.053 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.765.841 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.765.848 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.765.881 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.770.278 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.770.280 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.770.281 I llama_init_from_model: graph nodes  = 967
0.00.770.281 I llama_init_from_model: graph splits = 2
0.00.770.287 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.770.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.770.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.203 I main: llama threadpool init, n_threads = 4
0.00.821.244 I 
0.00.821.264 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.264 I 
0.00.821.393 I sampler seed: 1234
0.00.821.397 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.438 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.513.681 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.513.681 I llama_perf_context_print:        load time =     804.26 ms
0.01.513.682 I llama_perf_context_print: prompt eval time =      49.63 ms /     7 tokens (    7.09 ms per token,   141.04 tokens per second)
0.01.513.683 I llama_perf_context_print:        eval time =     639.79 ms /    63 runs   (   10.16 ms per token,    98.47 tokens per second)
0.01.513.683 I llama_perf_context_print:       total time =     693.18 ms /    70 tokens
0.01.513.936 I ggml_metal_free: deallocating

real	0m1.536s
user	0m0.121s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.415 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.057 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.318 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.320 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.320 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.323 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.324 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.170 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.185 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.989 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.990 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.990 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.990 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.991 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.991 I llama_model_loader: - type  f32:  194 tensors
0.00.025.992 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.992 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.993 I print_info: file format = GGUF V3 (latest)
0.00.025.993 I print_info: file type   = Q4_0
0.00.025.996 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.153 I load: special tokens cache size = 25
0.00.040.239 I load: token to piece cache size = 0.2984 MB
0.00.040.243 I print_info: arch             = gptneox
0.00.040.244 I print_info: vocab_only       = 0
0.00.040.244 I print_info: n_ctx_train      = 2048
0.00.040.244 I print_info: n_embd           = 2048
0.00.040.244 I print_info: n_layer          = 24
0.00.040.249 I print_info: n_head           = 16
0.00.040.250 I print_info: n_head_kv        = 16
0.00.040.250 I print_info: n_rot            = 32
0.00.040.250 I print_info: n_swa            = 0
0.00.040.250 I print_info: n_embd_head_k    = 128
0.00.040.250 I print_info: n_embd_head_v    = 128
0.00.040.251 I print_info: n_gqa            = 1
0.00.040.252 I print_info: n_embd_k_gqa     = 2048
0.00.040.253 I print_info: n_embd_v_gqa     = 2048
0.00.040.253 I print_info: f_norm_eps       = 1.0e-05
0.00.040.254 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.254 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.254 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.254 I print_info: f_logit_scale    = 0.0e+00
0.00.040.254 I print_info: n_ff             = 8192
0.00.040.255 I print_info: n_expert         = 0
0.00.040.255 I print_info: n_expert_used    = 0
0.00.040.255 I print_info: causal attn      = 1
0.00.040.255 I print_info: pooling type     = 0
0.00.040.255 I print_info: rope type        = 2
0.00.040.255 I print_info: rope scaling     = linear
0.00.040.256 I print_info: freq_base_train  = 10000.0
0.00.040.256 I print_info: freq_scale_train = 1
0.00.040.256 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.256 I print_info: rope_finetuned   = unknown
0.00.040.257 I print_info: ssm_d_conv       = 0
0.00.040.257 I print_info: ssm_d_inner      = 0
0.00.040.257 I print_info: ssm_d_state      = 0
0.00.040.257 I print_info: ssm_dt_rank      = 0
0.00.040.257 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.257 I print_info: model type       = 1.4B
0.00.040.258 I print_info: model params     = 1.41 B
0.00.040.261 I print_info: general.name     = 1.4B
0.00.040.261 I print_info: vocab type       = BPE
0.00.040.261 I print_info: n_vocab          = 50304
0.00.040.261 I print_info: n_merges         = 50009
0.00.040.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: LF token         = 187 ''
0.00.040.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.263 I print_info: max token length = 1024
0.00.040.263 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.131 I load_tensors: offloading output layer to GPU
0.00.599.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.163 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.164 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.600.824 I llama_init_from_model: n_seq_max     = 1
0.00.600.830 I llama_init_from_model: n_ctx         = 128
0.00.600.831 I llama_init_from_model: n_ctx_per_seq = 128
0.00.600.831 I llama_init_from_model: n_batch       = 128
0.00.600.831 I llama_init_from_model: n_ubatch      = 128
0.00.600.832 I llama_init_from_model: flash_attn    = 0
0.00.600.833 I llama_init_from_model: freq_base     = 10000.0
0.00.600.834 I llama_init_from_model: freq_scale    = 1
0.00.600.834 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.836 I ggml_metal_init: allocating
0.00.600.913 I ggml_metal_init: found device: Apple M4
0.00.600.926 I ggml_metal_init: picking default device: Apple M4
0.00.602.718 I ggml_metal_init: using embedded metal library
0.00.609.035 I ggml_metal_init: GPU name:   Apple M4
0.00.609.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.048 I ggml_metal_init: simdgroup reduction   = true
0.00.609.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.048 I ggml_metal_init: has residency sets    = true
0.00.609.049 I ggml_metal_init: has bfloat            = true
0.00.609.049 I ggml_metal_init: use bfloat            = true
0.00.609.051 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.054 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.388 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.020 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.024 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.066 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.581 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.583 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.584 I llama_init_from_model: graph nodes  = 967
0.00.635.584 I llama_init_from_model: graph splits = 2
0.00.635.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.072 I 
0.00.665.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.161 I perplexity: tokenizing the input ..
0.00.671.861 I perplexity: tokenization took 6.698 ms
0.00.671.879 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.232 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.810.583 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.810.596 I llama_perf_context_print:        load time =     655.00 ms
0.00.810.597 I llama_perf_context_print: prompt eval time =     136.36 ms /   128 tokens (    1.07 ms per token,   938.66 tokens per second)
0.00.810.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.598 I llama_perf_context_print:       total time =     145.53 ms /   129 tokens
0.00.811.028 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.080s
sys	0m0.134s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.783 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.342 I llama_model_loader: - type  f32:  194 tensors
0.00.025.342 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.342 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.343 I print_info: file format = GGUF V3 (latest)
0.00.025.343 I print_info: file type   = Q4_1
0.00.025.348 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.446 I load: special tokens cache size = 25
0.00.039.395 I load: token to piece cache size = 0.2984 MB
0.00.039.398 I print_info: arch             = gptneox
0.00.039.398 I print_info: vocab_only       = 0
0.00.039.398 I print_info: n_ctx_train      = 2048
0.00.039.399 I print_info: n_embd           = 2048
0.00.039.399 I print_info: n_layer          = 24
0.00.039.402 I print_info: n_head           = 16
0.00.039.402 I print_info: n_head_kv        = 16
0.00.039.403 I print_info: n_rot            = 32
0.00.039.403 I print_info: n_swa            = 0
0.00.039.404 I print_info: n_embd_head_k    = 128
0.00.039.404 I print_info: n_embd_head_v    = 128
0.00.039.405 I print_info: n_gqa            = 1
0.00.039.406 I print_info: n_embd_k_gqa     = 2048
0.00.039.406 I print_info: n_embd_v_gqa     = 2048
0.00.039.407 I print_info: f_norm_eps       = 1.0e-05
0.00.039.407 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.408 I print_info: f_logit_scale    = 0.0e+00
0.00.039.408 I print_info: n_ff             = 8192
0.00.039.409 I print_info: n_expert         = 0
0.00.039.409 I print_info: n_expert_used    = 0
0.00.039.409 I print_info: causal attn      = 1
0.00.039.409 I print_info: pooling type     = 0
0.00.039.411 I print_info: rope type        = 2
0.00.039.412 I print_info: rope scaling     = linear
0.00.039.412 I print_info: freq_base_train  = 10000.0
0.00.039.413 I print_info: freq_scale_train = 1
0.00.039.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.413 I print_info: rope_finetuned   = unknown
0.00.039.413 I print_info: ssm_d_conv       = 0
0.00.039.413 I print_info: ssm_d_inner      = 0
0.00.039.414 I print_info: ssm_d_state      = 0
0.00.039.414 I print_info: ssm_dt_rank      = 0
0.00.039.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.414 I print_info: model type       = 1.4B
0.00.039.414 I print_info: model params     = 1.41 B
0.00.039.415 I print_info: general.name     = 1.4B
0.00.039.415 I print_info: vocab type       = BPE
0.00.039.415 I print_info: n_vocab          = 50304
0.00.039.415 I print_info: n_merges         = 50009
0.00.039.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.418 I print_info: LF token         = 187 ''
0.00.039.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.419 I print_info: max token length = 1024
0.00.039.428 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.400 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.417 I load_tensors: offloading output layer to GPU
0.00.615.418 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.451 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.615.453 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.617.171 I llama_init_from_model: n_seq_max     = 1
0.00.617.173 I llama_init_from_model: n_ctx         = 2048
0.00.617.174 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.617.174 I llama_init_from_model: n_batch       = 2048
0.00.617.175 I llama_init_from_model: n_ubatch      = 512
0.00.617.176 I llama_init_from_model: flash_attn    = 0
0.00.617.178 I llama_init_from_model: freq_base     = 10000.0
0.00.617.179 I llama_init_from_model: freq_scale    = 1
0.00.617.181 I ggml_metal_init: allocating
0.00.617.260 I ggml_metal_init: found device: Apple M4
0.00.617.277 I ggml_metal_init: picking default device: Apple M4
0.00.619.220 I ggml_metal_init: using embedded metal library
0.00.626.192 I ggml_metal_init: GPU name:   Apple M4
0.00.626.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.196 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.198 I ggml_metal_init: simdgroup reduction   = true
0.00.626.198 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.198 I ggml_metal_init: has residency sets    = true
0.00.626.198 I ggml_metal_init: has bfloat            = true
0.00.626.199 I ggml_metal_init: use bfloat            = true
0.00.626.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.583 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.627 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.702.636 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.702.667 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.410 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.708.411 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.708.412 I llama_init_from_model: graph nodes  = 967
0.00.708.412 I llama_init_from_model: graph splits = 2
0.00.708.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.708.547 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.708.547 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.247 I main: llama threadpool init, n_threads = 4
0.00.763.286 I 
0.00.763.308 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.308 I 
0.00.763.474 I sampler seed: 1234
0.00.763.479 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.490 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.490 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.496 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.496.658 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.496.658 I llama_perf_context_print:        load time =     753.69 ms
0.01.496.659 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.39 tokens per second)
0.01.496.660 I llama_perf_context_print:        eval time =     681.68 ms /    63 runs   (   10.82 ms per token,    92.42 tokens per second)
0.01.496.664 I llama_perf_context_print:       total time =     734.19 ms /    70 tokens
0.01.496.950 I ggml_metal_free: deallocating

real	0m1.514s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.911 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.942 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.943 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.944 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.946 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.648 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.650 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.650 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.651 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.651 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.652 I llama_model_loader: - type  f32:  194 tensors
0.00.024.652 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.652 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.653 I print_info: file format = GGUF V3 (latest)
0.00.024.654 I print_info: file type   = Q4_1
0.00.024.655 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.976 I load: special tokens cache size = 25
0.00.039.067 I load: token to piece cache size = 0.2984 MB
0.00.039.072 I print_info: arch             = gptneox
0.00.039.072 I print_info: vocab_only       = 0
0.00.039.072 I print_info: n_ctx_train      = 2048
0.00.039.072 I print_info: n_embd           = 2048
0.00.039.073 I print_info: n_layer          = 24
0.00.039.077 I print_info: n_head           = 16
0.00.039.078 I print_info: n_head_kv        = 16
0.00.039.078 I print_info: n_rot            = 32
0.00.039.078 I print_info: n_swa            = 0
0.00.039.078 I print_info: n_embd_head_k    = 128
0.00.039.078 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.080 I print_info: n_embd_k_gqa     = 2048
0.00.039.080 I print_info: n_embd_v_gqa     = 2048
0.00.039.081 I print_info: f_norm_eps       = 1.0e-05
0.00.039.083 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.083 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.083 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.083 I print_info: f_logit_scale    = 0.0e+00
0.00.039.084 I print_info: n_ff             = 8192
0.00.039.084 I print_info: n_expert         = 0
0.00.039.084 I print_info: n_expert_used    = 0
0.00.039.085 I print_info: causal attn      = 1
0.00.039.085 I print_info: pooling type     = 0
0.00.039.085 I print_info: rope type        = 2
0.00.039.085 I print_info: rope scaling     = linear
0.00.039.085 I print_info: freq_base_train  = 10000.0
0.00.039.086 I print_info: freq_scale_train = 1
0.00.039.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.086 I print_info: rope_finetuned   = unknown
0.00.039.087 I print_info: ssm_d_conv       = 0
0.00.039.087 I print_info: ssm_d_inner      = 0
0.00.039.088 I print_info: ssm_d_state      = 0
0.00.039.088 I print_info: ssm_dt_rank      = 0
0.00.039.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.088 I print_info: model params     = 1.41 B
0.00.039.088 I print_info: general.name     = 1.4B
0.00.039.089 I print_info: vocab type       = BPE
0.00.039.089 I print_info: n_vocab          = 50304
0.00.039.089 I print_info: n_merges         = 50009
0.00.039.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: LF token         = 187 ''
0.00.039.091 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: max token length = 1024
0.00.039.092 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.072 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.091 I load_tensors: offloading output layer to GPU
0.00.617.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.132 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.617.134 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.618.865 I llama_init_from_model: n_seq_max     = 1
0.00.618.868 I llama_init_from_model: n_ctx         = 128
0.00.618.869 I llama_init_from_model: n_ctx_per_seq = 128
0.00.618.869 I llama_init_from_model: n_batch       = 128
0.00.618.870 I llama_init_from_model: n_ubatch      = 128
0.00.618.870 I llama_init_from_model: flash_attn    = 0
0.00.618.872 I llama_init_from_model: freq_base     = 10000.0
0.00.618.872 I llama_init_from_model: freq_scale    = 1
0.00.618.873 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.618.878 I ggml_metal_init: allocating
0.00.619.019 I ggml_metal_init: found device: Apple M4
0.00.619.033 I ggml_metal_init: picking default device: Apple M4
0.00.620.918 I ggml_metal_init: using embedded metal library
0.00.627.756 I ggml_metal_init: GPU name:   Apple M4
0.00.627.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.767 I ggml_metal_init: simdgroup reduction   = true
0.00.627.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.767 I ggml_metal_init: has residency sets    = true
0.00.627.767 I ggml_metal_init: has bfloat            = true
0.00.627.768 I ggml_metal_init: use bfloat            = true
0.00.627.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.594 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.650.145 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.650.149 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.650.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.653.428 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.653.430 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.653.430 I llama_init_from_model: graph nodes  = 967
0.00.653.431 I llama_init_from_model: graph splits = 2
0.00.653.434 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.661 I 
0.00.678.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.758 I perplexity: tokenizing the input ..
0.00.686.226 I perplexity: tokenization took 7.463 ms
0.00.686.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.127 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.823.490 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.823.506 I llama_perf_context_print:        load time =     669.74 ms
0.00.823.507 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.72 tokens per second)
0.00.823.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.508 I llama_perf_context_print:       total time =     144.85 ms /   129 tokens
0.00.823.895 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.081s
sys	0m0.121s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.014.999 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.022.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.435 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.435 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.021 I llama_model_loader: - type  f32:  194 tensors
0.00.031.021 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.021 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.022 I print_info: file format = GGUF V3 (latest)
0.00.031.022 I print_info: file type   = Q5_0
0.00.031.023 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.039.109 I load: special tokens cache size = 25
0.00.045.190 I load: token to piece cache size = 0.2984 MB
0.00.045.193 I print_info: arch             = gptneox
0.00.045.193 I print_info: vocab_only       = 0
0.00.045.193 I print_info: n_ctx_train      = 2048
0.00.045.194 I print_info: n_embd           = 2048
0.00.045.194 I print_info: n_layer          = 24
0.00.045.196 I print_info: n_head           = 16
0.00.045.197 I print_info: n_head_kv        = 16
0.00.045.197 I print_info: n_rot            = 32
0.00.045.198 I print_info: n_swa            = 0
0.00.045.199 I print_info: n_embd_head_k    = 128
0.00.045.199 I print_info: n_embd_head_v    = 128
0.00.045.201 I print_info: n_gqa            = 1
0.00.045.202 I print_info: n_embd_k_gqa     = 2048
0.00.045.203 I print_info: n_embd_v_gqa     = 2048
0.00.045.203 I print_info: f_norm_eps       = 1.0e-05
0.00.045.203 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.204 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.204 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.204 I print_info: f_logit_scale    = 0.0e+00
0.00.045.205 I print_info: n_ff             = 8192
0.00.045.205 I print_info: n_expert         = 0
0.00.045.205 I print_info: n_expert_used    = 0
0.00.045.205 I print_info: causal attn      = 1
0.00.045.205 I print_info: pooling type     = 0
0.00.045.207 I print_info: rope type        = 2
0.00.045.208 I print_info: rope scaling     = linear
0.00.045.208 I print_info: freq_base_train  = 10000.0
0.00.045.209 I print_info: freq_scale_train = 1
0.00.045.209 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.209 I print_info: rope_finetuned   = unknown
0.00.045.209 I print_info: ssm_d_conv       = 0
0.00.045.209 I print_info: ssm_d_inner      = 0
0.00.045.209 I print_info: ssm_d_state      = 0
0.00.045.210 I print_info: ssm_dt_rank      = 0
0.00.045.210 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.210 I print_info: model type       = 1.4B
0.00.045.210 I print_info: model params     = 1.41 B
0.00.045.210 I print_info: general.name     = 1.4B
0.00.045.215 I print_info: vocab type       = BPE
0.00.045.215 I print_info: n_vocab          = 50304
0.00.045.215 I print_info: n_merges         = 50009
0.00.045.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.216 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.216 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.216 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.217 I print_info: LF token         = 187 ''
0.00.045.217 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.217 I print_info: max token length = 1024
0.00.045.217 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.304 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.318 I load_tensors: offloading output layer to GPU
0.00.668.319 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.361 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.668.363 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.669.946 I llama_init_from_model: n_seq_max     = 1
0.00.669.949 I llama_init_from_model: n_ctx         = 2048
0.00.669.950 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.669.950 I llama_init_from_model: n_batch       = 2048
0.00.669.951 I llama_init_from_model: n_ubatch      = 512
0.00.669.951 I llama_init_from_model: flash_attn    = 0
0.00.669.953 I llama_init_from_model: freq_base     = 10000.0
0.00.669.954 I llama_init_from_model: freq_scale    = 1
0.00.669.961 I ggml_metal_init: allocating
0.00.670.037 I ggml_metal_init: found device: Apple M4
0.00.670.050 I ggml_metal_init: picking default device: Apple M4
0.00.671.885 I ggml_metal_init: using embedded metal library
0.00.678.495 I ggml_metal_init: GPU name:   Apple M4
0.00.678.499 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.499 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.500 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.500 I ggml_metal_init: simdgroup reduction   = true
0.00.678.501 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.501 I ggml_metal_init: has residency sets    = true
0.00.678.501 I ggml_metal_init: has bfloat            = true
0.00.678.501 I ggml_metal_init: use bfloat            = true
0.00.678.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.504 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.855 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.749.850 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.749.857 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.749.892 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.754.253 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.754.255 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.754.255 I llama_init_from_model: graph nodes  = 967
0.00.754.256 I llama_init_from_model: graph splits = 2
0.00.754.262 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.378 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.390 I main: llama threadpool init, n_threads = 4
0.00.813.428 I 
0.00.813.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.450 I 
0.00.813.614 I sampler seed: 1234
0.00.813.618 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.813.628 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.813.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.813.629 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.603.416 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.603.417 I llama_perf_context_print:        load time =     797.62 ms
0.01.603.418 I llama_perf_context_print: prompt eval time =      53.07 ms /     7 tokens (    7.58 ms per token,   131.90 tokens per second)
0.01.603.419 I llama_perf_context_print:        eval time =     733.79 ms /    63 runs   (   11.65 ms per token,    85.86 tokens per second)
0.01.603.419 I llama_perf_context_print:       total time =     790.80 ms /    70 tokens
0.01.603.629 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.109s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.973 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.871 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.877 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.879 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.881 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.379 I llama_model_loader: - type  f32:  194 tensors
0.00.025.380 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.380 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.381 I print_info: file format = GGUF V3 (latest)
0.00.025.381 I print_info: file type   = Q5_0
0.00.025.382 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.422 I load: special tokens cache size = 25
0.00.039.417 I load: token to piece cache size = 0.2984 MB
0.00.039.421 I print_info: arch             = gptneox
0.00.039.422 I print_info: vocab_only       = 0
0.00.039.422 I print_info: n_ctx_train      = 2048
0.00.039.422 I print_info: n_embd           = 2048
0.00.039.422 I print_info: n_layer          = 24
0.00.039.427 I print_info: n_head           = 16
0.00.039.427 I print_info: n_head_kv        = 16
0.00.039.428 I print_info: n_rot            = 32
0.00.039.428 I print_info: n_swa            = 0
0.00.039.428 I print_info: n_embd_head_k    = 128
0.00.039.428 I print_info: n_embd_head_v    = 128
0.00.039.429 I print_info: n_gqa            = 1
0.00.039.430 I print_info: n_embd_k_gqa     = 2048
0.00.039.430 I print_info: n_embd_v_gqa     = 2048
0.00.039.431 I print_info: f_norm_eps       = 1.0e-05
0.00.039.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.432 I print_info: f_logit_scale    = 0.0e+00
0.00.039.432 I print_info: n_ff             = 8192
0.00.039.432 I print_info: n_expert         = 0
0.00.039.433 I print_info: n_expert_used    = 0
0.00.039.433 I print_info: causal attn      = 1
0.00.039.433 I print_info: pooling type     = 0
0.00.039.433 I print_info: rope type        = 2
0.00.039.433 I print_info: rope scaling     = linear
0.00.039.433 I print_info: freq_base_train  = 10000.0
0.00.039.434 I print_info: freq_scale_train = 1
0.00.039.434 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.434 I print_info: rope_finetuned   = unknown
0.00.039.434 I print_info: ssm_d_conv       = 0
0.00.039.434 I print_info: ssm_d_inner      = 0
0.00.039.435 I print_info: ssm_d_state      = 0
0.00.039.435 I print_info: ssm_dt_rank      = 0
0.00.039.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.435 I print_info: model type       = 1.4B
0.00.039.435 I print_info: model params     = 1.41 B
0.00.039.439 I print_info: general.name     = 1.4B
0.00.039.439 I print_info: vocab type       = BPE
0.00.039.439 I print_info: n_vocab          = 50304
0.00.039.439 I print_info: n_merges         = 50009
0.00.039.440 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.440 I print_info: LF token         = 187 ''
0.00.039.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.441 I print_info: max token length = 1024
0.00.039.441 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.715 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.732 I load_tensors: offloading output layer to GPU
0.00.656.733 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.772 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.656.774 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.658.352 I llama_init_from_model: n_seq_max     = 1
0.00.658.355 I llama_init_from_model: n_ctx         = 128
0.00.658.356 I llama_init_from_model: n_ctx_per_seq = 128
0.00.658.356 I llama_init_from_model: n_batch       = 128
0.00.658.357 I llama_init_from_model: n_ubatch      = 128
0.00.658.357 I llama_init_from_model: flash_attn    = 0
0.00.658.360 I llama_init_from_model: freq_base     = 10000.0
0.00.658.360 I llama_init_from_model: freq_scale    = 1
0.00.658.361 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.658.363 I ggml_metal_init: allocating
0.00.658.471 I ggml_metal_init: found device: Apple M4
0.00.658.486 I ggml_metal_init: picking default device: Apple M4
0.00.660.336 I ggml_metal_init: using embedded metal library
0.00.667.119 I ggml_metal_init: GPU name:   Apple M4
0.00.667.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.128 I ggml_metal_init: simdgroup reduction   = true
0.00.667.129 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.129 I ggml_metal_init: has residency sets    = true
0.00.667.129 I ggml_metal_init: has bfloat            = true
0.00.667.129 I ggml_metal_init: use bfloat            = true
0.00.667.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.318 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.688.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.688.919 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.688.962 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.692.169 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.692.170 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.692.171 I llama_init_from_model: graph nodes  = 967
0.00.692.171 I llama_init_from_model: graph splits = 2
0.00.692.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.692.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.976 I 
0.00.721.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.075 I perplexity: tokenizing the input ..
0.00.727.341 I perplexity: tokenization took 6.264 ms
0.00.727.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.116 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.868.464 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.868.482 I llama_perf_context_print:        load time =     710.99 ms
0.00.868.483 I llama_perf_context_print: prompt eval time =     139.54 ms /   128 tokens (    1.09 ms per token,   917.33 tokens per second)
0.00.868.484 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.868.485 I llama_perf_context_print:       total time =     147.51 ms /   129 tokens
0.00.868.917 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.078s
sys	0m0.126s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.572 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.578 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.578 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.372 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.104 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.104 I llama_model_loader: - type  f32:  194 tensors
0.00.025.104 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.105 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.105 I print_info: file format = GGUF V3 (latest)
0.00.025.106 I print_info: file type   = Q5_1
0.00.025.107 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.860 I load: special tokens cache size = 25
0.00.038.857 I load: token to piece cache size = 0.2984 MB
0.00.038.860 I print_info: arch             = gptneox
0.00.038.860 I print_info: vocab_only       = 0
0.00.038.860 I print_info: n_ctx_train      = 2048
0.00.038.860 I print_info: n_embd           = 2048
0.00.038.861 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.864 I print_info: n_head_kv        = 16
0.00.038.864 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.864 I print_info: n_embd_head_k    = 128
0.00.038.865 I print_info: n_embd_head_v    = 128
0.00.038.865 I print_info: n_gqa            = 1
0.00.038.866 I print_info: n_embd_k_gqa     = 2048
0.00.038.867 I print_info: n_embd_v_gqa     = 2048
0.00.038.867 I print_info: f_norm_eps       = 1.0e-05
0.00.038.868 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.868 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.868 I print_info: f_logit_scale    = 0.0e+00
0.00.038.869 I print_info: n_ff             = 8192
0.00.038.869 I print_info: n_expert         = 0
0.00.038.869 I print_info: n_expert_used    = 0
0.00.038.869 I print_info: causal attn      = 1
0.00.038.869 I print_info: pooling type     = 0
0.00.038.871 I print_info: rope type        = 2
0.00.038.873 I print_info: rope scaling     = linear
0.00.038.873 I print_info: freq_base_train  = 10000.0
0.00.038.873 I print_info: freq_scale_train = 1
0.00.038.873 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.874 I print_info: rope_finetuned   = unknown
0.00.038.874 I print_info: ssm_d_conv       = 0
0.00.038.874 I print_info: ssm_d_inner      = 0
0.00.038.874 I print_info: ssm_d_state      = 0
0.00.038.874 I print_info: ssm_dt_rank      = 0
0.00.038.874 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.875 I print_info: model type       = 1.4B
0.00.038.875 I print_info: model params     = 1.41 B
0.00.038.875 I print_info: general.name     = 1.4B
0.00.038.876 I print_info: vocab type       = BPE
0.00.038.876 I print_info: n_vocab          = 50304
0.00.038.876 I print_info: n_merges         = 50009
0.00.038.876 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.876 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.877 I print_info: LF token         = 187 ''
0.00.038.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: max token length = 1024
0.00.038.878 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.174 I load_tensors: offloading output layer to GPU
0.00.626.174 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.196 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.626.199 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.627.786 I llama_init_from_model: n_seq_max     = 1
0.00.627.788 I llama_init_from_model: n_ctx         = 2048
0.00.627.788 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.627.789 I llama_init_from_model: n_batch       = 2048
0.00.627.789 I llama_init_from_model: n_ubatch      = 512
0.00.627.789 I llama_init_from_model: flash_attn    = 0
0.00.627.790 I llama_init_from_model: freq_base     = 10000.0
0.00.627.791 I llama_init_from_model: freq_scale    = 1
0.00.627.792 I ggml_metal_init: allocating
0.00.627.811 I ggml_metal_init: found device: Apple M4
0.00.627.823 I ggml_metal_init: picking default device: Apple M4
0.00.629.373 I ggml_metal_init: using embedded metal library
0.00.635.481 I ggml_metal_init: GPU name:   Apple M4
0.00.635.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.486 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.487 I ggml_metal_init: simdgroup reduction   = true
0.00.635.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.487 I ggml_metal_init: has residency sets    = true
0.00.635.488 I ggml_metal_init: has bfloat            = true
0.00.635.488 I ggml_metal_init: use bfloat            = true
0.00.635.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.490 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.386 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.707.652 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.300 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.302 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.302 I llama_init_from_model: graph nodes  = 967
0.00.712.303 I llama_init_from_model: graph splits = 2
0.00.712.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.246 I main: llama threadpool init, n_threads = 4
0.00.773.288 I 
0.00.773.310 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.310 I 
0.00.773.478 I sampler seed: 1234
0.00.773.482 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.527 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.531 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.531 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.619.592 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47113.47 tokens per second)
0.01.619.593 I llama_perf_context_print:        load time =     763.41 ms
0.01.619.593 I llama_perf_context_print: prompt eval time =      52.22 ms /     7 tokens (    7.46 ms per token,   134.05 tokens per second)
0.01.619.594 I llama_perf_context_print:        eval time =     791.19 ms /    63 runs   (   12.56 ms per token,    79.63 tokens per second)
0.01.619.594 I llama_perf_context_print:       total time =     847.06 ms /    70 tokens
0.01.619.845 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.108s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.781 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.956 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.962 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.970 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.970 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.970 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.971 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.971 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.972 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.974 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.975 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.523 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.523 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.524 I llama_model_loader: - type  f32:  194 tensors
0.00.025.525 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.525 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.526 I print_info: file format = GGUF V3 (latest)
0.00.025.531 I print_info: file type   = Q5_1
0.00.025.533 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.508 I load: special tokens cache size = 25
0.00.039.532 I load: token to piece cache size = 0.2984 MB
0.00.039.537 I print_info: arch             = gptneox
0.00.039.537 I print_info: vocab_only       = 0
0.00.039.537 I print_info: n_ctx_train      = 2048
0.00.039.537 I print_info: n_embd           = 2048
0.00.039.538 I print_info: n_layer          = 24
0.00.039.542 I print_info: n_head           = 16
0.00.039.542 I print_info: n_head_kv        = 16
0.00.039.543 I print_info: n_rot            = 32
0.00.039.543 I print_info: n_swa            = 0
0.00.039.543 I print_info: n_embd_head_k    = 128
0.00.039.543 I print_info: n_embd_head_v    = 128
0.00.039.544 I print_info: n_gqa            = 1
0.00.039.545 I print_info: n_embd_k_gqa     = 2048
0.00.039.545 I print_info: n_embd_v_gqa     = 2048
0.00.039.546 I print_info: f_norm_eps       = 1.0e-05
0.00.039.546 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.546 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.547 I print_info: f_logit_scale    = 0.0e+00
0.00.039.547 I print_info: n_ff             = 8192
0.00.039.547 I print_info: n_expert         = 0
0.00.039.548 I print_info: n_expert_used    = 0
0.00.039.548 I print_info: causal attn      = 1
0.00.039.548 I print_info: pooling type     = 0
0.00.039.548 I print_info: rope type        = 2
0.00.039.548 I print_info: rope scaling     = linear
0.00.039.549 I print_info: freq_base_train  = 10000.0
0.00.039.549 I print_info: freq_scale_train = 1
0.00.039.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.552 I print_info: rope_finetuned   = unknown
0.00.039.552 I print_info: ssm_d_conv       = 0
0.00.039.552 I print_info: ssm_d_inner      = 0
0.00.039.552 I print_info: ssm_d_state      = 0
0.00.039.552 I print_info: ssm_dt_rank      = 0
0.00.039.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.553 I print_info: model type       = 1.4B
0.00.039.554 I print_info: model params     = 1.41 B
0.00.039.554 I print_info: general.name     = 1.4B
0.00.039.555 I print_info: vocab type       = BPE
0.00.039.556 I print_info: n_vocab          = 50304
0.00.039.556 I print_info: n_merges         = 50009
0.00.039.556 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.557 I print_info: LF token         = 187 ''
0.00.039.557 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.557 I print_info: max token length = 1024
0.00.039.558 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.621.070 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.090 I load_tensors: offloading output layer to GPU
0.00.621.091 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.132 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.134 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.622.575 I llama_init_from_model: n_seq_max     = 1
0.00.622.581 I llama_init_from_model: n_ctx         = 128
0.00.622.582 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.582 I llama_init_from_model: n_batch       = 128
0.00.622.583 I llama_init_from_model: n_ubatch      = 128
0.00.622.583 I llama_init_from_model: flash_attn    = 0
0.00.622.586 I llama_init_from_model: freq_base     = 10000.0
0.00.622.587 I llama_init_from_model: freq_scale    = 1
0.00.622.588 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.590 I ggml_metal_init: allocating
0.00.622.727 I ggml_metal_init: found device: Apple M4
0.00.622.746 I ggml_metal_init: picking default device: Apple M4
0.00.624.703 I ggml_metal_init: using embedded metal library
0.00.631.856 I ggml_metal_init: GPU name:   Apple M4
0.00.631.867 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.868 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.869 I ggml_metal_init: simdgroup reduction   = true
0.00.631.870 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.870 I ggml_metal_init: has residency sets    = true
0.00.631.870 I ggml_metal_init: has bfloat            = true
0.00.631.870 I ggml_metal_init: use bfloat            = true
0.00.631.872 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.876 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.025 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.028 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.065 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.657.337 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.657.339 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.657.339 I llama_init_from_model: graph nodes  = 967
0.00.657.340 I llama_init_from_model: graph splits = 2
0.00.657.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.657.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.973 I 
0.00.686.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.056 I perplexity: tokenizing the input ..
0.00.692.801 I perplexity: tokenization took 6.741 ms
0.00.692.822 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.841.290 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.842.622 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.842.636 I llama_perf_context_print:        load time =     677.19 ms
0.00.842.637 I llama_perf_context_print: prompt eval time =     147.42 ms /   128 tokens (    1.15 ms per token,   868.27 tokens per second)
0.00.842.637 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.638 I llama_perf_context_print:       total time =     156.66 ms /   129 tokens
0.00.843.094 I ggml_metal_free: deallocating

real	0m0.857s
user	0m0.082s
sys	0m0.153s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.381 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.386 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.387 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.388 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.394 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.750 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.751 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.752 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.752 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.752 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.753 I llama_model_loader: - type  f32:  194 tensors
0.00.024.753 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.753 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.754 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.754 I print_info: file format = GGUF V3 (latest)
0.00.024.755 I print_info: file type   = Q2_K - Medium
0.00.024.756 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.543 I load: special tokens cache size = 25
0.00.038.591 I load: token to piece cache size = 0.2984 MB
0.00.038.594 I print_info: arch             = gptneox
0.00.038.594 I print_info: vocab_only       = 0
0.00.038.594 I print_info: n_ctx_train      = 2048
0.00.038.595 I print_info: n_embd           = 2048
0.00.038.595 I print_info: n_layer          = 24
0.00.038.598 I print_info: n_head           = 16
0.00.038.599 I print_info: n_head_kv        = 16
0.00.038.599 I print_info: n_rot            = 32
0.00.038.599 I print_info: n_swa            = 0
0.00.038.599 I print_info: n_embd_head_k    = 128
0.00.038.599 I print_info: n_embd_head_v    = 128
0.00.038.600 I print_info: n_gqa            = 1
0.00.038.601 I print_info: n_embd_k_gqa     = 2048
0.00.038.601 I print_info: n_embd_v_gqa     = 2048
0.00.038.602 I print_info: f_norm_eps       = 1.0e-05
0.00.038.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.603 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.603 I print_info: f_logit_scale    = 0.0e+00
0.00.038.604 I print_info: n_ff             = 8192
0.00.038.604 I print_info: n_expert         = 0
0.00.038.604 I print_info: n_expert_used    = 0
0.00.038.604 I print_info: causal attn      = 1
0.00.038.604 I print_info: pooling type     = 0
0.00.038.604 I print_info: rope type        = 2
0.00.038.604 I print_info: rope scaling     = linear
0.00.038.605 I print_info: freq_base_train  = 10000.0
0.00.038.605 I print_info: freq_scale_train = 1
0.00.038.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.606 I print_info: rope_finetuned   = unknown
0.00.038.606 I print_info: ssm_d_conv       = 0
0.00.038.606 I print_info: ssm_d_inner      = 0
0.00.038.608 I print_info: ssm_d_state      = 0
0.00.038.608 I print_info: ssm_dt_rank      = 0
0.00.038.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.609 I print_info: model type       = 1.4B
0.00.038.609 I print_info: model params     = 1.41 B
0.00.038.609 I print_info: general.name     = 1.4B
0.00.038.610 I print_info: vocab type       = BPE
0.00.038.610 I print_info: n_vocab          = 50304
0.00.038.610 I print_info: n_merges         = 50009
0.00.038.611 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.611 I print_info: LF token         = 187 ''
0.00.038.612 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.612 I print_info: max token length = 1024
0.00.038.612 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.338.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.074 I load_tensors: offloading output layer to GPU
0.00.338.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.112 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.115 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.339.576 I llama_init_from_model: n_seq_max     = 1
0.00.339.582 I llama_init_from_model: n_ctx         = 2048
0.00.339.583 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.339.583 I llama_init_from_model: n_batch       = 2048
0.00.339.584 I llama_init_from_model: n_ubatch      = 512
0.00.339.584 I llama_init_from_model: flash_attn    = 0
0.00.339.586 I llama_init_from_model: freq_base     = 10000.0
0.00.339.586 I llama_init_from_model: freq_scale    = 1
0.00.339.589 I ggml_metal_init: allocating
0.00.339.670 I ggml_metal_init: found device: Apple M4
0.00.339.685 I ggml_metal_init: picking default device: Apple M4
0.00.342.179 I ggml_metal_init: using embedded metal library
0.00.348.597 I ggml_metal_init: GPU name:   Apple M4
0.00.348.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.604 I ggml_metal_init: simdgroup reduction   = true
0.00.348.604 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.604 I ggml_metal_init: has residency sets    = true
0.00.348.605 I ggml_metal_init: has bfloat            = true
0.00.348.605 I ggml_metal_init: use bfloat            = true
0.00.348.606 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.616 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.768 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.423.116 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.423.123 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.423.152 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.427.432 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.427.434 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.427.434 I llama_init_from_model: graph nodes  = 967
0.00.427.435 I llama_init_from_model: graph splits = 2
0.00.427.440 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.427.573 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.427.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.273 I main: llama threadpool init, n_threads = 4
0.00.489.312 I 
0.00.489.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.334 I 
0.00.489.513 I sampler seed: 1234
0.00.489.518 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.489.528 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.489.529 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.489.529 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.171.785 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.171.786 I llama_perf_context_print:        load time =     478.65 ms
0.01.171.786 I llama_perf_context_print: prompt eval time =      44.22 ms /     7 tokens (    6.32 ms per token,   158.31 tokens per second)
0.01.171.788 I llama_perf_context_print:        eval time =     635.20 ms /    63 runs   (   10.08 ms per token,    99.18 tokens per second)
0.01.171.788 I llama_perf_context_print:       total time =     683.22 ms /    70 tokens
0.01.172.003 I ggml_metal_free: deallocating

real	0m1.191s
user	0m0.110s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.966 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.889 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.896 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.902 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.903 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.903 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.903 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.904 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.905 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.905 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.908 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.908 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.910 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.910 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.910 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.740 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.617 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.617 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.617 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.618 I print_info: file format = GGUF V3 (latest)
0.00.025.619 I print_info: file type   = Q2_K - Medium
0.00.025.620 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.001 I load: special tokens cache size = 25
0.00.039.930 I load: token to piece cache size = 0.2984 MB
0.00.039.934 I print_info: arch             = gptneox
0.00.039.935 I print_info: vocab_only       = 0
0.00.039.935 I print_info: n_ctx_train      = 2048
0.00.039.935 I print_info: n_embd           = 2048
0.00.039.935 I print_info: n_layer          = 24
0.00.039.940 I print_info: n_head           = 16
0.00.039.941 I print_info: n_head_kv        = 16
0.00.039.941 I print_info: n_rot            = 32
0.00.039.941 I print_info: n_swa            = 0
0.00.039.943 I print_info: n_embd_head_k    = 128
0.00.039.943 I print_info: n_embd_head_v    = 128
0.00.039.944 I print_info: n_gqa            = 1
0.00.039.945 I print_info: n_embd_k_gqa     = 2048
0.00.039.945 I print_info: n_embd_v_gqa     = 2048
0.00.039.946 I print_info: f_norm_eps       = 1.0e-05
0.00.039.946 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.946 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.946 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.947 I print_info: f_logit_scale    = 0.0e+00
0.00.039.947 I print_info: n_ff             = 8192
0.00.039.947 I print_info: n_expert         = 0
0.00.039.947 I print_info: n_expert_used    = 0
0.00.039.949 I print_info: causal attn      = 1
0.00.039.949 I print_info: pooling type     = 0
0.00.039.949 I print_info: rope type        = 2
0.00.039.950 I print_info: rope scaling     = linear
0.00.039.950 I print_info: freq_base_train  = 10000.0
0.00.039.950 I print_info: freq_scale_train = 1
0.00.039.950 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.951 I print_info: rope_finetuned   = unknown
0.00.039.951 I print_info: ssm_d_conv       = 0
0.00.039.951 I print_info: ssm_d_inner      = 0
0.00.039.951 I print_info: ssm_d_state      = 0
0.00.039.951 I print_info: ssm_dt_rank      = 0
0.00.039.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.951 I print_info: model type       = 1.4B
0.00.039.953 I print_info: model params     = 1.41 B
0.00.039.953 I print_info: general.name     = 1.4B
0.00.039.954 I print_info: vocab type       = BPE
0.00.039.954 I print_info: n_vocab          = 50304
0.00.039.954 I print_info: n_merges         = 50009
0.00.039.954 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.954 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.954 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.955 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.955 I print_info: LF token         = 187 ''
0.00.039.955 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.956 I print_info: max token length = 1024
0.00.039.956 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.346.246 I load_tensors: offloading 24 repeating layers to GPU
0.00.346.261 I load_tensors: offloading output layer to GPU
0.00.346.262 I load_tensors: offloaded 25/25 layers to GPU
0.00.346.295 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.346.302 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.347.732 I llama_init_from_model: n_seq_max     = 1
0.00.347.737 I llama_init_from_model: n_ctx         = 128
0.00.347.738 I llama_init_from_model: n_ctx_per_seq = 128
0.00.347.738 I llama_init_from_model: n_batch       = 128
0.00.347.738 I llama_init_from_model: n_ubatch      = 128
0.00.347.739 I llama_init_from_model: flash_attn    = 0
0.00.347.741 I llama_init_from_model: freq_base     = 10000.0
0.00.347.742 I llama_init_from_model: freq_scale    = 1
0.00.347.742 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.347.745 I ggml_metal_init: allocating
0.00.347.827 I ggml_metal_init: found device: Apple M4
0.00.347.848 I ggml_metal_init: picking default device: Apple M4
0.00.349.734 I ggml_metal_init: using embedded metal library
0.00.355.692 I ggml_metal_init: GPU name:   Apple M4
0.00.355.710 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.712 I ggml_metal_init: simdgroup reduction   = true
0.00.355.712 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.713 I ggml_metal_init: has residency sets    = true
0.00.355.713 I ggml_metal_init: has bfloat            = true
0.00.355.714 I ggml_metal_init: use bfloat            = true
0.00.355.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.837 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.381.730 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.381.736 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.381.807 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.385.252 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.385.254 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.385.255 I llama_init_from_model: graph nodes  = 967
0.00.385.256 I llama_init_from_model: graph splits = 2
0.00.385.259 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.385.259 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.412.749 I 
0.00.412.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.412.832 I perplexity: tokenizing the input ..
0.00.419.357 I perplexity: tokenization took 6.523 ms
0.00.419.372 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.551.711 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.553.058 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.553.070 I llama_perf_context_print:        load time =     402.78 ms
0.00.553.071 I llama_perf_context_print: prompt eval time =     131.71 ms /   128 tokens (    1.03 ms per token,   971.83 tokens per second)
0.00.553.073 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.553.075 I llama_perf_context_print:       total time =     140.32 ms /   129 tokens
0.00.553.501 I ggml_metal_free: deallocating

real	0m0.569s
user	0m0.082s
sys	0m0.095s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.908 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.625 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.641 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.641 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.645 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.645 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.645 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.648 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.649 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.649 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.487 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.351 I llama_model_loader: - type  f32:  194 tensors
0.00.025.351 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.352 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.352 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.352 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.353 I print_info: file format = GGUF V3 (latest)
0.00.025.353 I print_info: file type   = Q3_K - Medium
0.00.025.354 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.108 I load: special tokens cache size = 25
0.00.038.970 I load: token to piece cache size = 0.2984 MB
0.00.038.973 I print_info: arch             = gptneox
0.00.038.974 I print_info: vocab_only       = 0
0.00.038.974 I print_info: n_ctx_train      = 2048
0.00.038.974 I print_info: n_embd           = 2048
0.00.038.974 I print_info: n_layer          = 24
0.00.038.977 I print_info: n_head           = 16
0.00.038.978 I print_info: n_head_kv        = 16
0.00.038.978 I print_info: n_rot            = 32
0.00.038.980 I print_info: n_swa            = 0
0.00.038.980 I print_info: n_embd_head_k    = 128
0.00.038.980 I print_info: n_embd_head_v    = 128
0.00.038.981 I print_info: n_gqa            = 1
0.00.038.982 I print_info: n_embd_k_gqa     = 2048
0.00.038.987 I print_info: n_embd_v_gqa     = 2048
0.00.038.987 I print_info: f_norm_eps       = 1.0e-05
0.00.038.989 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.989 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.990 I print_info: f_logit_scale    = 0.0e+00
0.00.038.990 I print_info: n_ff             = 8192
0.00.038.991 I print_info: n_expert         = 0
0.00.038.991 I print_info: n_expert_used    = 0
0.00.038.992 I print_info: causal attn      = 1
0.00.038.994 I print_info: pooling type     = 0
0.00.038.994 I print_info: rope type        = 2
0.00.038.995 I print_info: rope scaling     = linear
0.00.038.995 I print_info: freq_base_train  = 10000.0
0.00.038.995 I print_info: freq_scale_train = 1
0.00.038.995 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.998 I print_info: rope_finetuned   = unknown
0.00.038.998 I print_info: ssm_d_conv       = 0
0.00.038.998 I print_info: ssm_d_inner      = 0
0.00.038.998 I print_info: ssm_d_state      = 0
0.00.038.998 I print_info: ssm_dt_rank      = 0
0.00.038.998 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.999 I print_info: model type       = 1.4B
0.00.038.999 I print_info: model params     = 1.41 B
0.00.038.999 I print_info: general.name     = 1.4B
0.00.039.000 I print_info: vocab type       = BPE
0.00.039.000 I print_info: n_vocab          = 50304
0.00.039.000 I print_info: n_merges         = 50009
0.00.039.000 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: LF token         = 187 ''
0.00.039.002 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.002 I print_info: max token length = 1024
0.00.039.003 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.291 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.307 I load_tensors: offloading output layer to GPU
0.00.442.307 I load_tensors: offloaded 25/25 layers to GPU
0.00.442.342 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.442.344 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.035 I llama_init_from_model: n_seq_max     = 1
0.00.444.038 I llama_init_from_model: n_ctx         = 2048
0.00.444.039 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.039 I llama_init_from_model: n_batch       = 2048
0.00.444.040 I llama_init_from_model: n_ubatch      = 512
0.00.444.040 I llama_init_from_model: flash_attn    = 0
0.00.444.043 I llama_init_from_model: freq_base     = 10000.0
0.00.444.043 I llama_init_from_model: freq_scale    = 1
0.00.444.048 I ggml_metal_init: allocating
0.00.444.126 I ggml_metal_init: found device: Apple M4
0.00.444.139 I ggml_metal_init: picking default device: Apple M4
0.00.446.082 I ggml_metal_init: using embedded metal library
0.00.452.101 I ggml_metal_init: GPU name:   Apple M4
0.00.452.108 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.108 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.109 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.110 I ggml_metal_init: simdgroup reduction   = true
0.00.452.110 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.111 I ggml_metal_init: has residency sets    = true
0.00.452.111 I ggml_metal_init: has bfloat            = true
0.00.452.111 I ggml_metal_init: use bfloat            = true
0.00.452.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.795 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.547.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.547.598 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.547.639 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.551.917 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.551.919 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.551.919 I llama_init_from_model: graph nodes  = 967
0.00.551.920 I llama_init_from_model: graph splits = 2
0.00.551.926 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.552.067 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.552.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.083 I main: llama threadpool init, n_threads = 4
0.00.610.127 I 
0.00.610.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.151 I 
0.00.610.304 I sampler seed: 1234
0.00.610.309 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.610.343 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.610.347 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.610.347 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.359.743 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49824.56 tokens per second)
0.01.359.743 I llama_perf_context_print:        load time =     600.47 ms
0.01.359.747 I llama_perf_context_print: prompt eval time =      48.36 ms /     7 tokens (    6.91 ms per token,   144.75 tokens per second)
0.01.359.749 I llama_perf_context_print:        eval time =     698.02 ms /    63 runs   (   11.08 ms per token,    90.25 tokens per second)
0.01.359.750 I llama_perf_context_print:       total time =     750.37 ms /    70 tokens
0.01.359.951 I ggml_metal_free: deallocating

real	0m1.376s
user	0m0.112s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.227 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.347 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.191 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.143 I llama_model_loader: - type  f32:  194 tensors
0.00.025.144 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.144 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.144 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.144 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.145 I print_info: file format = GGUF V3 (latest)
0.00.025.148 I print_info: file type   = Q3_K - Medium
0.00.025.150 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.540 I load: special tokens cache size = 25
0.00.039.568 I load: token to piece cache size = 0.2984 MB
0.00.039.574 I print_info: arch             = gptneox
0.00.039.574 I print_info: vocab_only       = 0
0.00.039.574 I print_info: n_ctx_train      = 2048
0.00.039.574 I print_info: n_embd           = 2048
0.00.039.574 I print_info: n_layer          = 24
0.00.039.579 I print_info: n_head           = 16
0.00.039.580 I print_info: n_head_kv        = 16
0.00.039.580 I print_info: n_rot            = 32
0.00.039.581 I print_info: n_swa            = 0
0.00.039.581 I print_info: n_embd_head_k    = 128
0.00.039.581 I print_info: n_embd_head_v    = 128
0.00.039.582 I print_info: n_gqa            = 1
0.00.039.583 I print_info: n_embd_k_gqa     = 2048
0.00.039.583 I print_info: n_embd_v_gqa     = 2048
0.00.039.584 I print_info: f_norm_eps       = 1.0e-05
0.00.039.586 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.587 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.588 I print_info: f_logit_scale    = 0.0e+00
0.00.039.589 I print_info: n_ff             = 8192
0.00.039.589 I print_info: n_expert         = 0
0.00.039.589 I print_info: n_expert_used    = 0
0.00.039.590 I print_info: causal attn      = 1
0.00.039.590 I print_info: pooling type     = 0
0.00.039.590 I print_info: rope type        = 2
0.00.039.591 I print_info: rope scaling     = linear
0.00.039.591 I print_info: freq_base_train  = 10000.0
0.00.039.592 I print_info: freq_scale_train = 1
0.00.039.592 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.593 I print_info: rope_finetuned   = unknown
0.00.039.594 I print_info: ssm_d_conv       = 0
0.00.039.594 I print_info: ssm_d_inner      = 0
0.00.039.594 I print_info: ssm_d_state      = 0
0.00.039.594 I print_info: ssm_dt_rank      = 0
0.00.039.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.594 I print_info: model type       = 1.4B
0.00.039.595 I print_info: model params     = 1.41 B
0.00.039.595 I print_info: general.name     = 1.4B
0.00.039.595 I print_info: vocab type       = BPE
0.00.039.595 I print_info: n_vocab          = 50304
0.00.039.596 I print_info: n_merges         = 50009
0.00.039.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.596 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.596 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.596 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.596 I print_info: LF token         = 187 ''
0.00.039.597 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.597 I print_info: max token length = 1024
0.00.039.597 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.450.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.450.289 I load_tensors: offloading output layer to GPU
0.00.450.290 I load_tensors: offloaded 25/25 layers to GPU
0.00.450.351 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.450.353 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.451.723 I llama_init_from_model: n_seq_max     = 1
0.00.451.726 I llama_init_from_model: n_ctx         = 128
0.00.451.727 I llama_init_from_model: n_ctx_per_seq = 128
0.00.451.727 I llama_init_from_model: n_batch       = 128
0.00.451.728 I llama_init_from_model: n_ubatch      = 128
0.00.451.728 I llama_init_from_model: flash_attn    = 0
0.00.451.732 I llama_init_from_model: freq_base     = 10000.0
0.00.451.732 I llama_init_from_model: freq_scale    = 1
0.00.451.733 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.451.736 I ggml_metal_init: allocating
0.00.451.819 I ggml_metal_init: found device: Apple M4
0.00.451.835 I ggml_metal_init: picking default device: Apple M4
0.00.453.726 I ggml_metal_init: using embedded metal library
0.00.459.650 I ggml_metal_init: GPU name:   Apple M4
0.00.459.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.459.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.459.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.459.674 I ggml_metal_init: simdgroup reduction   = true
0.00.459.675 I ggml_metal_init: simdgroup matrix mul. = true
0.00.459.675 I ggml_metal_init: has residency sets    = true
0.00.459.675 I ggml_metal_init: has bfloat            = true
0.00.459.676 I ggml_metal_init: use bfloat            = true
0.00.459.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.459.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.480.488 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.484.018 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.484.024 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.484.086 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.487.390 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.487.392 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.487.393 I llama_init_from_model: graph nodes  = 967
0.00.487.393 I llama_init_from_model: graph splits = 2
0.00.487.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.487.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.511.660 I 
0.00.511.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.511.735 I perplexity: tokenizing the input ..
0.00.518.123 I perplexity: tokenization took 6.386 ms
0.00.518.141 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.650.200 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.651.513 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.651.533 I llama_perf_context_print:        load time =     502.43 ms
0.00.651.534 I llama_perf_context_print: prompt eval time =     131.61 ms /   128 tokens (    1.03 ms per token,   972.59 tokens per second)
0.00.651.535 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.651.536 I llama_perf_context_print:       total time =     139.88 ms /   129 tokens
0.00.651.957 I ggml_metal_free: deallocating

real	0m0.665s
user	0m0.081s
sys	0m0.120s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.292 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.296 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.909 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.911 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.911 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.911 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.912 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.912 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.912 I llama_model_loader: - type  f32:  194 tensors
0.00.024.913 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.913 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.913 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.914 I print_info: file format = GGUF V3 (latest)
0.00.024.914 I print_info: file type   = Q4_K - Medium
0.00.024.915 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.058 I load: special tokens cache size = 25
0.00.039.122 I load: token to piece cache size = 0.2984 MB
0.00.039.125 I print_info: arch             = gptneox
0.00.039.125 I print_info: vocab_only       = 0
0.00.039.125 I print_info: n_ctx_train      = 2048
0.00.039.125 I print_info: n_embd           = 2048
0.00.039.126 I print_info: n_layer          = 24
0.00.039.128 I print_info: n_head           = 16
0.00.039.129 I print_info: n_head_kv        = 16
0.00.039.129 I print_info: n_rot            = 32
0.00.039.129 I print_info: n_swa            = 0
0.00.039.129 I print_info: n_embd_head_k    = 128
0.00.039.130 I print_info: n_embd_head_v    = 128
0.00.039.130 I print_info: n_gqa            = 1
0.00.039.131 I print_info: n_embd_k_gqa     = 2048
0.00.039.132 I print_info: n_embd_v_gqa     = 2048
0.00.039.133 I print_info: f_norm_eps       = 1.0e-05
0.00.039.133 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.133 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.133 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.133 I print_info: f_logit_scale    = 0.0e+00
0.00.039.134 I print_info: n_ff             = 8192
0.00.039.134 I print_info: n_expert         = 0
0.00.039.134 I print_info: n_expert_used    = 0
0.00.039.135 I print_info: causal attn      = 1
0.00.039.136 I print_info: pooling type     = 0
0.00.039.138 I print_info: rope type        = 2
0.00.039.138 I print_info: rope scaling     = linear
0.00.039.139 I print_info: freq_base_train  = 10000.0
0.00.039.139 I print_info: freq_scale_train = 1
0.00.039.139 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.139 I print_info: rope_finetuned   = unknown
0.00.039.139 I print_info: ssm_d_conv       = 0
0.00.039.140 I print_info: ssm_d_inner      = 0
0.00.039.140 I print_info: ssm_d_state      = 0
0.00.039.140 I print_info: ssm_dt_rank      = 0
0.00.039.140 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.140 I print_info: model type       = 1.4B
0.00.039.141 I print_info: model params     = 1.41 B
0.00.039.141 I print_info: general.name     = 1.4B
0.00.039.141 I print_info: vocab type       = BPE
0.00.039.141 I print_info: n_vocab          = 50304
0.00.039.141 I print_info: n_merges         = 50009
0.00.039.142 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: LF token         = 187 ''
0.00.039.147 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.147 I print_info: max token length = 1024
0.00.039.147 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.314 I load_tensors: offloading output layer to GPU
0.00.515.315 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.348 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.350 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.517.049 I llama_init_from_model: n_seq_max     = 1
0.00.517.053 I llama_init_from_model: n_ctx         = 2048
0.00.517.053 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.517.054 I llama_init_from_model: n_batch       = 2048
0.00.517.054 I llama_init_from_model: n_ubatch      = 512
0.00.517.054 I llama_init_from_model: flash_attn    = 0
0.00.517.057 I llama_init_from_model: freq_base     = 10000.0
0.00.517.057 I llama_init_from_model: freq_scale    = 1
0.00.517.059 I ggml_metal_init: allocating
0.00.517.131 I ggml_metal_init: found device: Apple M4
0.00.517.149 I ggml_metal_init: picking default device: Apple M4
0.00.519.048 I ggml_metal_init: using embedded metal library
0.00.525.731 I ggml_metal_init: GPU name:   Apple M4
0.00.525.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.737 I ggml_metal_init: simdgroup reduction   = true
0.00.525.737 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.737 I ggml_metal_init: has residency sets    = true
0.00.525.738 I ggml_metal_init: has bfloat            = true
0.00.525.738 I ggml_metal_init: use bfloat            = true
0.00.525.739 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.740 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.689 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.304 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.605.310 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.605.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.610.111 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.610.114 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.610.114 I llama_init_from_model: graph nodes  = 967
0.00.610.114 I llama_init_from_model: graph splits = 2
0.00.610.121 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.610.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.610.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.266 I main: llama threadpool init, n_threads = 4
0.00.669.314 I 
0.00.669.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.341 I 
0.00.669.497 I sampler seed: 1234
0.00.669.502 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.523 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.523 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.432.497 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.432.498 I llama_perf_context_print:        load time =     659.71 ms
0.01.432.499 I llama_perf_context_print: prompt eval time =      57.99 ms /     7 tokens (    8.28 ms per token,   120.70 tokens per second)
0.01.432.500 I llama_perf_context_print:        eval time =     702.04 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.432.501 I llama_perf_context_print:       total time =     763.94 ms /    70 tokens
0.01.432.763 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.112s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.255 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.567 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.571 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.334 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.335 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.336 I llama_model_loader: - type  f32:  194 tensors
0.00.025.336 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.337 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.337 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.338 I print_info: file format = GGUF V3 (latest)
0.00.025.338 I print_info: file type   = Q4_K - Medium
0.00.025.339 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.765 I load: special tokens cache size = 25
0.00.039.820 I load: token to piece cache size = 0.2984 MB
0.00.039.824 I print_info: arch             = gptneox
0.00.039.824 I print_info: vocab_only       = 0
0.00.039.825 I print_info: n_ctx_train      = 2048
0.00.039.825 I print_info: n_embd           = 2048
0.00.039.825 I print_info: n_layer          = 24
0.00.039.829 I print_info: n_head           = 16
0.00.039.830 I print_info: n_head_kv        = 16
0.00.039.830 I print_info: n_rot            = 32
0.00.039.830 I print_info: n_swa            = 0
0.00.039.831 I print_info: n_embd_head_k    = 128
0.00.039.831 I print_info: n_embd_head_v    = 128
0.00.039.834 I print_info: n_gqa            = 1
0.00.039.835 I print_info: n_embd_k_gqa     = 2048
0.00.039.835 I print_info: n_embd_v_gqa     = 2048
0.00.039.836 I print_info: f_norm_eps       = 1.0e-05
0.00.039.836 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.836 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.840 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.840 I print_info: f_logit_scale    = 0.0e+00
0.00.039.842 I print_info: n_ff             = 8192
0.00.039.842 I print_info: n_expert         = 0
0.00.039.842 I print_info: n_expert_used    = 0
0.00.039.842 I print_info: causal attn      = 1
0.00.039.843 I print_info: pooling type     = 0
0.00.039.844 I print_info: rope type        = 2
0.00.039.844 I print_info: rope scaling     = linear
0.00.039.844 I print_info: freq_base_train  = 10000.0
0.00.039.845 I print_info: freq_scale_train = 1
0.00.039.845 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.845 I print_info: rope_finetuned   = unknown
0.00.039.845 I print_info: ssm_d_conv       = 0
0.00.039.845 I print_info: ssm_d_inner      = 0
0.00.039.845 I print_info: ssm_d_state      = 0
0.00.039.846 I print_info: ssm_dt_rank      = 0
0.00.039.846 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.846 I print_info: model type       = 1.4B
0.00.039.846 I print_info: model params     = 1.41 B
0.00.039.847 I print_info: general.name     = 1.4B
0.00.039.850 I print_info: vocab type       = BPE
0.00.039.850 I print_info: n_vocab          = 50304
0.00.039.850 I print_info: n_merges         = 50009
0.00.039.850 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.850 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: LF token         = 187 ''
0.00.039.851 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.851 I print_info: max token length = 1024
0.00.039.852 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.530.293 I load_tensors: offloading 24 repeating layers to GPU
0.00.530.316 I load_tensors: offloading output layer to GPU
0.00.530.317 I load_tensors: offloaded 25/25 layers to GPU
0.00.530.354 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.530.356 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.531.810 I llama_init_from_model: n_seq_max     = 1
0.00.531.814 I llama_init_from_model: n_ctx         = 128
0.00.531.814 I llama_init_from_model: n_ctx_per_seq = 128
0.00.531.815 I llama_init_from_model: n_batch       = 128
0.00.531.816 I llama_init_from_model: n_ubatch      = 128
0.00.531.816 I llama_init_from_model: flash_attn    = 0
0.00.531.819 I llama_init_from_model: freq_base     = 10000.0
0.00.531.820 I llama_init_from_model: freq_scale    = 1
0.00.531.820 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.531.822 I ggml_metal_init: allocating
0.00.531.911 I ggml_metal_init: found device: Apple M4
0.00.531.926 I ggml_metal_init: picking default device: Apple M4
0.00.533.778 I ggml_metal_init: using embedded metal library
0.00.540.617 I ggml_metal_init: GPU name:   Apple M4
0.00.540.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.540.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.540.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.540.628 I ggml_metal_init: simdgroup reduction   = true
0.00.540.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.540.629 I ggml_metal_init: has residency sets    = true
0.00.540.629 I ggml_metal_init: has bfloat            = true
0.00.540.629 I ggml_metal_init: use bfloat            = true
0.00.540.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.540.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.641 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.563.164 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.563.170 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.563.212 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.566.537 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.566.539 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.566.539 I llama_init_from_model: graph nodes  = 967
0.00.566.540 I llama_init_from_model: graph splits = 2
0.00.566.542 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.566.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.593.043 I 
0.00.593.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.593.118 I perplexity: tokenizing the input ..
0.00.599.855 I perplexity: tokenization took 6.733 ms
0.00.599.881 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.745.835 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.747.187 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.747.201 I llama_perf_context_print:        load time =     583.78 ms
0.00.747.202 I llama_perf_context_print: prompt eval time =     145.01 ms /   128 tokens (    1.13 ms per token,   882.70 tokens per second)
0.00.747.203 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.203 I llama_perf_context_print:       total time =     154.16 ms /   129 tokens
0.00.747.648 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.081s
sys	0m0.130s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.223 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.228 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.230 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.231 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.243 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.020 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.739 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.739 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.740 I llama_model_loader: - type  f32:  194 tensors
0.00.026.740 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.740 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.741 I print_info: file format = GGUF V3 (latest)
0.00.026.741 I print_info: file type   = Q5_K - Medium
0.00.026.742 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.603 I load: special tokens cache size = 25
0.00.040.549 I load: token to piece cache size = 0.2984 MB
0.00.040.552 I print_info: arch             = gptneox
0.00.040.552 I print_info: vocab_only       = 0
0.00.040.552 I print_info: n_ctx_train      = 2048
0.00.040.552 I print_info: n_embd           = 2048
0.00.040.552 I print_info: n_layer          = 24
0.00.040.555 I print_info: n_head           = 16
0.00.040.556 I print_info: n_head_kv        = 16
0.00.040.556 I print_info: n_rot            = 32
0.00.040.556 I print_info: n_swa            = 0
0.00.040.556 I print_info: n_embd_head_k    = 128
0.00.040.556 I print_info: n_embd_head_v    = 128
0.00.040.557 I print_info: n_gqa            = 1
0.00.040.558 I print_info: n_embd_k_gqa     = 2048
0.00.040.559 I print_info: n_embd_v_gqa     = 2048
0.00.040.559 I print_info: f_norm_eps       = 1.0e-05
0.00.040.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.560 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.560 I print_info: f_logit_scale    = 0.0e+00
0.00.040.561 I print_info: n_ff             = 8192
0.00.040.561 I print_info: n_expert         = 0
0.00.040.561 I print_info: n_expert_used    = 0
0.00.040.561 I print_info: causal attn      = 1
0.00.040.562 I print_info: pooling type     = 0
0.00.040.563 I print_info: rope type        = 2
0.00.040.565 I print_info: rope scaling     = linear
0.00.040.565 I print_info: freq_base_train  = 10000.0
0.00.040.565 I print_info: freq_scale_train = 1
0.00.040.565 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.566 I print_info: rope_finetuned   = unknown
0.00.040.566 I print_info: ssm_d_conv       = 0
0.00.040.566 I print_info: ssm_d_inner      = 0
0.00.040.567 I print_info: ssm_d_state      = 0
0.00.040.568 I print_info: ssm_dt_rank      = 0
0.00.040.568 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.568 I print_info: model type       = 1.4B
0.00.040.568 I print_info: model params     = 1.41 B
0.00.040.568 I print_info: general.name     = 1.4B
0.00.040.569 I print_info: vocab type       = BPE
0.00.040.569 I print_info: n_vocab          = 50304
0.00.040.569 I print_info: n_merges         = 50009
0.00.040.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.570 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.570 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.570 I print_info: LF token         = 187 ''
0.00.040.570 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.571 I print_info: max token length = 1024
0.00.040.571 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.685 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.700 I load_tensors: offloading output layer to GPU
0.00.601.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.736 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.737 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.603.167 I llama_init_from_model: n_seq_max     = 1
0.00.603.170 I llama_init_from_model: n_ctx         = 2048
0.00.603.170 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.171 I llama_init_from_model: n_batch       = 2048
0.00.603.171 I llama_init_from_model: n_ubatch      = 512
0.00.603.172 I llama_init_from_model: flash_attn    = 0
0.00.603.174 I llama_init_from_model: freq_base     = 10000.0
0.00.603.174 I llama_init_from_model: freq_scale    = 1
0.00.603.176 I ggml_metal_init: allocating
0.00.603.261 I ggml_metal_init: found device: Apple M4
0.00.603.276 I ggml_metal_init: picking default device: Apple M4
0.00.605.219 I ggml_metal_init: using embedded metal library
0.00.611.872 I ggml_metal_init: GPU name:   Apple M4
0.00.611.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.876 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.877 I ggml_metal_init: simdgroup reduction   = true
0.00.611.877 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.877 I ggml_metal_init: has residency sets    = true
0.00.611.877 I ggml_metal_init: has bfloat            = true
0.00.611.878 I ggml_metal_init: use bfloat            = true
0.00.611.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.470 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.393 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.703.399 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.703.434 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.603 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.707.605 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.707.605 I llama_init_from_model: graph nodes  = 967
0.00.707.606 I llama_init_from_model: graph splits = 2
0.00.707.611 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.736 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.381 I main: llama threadpool init, n_threads = 4
0.00.772.425 I 
0.00.772.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.446 I 
0.00.772.589 I sampler seed: 1234
0.00.772.593 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.605 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.605 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.605 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.613.230 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.613.230 I llama_perf_context_print:        load time =     761.20 ms
0.01.613.232 I llama_perf_context_print: prompt eval time =      52.69 ms /     7 tokens (    7.53 ms per token,   132.87 tokens per second)
0.01.613.232 I llama_perf_context_print:        eval time =     784.94 ms /    63 runs   (   12.46 ms per token,    80.26 tokens per second)
0.01.613.233 I llama_perf_context_print:       total time =     841.57 ms /    70 tokens
0.01.613.481 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.109s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.529 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.512 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.315 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.076 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.076 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.077 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.077 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.078 I llama_model_loader: - type  f32:  194 tensors
0.00.026.078 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.079 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.079 I print_info: file format = GGUF V3 (latest)
0.00.026.080 I print_info: file type   = Q5_K - Medium
0.00.026.083 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.057 I load: special tokens cache size = 25
0.00.040.123 I load: token to piece cache size = 0.2984 MB
0.00.040.128 I print_info: arch             = gptneox
0.00.040.128 I print_info: vocab_only       = 0
0.00.040.128 I print_info: n_ctx_train      = 2048
0.00.040.128 I print_info: n_embd           = 2048
0.00.040.129 I print_info: n_layer          = 24
0.00.040.133 I print_info: n_head           = 16
0.00.040.134 I print_info: n_head_kv        = 16
0.00.040.134 I print_info: n_rot            = 32
0.00.040.134 I print_info: n_swa            = 0
0.00.040.134 I print_info: n_embd_head_k    = 128
0.00.040.135 I print_info: n_embd_head_v    = 128
0.00.040.135 I print_info: n_gqa            = 1
0.00.040.136 I print_info: n_embd_k_gqa     = 2048
0.00.040.137 I print_info: n_embd_v_gqa     = 2048
0.00.040.137 I print_info: f_norm_eps       = 1.0e-05
0.00.040.138 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.138 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.139 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.139 I print_info: f_logit_scale    = 0.0e+00
0.00.040.140 I print_info: n_ff             = 8192
0.00.040.140 I print_info: n_expert         = 0
0.00.040.140 I print_info: n_expert_used    = 0
0.00.040.140 I print_info: causal attn      = 1
0.00.040.140 I print_info: pooling type     = 0
0.00.040.140 I print_info: rope type        = 2
0.00.040.141 I print_info: rope scaling     = linear
0.00.040.141 I print_info: freq_base_train  = 10000.0
0.00.040.143 I print_info: freq_scale_train = 1
0.00.040.143 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.144 I print_info: rope_finetuned   = unknown
0.00.040.144 I print_info: ssm_d_conv       = 0
0.00.040.144 I print_info: ssm_d_inner      = 0
0.00.040.144 I print_info: ssm_d_state      = 0
0.00.040.144 I print_info: ssm_dt_rank      = 0
0.00.040.144 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.144 I print_info: model type       = 1.4B
0.00.040.145 I print_info: model params     = 1.41 B
0.00.040.145 I print_info: general.name     = 1.4B
0.00.040.145 I print_info: vocab type       = BPE
0.00.040.146 I print_info: n_vocab          = 50304
0.00.040.147 I print_info: n_merges         = 50009
0.00.040.147 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.147 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.147 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.147 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.148 I print_info: LF token         = 187 ''
0.00.040.148 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.148 I print_info: max token length = 1024
0.00.040.148 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.045 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.065 I load_tensors: offloading output layer to GPU
0.00.609.066 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.122 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.609.124 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.610.466 I llama_init_from_model: n_seq_max     = 1
0.00.610.469 I llama_init_from_model: n_ctx         = 128
0.00.610.470 I llama_init_from_model: n_ctx_per_seq = 128
0.00.610.470 I llama_init_from_model: n_batch       = 128
0.00.610.470 I llama_init_from_model: n_ubatch      = 128
0.00.610.471 I llama_init_from_model: flash_attn    = 0
0.00.610.473 I llama_init_from_model: freq_base     = 10000.0
0.00.610.473 I llama_init_from_model: freq_scale    = 1
0.00.610.474 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.477 I ggml_metal_init: allocating
0.00.610.562 I ggml_metal_init: found device: Apple M4
0.00.610.578 I ggml_metal_init: picking default device: Apple M4
0.00.612.457 I ggml_metal_init: using embedded metal library
0.00.619.689 I ggml_metal_init: GPU name:   Apple M4
0.00.619.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.702 I ggml_metal_init: simdgroup reduction   = true
0.00.619.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.703 I ggml_metal_init: has residency sets    = true
0.00.619.703 I ggml_metal_init: has bfloat            = true
0.00.619.703 I ggml_metal_init: use bfloat            = true
0.00.619.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.712 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.884 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.919 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.179 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.180 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.181 I llama_init_from_model: graph nodes  = 967
0.00.646.181 I llama_init_from_model: graph splits = 2
0.00.646.184 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.430 I 
0.00.679.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.501 I perplexity: tokenizing the input ..
0.00.685.388 I perplexity: tokenization took 5.887 ms
0.00.685.399 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.713 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.823.045 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.823.061 I llama_perf_context_print:        load time =     668.89 ms
0.00.823.061 I llama_perf_context_print: prompt eval time =     136.06 ms /   128 tokens (    1.06 ms per token,   940.77 tokens per second)
0.00.823.062 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.062 I llama_perf_context_print:       total time =     143.63 ms /   129 tokens
0.00.823.474 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.080s
sys	0m0.150s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.782 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.721 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.727 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.729 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.731 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.733 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.061 I llama_model_loader: - type  f32:  194 tensors
0.00.025.061 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.062 I print_info: file format = GGUF V3 (latest)
0.00.025.062 I print_info: file type   = Q6_K
0.00.025.063 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.835 I load: special tokens cache size = 25
0.00.038.784 I load: token to piece cache size = 0.2984 MB
0.00.038.786 I print_info: arch             = gptneox
0.00.038.786 I print_info: vocab_only       = 0
0.00.038.787 I print_info: n_ctx_train      = 2048
0.00.038.787 I print_info: n_embd           = 2048
0.00.038.787 I print_info: n_layer          = 24
0.00.038.790 I print_info: n_head           = 16
0.00.038.791 I print_info: n_head_kv        = 16
0.00.038.791 I print_info: n_rot            = 32
0.00.038.791 I print_info: n_swa            = 0
0.00.038.793 I print_info: n_embd_head_k    = 128
0.00.038.793 I print_info: n_embd_head_v    = 128
0.00.038.794 I print_info: n_gqa            = 1
0.00.038.795 I print_info: n_embd_k_gqa     = 2048
0.00.038.800 I print_info: n_embd_v_gqa     = 2048
0.00.038.801 I print_info: f_norm_eps       = 1.0e-05
0.00.038.801 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.802 I print_info: f_logit_scale    = 0.0e+00
0.00.038.803 I print_info: n_ff             = 8192
0.00.038.803 I print_info: n_expert         = 0
0.00.038.807 I print_info: n_expert_used    = 0
0.00.038.807 I print_info: causal attn      = 1
0.00.038.809 I print_info: pooling type     = 0
0.00.038.809 I print_info: rope type        = 2
0.00.038.811 I print_info: rope scaling     = linear
0.00.038.811 I print_info: freq_base_train  = 10000.0
0.00.038.811 I print_info: freq_scale_train = 1
0.00.038.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.812 I print_info: rope_finetuned   = unknown
0.00.038.812 I print_info: ssm_d_conv       = 0
0.00.038.812 I print_info: ssm_d_inner      = 0
0.00.038.812 I print_info: ssm_d_state      = 0
0.00.038.812 I print_info: ssm_dt_rank      = 0
0.00.038.812 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.812 I print_info: model type       = 1.4B
0.00.038.813 I print_info: model params     = 1.41 B
0.00.038.813 I print_info: general.name     = 1.4B
0.00.038.813 I print_info: vocab type       = BPE
0.00.038.813 I print_info: n_vocab          = 50304
0.00.038.813 I print_info: n_merges         = 50009
0.00.038.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.815 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.815 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.815 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.815 I print_info: LF token         = 187 ''
0.00.038.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.816 I print_info: max token length = 1024
0.00.038.816 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.254 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.258 I load_tensors: offloading output layer to GPU
0.00.666.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.282 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.666.285 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.668.042 I llama_init_from_model: n_seq_max     = 1
0.00.668.044 I llama_init_from_model: n_ctx         = 2048
0.00.668.044 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.668.045 I llama_init_from_model: n_batch       = 2048
0.00.668.045 I llama_init_from_model: n_ubatch      = 512
0.00.668.046 I llama_init_from_model: flash_attn    = 0
0.00.668.047 I llama_init_from_model: freq_base     = 10000.0
0.00.668.047 I llama_init_from_model: freq_scale    = 1
0.00.668.049 I ggml_metal_init: allocating
0.00.668.098 I ggml_metal_init: found device: Apple M4
0.00.668.111 I ggml_metal_init: picking default device: Apple M4
0.00.669.592 I ggml_metal_init: using embedded metal library
0.00.675.588 I ggml_metal_init: GPU name:   Apple M4
0.00.675.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.593 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.593 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.594 I ggml_metal_init: simdgroup reduction   = true
0.00.675.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.594 I ggml_metal_init: has residency sets    = true
0.00.675.594 I ggml_metal_init: has bfloat            = true
0.00.675.595 I ggml_metal_init: use bfloat            = true
0.00.675.596 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.977 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.676 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.682 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.719 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.960 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.962 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.962 I llama_init_from_model: graph nodes  = 967
0.00.748.963 I llama_init_from_model: graph splits = 2
0.00.748.969 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.844 I main: llama threadpool init, n_threads = 4
0.00.813.891 I 
0.00.813.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.916 I 
0.00.814.086 I sampler seed: 1234
0.00.814.091 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.102 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.102 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.102 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.686.649 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.01.686.650 I llama_perf_context_print:        load time =     804.29 ms
0.01.686.651 I llama_perf_context_print: prompt eval time =      57.50 ms /     7 tokens (    8.21 ms per token,   121.74 tokens per second)
0.01.686.653 I llama_perf_context_print:        eval time =     812.15 ms /    63 runs   (   12.89 ms per token,    77.57 tokens per second)
0.01.686.653 I llama_perf_context_print:       total time =     873.58 ms /    70 tokens
0.01.686.912 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.108s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4807 (2679c3b5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.446 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.519 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.532 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.534 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.539 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.282 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.282 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.283 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.283 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.284 I llama_model_loader: - type  f32:  194 tensors
0.00.025.284 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.285 I print_info: file format = GGUF V3 (latest)
0.00.025.285 I print_info: file type   = Q6_K
0.00.025.286 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.319 I load: special tokens cache size = 25
0.00.039.371 I load: token to piece cache size = 0.2984 MB
0.00.039.376 I print_info: arch             = gptneox
0.00.039.376 I print_info: vocab_only       = 0
0.00.039.377 I print_info: n_ctx_train      = 2048
0.00.039.377 I print_info: n_embd           = 2048
0.00.039.377 I print_info: n_layer          = 24
0.00.039.382 I print_info: n_head           = 16
0.00.039.382 I print_info: n_head_kv        = 16
0.00.039.383 I print_info: n_rot            = 32
0.00.039.385 I print_info: n_swa            = 0
0.00.039.385 I print_info: n_embd_head_k    = 128
0.00.039.385 I print_info: n_embd_head_v    = 128
0.00.039.386 I print_info: n_gqa            = 1
0.00.039.387 I print_info: n_embd_k_gqa     = 2048
0.00.039.388 I print_info: n_embd_v_gqa     = 2048
0.00.039.388 I print_info: f_norm_eps       = 1.0e-05
0.00.039.389 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.389 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.389 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.390 I print_info: f_logit_scale    = 0.0e+00
0.00.039.390 I print_info: n_ff             = 8192
0.00.039.390 I print_info: n_expert         = 0
0.00.039.392 I print_info: n_expert_used    = 0
0.00.039.393 I print_info: causal attn      = 1
0.00.039.393 I print_info: pooling type     = 0
0.00.039.393 I print_info: rope type        = 2
0.00.039.393 I print_info: rope scaling     = linear
0.00.039.394 I print_info: freq_base_train  = 10000.0
0.00.039.394 I print_info: freq_scale_train = 1
0.00.039.394 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.394 I print_info: rope_finetuned   = unknown
0.00.039.395 I print_info: ssm_d_conv       = 0
0.00.039.395 I print_info: ssm_d_inner      = 0
0.00.039.395 I print_info: ssm_d_state      = 0
0.00.039.395 I print_info: ssm_dt_rank      = 0
0.00.039.395 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.395 I print_info: model type       = 1.4B
0.00.039.396 I print_info: model params     = 1.41 B
0.00.039.396 I print_info: general.name     = 1.4B
0.00.039.396 I print_info: vocab type       = BPE
0.00.039.398 I print_info: n_vocab          = 50304
0.00.039.398 I print_info: n_merges         = 50009
0.00.039.398 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.398 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.399 I print_info: LF token         = 187 ''
0.00.039.399 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.399 I print_info: max token length = 1024
0.00.039.400 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.470 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.483 I load_tensors: offloading output layer to GPU
0.00.614.484 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.542 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.614.546 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.616.038 I llama_init_from_model: n_seq_max     = 1
0.00.616.040 I llama_init_from_model: n_ctx         = 128
0.00.616.041 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.041 I llama_init_from_model: n_batch       = 128
0.00.616.042 I llama_init_from_model: n_ubatch      = 128
0.00.616.042 I llama_init_from_model: flash_attn    = 0
0.00.616.044 I llama_init_from_model: freq_base     = 10000.0
0.00.616.045 I llama_init_from_model: freq_scale    = 1
0.00.616.045 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.050 I ggml_metal_init: allocating
0.00.616.151 I ggml_metal_init: found device: Apple M4
0.00.616.165 I ggml_metal_init: picking default device: Apple M4
0.00.618.061 I ggml_metal_init: using embedded metal library
0.00.624.621 I ggml_metal_init: GPU name:   Apple M4
0.00.624.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.628 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.629 I ggml_metal_init: simdgroup reduction   = true
0.00.624.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.630 I ggml_metal_init: has residency sets    = true
0.00.624.630 I ggml_metal_init: has bfloat            = true
0.00.624.630 I ggml_metal_init: use bfloat            = true
0.00.624.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.636 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.816 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.183 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.646.188 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.242 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.649.441 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.649.442 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.649.443 I llama_init_from_model: graph nodes  = 967
0.00.649.443 I llama_init_from_model: graph splits = 2
0.00.649.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.649.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.415 I 
0.00.680.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.506 I perplexity: tokenizing the input ..
0.00.687.055 I perplexity: tokenization took 6.547 ms
0.00.687.070 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.121 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.820.416 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.820.431 I llama_perf_context_print:        load time =     670.96 ms
0.00.820.432 I llama_perf_context_print: prompt eval time =     131.42 ms /   128 tokens (    1.03 ms per token,   973.97 tokens per second)
0.00.820.433 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.433 I llama_perf_context_print:       total time =     140.02 ms /   129 tokens
0.00.820.900 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.080s
sys	0m0.142s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4807 (2679c3b5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10b6079a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10b607e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10b6082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10b608740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10b608bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10b609020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10b609490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10b609900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10b609d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10b60a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10b60a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10b60acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10b60b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10b60bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10b60c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10b60cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10b60d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10b60dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10b60e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10b60ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10b60f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10b60fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10b610180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10b610a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10b611140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10b611400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10b6116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10b611b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10b612250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10b6126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10b612c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10b613190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10b613600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10b6138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10b613d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10b6141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10b614610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10b614a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10b614ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10b615360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10b6157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10b615c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10b6160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10b616520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10b616990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10b616e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10b617270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10b6176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10b617e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10b6182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10b618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10b618bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10b619030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10b6194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10b619910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10b61a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10b61a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10b61a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10b61ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10b61b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10b61b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10b61b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10b61be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10b61c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10b61c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10b61cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10b61d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10b61d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10b61dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10b61e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10b61e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10b61eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10b61f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10b61f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10b61fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10b6200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10b6206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10b620c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10b621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10b6217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10b621d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10b622310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10b6228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10b622e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10b623420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10b6239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10b623f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10b624530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10b624ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10b625090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10b625640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10b625bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10b6261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10b626750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10b626d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10b6272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10b627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10b6179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10b627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10b628430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10b6288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10b628e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10b629400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10b6299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10b629f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10b62a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10b62aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10b62b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10b62b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10b62bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10b62c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10b62c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10b62cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10b62d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10b62d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10b62dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10b62e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10b62e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10b62eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10b62f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10b62f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10b62fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10b62ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10b630490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10b630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10b630e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10b631390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10b631890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10b631d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10b632290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10b632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10b632c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10b633190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10b633690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10b633b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10b634090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10b634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10b634a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10b634f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10b635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10b635990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10b635e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10b636390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10b636890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10b636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10b637290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10b637790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10b637c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10b638190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10b638690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10b638b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10b639090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10b639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10b639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10b639f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10b63a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10b63a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10b63ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10b63b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10b63b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10b63bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10b63c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10b63c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10b63cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10b63d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10b63d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10b63db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10b63e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10b63e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10b63ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10b63ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10b63f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10b63f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10b63fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10b640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10b640890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10b640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10b641290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10b641790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10b641c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10b642190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10b642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10b642b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10b643090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10b643590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10b643a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10b643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10b644490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10b644990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10b644e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10b645390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10b645890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10b645d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10b646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10b646840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10b646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10b6473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10b647950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10b647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10b648570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10b648b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10b649370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10b649810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10b649ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10b64a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10b64a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10b64aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10b64b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10b64b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10b64bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10b64c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10b64c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10b64cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10b64d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10b64d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10b64df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10b64e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10b64e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10b64eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10b64f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10b64f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10b64fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10b650430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10b650980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10b650ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10b651420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10b651970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10b651ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10b652410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10b652960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10b652eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10b653400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10b653950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10b653ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10b6543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10b654940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10b654e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10b6553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10b655930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10b655e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10b6563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10b656920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10b656e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10b6573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10b657910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10b657e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10b6583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10b658900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10b658e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10b6593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10b6598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10b659e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10b65a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10b65a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10b65ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10b65b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10b65b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10b65be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10b65c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10b65c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10b65ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10b65d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10b65d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10b65de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10b65e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10b65e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10b65edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10b65f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10b65f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10b65fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10b660070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10b660510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10b6609b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10b660e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10b6612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10b661790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10b661c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10b6620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10b662570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10b662a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10b662eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10b663350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10b6637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10b663c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10b664130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10b6645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10b664a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10b664f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10b6653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10b665850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10b665cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10b666190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10b6666e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10b666e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10b667520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10b667c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10b668360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10b668620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10b668e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10b6690d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10b6696e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.743.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.451 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10b61fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10b625350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10b61f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10b627570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10b624da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10b62c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10b62be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10b62b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10b626fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10b621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10b629c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10b646b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10b626a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10b6214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10b6247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10b623130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10b6296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10b646550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10b62b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10b626460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10b620f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10b624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10b622b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10b629110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10b62ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10b625eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10b620960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10b623c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10b628b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10b62a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10b625900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10b6236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10b62a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10b669390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10b647c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10b648830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10b64a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10b6106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10b60a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10b61aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10b6688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10b619bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10b627b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10b64a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10b648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10b611df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10b669b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10b669e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10b66a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10b66a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10b66a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10b66a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10b66abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10b66ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10b66b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10b66b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10b66b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10b66b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10b66bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10b66bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10b66c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10b66c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10b66c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10b66ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10b66ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10b66cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10b66d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10b66d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10b66d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10b66da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10b66dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10b66e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10b66e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10b66e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10b66e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10b66eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10b66edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10b66f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10b66f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10b66f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10b66f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10b66fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10b66fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10b670100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10b6703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10b670680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10b670940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10b670c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10b670ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10b671180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10b671440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10b671700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10b6719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10b671c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10b671f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10b672200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10b6724c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10b672780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10b672a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10b672d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10b672fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10b673280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10b673540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10b673800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10b673ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10b673d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10b674040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10b674300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10b6745c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10b674880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10b674b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10b674e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10b6750c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10b675380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10b675640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10b675900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10b675bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10b675e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10b676140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10b676400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10b6766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10b676980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10b676c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10b676f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10b6771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10b677480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10b677740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10b677a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10b677cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10b677f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10b678240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10b678500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10b6787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10b678a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10b678d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10b679000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10b6792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10b679580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10b679840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10b679b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10b679dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10b67a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10b67a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10b67a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10b67a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10b67ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10b67ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10b67b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10b67b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10b67b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10b67b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10b67bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10b67bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10b67c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10b67c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10b67c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10b67c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10b67cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10b67cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10b67d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10b67d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10b67d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10b67da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10b67dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10b67dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10b67e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10b67e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10b67e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10b67eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10b67ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10b67f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10b67f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10b67f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10b67f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10b67fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10b67fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10b6800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10b680380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10b680640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10b680900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10b680bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10b680e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10b681140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10b681400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10b6816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10b681980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10b681c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10b681f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10b6821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10b682480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10b682740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10b682a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10b682cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10b682f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10b683240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10b683500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10b6837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10b683a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10b683d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10b684000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10b6842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10b684580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10b684840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10b684b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10b684dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10b685080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10b685340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10b685600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10b6858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10b685b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10b686150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10b686410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10b6866d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10b686990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10b686c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10b686f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10b6871d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10b687490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10b687750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10b687a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10b687cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10b687f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10b688250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10b688510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10b6887d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10b688a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10b688d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10b689010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10b689560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10b689ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10b68a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10b68a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10b68aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10b68aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10b68b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10b68ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10b68bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10b68c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10b68ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10b68cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10b68d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10b68da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10b68dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10b68e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10b68ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10b68efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10b68f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10b68fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10b68ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10b6904f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10b690a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10b690f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10b6914e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10b691a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10b691f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10b6924d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10b692a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10b692f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10b6934c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10b693a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10b693f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10b6944b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10b694a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10b694f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10b6954a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10b6959f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10b695f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10b6963e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10b696880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10b696d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10b6971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10b697660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10b697b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10b697fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10b698440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10b6988e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10b698d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10b699220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10b6996c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10b699b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10b69a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10b69a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10b69a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10b69ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10b69b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10b69b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10b69bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10b69c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10b69c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10b69c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10b69ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10b69d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10b69d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10b69df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10b69e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10b69ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10b69f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10b69f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10b69ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10b6a0220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10b6a0830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x109f06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109f07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109f07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109f08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x109f08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109f09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x109f09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x109f0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x109f0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x109f0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x109f0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109f0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109f0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x109f0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x109f0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109f0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x109f0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x109f0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x109f0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x109f0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109f0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x109f0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109f0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109f0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109f0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109f102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109f10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109f10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109f10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109f11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109f118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109f11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109f121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109f12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109f12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109f12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109f13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109f137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109f13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109f14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x109f149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109f14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109f15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x109f156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109f15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x109f15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109f16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x109f16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109f17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109f17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109f17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109f18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x109f184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109f18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109f18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109f19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109f196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109f19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x109f19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109f1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x109f1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x109f1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109f1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109f1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x109f1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109f1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109f1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109f1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109f1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x109f1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x109f1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109f1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109f1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x109f1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109f1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x109f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109f1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109f1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109f1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109f20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109f20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109f20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109f20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109f212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109f21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109f21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109f22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x109f22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109f22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x109f234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x109f23a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109f24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x109f245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x109f24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109f25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109f256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x109f25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109f26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x109f267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109f26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x109f27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109f278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109f27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x109f282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109f28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109f291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109f296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109f2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109f2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109f2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x109f2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109f2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109f2b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109f2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109f2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x109f2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109f2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109f2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x109f2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x109f2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109f2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x109f2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109f2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109f2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x109f2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109f2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109f2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109f304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x109f309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x109f30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109f313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x109f318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x109f31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x109f322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109f327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109f32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109f331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109f336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x109f33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109f340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x109f345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109f34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109f34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109f354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109f359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109f35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x109f363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x109f368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109f36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x109f372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109f377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x109f37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109f381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x109f386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109f38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109f390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x109f395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109f39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x109f39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109f3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x109f3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109f3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109f3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x109f3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109f3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109f3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x109f3cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109f3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109f3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109f3dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109f3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x109f3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109f3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x109f3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109f3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x109f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x109f3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109f403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x109f408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x109f40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x109f41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x109f419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x109f41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109f425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109f42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109f431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109f439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x109f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x109f44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109f44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109f44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x109f45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109f459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109f45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109f46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109f46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109f47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x109f47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109f48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109f48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x109f48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109f48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109f49540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109f49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109f49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109f4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109f4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109f4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109f4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109f4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109f4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109f4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109f4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109f4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109f4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109f4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109f4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109f4e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109f4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109f4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109f4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109f4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109f4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109f504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109f50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x109f50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109f514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109f51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x109f51f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x109f524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109f52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x109f52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109f534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x109f539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x109f53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109f54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109f549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x109f54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x109f55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x109f559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x109f55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x109f56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x109f569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x109f56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x109f57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x109f579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x109f57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109f58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109f589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109f58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109f59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109f598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109f59d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x109f5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109f5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109f5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109f5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109f5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109f5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109f5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109f5c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109f5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109f5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109f5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109f5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109f5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x109f5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x109f5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x109f5e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x109f5ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x109f5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x109f5f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x109f5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x109f5fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x109f60340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x109f607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109f60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109f61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109f61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109f62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109f629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109f62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109f63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109f63720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109f63d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.826s
user	0m0.286s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4807 (2679c3b5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15360e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15360ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15360f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15360fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153610030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1536105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153610b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153611140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1536116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153611bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1536120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1536125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153613110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1536138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1536140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1536147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153614f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153615630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153615d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153616520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153616c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153617360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153617a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153618a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153618d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153619310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153619f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15361a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15361a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15361ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15361aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15361b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15361bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15361bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15361c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15361c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15361cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15361d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15361d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15361db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15361dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15361e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15361e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15361ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15361f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15361f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153620110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153620720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153620d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153621340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153621950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153621f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153622570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153622d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153623200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1536236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153623960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153623f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153624a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153624ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153625800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1536265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153626a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153626f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1536273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153627d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1536281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1536286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153628c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153629190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1536296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153629c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15362a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15362a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15362ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15362b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15362b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15362bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15362c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15362c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15362cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15362d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15362d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15362dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15362e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15362e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15362ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15362f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15362f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15362fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153630120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15361fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153630590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153630d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153631290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1536317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153631d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153632280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1536327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153632d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1536337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153633d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153634260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1536347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153634d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x153635250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1536356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1536364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153636970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153636e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1536372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153637bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153638090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153638530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1536389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153639310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1536397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15363a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15363a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15363aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15363aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15363b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15363b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15363bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15363c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15363c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15363ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15363cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15363d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15363d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15363dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15363e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15363e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15363eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15363ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15363f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15363f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15363fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153640210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1536406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153640b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153640ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153641dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153642710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153642bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1536434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1536442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153644770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153644c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1536450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153645550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1536459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153645e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153646330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1536467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153646c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153647110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1536475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153647a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153647ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153648390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153648830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153648cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153649170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153649610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153649ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153649f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15364a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15364a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15364ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15364b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15364b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15364bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15364bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15364c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15364c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15364cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15364d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15364d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15364dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15364e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15364e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15364ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15364f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15364fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15364fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1536503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1536509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1536511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153651680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x153651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153651fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153652770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153652cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153653210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153653760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153653cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153654200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153654750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153654ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1536551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153655740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153655c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1536561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153656730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153656c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1536571d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153657720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153657c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1536581c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153658710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153658c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1536591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153659700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153659c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15365a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15365a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15365ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15365b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15365b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15365bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15365c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15365c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15365cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15365d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15365d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15365dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15365e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15365e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15365ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15365f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15365f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15365fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153660140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153660690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153660be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153661130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153661680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153661bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153662120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153662670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153662bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153663660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153663bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153664100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153664650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153664ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1536650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153665590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153665a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153665ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153666370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153666810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153666cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153667150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1536675f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153667a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153667f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1536683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153668870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153668d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1536691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153669650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x153669af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x153669f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15366a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15366a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15366ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15366b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15366b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15366bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15366bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15366c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15366c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15366d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15366d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15366df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15366e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15366e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15366f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15366f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15366f9e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.109.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.109.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153707bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153708060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1537084d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153708940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153708db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153709220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153709690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153709b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153709f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15370a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15370a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15370af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15370ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15370c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15370ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15370d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15370d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15370df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15370e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15370edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15370f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15370fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x153710330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153710a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x153711170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153711430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1537116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153711b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153711fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153712440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1537128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x153712de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153713250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153713980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153713df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153714260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1537146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153714b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153714fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153715420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153715d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153716170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1537165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153716a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153716ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x153717330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1537177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153717c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153718080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1537184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153718960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153718dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153719240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1537196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153719c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15371a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15371a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15371aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15371ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15371b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15371b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15371bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15371c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15371c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15371c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15371cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15371d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15371d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15371dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15371df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15371e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15371e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15371ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15371f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15371f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15371f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15371fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1537202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153720730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153720ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153721010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153721480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1537218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153721d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1537221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153722640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153722ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153722f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153723390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153723800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153723c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1537240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153724550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1537249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153724e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1537252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153725710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153725ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x153726460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1537268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153726d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1537271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x153727620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153727a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153727f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1537287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153728c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1537290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x153729530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1537299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153729e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15372a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15372a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15372ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15372afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15372b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15372b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15372bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15372c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15372c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15372ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15372cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15372d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15372d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15372dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15372e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15372e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15372e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15372edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15372f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15372f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15372fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15372ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153730420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x153730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153731170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1537315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153731a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153731ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153732330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1537327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153732c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153733080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1537334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153733960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153733dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153734240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1537346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153734b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153735400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153735870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153735ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153736150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1537365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153736a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153736ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153737310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153737780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153737bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153738060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153738c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153738f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153739af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153739f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15373a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15373a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15373acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15373b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15373b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15373ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15373be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15373c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15373c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15373cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15373d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15373d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15373d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15373dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15373e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15373e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15373ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15373ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15373f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15373f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15373fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153740570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1537409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153740e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1537412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153741730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153741ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153742010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153742480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1537429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153742ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153743360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1537437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x153743c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1537440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1537445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153744ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153745910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153745ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153746490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153746a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1537475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153748150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153748710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153748cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153749290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153749850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153749e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15374a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15374a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15374af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15374b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15374bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15374c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15374c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15374cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15374d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15374d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15374dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15374e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15374e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15374ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15374f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15374fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15374ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153750590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153750b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153751110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1537516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153751c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153752250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153752810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153752dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153753390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153753950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1537544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153754a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x153755050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153755610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153755bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153756190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153756750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153756d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1537572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153757890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153757e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153758410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1537589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153758f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153759550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153759b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15375a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15375a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15375aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15375af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15375b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15375b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15375be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15375c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15375c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15375cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15375d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15375d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15375dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15375e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15375e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15375eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15375f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15375f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15375fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15375ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x153760410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x153760910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x153760e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x153761310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153761810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153762220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153762940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153763060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153763780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153763a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153764230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1537644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153764b00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156e044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156e04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156e04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156e05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156e056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156e05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156e05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156e063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156e06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156e06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156e07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156e07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156e08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156e08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156e09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156e09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156e0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156e0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156e0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156e0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156e0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156e0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156e0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156e0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156e0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156e0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156e0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156e0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156e0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156e0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156e0f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156e0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156e0fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156e102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156e10710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156e10b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156e10ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156e11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156e118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156e11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156e121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156e12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156e12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156e12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156e13370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156e137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156e13c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156e140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156e14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156e149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156e14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156e15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156e156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156e15b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156e15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156e16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156e16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156e16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156e17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156e17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156e17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156e18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156e184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156e18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156e18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156e19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156e196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156e19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156e19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156e1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156e1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156e1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156e1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156e1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156e1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156e1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156e1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156e1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156e1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156e1d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156e1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156e1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156e1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156e1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156e1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156e1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156e1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156e1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156e1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156e20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156e20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156e20a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156e20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156e212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156e21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156e21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156e22420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156e22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156e22ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156e234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156e23a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156e24000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156e245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156e24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156e25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156e256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156e25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156e26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156e267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156e26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156e27330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156e278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156e27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156e282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156e287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156e28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156e291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156e296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156e29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156e2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156e2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156e2aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156e2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156e2b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156e2b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156e2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156e2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156e2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156e2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156e2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156e2dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156e2e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156e2e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156e2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156e2f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156e2f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156e2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156e2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156e304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156e309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156e30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156e313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156e318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156e31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156e322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156e327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156e32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156e331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156e336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156e33be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156e340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156e345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156e34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156e34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156e354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156e359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156e35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156e363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156e368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156e372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156e377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156e37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156e381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156e386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156e38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156e390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156e395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156e39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156e39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156e3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156e3a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156e3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156e3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156e3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156e3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156e3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156e3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156e3cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156e3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156e3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156e3dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156e3e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156e3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156e3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156e3efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156e3f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156e3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156e3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156e403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156e408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156e40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156e41440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156e419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156e41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156e425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156e42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156e431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156e439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156e43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156e44120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156e44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156e44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156e45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156e459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156e45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156e46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156e46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156e47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156e47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156e47ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156e48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156e48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156e48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156e48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156e49540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156e49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156e49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156e4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156e4aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156e4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156e4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156e4ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156e4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156e4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156e4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156e4cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156e4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156e4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156e4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156e4e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156e4ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156e4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156e4f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156e4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156e4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156e504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156e50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156e514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156e51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156e51f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156e524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156e52a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156e52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156e534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156e539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156e53f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156e54490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156e549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156e54f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156e55480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156e559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156e55f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156e56470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156e569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156e56f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156e57460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156e579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156e57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156e58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156e589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156e58ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156e59440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156e598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156e59d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156e5a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156e5a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156e5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156e5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156e5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156e5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156e5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156e5c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156e5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156e5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156e5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156e5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156e5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x156e5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x156e5e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x156e5e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x156e5ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x156e5f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x156e5f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x156e5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x156e5fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x156e60340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x156e607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156e60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156e61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156e61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156e62290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156e629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156e62c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156e63460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156e63720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156e63d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.971s
user	0m0.232s
sys	0m0.168s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.77 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.79 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.56 sec*proc (2 tests)

Total Test time (real) =   2.57 sec
        2.59 real         0.53 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.26 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.56 sec*proc (2 tests)

Total Test time (real) =   0.57 sec
        0.57 real         0.13 user         0.08 sys
```
