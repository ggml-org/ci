Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.1s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.335s
user	0m0.556s
sys	0m0.798s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target xxhash
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target build_info
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX shared library libllama.dylib
[ 20%] Built target llama-gguf
[ 20%] Built target llama
[ 20%] Built target llama-gguf-hash
[ 20%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 20%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 22%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Linking CXX executable ../../bin/llama-simple-chat
[ 26%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 26%] Linking CXX executable ../../bin/llama-quantize-stats
[ 26%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 27%] Linking CXX executable ../../bin/llama-simple
[ 27%] Built target llava
[ 28%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Linking CXX static library libllava_static.a
[ 31%] Linking CXX static library libcommon.a
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Built target test-c
[ 31%] Built target llama-simple
[ 31%] Built target llama-quantize-stats
[ 31%] Built target llama-simple-chat
[ 31%] Built target llava_static
[ 31%] Built target common
[ 31%] Built target llava_shared
[ 31%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-grammar-integration
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 43%] Linking CXX executable ../bin/test-log
[ 43%] Linking CXX executable ../bin/test-chat-template
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-log
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-sampling
[ 46%] Built target test-chat-template
[ 47%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 47%] Built target test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Built target test-arg-parser
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 49%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 52%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 54%] Linking CXX executable ../bin/test-autorelease
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 56%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-rope
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../../bin/llama-batched
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-barrier
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Built target llama-batched-bench
[ 63%] Built target test-quantize-perf
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Built target test-rope
[ 64%] Built target llama-batched
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-eval-callback
[ 65%] Built target test-json-schema-to-grammar
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 66%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Built target llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Built target llama-infill
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-lookup
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Generating completion.js.hpp
[ 84%] Built target llama-lookup-merge
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-cli
[ 84%] Generating deps_daisyui.min.css.hpp
[ 85%] Generating deps_markdown-it.js.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 87%] Built target llama-quantize
[ 87%] Built target llama-parallel
[ 87%] Built target llama-perplexity
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Built target llama-passkey
[ 88%] Built target llama-retrieval
[ 89%] Generating deps_tailwindcss.js.hpp
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Generating deps_vue.esm-browser.js.hpp
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 94%] Generating index.html.hpp
[ 94%] Built target llama-speculative-simple
[ 94%] Built target llama-tokenize
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.502s
user	0m5.909s
sys	0m9.443s

main: quantize time =  5702.78 ms
main:    total time =  5702.79 ms

main: quantize time =  2139.70 ms
main:    total time =  2139.70 ms

main: quantize time =  2420.69 ms
main:    total time =  2420.69 ms

main: quantize time =  2007.15 ms
main:    total time =  2007.15 ms

main: quantize time =  3198.70 ms
main:    total time =  3198.70 ms

main: quantize time =  5166.85 ms
main:    total time =  5166.85 ms

main: quantize time =  5775.49 ms
main:    total time =  5775.49 ms

main: quantize time =  6929.20 ms
main:    total time =  6929.20 ms

main: quantize time =  5894.67 ms
main:    total time =  5894.67 ms

main: quantize time =  4525.71 ms
main:    total time =  4525.71 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.167 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.517 I main: llama backend init
0.00.000.548 I main: load the model and apply lora adapter, if any
0.00.030.250 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.833 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.848 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.849 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.850 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.850 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.851 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.856 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.856 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.859 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.860 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.861 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.905 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.906 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.906 I llama_model_loader: - type  f32:  194 tensors
0.00.059.907 I llama_model_loader: - type  f16:   98 tensors
0.00.088.732 I llm_load_vocab: special tokens cache size = 25
0.00.095.422 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.424 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.425 I llm_load_print_meta: arch             = gptneox
0.00.095.425 I llm_load_print_meta: vocab type       = BPE
0.00.095.425 I llm_load_print_meta: n_vocab          = 50304
0.00.095.425 I llm_load_print_meta: n_merges         = 50009
0.00.095.426 I llm_load_print_meta: vocab_only       = 0
0.00.095.426 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.426 I llm_load_print_meta: n_embd           = 2048
0.00.095.426 I llm_load_print_meta: n_layer          = 24
0.00.095.429 I llm_load_print_meta: n_head           = 16
0.00.095.430 I llm_load_print_meta: n_head_kv        = 16
0.00.095.430 I llm_load_print_meta: n_rot            = 32
0.00.095.432 I llm_load_print_meta: n_swa            = 0
0.00.095.432 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.432 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.433 I llm_load_print_meta: n_gqa            = 1
0.00.095.434 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.434 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.435 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.435 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.435 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.435 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.436 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.436 I llm_load_print_meta: n_ff             = 8192
0.00.095.436 I llm_load_print_meta: n_expert         = 0
0.00.095.437 I llm_load_print_meta: n_expert_used    = 0
0.00.095.437 I llm_load_print_meta: causal attn      = 1
0.00.095.437 I llm_load_print_meta: pooling type     = 0
0.00.095.437 I llm_load_print_meta: rope type        = 2
0.00.095.437 I llm_load_print_meta: rope scaling     = linear
0.00.095.438 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.438 I llm_load_print_meta: freq_scale_train = 1
0.00.095.438 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.438 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.438 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.438 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.439 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.439 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.439 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.446 I llm_load_print_meta: model type       = 1.4B
0.00.095.447 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.447 I llm_load_print_meta: model params     = 1.41 B
0.00.095.448 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.448 I llm_load_print_meta: general.name     = 1.4B
0.00.095.448 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.448 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.449 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.449 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.449 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.095.449 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.449 I llm_load_print_meta: max token length = 1024
0.00.097.439 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.439 I llm_load_tensors: offloading output layer to GPU
0.00.097.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.451 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.452 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.346 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.347 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.347 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.347 I llama_new_context_with_model: n_batch       = 2048
0.00.098.347 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.348 I llama_new_context_with_model: flash_attn    = 0
0.00.098.348 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.348 I llama_new_context_with_model: freq_scale    = 1
0.00.098.348 I ggml_metal_init: allocating
0.00.098.352 I ggml_metal_init: found device: Apple M4
0.00.098.353 I ggml_metal_init: picking default device: Apple M4
0.00.098.941 I ggml_metal_init: using embedded metal library
0.00.107.190 I ggml_metal_init: GPU name:   Apple M4
0.00.107.192 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.107.192 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.107.193 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.107.193 I ggml_metal_init: simdgroup reduction   = true
0.00.107.193 I ggml_metal_init: simdgroup matrix mul. = true
0.00.107.193 I ggml_metal_init: has bfloat            = true
0.00.107.193 I ggml_metal_init: use bfloat            = true
0.00.107.194 I ggml_metal_init: hasUnifiedMemory      = true
0.00.107.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.142.470 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.142.475 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.142.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.143.432 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.143.433 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.143.433 I llama_new_context_with_model: graph nodes  = 967
0.00.143.434 I llama_new_context_with_model: graph splits = 2
0.00.143.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.219.482 I main: llama threadpool init, n_threads = 4
0.00.219.519 I 
0.00.219.538 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.219.538 I 
0.00.219.619 I sampler seed: 1234
0.00.219.623 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.219.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.219.649 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.219.649 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.083.048 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54953.56 tokens per second)
0.02.083.049 I llama_perf_context_print:        load time =     189.22 ms
0.02.083.050 I llama_perf_context_print: prompt eval time =      38.95 ms /     7 tokens (    5.56 ms per token,   179.71 tokens per second)
0.02.083.050 I llama_perf_context_print:        eval time =    1821.49 ms /    63 runs   (   28.91 ms per token,    34.59 tokens per second)
0.02.083.052 I llama_perf_context_print:       total time =    1863.57 ms /    70 tokens
0.02.083.237 I ggml_metal_free: deallocating

real	0m2.404s
user	0m0.143s
sys	0m0.094s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.062 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.102 I main: load the model and apply lora adapter, if any
0.00.009.745 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.405 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.412 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.414 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.654 I llama_model_loader: - type  f32:  194 tensors
0.00.037.654 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.270 I llm_load_vocab: special tokens cache size = 25
0.00.068.680 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.684 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.684 I llm_load_print_meta: arch             = gptneox
0.00.068.685 I llm_load_print_meta: vocab type       = BPE
0.00.068.685 I llm_load_print_meta: n_vocab          = 50304
0.00.068.685 I llm_load_print_meta: n_merges         = 50009
0.00.068.685 I llm_load_print_meta: vocab_only       = 0
0.00.068.686 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.686 I llm_load_print_meta: n_embd           = 2048
0.00.068.686 I llm_load_print_meta: n_layer          = 24
0.00.068.690 I llm_load_print_meta: n_head           = 16
0.00.068.691 I llm_load_print_meta: n_head_kv        = 16
0.00.068.691 I llm_load_print_meta: n_rot            = 32
0.00.068.691 I llm_load_print_meta: n_swa            = 0
0.00.068.691 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.691 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.692 I llm_load_print_meta: n_gqa            = 1
0.00.068.693 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.693 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.694 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.694 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.694 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.695 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.695 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.695 I llm_load_print_meta: n_ff             = 8192
0.00.068.698 I llm_load_print_meta: n_expert         = 0
0.00.068.698 I llm_load_print_meta: n_expert_used    = 0
0.00.068.698 I llm_load_print_meta: causal attn      = 1
0.00.068.698 I llm_load_print_meta: pooling type     = 0
0.00.068.698 I llm_load_print_meta: rope type        = 2
0.00.068.698 I llm_load_print_meta: rope scaling     = linear
0.00.068.699 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.699 I llm_load_print_meta: freq_scale_train = 1
0.00.068.699 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.699 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.700 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.700 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.700 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.713 I llm_load_print_meta: model type       = 1.4B
0.00.068.714 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.714 I llm_load_print_meta: model params     = 1.41 B
0.00.068.714 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.715 I llm_load_print_meta: general.name     = 1.4B
0.00.068.715 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.715 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.715 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.715 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.716 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.716 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.716 I llm_load_print_meta: max token length = 1024
0.00.071.205 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.205 I llm_load_tensors: offloading output layer to GPU
0.00.071.206 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.216 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.217 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.262 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.263 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.263 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.264 I llama_new_context_with_model: n_batch       = 2048
0.00.072.264 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.264 I llama_new_context_with_model: flash_attn    = 0
0.00.072.265 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.265 I llama_new_context_with_model: freq_scale    = 1
0.00.072.265 I ggml_metal_init: allocating
0.00.072.273 I ggml_metal_init: found device: Apple M4
0.00.072.275 I ggml_metal_init: picking default device: Apple M4
0.00.073.028 I ggml_metal_init: using embedded metal library
0.00.075.426 I ggml_metal_init: GPU name:   Apple M4
0.00.075.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.429 I ggml_metal_init: simdgroup reduction   = true
0.00.075.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.430 I ggml_metal_init: has bfloat            = true
0.00.075.430 I ggml_metal_init: use bfloat            = true
0.00.075.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.431 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.661 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.109.676 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.701 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.756 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.110.758 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.110.758 I llama_new_context_with_model: graph nodes  = 967
0.00.110.758 I llama_new_context_with_model: graph splits = 2
0.00.110.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.377.248 I main: llama threadpool init, n_threads = 4
0.01.377.322 I 
0.01.377.363 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.01.377.363 I 
0.01.377.908 I sampler seed: 1234
0.01.377.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.377.982 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.377.985 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.377.985 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.479.843 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.02.479.844 I llama_perf_context_print:        load time =    1367.50 ms
0.02.479.844 I llama_perf_context_print: prompt eval time =      42.76 ms /     7 tokens (    6.11 ms per token,   163.71 tokens per second)
0.02.479.845 I llama_perf_context_print:        eval time =    1056.15 ms /    63 runs   (   16.76 ms per token,    59.65 tokens per second)
0.02.479.846 I llama_perf_context_print:       total time =    1102.60 ms /    70 tokens
0.02.480.024 I ggml_metal_free: deallocating

real	0m2.497s
user	0m0.124s
sys	0m0.269s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.018.904 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.678 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.685 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.690 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.690 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.691 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.749 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.634 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.635 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.637 I llama_model_loader: - type  f32:  194 tensors
0.00.048.637 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.638 I llama_model_loader: - type q6_K:    1 tensors
0.00.082.615 I llm_load_vocab: special tokens cache size = 25
0.00.093.173 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.177 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.177 I llm_load_print_meta: arch             = gptneox
0.00.093.178 I llm_load_print_meta: vocab type       = BPE
0.00.093.178 I llm_load_print_meta: n_vocab          = 50304
0.00.093.178 I llm_load_print_meta: n_merges         = 50009
0.00.093.179 I llm_load_print_meta: vocab_only       = 0
0.00.093.179 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.179 I llm_load_print_meta: n_embd           = 2048
0.00.093.179 I llm_load_print_meta: n_layer          = 24
0.00.093.183 I llm_load_print_meta: n_head           = 16
0.00.093.184 I llm_load_print_meta: n_head_kv        = 16
0.00.093.184 I llm_load_print_meta: n_rot            = 32
0.00.093.184 I llm_load_print_meta: n_swa            = 0
0.00.093.185 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.185 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.186 I llm_load_print_meta: n_gqa            = 1
0.00.093.187 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.188 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.188 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.189 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.189 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.189 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.189 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.190 I llm_load_print_meta: n_ff             = 8192
0.00.093.192 I llm_load_print_meta: n_expert         = 0
0.00.093.192 I llm_load_print_meta: n_expert_used    = 0
0.00.093.192 I llm_load_print_meta: causal attn      = 1
0.00.093.193 I llm_load_print_meta: pooling type     = 0
0.00.093.194 I llm_load_print_meta: rope type        = 2
0.00.093.194 I llm_load_print_meta: rope scaling     = linear
0.00.093.195 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.195 I llm_load_print_meta: freq_scale_train = 1
0.00.093.195 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.196 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.196 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.196 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.196 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.196 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.196 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.210 I llm_load_print_meta: model type       = 1.4B
0.00.093.211 I llm_load_print_meta: model ftype      = Q4_0
0.00.093.211 I llm_load_print_meta: model params     = 1.41 B
0.00.093.212 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.093.212 I llm_load_print_meta: general.name     = 1.4B
0.00.093.213 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.213 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.215 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.216 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.216 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.216 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.217 I llm_load_print_meta: max token length = 1024
0.00.096.203 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.204 I llm_load_tensors: offloading output layer to GPU
0.00.096.204 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.216 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.096.217 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.097.746 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.748 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.748 I llama_new_context_with_model: n_batch       = 2048
0.00.097.748 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.749 I llama_new_context_with_model: flash_attn    = 0
0.00.097.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.750 I llama_new_context_with_model: freq_scale    = 1
0.00.097.750 I ggml_metal_init: allocating
0.00.097.759 I ggml_metal_init: found device: Apple M4
0.00.097.762 I ggml_metal_init: picking default device: Apple M4
0.00.098.606 I ggml_metal_init: using embedded metal library
0.00.101.586 I ggml_metal_init: GPU name:   Apple M4
0.00.101.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.589 I ggml_metal_init: simdgroup reduction   = true
0.00.101.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.589 I ggml_metal_init: has bfloat            = true
0.00.101.590 I ggml_metal_init: use bfloat            = true
0.00.101.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.135.031 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.039 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.062 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.136.037 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.136.039 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.136.039 I llama_new_context_with_model: graph nodes  = 967
0.00.136.039 I llama_new_context_with_model: graph splits = 2
0.00.136.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.972 I main: llama threadpool init, n_threads = 4
0.00.943.046 I 
0.00.943.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.943.079 I 
0.00.943.641 I sampler seed: 1234
0.00.943.649 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.943.709 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.943.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.943.710 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.633.800 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.633.801 I llama_perf_context_print:        load time =     924.06 ms
0.01.633.802 I llama_perf_context_print: prompt eval time =      42.68 ms /     7 tokens (    6.10 ms per token,   164.03 tokens per second)
0.01.633.803 I llama_perf_context_print:        eval time =     644.37 ms /    63 runs   (   10.23 ms per token,    97.77 tokens per second)
0.01.633.803 I llama_perf_context_print:       total time =     690.83 ms /    70 tokens
0.01.633.990 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.144s
sys	0m0.172s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.017.181 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.033.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.713 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.714 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.714 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.715 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.715 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.720 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.722 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.722 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.648 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.122 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.124 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.124 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.125 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.125 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.125 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.046.126 I llama_model_loader: - type  f32:  194 tensors
0.00.046.126 I llama_model_loader: - type q4_1:   97 tensors
0.00.046.127 I llama_model_loader: - type q6_K:    1 tensors
0.00.083.496 I llm_load_vocab: special tokens cache size = 25
0.00.093.343 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.347 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.348 I llm_load_print_meta: arch             = gptneox
0.00.093.348 I llm_load_print_meta: vocab type       = BPE
0.00.093.349 I llm_load_print_meta: n_vocab          = 50304
0.00.093.349 I llm_load_print_meta: n_merges         = 50009
0.00.093.349 I llm_load_print_meta: vocab_only       = 0
0.00.093.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.349 I llm_load_print_meta: n_embd           = 2048
0.00.093.350 I llm_load_print_meta: n_layer          = 24
0.00.093.354 I llm_load_print_meta: n_head           = 16
0.00.093.355 I llm_load_print_meta: n_head_kv        = 16
0.00.093.355 I llm_load_print_meta: n_rot            = 32
0.00.093.355 I llm_load_print_meta: n_swa            = 0
0.00.093.355 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.359 I llm_load_print_meta: n_gqa            = 1
0.00.093.360 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.361 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.362 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.364 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.364 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.364 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.364 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.365 I llm_load_print_meta: n_ff             = 8192
0.00.093.365 I llm_load_print_meta: n_expert         = 0
0.00.093.366 I llm_load_print_meta: n_expert_used    = 0
0.00.093.367 I llm_load_print_meta: causal attn      = 1
0.00.093.367 I llm_load_print_meta: pooling type     = 0
0.00.093.367 I llm_load_print_meta: rope type        = 2
0.00.093.368 I llm_load_print_meta: rope scaling     = linear
0.00.093.368 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.373 I llm_load_print_meta: freq_scale_train = 1
0.00.093.373 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.374 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.374 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.375 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.377 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.378 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.390 I llm_load_print_meta: model type       = 1.4B
0.00.093.390 I llm_load_print_meta: model ftype      = Q4_1
0.00.093.391 I llm_load_print_meta: model params     = 1.41 B
0.00.093.391 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.093.392 I llm_load_print_meta: general.name     = 1.4B
0.00.093.392 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.392 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.392 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.393 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.393 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.393 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.394 I llm_load_print_meta: max token length = 1024
0.00.095.595 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.595 I llm_load_tensors: offloading output layer to GPU
0.00.095.596 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.606 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.095.607 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.096.843 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.844 I llama_new_context_with_model: n_ctx         = 2048
0.00.096.845 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.096.845 I llama_new_context_with_model: n_batch       = 2048
0.00.096.845 I llama_new_context_with_model: n_ubatch      = 512
0.00.096.845 I llama_new_context_with_model: flash_attn    = 0
0.00.096.846 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.846 I llama_new_context_with_model: freq_scale    = 1
0.00.096.847 I ggml_metal_init: allocating
0.00.096.853 I ggml_metal_init: found device: Apple M4
0.00.096.858 I ggml_metal_init: picking default device: Apple M4
0.00.097.611 I ggml_metal_init: using embedded metal library
0.00.100.333 I ggml_metal_init: GPU name:   Apple M4
0.00.100.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.336 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.336 I ggml_metal_init: simdgroup reduction   = true
0.00.100.337 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.337 I ggml_metal_init: has bfloat            = true
0.00.100.338 I ggml_metal_init: use bfloat            = true
0.00.100.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.339 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.131.705 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.719 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.739 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.132.623 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.132.624 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.132.624 I llama_new_context_with_model: graph nodes  = 967
0.00.132.625 I llama_new_context_with_model: graph splits = 2
0.00.132.646 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.826.351 I main: llama threadpool init, n_threads = 4
0.00.826.427 I 
0.00.826.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.826.476 I 
0.00.826.775 I sampler seed: 1234
0.00.826.784 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.826.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.826.845 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.826.845 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.565.335 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56573.71 tokens per second)
0.01.565.335 I llama_perf_context_print:        load time =     809.16 ms
0.01.565.336 I llama_perf_context_print: prompt eval time =      41.73 ms /     7 tokens (    5.96 ms per token,   167.74 tokens per second)
0.01.565.337 I llama_perf_context_print:        eval time =     693.79 ms /    63 runs   (   11.01 ms per token,    90.81 tokens per second)
0.01.565.337 I llama_perf_context_print:       total time =     738.99 ms /    70 tokens
0.01.565.491 I ggml_metal_free: deallocating

real	0m1.602s
user	0m0.157s
sys	0m0.171s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.524 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.814 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.823 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.823 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.824 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.824 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.830 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.830 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.807 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.808 I llama_model_loader: - type  f32:  194 tensors
0.00.028.809 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.809 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.415 I llm_load_vocab: special tokens cache size = 25
0.00.055.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.442 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.442 I llm_load_print_meta: arch             = gptneox
0.00.055.442 I llm_load_print_meta: vocab type       = BPE
0.00.055.443 I llm_load_print_meta: n_vocab          = 50304
0.00.055.443 I llm_load_print_meta: n_merges         = 50009
0.00.055.443 I llm_load_print_meta: vocab_only       = 0
0.00.055.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.443 I llm_load_print_meta: n_embd           = 2048
0.00.055.444 I llm_load_print_meta: n_layer          = 24
0.00.055.446 I llm_load_print_meta: n_head           = 16
0.00.055.447 I llm_load_print_meta: n_head_kv        = 16
0.00.055.447 I llm_load_print_meta: n_rot            = 32
0.00.055.447 I llm_load_print_meta: n_swa            = 0
0.00.055.448 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.448 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.451 I llm_load_print_meta: n_gqa            = 1
0.00.055.451 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.452 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.453 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.453 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.453 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.453 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.454 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.454 I llm_load_print_meta: n_ff             = 8192
0.00.055.454 I llm_load_print_meta: n_expert         = 0
0.00.055.455 I llm_load_print_meta: n_expert_used    = 0
0.00.055.455 I llm_load_print_meta: causal attn      = 1
0.00.055.455 I llm_load_print_meta: pooling type     = 0
0.00.055.455 I llm_load_print_meta: rope type        = 2
0.00.055.455 I llm_load_print_meta: rope scaling     = linear
0.00.055.456 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.456 I llm_load_print_meta: freq_scale_train = 1
0.00.055.456 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.456 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.457 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.457 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.457 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.457 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.458 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.470 I llm_load_print_meta: model type       = 1.4B
0.00.055.470 I llm_load_print_meta: model ftype      = Q5_0
0.00.055.471 I llm_load_print_meta: model params     = 1.41 B
0.00.055.471 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.055.471 I llm_load_print_meta: general.name     = 1.4B
0.00.055.472 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.472 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.472 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.472 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.472 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.472 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.473 I llm_load_print_meta: max token length = 1024
0.00.057.456 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.456 I llm_load_tensors: offloading output layer to GPU
0.00.057.457 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.466 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.467 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.058.435 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.436 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.436 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.437 I llama_new_context_with_model: n_batch       = 2048
0.00.058.437 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.437 I llama_new_context_with_model: flash_attn    = 0
0.00.058.437 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.438 I llama_new_context_with_model: freq_scale    = 1
0.00.058.438 I ggml_metal_init: allocating
0.00.058.445 I ggml_metal_init: found device: Apple M4
0.00.058.447 I ggml_metal_init: picking default device: Apple M4
0.00.059.004 I ggml_metal_init: using embedded metal library
0.00.060.948 I ggml_metal_init: GPU name:   Apple M4
0.00.060.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.950 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.951 I ggml_metal_init: simdgroup reduction   = true
0.00.060.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.951 I ggml_metal_init: has bfloat            = true
0.00.060.951 I ggml_metal_init: use bfloat            = true
0.00.060.952 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.952 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.744 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.749 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.766 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.897 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.899 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.899 I llama_new_context_with_model: graph nodes  = 967
0.00.089.899 I llama_new_context_with_model: graph splits = 2
0.00.089.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.806.402 I main: llama threadpool init, n_threads = 4
0.00.806.442 I 
0.00.806.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.806.459 I 
0.00.806.672 I sampler seed: 1234
0.00.806.676 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.806.717 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.806.717 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.806.718 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.597.461 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.597.461 I llama_perf_context_print:        load time =     794.87 ms
0.01.597.462 I llama_perf_context_print: prompt eval time =      36.61 ms /     7 tokens (    5.23 ms per token,   191.20 tokens per second)
0.01.597.463 I llama_perf_context_print:        eval time =     751.12 ms /    63 runs   (   11.92 ms per token,    83.87 tokens per second)
0.01.597.463 I llama_perf_context_print:       total time =     791.06 ms /    70 tokens
0.01.597.632 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.466 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.190 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.191 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.191 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.191 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.208 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.089 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.091 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.091 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.091 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.092 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.092 I llama_model_loader: - type  f32:  194 tensors
0.00.028.093 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.093 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.458 I llm_load_vocab: special tokens cache size = 25
0.00.055.641 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.644 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.644 I llm_load_print_meta: arch             = gptneox
0.00.055.645 I llm_load_print_meta: vocab type       = BPE
0.00.055.645 I llm_load_print_meta: n_vocab          = 50304
0.00.055.645 I llm_load_print_meta: n_merges         = 50009
0.00.055.645 I llm_load_print_meta: vocab_only       = 0
0.00.055.645 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.646 I llm_load_print_meta: n_embd           = 2048
0.00.055.646 I llm_load_print_meta: n_layer          = 24
0.00.055.649 I llm_load_print_meta: n_head           = 16
0.00.055.649 I llm_load_print_meta: n_head_kv        = 16
0.00.055.650 I llm_load_print_meta: n_rot            = 32
0.00.055.650 I llm_load_print_meta: n_swa            = 0
0.00.055.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.651 I llm_load_print_meta: n_gqa            = 1
0.00.055.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.652 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.653 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.653 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.653 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.654 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.654 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.654 I llm_load_print_meta: n_ff             = 8192
0.00.055.655 I llm_load_print_meta: n_expert         = 0
0.00.055.655 I llm_load_print_meta: n_expert_used    = 0
0.00.055.657 I llm_load_print_meta: causal attn      = 1
0.00.055.659 I llm_load_print_meta: pooling type     = 0
0.00.055.659 I llm_load_print_meta: rope type        = 2
0.00.055.659 I llm_load_print_meta: rope scaling     = linear
0.00.055.660 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.660 I llm_load_print_meta: freq_scale_train = 1
0.00.055.660 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.660 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.660 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.660 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.661 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.661 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.661 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.673 I llm_load_print_meta: model type       = 1.4B
0.00.055.673 I llm_load_print_meta: model ftype      = Q5_1
0.00.055.674 I llm_load_print_meta: model params     = 1.41 B
0.00.055.674 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.055.675 I llm_load_print_meta: general.name     = 1.4B
0.00.055.675 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.675 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.675 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.676 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.676 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.676 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.676 I llm_load_print_meta: max token length = 1024
0.00.057.702 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.703 I llm_load_tensors: offloading output layer to GPU
0.00.057.703 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.713 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.057.714 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.058.682 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.683 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.683 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.683 I llama_new_context_with_model: n_batch       = 2048
0.00.058.683 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.683 I llama_new_context_with_model: flash_attn    = 0
0.00.058.684 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.684 I llama_new_context_with_model: freq_scale    = 1
0.00.058.684 I ggml_metal_init: allocating
0.00.058.688 I ggml_metal_init: found device: Apple M4
0.00.058.690 I ggml_metal_init: picking default device: Apple M4
0.00.059.261 I ggml_metal_init: using embedded metal library
0.00.061.204 I ggml_metal_init: GPU name:   Apple M4
0.00.061.205 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.206 I ggml_metal_init: simdgroup reduction   = true
0.00.061.208 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.208 I ggml_metal_init: has bfloat            = true
0.00.061.208 I ggml_metal_init: use bfloat            = true
0.00.061.209 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.210 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.450 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.458 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.477 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.576 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.578 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.578 I llama_new_context_with_model: graph nodes  = 967
0.00.093.579 I llama_new_context_with_model: graph splits = 2
0.00.093.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.515 I main: llama threadpool init, n_threads = 4
0.00.782.549 I 
0.00.782.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.782.576 I 
0.00.782.851 I sampler seed: 1234
0.00.782.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.877 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.877 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.877 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.638.504 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.638.505 I llama_perf_context_print:        load time =     771.04 ms
0.01.638.505 I llama_perf_context_print: prompt eval time =      36.57 ms /     7 tokens (    5.22 ms per token,   191.41 tokens per second)
0.01.638.506 I llama_perf_context_print:        eval time =     816.04 ms /    63 runs   (   12.95 ms per token,    77.20 tokens per second)
0.01.638.506 I llama_perf_context_print:       total time =     855.99 ms /    70 tokens
0.01.638.686 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.113s
sys	0m0.177s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.066 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.640 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.647 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.648 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.648 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.648 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.650 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.650 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.651 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.653 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.401 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.402 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.402 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.403 I llama_model_loader: - type  f32:  194 tensors
0.00.024.403 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.403 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.403 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.847 I llm_load_vocab: special tokens cache size = 25
0.00.050.899 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.901 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.902 I llm_load_print_meta: arch             = gptneox
0.00.050.902 I llm_load_print_meta: vocab type       = BPE
0.00.050.903 I llm_load_print_meta: n_vocab          = 50304
0.00.050.903 I llm_load_print_meta: n_merges         = 50009
0.00.050.903 I llm_load_print_meta: vocab_only       = 0
0.00.050.903 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.903 I llm_load_print_meta: n_embd           = 2048
0.00.050.903 I llm_load_print_meta: n_layer          = 24
0.00.050.906 I llm_load_print_meta: n_head           = 16
0.00.050.907 I llm_load_print_meta: n_head_kv        = 16
0.00.050.907 I llm_load_print_meta: n_rot            = 32
0.00.050.907 I llm_load_print_meta: n_swa            = 0
0.00.050.907 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.907 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.908 I llm_load_print_meta: n_gqa            = 1
0.00.050.909 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.909 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.910 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.910 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.910 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.910 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.910 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.911 I llm_load_print_meta: n_ff             = 8192
0.00.050.911 I llm_load_print_meta: n_expert         = 0
0.00.050.911 I llm_load_print_meta: n_expert_used    = 0
0.00.050.911 I llm_load_print_meta: causal attn      = 1
0.00.050.912 I llm_load_print_meta: pooling type     = 0
0.00.050.912 I llm_load_print_meta: rope type        = 2
0.00.050.912 I llm_load_print_meta: rope scaling     = linear
0.00.050.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.913 I llm_load_print_meta: freq_scale_train = 1
0.00.050.913 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.913 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.914 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.914 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.925 I llm_load_print_meta: model type       = 1.4B
0.00.050.926 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.926 I llm_load_print_meta: model params     = 1.41 B
0.00.050.927 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.927 I llm_load_print_meta: general.name     = 1.4B
0.00.050.927 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.927 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.927 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.927 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.928 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.928 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.928 I llm_load_print_meta: max token length = 1024
0.00.052.543 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.543 I llm_load_tensors: offloading output layer to GPU
0.00.052.544 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.553 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.554 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.399 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.399 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.400 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.400 I llama_new_context_with_model: n_batch       = 2048
0.00.053.400 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.400 I llama_new_context_with_model: flash_attn    = 0
0.00.053.401 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.401 I llama_new_context_with_model: freq_scale    = 1
0.00.053.401 I ggml_metal_init: allocating
0.00.053.404 I ggml_metal_init: found device: Apple M4
0.00.053.406 I ggml_metal_init: picking default device: Apple M4
0.00.053.980 I ggml_metal_init: using embedded metal library
0.00.055.961 I ggml_metal_init: GPU name:   Apple M4
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.964 I ggml_metal_init: simdgroup reduction   = true
0.00.055.964 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.964 I ggml_metal_init: has bfloat            = true
0.00.055.964 I ggml_metal_init: use bfloat            = true
0.00.055.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.767 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.773 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.792 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.710 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.711 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.711 I llama_new_context_with_model: graph nodes  = 967
0.00.084.712 I llama_new_context_with_model: graph splits = 2
0.00.084.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.544 I main: llama threadpool init, n_threads = 4
0.00.480.591 I 
0.00.480.611 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.480.611 I 
0.00.480.776 I sampler seed: 1234
0.00.480.781 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.480.792 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.480.792 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.480.793 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.162.378 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.162.379 I llama_perf_context_print:        load time =     470.47 ms
0.01.162.380 I llama_perf_context_print: prompt eval time =      35.90 ms /     7 tokens (    5.13 ms per token,   195.00 tokens per second)
0.01.162.380 I llama_perf_context_print:        eval time =     642.60 ms /    63 runs   (   10.20 ms per token,    98.04 tokens per second)
0.01.162.381 I llama_perf_context_print:       total time =     681.84 ms /    70 tokens
0.01.162.545 I ggml_metal_free: deallocating

real	0m1.181s
user	0m0.110s
sys	0m0.111s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.178 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.064 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.068 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.076 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.077 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.079 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.079 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.079 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.080 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.082 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.976 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.023 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.880 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.881 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.883 I llama_model_loader: - type  f32:  194 tensors
0.00.024.883 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.883 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.883 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.884 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.435 I llm_load_vocab: special tokens cache size = 25
0.00.051.555 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.558 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.559 I llm_load_print_meta: arch             = gptneox
0.00.051.559 I llm_load_print_meta: vocab type       = BPE
0.00.051.559 I llm_load_print_meta: n_vocab          = 50304
0.00.051.559 I llm_load_print_meta: n_merges         = 50009
0.00.051.559 I llm_load_print_meta: vocab_only       = 0
0.00.051.560 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.560 I llm_load_print_meta: n_embd           = 2048
0.00.051.560 I llm_load_print_meta: n_layer          = 24
0.00.051.563 I llm_load_print_meta: n_head           = 16
0.00.051.564 I llm_load_print_meta: n_head_kv        = 16
0.00.051.565 I llm_load_print_meta: n_rot            = 32
0.00.051.565 I llm_load_print_meta: n_swa            = 0
0.00.051.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.565 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.566 I llm_load_print_meta: n_gqa            = 1
0.00.051.567 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.568 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.568 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.569 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.571 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.571 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.571 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.572 I llm_load_print_meta: n_ff             = 8192
0.00.051.572 I llm_load_print_meta: n_expert         = 0
0.00.051.572 I llm_load_print_meta: n_expert_used    = 0
0.00.051.572 I llm_load_print_meta: causal attn      = 1
0.00.051.572 I llm_load_print_meta: pooling type     = 0
0.00.051.573 I llm_load_print_meta: rope type        = 2
0.00.051.573 I llm_load_print_meta: rope scaling     = linear
0.00.051.573 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.574 I llm_load_print_meta: freq_scale_train = 1
0.00.051.574 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.576 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.576 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.576 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.576 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.576 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.576 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.583 I llm_load_print_meta: model type       = 1.4B
0.00.051.583 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.584 I llm_load_print_meta: model params     = 1.41 B
0.00.051.584 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.584 I llm_load_print_meta: general.name     = 1.4B
0.00.051.585 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.585 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.586 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.586 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.586 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.586 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.587 I llm_load_print_meta: max token length = 1024
0.00.053.378 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.378 I llm_load_tensors: offloading output layer to GPU
0.00.053.378 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.383 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.385 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.271 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.272 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.272 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.272 I llama_new_context_with_model: n_batch       = 2048
0.00.054.272 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.273 I llama_new_context_with_model: flash_attn    = 0
0.00.054.273 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.273 I llama_new_context_with_model: freq_scale    = 1
0.00.054.274 I ggml_metal_init: allocating
0.00.054.277 I ggml_metal_init: found device: Apple M4
0.00.054.279 I ggml_metal_init: picking default device: Apple M4
0.00.054.844 I ggml_metal_init: using embedded metal library
0.00.056.768 I ggml_metal_init: GPU name:   Apple M4
0.00.056.769 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.770 I ggml_metal_init: simdgroup reduction   = true
0.00.056.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.770 I ggml_metal_init: has bfloat            = true
0.00.056.770 I ggml_metal_init: use bfloat            = true
0.00.056.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.937 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.942 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.960 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.967 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.968 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.969 I llama_new_context_with_model: graph nodes  = 967
0.00.084.969 I llama_new_context_with_model: graph splits = 2
0.00.084.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.845 I main: llama threadpool init, n_threads = 4
0.00.545.881 I 
0.00.545.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.545.901 I 
0.00.546.050 I sampler seed: 1234
0.00.546.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.546.086 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.546.086 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.546.086 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.291.945 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.291.946 I llama_perf_context_print:        load time =     536.66 ms
0.01.291.947 I llama_perf_context_print: prompt eval time =      35.59 ms /     7 tokens (    5.08 ms per token,   196.71 tokens per second)
0.01.291.948 I llama_perf_context_print:        eval time =     707.17 ms /    63 runs   (   11.22 ms per token,    89.09 tokens per second)
0.01.291.949 I llama_perf_context_print:       total time =     746.10 ms /    70 tokens
0.01.292.127 I ggml_metal_free: deallocating

real	0m1.309s
user	0m0.110s
sys	0m0.125s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.368 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.973 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.977 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.978 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.980 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.981 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.981 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.982 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.984 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.984 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.985 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.875 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.933 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.807 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.807 I llama_model_loader: - type  f32:  194 tensors
0.00.025.807 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.808 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.808 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.063 I llm_load_vocab: special tokens cache size = 25
0.00.052.219 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.222 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.222 I llm_load_print_meta: arch             = gptneox
0.00.052.222 I llm_load_print_meta: vocab type       = BPE
0.00.052.223 I llm_load_print_meta: n_vocab          = 50304
0.00.052.223 I llm_load_print_meta: n_merges         = 50009
0.00.052.223 I llm_load_print_meta: vocab_only       = 0
0.00.052.223 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.223 I llm_load_print_meta: n_embd           = 2048
0.00.052.224 I llm_load_print_meta: n_layer          = 24
0.00.052.227 I llm_load_print_meta: n_head           = 16
0.00.052.228 I llm_load_print_meta: n_head_kv        = 16
0.00.052.228 I llm_load_print_meta: n_rot            = 32
0.00.052.228 I llm_load_print_meta: n_swa            = 0
0.00.052.228 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.228 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.229 I llm_load_print_meta: n_gqa            = 1
0.00.052.230 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.231 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.232 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.232 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.232 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.232 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.234 I llm_load_print_meta: n_ff             = 8192
0.00.052.234 I llm_load_print_meta: n_expert         = 0
0.00.052.235 I llm_load_print_meta: n_expert_used    = 0
0.00.052.236 I llm_load_print_meta: causal attn      = 1
0.00.052.236 I llm_load_print_meta: pooling type     = 0
0.00.052.237 I llm_load_print_meta: rope type        = 2
0.00.052.237 I llm_load_print_meta: rope scaling     = linear
0.00.052.237 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.237 I llm_load_print_meta: freq_scale_train = 1
0.00.052.238 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.238 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.238 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.238 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.238 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.238 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.239 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.245 I llm_load_print_meta: model type       = 1.4B
0.00.052.245 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.245 I llm_load_print_meta: model params     = 1.41 B
0.00.052.246 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.246 I llm_load_print_meta: general.name     = 1.4B
0.00.052.246 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.247 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.247 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.247 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.247 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.247 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.248 I llm_load_print_meta: max token length = 1024
0.00.053.773 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.773 I llm_load_tensors: offloading output layer to GPU
0.00.053.773 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.777 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.778 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.598 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.599 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.599 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.599 I llama_new_context_with_model: n_batch       = 2048
0.00.054.599 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.599 I llama_new_context_with_model: flash_attn    = 0
0.00.054.600 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.600 I llama_new_context_with_model: freq_scale    = 1
0.00.054.600 I ggml_metal_init: allocating
0.00.054.603 I ggml_metal_init: found device: Apple M4
0.00.054.605 I ggml_metal_init: picking default device: Apple M4
0.00.055.146 I ggml_metal_init: using embedded metal library
0.00.057.090 I ggml_metal_init: GPU name:   Apple M4
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.094 I ggml_metal_init: simdgroup reduction   = true
0.00.057.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.094 I ggml_metal_init: has bfloat            = true
0.00.057.094 I ggml_metal_init: use bfloat            = true
0.00.057.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.906 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.913 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.936 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.881 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.882 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.883 I llama_new_context_with_model: graph nodes  = 967
0.00.085.883 I llama_new_context_with_model: graph splits = 2
0.00.085.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.950 I main: llama threadpool init, n_threads = 4
0.00.625.987 I 
0.00.626.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.626.010 I 
0.00.626.299 I sampler seed: 1234
0.00.626.303 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.626.313 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.626.313 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.626.314 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.383.367 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.383.368 I llama_perf_context_print:        load time =     615.58 ms
0.01.383.368 I llama_perf_context_print: prompt eval time =      40.26 ms /     7 tokens (    5.75 ms per token,   173.86 tokens per second)
0.01.383.369 I llama_perf_context_print:        eval time =     713.87 ms /    63 runs   (   11.33 ms per token,    88.25 tokens per second)
0.01.383.370 I llama_perf_context_print:       total time =     757.42 ms /    70 tokens
0.01.383.542 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.830 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.271 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.275 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.277 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.278 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.279 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.279 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.280 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.280 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.286 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.286 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.146 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.147 I llama_model_loader: - type  f32:  194 tensors
0.00.027.147 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.148 I llama_model_loader: - type q6_K:   37 tensors
0.00.048.774 I llm_load_vocab: special tokens cache size = 25
0.00.054.972 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.974 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.975 I llm_load_print_meta: arch             = gptneox
0.00.054.975 I llm_load_print_meta: vocab type       = BPE
0.00.054.975 I llm_load_print_meta: n_vocab          = 50304
0.00.054.976 I llm_load_print_meta: n_merges         = 50009
0.00.054.976 I llm_load_print_meta: vocab_only       = 0
0.00.054.976 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.976 I llm_load_print_meta: n_embd           = 2048
0.00.054.976 I llm_load_print_meta: n_layer          = 24
0.00.054.979 I llm_load_print_meta: n_head           = 16
0.00.054.980 I llm_load_print_meta: n_head_kv        = 16
0.00.054.980 I llm_load_print_meta: n_rot            = 32
0.00.054.980 I llm_load_print_meta: n_swa            = 0
0.00.054.981 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.981 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.982 I llm_load_print_meta: n_gqa            = 1
0.00.054.983 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.988 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.989 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.989 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.990 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.990 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.990 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.991 I llm_load_print_meta: n_ff             = 8192
0.00.054.991 I llm_load_print_meta: n_expert         = 0
0.00.054.991 I llm_load_print_meta: n_expert_used    = 0
0.00.054.993 I llm_load_print_meta: causal attn      = 1
0.00.054.994 I llm_load_print_meta: pooling type     = 0
0.00.054.994 I llm_load_print_meta: rope type        = 2
0.00.054.995 I llm_load_print_meta: rope scaling     = linear
0.00.054.995 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.995 I llm_load_print_meta: freq_scale_train = 1
0.00.054.996 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.996 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.996 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.996 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.996 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.996 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.997 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.009 I llm_load_print_meta: model type       = 1.4B
0.00.055.009 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.055.009 I llm_load_print_meta: model params     = 1.41 B
0.00.055.010 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.055.010 I llm_load_print_meta: general.name     = 1.4B
0.00.055.010 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.010 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.011 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.011 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.011 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.012 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.013 I llm_load_print_meta: max token length = 1024
0.00.057.128 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.128 I llm_load_tensors: offloading output layer to GPU
0.00.057.128 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.138 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.057.139 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.058.101 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.102 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.102 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.102 I llama_new_context_with_model: n_batch       = 2048
0.00.058.102 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.102 I llama_new_context_with_model: flash_attn    = 0
0.00.058.103 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.103 I llama_new_context_with_model: freq_scale    = 1
0.00.058.104 I ggml_metal_init: allocating
0.00.058.109 I ggml_metal_init: found device: Apple M4
0.00.058.112 I ggml_metal_init: picking default device: Apple M4
0.00.058.642 I ggml_metal_init: using embedded metal library
0.00.060.571 I ggml_metal_init: GPU name:   Apple M4
0.00.060.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.574 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.574 I ggml_metal_init: simdgroup reduction   = true
0.00.060.575 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.575 I ggml_metal_init: has bfloat            = true
0.00.060.575 I ggml_metal_init: use bfloat            = true
0.00.060.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.210 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.218 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.233 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.226 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.227 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.227 I llama_new_context_with_model: graph nodes  = 967
0.00.089.228 I llama_new_context_with_model: graph splits = 2
0.00.089.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.639 I main: llama threadpool init, n_threads = 4
0.00.712.675 I 
0.00.712.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.712.704 I 
0.00.712.935 I sampler seed: 1234
0.00.712.939 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.951 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.951 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.553.882 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.01.553.883 I llama_perf_context_print:        load time =     700.81 ms
0.01.553.884 I llama_perf_context_print: prompt eval time =      38.64 ms /     7 tokens (    5.52 ms per token,   181.15 tokens per second)
0.01.553.885 I llama_perf_context_print:        eval time =     799.29 ms /    63 runs   (   12.69 ms per token,    78.82 tokens per second)
0.01.553.885 I llama_perf_context_print:       total time =     841.25 ms /    70 tokens
0.01.554.055 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.681 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.245 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.245 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.250 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.104 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.927 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.927 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.927 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.928 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.928 I llama_model_loader: - type  f32:  194 tensors
0.00.025.929 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.490 I llm_load_vocab: special tokens cache size = 25
0.00.052.719 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.721 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.722 I llm_load_print_meta: arch             = gptneox
0.00.052.722 I llm_load_print_meta: vocab type       = BPE
0.00.052.722 I llm_load_print_meta: n_vocab          = 50304
0.00.052.722 I llm_load_print_meta: n_merges         = 50009
0.00.052.723 I llm_load_print_meta: vocab_only       = 0
0.00.052.723 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.723 I llm_load_print_meta: n_embd           = 2048
0.00.052.723 I llm_load_print_meta: n_layer          = 24
0.00.052.726 I llm_load_print_meta: n_head           = 16
0.00.052.727 I llm_load_print_meta: n_head_kv        = 16
0.00.052.727 I llm_load_print_meta: n_rot            = 32
0.00.052.727 I llm_load_print_meta: n_swa            = 0
0.00.052.727 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.728 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.730 I llm_load_print_meta: n_gqa            = 1
0.00.052.730 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.731 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.732 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.732 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.732 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.732 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.732 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.733 I llm_load_print_meta: n_ff             = 8192
0.00.052.733 I llm_load_print_meta: n_expert         = 0
0.00.052.733 I llm_load_print_meta: n_expert_used    = 0
0.00.052.734 I llm_load_print_meta: causal attn      = 1
0.00.052.735 I llm_load_print_meta: pooling type     = 0
0.00.052.737 I llm_load_print_meta: rope type        = 2
0.00.052.737 I llm_load_print_meta: rope scaling     = linear
0.00.052.738 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.738 I llm_load_print_meta: freq_scale_train = 1
0.00.052.738 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.738 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.739 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.739 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.739 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.739 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.739 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.751 I llm_load_print_meta: model type       = 1.4B
0.00.052.751 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.751 I llm_load_print_meta: model params     = 1.41 B
0.00.052.752 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.752 I llm_load_print_meta: general.name     = 1.4B
0.00.052.752 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.752 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.752 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.753 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.753 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.753 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.753 I llm_load_print_meta: max token length = 1024
0.00.054.760 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.760 I llm_load_tensors: offloading output layer to GPU
0.00.054.760 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.770 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.771 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.707 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.707 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.708 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.708 I llama_new_context_with_model: n_batch       = 2048
0.00.055.708 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.708 I llama_new_context_with_model: flash_attn    = 0
0.00.055.709 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.709 I llama_new_context_with_model: freq_scale    = 1
0.00.055.709 I ggml_metal_init: allocating
0.00.055.716 I ggml_metal_init: found device: Apple M4
0.00.055.719 I ggml_metal_init: picking default device: Apple M4
0.00.056.265 I ggml_metal_init: using embedded metal library
0.00.058.217 I ggml_metal_init: GPU name:   Apple M4
0.00.058.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.219 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.219 I ggml_metal_init: simdgroup reduction   = true
0.00.058.221 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.221 I ggml_metal_init: has bfloat            = true
0.00.058.221 I ggml_metal_init: use bfloat            = true
0.00.058.221 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.222 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.843 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.848 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.867 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.870 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.872 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.872 I llama_new_context_with_model: graph nodes  = 967
0.00.087.873 I llama_new_context_with_model: graph splits = 2
0.00.087.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.453 I main: llama threadpool init, n_threads = 4
0.00.770.488 I 
0.00.770.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.770.515 I 
0.00.770.743 I sampler seed: 1234
0.00.770.747 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.758 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.759 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.759 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.641.246 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.641.246 I llama_perf_context_print:        load time =     759.77 ms
0.01.641.247 I llama_perf_context_print: prompt eval time =      38.52 ms /     7 tokens (    5.50 ms per token,   181.74 tokens per second)
0.01.641.248 I llama_perf_context_print:        eval time =     828.81 ms /    63 runs   (   13.16 ms per token,    76.01 tokens per second)
0.01.641.248 I llama_perf_context_print:       total time =     870.79 ms /    70 tokens
0.01.641.432 I ggml_metal_free: deallocating

real	0m1.661s
user	0m0.109s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.584 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.459 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.387 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.392 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.395 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.402 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.483 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.579 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.580 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.583 I llama_model_loader: - type  f32:  194 tensors
0.00.050.583 I llama_model_loader: - type  f16:   98 tensors
0.00.078.349 I llm_load_vocab: special tokens cache size = 25
0.00.084.912 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.914 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.915 I llm_load_print_meta: arch             = gptneox
0.00.084.915 I llm_load_print_meta: vocab type       = BPE
0.00.084.915 I llm_load_print_meta: n_vocab          = 50304
0.00.084.915 I llm_load_print_meta: n_merges         = 50009
0.00.084.915 I llm_load_print_meta: vocab_only       = 0
0.00.084.916 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.916 I llm_load_print_meta: n_embd           = 2048
0.00.084.916 I llm_load_print_meta: n_layer          = 24
0.00.084.918 I llm_load_print_meta: n_head           = 16
0.00.084.919 I llm_load_print_meta: n_head_kv        = 16
0.00.084.919 I llm_load_print_meta: n_rot            = 32
0.00.084.919 I llm_load_print_meta: n_swa            = 0
0.00.084.919 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.919 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.920 I llm_load_print_meta: n_gqa            = 1
0.00.084.921 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.921 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.922 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.922 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.922 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.922 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.923 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.923 I llm_load_print_meta: n_ff             = 8192
0.00.084.923 I llm_load_print_meta: n_expert         = 0
0.00.084.923 I llm_load_print_meta: n_expert_used    = 0
0.00.084.923 I llm_load_print_meta: causal attn      = 1
0.00.084.924 I llm_load_print_meta: pooling type     = 0
0.00.084.924 I llm_load_print_meta: rope type        = 2
0.00.084.924 I llm_load_print_meta: rope scaling     = linear
0.00.084.924 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.924 I llm_load_print_meta: freq_scale_train = 1
0.00.084.925 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.925 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.925 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.927 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.927 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.927 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.927 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.933 I llm_load_print_meta: model type       = 1.4B
0.00.084.934 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.935 I llm_load_print_meta: model params     = 1.41 B
0.00.084.936 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.936 I llm_load_print_meta: general.name     = 1.4B
0.00.084.936 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.936 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.936 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.937 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.937 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.937 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.937 I llm_load_print_meta: max token length = 1024
0.00.086.848 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.848 I llm_load_tensors: offloading output layer to GPU
0.00.086.849 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.853 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.854 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.012 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.013 I llama_new_context_with_model: n_ctx         = 128
0.00.088.013 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.013 I llama_new_context_with_model: n_batch       = 128
0.00.088.013 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.013 I llama_new_context_with_model: flash_attn    = 0
0.00.088.014 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.014 I llama_new_context_with_model: freq_scale    = 1
0.00.088.015 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.015 I ggml_metal_init: allocating
0.00.088.021 I ggml_metal_init: found device: Apple M4
0.00.088.023 I ggml_metal_init: picking default device: Apple M4
0.00.088.581 I ggml_metal_init: using embedded metal library
0.00.090.647 I ggml_metal_init: GPU name:   Apple M4
0.00.090.649 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.649 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.650 I ggml_metal_init: simdgroup reduction   = true
0.00.090.650 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.650 I ggml_metal_init: has bfloat            = true
0.00.090.650 I ggml_metal_init: use bfloat            = true
0.00.090.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.651 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.315 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.317 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.331 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.230 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.231 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.231 I llama_new_context_with_model: graph nodes  = 967
0.00.101.232 I llama_new_context_with_model: graph splits = 2
0.00.101.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.959.740 I 
0.00.959.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.959.809 I perplexity: tokenizing the input ..
0.00.972.542 I perplexity: tokenization took 12.73 ms
0.00.972.578 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.093.253 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.094.873 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.094.895 I llama_perf_context_print:        load time =     939.27 ms
0.01.094.897 I llama_perf_context_print: prompt eval time =     119.64 ms /   128 tokens (    0.93 ms per token,  1069.84 tokens per second)
0.01.094.898 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.094.899 I llama_perf_context_print:       total time =     135.16 ms /   129 tokens
0.01.095.586 I ggml_metal_free: deallocating

real	0m1.295s
user	0m0.123s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.129 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.261 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.083 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.086 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.088 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.088 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.232 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.627 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.628 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.629 I llama_model_loader: - type  f32:  194 tensors
0.00.029.630 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.400 I llm_load_vocab: special tokens cache size = 25
0.00.059.679 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.682 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.682 I llm_load_print_meta: arch             = gptneox
0.00.059.683 I llm_load_print_meta: vocab type       = BPE
0.00.059.683 I llm_load_print_meta: n_vocab          = 50304
0.00.059.683 I llm_load_print_meta: n_merges         = 50009
0.00.059.683 I llm_load_print_meta: vocab_only       = 0
0.00.059.683 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.683 I llm_load_print_meta: n_embd           = 2048
0.00.059.684 I llm_load_print_meta: n_layer          = 24
0.00.059.687 I llm_load_print_meta: n_head           = 16
0.00.059.687 I llm_load_print_meta: n_head_kv        = 16
0.00.059.688 I llm_load_print_meta: n_rot            = 32
0.00.059.688 I llm_load_print_meta: n_swa            = 0
0.00.059.688 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.688 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.689 I llm_load_print_meta: n_gqa            = 1
0.00.059.689 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.690 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.690 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.691 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.691 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.691 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.692 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.693 I llm_load_print_meta: n_ff             = 8192
0.00.059.693 I llm_load_print_meta: n_expert         = 0
0.00.059.693 I llm_load_print_meta: n_expert_used    = 0
0.00.059.693 I llm_load_print_meta: causal attn      = 1
0.00.059.693 I llm_load_print_meta: pooling type     = 0
0.00.059.693 I llm_load_print_meta: rope type        = 2
0.00.059.693 I llm_load_print_meta: rope scaling     = linear
0.00.059.694 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.694 I llm_load_print_meta: freq_scale_train = 1
0.00.059.694 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.694 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.695 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.695 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.695 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.695 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.695 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.706 I llm_load_print_meta: model type       = 1.4B
0.00.059.706 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.707 I llm_load_print_meta: model params     = 1.41 B
0.00.059.707 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.707 I llm_load_print_meta: general.name     = 1.4B
0.00.059.707 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.708 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.708 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.708 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.708 I llm_load_print_meta: max token length = 1024
0.00.061.360 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.360 I llm_load_tensors: offloading output layer to GPU
0.00.061.360 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.369 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.370 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.206 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.207 I llama_new_context_with_model: n_ctx         = 128
0.00.062.207 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.207 I llama_new_context_with_model: n_batch       = 128
0.00.062.207 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.208 I llama_new_context_with_model: flash_attn    = 0
0.00.062.208 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.208 I llama_new_context_with_model: freq_scale    = 1
0.00.062.209 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.209 I ggml_metal_init: allocating
0.00.062.212 I ggml_metal_init: found device: Apple M4
0.00.062.214 I ggml_metal_init: picking default device: Apple M4
0.00.062.771 I ggml_metal_init: using embedded metal library
0.00.064.685 I ggml_metal_init: GPU name:   Apple M4
0.00.064.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.686 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.687 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.687 I ggml_metal_init: simdgroup reduction   = true
0.00.064.687 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.687 I ggml_metal_init: has bfloat            = true
0.00.064.687 I ggml_metal_init: use bfloat            = true
0.00.064.688 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.508 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.515 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.527 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.430 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.431 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.432 I llama_new_context_with_model: graph nodes  = 967
0.00.074.432 I llama_new_context_with_model: graph splits = 2
0.00.074.444 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.516 I 
0.00.832.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.832.544 I perplexity: tokenizing the input ..
0.00.839.896 I perplexity: tokenization took 7.35 ms
0.00.839.908 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.961.810 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.963.029 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.963.046 I llama_perf_context_print:        load time =     822.25 ms
0.00.963.047 I llama_perf_context_print: prompt eval time =     121.68 ms /   128 tokens (    0.95 ms per token,  1051.97 tokens per second)
0.00.963.048 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.963.048 I llama_perf_context_print:       total time =     130.53 ms /   129 tokens
0.00.963.471 I ggml_metal_free: deallocating

real	0m0.982s
user	0m0.087s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.054 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.022 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.880 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.881 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.883 I llama_model_loader: - type  f32:  194 tensors
0.00.024.883 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.883 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.226 I llm_load_vocab: special tokens cache size = 25
0.00.052.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.315 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.315 I llm_load_print_meta: arch             = gptneox
0.00.052.316 I llm_load_print_meta: vocab type       = BPE
0.00.052.316 I llm_load_print_meta: n_vocab          = 50304
0.00.052.316 I llm_load_print_meta: n_merges         = 50009
0.00.052.316 I llm_load_print_meta: vocab_only       = 0
0.00.052.316 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.317 I llm_load_print_meta: n_embd           = 2048
0.00.052.317 I llm_load_print_meta: n_layer          = 24
0.00.052.320 I llm_load_print_meta: n_head           = 16
0.00.052.321 I llm_load_print_meta: n_head_kv        = 16
0.00.052.321 I llm_load_print_meta: n_rot            = 32
0.00.052.321 I llm_load_print_meta: n_swa            = 0
0.00.052.321 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.321 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.324 I llm_load_print_meta: n_gqa            = 1
0.00.052.325 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.325 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.326 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.327 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.327 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.328 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.328 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.328 I llm_load_print_meta: n_ff             = 8192
0.00.052.329 I llm_load_print_meta: n_expert         = 0
0.00.052.329 I llm_load_print_meta: n_expert_used    = 0
0.00.052.329 I llm_load_print_meta: causal attn      = 1
0.00.052.329 I llm_load_print_meta: pooling type     = 0
0.00.052.329 I llm_load_print_meta: rope type        = 2
0.00.052.329 I llm_load_print_meta: rope scaling     = linear
0.00.052.330 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.330 I llm_load_print_meta: freq_scale_train = 1
0.00.052.330 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.330 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.331 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.331 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.331 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.331 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.331 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.343 I llm_load_print_meta: model type       = 1.4B
0.00.052.343 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.344 I llm_load_print_meta: model params     = 1.41 B
0.00.052.344 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.344 I llm_load_print_meta: general.name     = 1.4B
0.00.052.344 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.345 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.345 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.345 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.345 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.346 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.347 I llm_load_print_meta: max token length = 1024
0.00.054.365 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.365 I llm_load_tensors: offloading output layer to GPU
0.00.054.365 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.376 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.377 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.394 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.395 I llama_new_context_with_model: n_ctx         = 128
0.00.055.395 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.395 I llama_new_context_with_model: n_batch       = 128
0.00.055.395 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.396 I llama_new_context_with_model: flash_attn    = 0
0.00.055.396 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.396 I llama_new_context_with_model: freq_scale    = 1
0.00.055.397 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.397 I ggml_metal_init: allocating
0.00.055.402 I ggml_metal_init: found device: Apple M4
0.00.055.404 I ggml_metal_init: picking default device: Apple M4
0.00.055.991 I ggml_metal_init: using embedded metal library
0.00.057.879 I ggml_metal_init: GPU name:   Apple M4
0.00.057.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.882 I ggml_metal_init: simdgroup reduction   = true
0.00.057.882 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.882 I ggml_metal_init: has bfloat            = true
0.00.057.882 I ggml_metal_init: use bfloat            = true
0.00.057.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.883 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.094 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.096 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.110 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.011 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.012 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.013 I llama_new_context_with_model: graph nodes  = 967
0.00.068.013 I llama_new_context_with_model: graph splits = 2
0.00.068.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.391 I 
0.00.627.416 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.627.419 I perplexity: tokenizing the input ..
0.00.635.482 I perplexity: tokenization took 8.06 ms
0.00.635.493 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.758.265 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.759.492 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.759.511 I llama_perf_context_print:        load time =     617.33 ms
0.00.759.512 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.55 tokens per second)
0.00.759.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.514 I llama_perf_context_print:       total time =     132.12 ms /   129 tokens
0.00.760.002 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.078s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.257 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.071 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.073 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.074 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.075 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.075 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.076 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.078 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.078 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.976 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.970 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.972 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.972 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.973 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.973 I llama_model_loader: - type  f32:  194 tensors
0.00.023.974 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.974 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.198 I llm_load_vocab: special tokens cache size = 25
0.00.050.202 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.205 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.205 I llm_load_print_meta: arch             = gptneox
0.00.050.206 I llm_load_print_meta: vocab type       = BPE
0.00.050.206 I llm_load_print_meta: n_vocab          = 50304
0.00.050.206 I llm_load_print_meta: n_merges         = 50009
0.00.050.206 I llm_load_print_meta: vocab_only       = 0
0.00.050.206 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.207 I llm_load_print_meta: n_embd           = 2048
0.00.050.207 I llm_load_print_meta: n_layer          = 24
0.00.050.209 I llm_load_print_meta: n_head           = 16
0.00.050.210 I llm_load_print_meta: n_head_kv        = 16
0.00.050.210 I llm_load_print_meta: n_rot            = 32
0.00.050.210 I llm_load_print_meta: n_swa            = 0
0.00.050.211 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.212 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.214 I llm_load_print_meta: n_gqa            = 1
0.00.050.214 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.215 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.216 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.216 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.216 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.216 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.216 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.217 I llm_load_print_meta: n_ff             = 8192
0.00.050.217 I llm_load_print_meta: n_expert         = 0
0.00.050.218 I llm_load_print_meta: n_expert_used    = 0
0.00.050.218 I llm_load_print_meta: causal attn      = 1
0.00.050.218 I llm_load_print_meta: pooling type     = 0
0.00.050.218 I llm_load_print_meta: rope type        = 2
0.00.050.218 I llm_load_print_meta: rope scaling     = linear
0.00.050.218 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.223 I llm_load_print_meta: freq_scale_train = 1
0.00.050.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.224 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.225 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.225 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.226 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.226 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.232 I llm_load_print_meta: model type       = 1.4B
0.00.050.233 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.233 I llm_load_print_meta: model params     = 1.41 B
0.00.050.234 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.234 I llm_load_print_meta: general.name     = 1.4B
0.00.050.234 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.236 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.237 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.237 I llm_load_print_meta: max token length = 1024
0.00.051.794 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.794 I llm_load_tensors: offloading output layer to GPU
0.00.051.794 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.799 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.799 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.604 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.604 I llama_new_context_with_model: n_ctx         = 128
0.00.052.604 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.605 I llama_new_context_with_model: n_batch       = 128
0.00.052.605 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.605 I llama_new_context_with_model: flash_attn    = 0
0.00.052.605 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.606 I llama_new_context_with_model: freq_scale    = 1
0.00.052.606 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.606 I ggml_metal_init: allocating
0.00.052.612 I ggml_metal_init: found device: Apple M4
0.00.052.614 I ggml_metal_init: picking default device: Apple M4
0.00.053.123 I ggml_metal_init: using embedded metal library
0.00.055.015 I ggml_metal_init: GPU name:   Apple M4
0.00.055.016 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.017 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.017 I ggml_metal_init: simdgroup reduction   = true
0.00.055.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.018 I ggml_metal_init: has bfloat            = true
0.00.055.018 I ggml_metal_init: use bfloat            = true
0.00.055.018 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.259 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.263 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.278 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.163 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.164 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.164 I llama_new_context_with_model: graph nodes  = 967
0.00.065.165 I llama_new_context_with_model: graph splits = 2
0.00.065.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.549 I 
0.00.636.570 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.636.574 I perplexity: tokenizing the input ..
0.00.644.712 I perplexity: tokenization took 8.136 ms
0.00.644.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.892 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.768.057 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.768.072 I llama_perf_context_print:        load time =     627.29 ms
0.00.768.074 I llama_perf_context_print: prompt eval time =     121.95 ms /   128 tokens (    0.95 ms per token,  1049.65 tokens per second)
0.00.768.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.077 I llama_perf_context_print:       total time =     131.52 ms /   129 tokens
0.00.768.487 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.077s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.532 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.439 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.447 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.354 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.444 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.302 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.303 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.304 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.305 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.305 I llama_model_loader: - type  f32:  194 tensors
0.00.025.306 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.306 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.585 I llm_load_vocab: special tokens cache size = 25
0.00.052.741 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.743 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.744 I llm_load_print_meta: arch             = gptneox
0.00.052.744 I llm_load_print_meta: vocab type       = BPE
0.00.052.745 I llm_load_print_meta: n_vocab          = 50304
0.00.052.745 I llm_load_print_meta: n_merges         = 50009
0.00.052.745 I llm_load_print_meta: vocab_only       = 0
0.00.052.745 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.745 I llm_load_print_meta: n_embd           = 2048
0.00.052.746 I llm_load_print_meta: n_layer          = 24
0.00.052.748 I llm_load_print_meta: n_head           = 16
0.00.052.749 I llm_load_print_meta: n_head_kv        = 16
0.00.052.749 I llm_load_print_meta: n_rot            = 32
0.00.052.749 I llm_load_print_meta: n_swa            = 0
0.00.052.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.750 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.750 I llm_load_print_meta: n_gqa            = 1
0.00.052.751 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.754 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.754 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.755 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.755 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.756 I llm_load_print_meta: n_ff             = 8192
0.00.052.758 I llm_load_print_meta: n_expert         = 0
0.00.052.758 I llm_load_print_meta: n_expert_used    = 0
0.00.052.758 I llm_load_print_meta: causal attn      = 1
0.00.052.758 I llm_load_print_meta: pooling type     = 0
0.00.052.759 I llm_load_print_meta: rope type        = 2
0.00.052.759 I llm_load_print_meta: rope scaling     = linear
0.00.052.759 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.759 I llm_load_print_meta: freq_scale_train = 1
0.00.052.760 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.773 I llm_load_print_meta: model type       = 1.4B
0.00.052.773 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.774 I llm_load_print_meta: model params     = 1.41 B
0.00.052.774 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.774 I llm_load_print_meta: general.name     = 1.4B
0.00.052.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.775 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.775 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.775 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.776 I llm_load_print_meta: max token length = 1024
0.00.054.814 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.814 I llm_load_tensors: offloading output layer to GPU
0.00.054.814 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.825 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.826 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.738 I llama_new_context_with_model: n_ctx         = 128
0.00.055.738 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.738 I llama_new_context_with_model: n_batch       = 128
0.00.055.738 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.738 I llama_new_context_with_model: flash_attn    = 0
0.00.055.739 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.739 I llama_new_context_with_model: freq_scale    = 1
0.00.055.739 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.740 I ggml_metal_init: allocating
0.00.055.745 I ggml_metal_init: found device: Apple M4
0.00.055.747 I ggml_metal_init: picking default device: Apple M4
0.00.056.287 I ggml_metal_init: using embedded metal library
0.00.058.238 I ggml_metal_init: GPU name:   Apple M4
0.00.058.240 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.241 I ggml_metal_init: simdgroup reduction   = true
0.00.058.241 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.241 I ggml_metal_init: has bfloat            = true
0.00.058.241 I ggml_metal_init: use bfloat            = true
0.00.058.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.453 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.458 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.474 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.353 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.354 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.354 I llama_new_context_with_model: graph nodes  = 967
0.00.068.354 I llama_new_context_with_model: graph splits = 2
0.00.068.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.532 I 
0.00.677.551 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.677.553 I perplexity: tokenizing the input ..
0.00.684.768 I perplexity: tokenization took 7.214 ms
0.00.684.778 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.752 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.821.006 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.821.020 I llama_perf_context_print:        load time =     667.00 ms
0.00.821.021 I llama_perf_context_print: prompt eval time =     134.75 ms /   128 tokens (    1.05 ms per token,   949.94 tokens per second)
0.00.821.022 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.022 I llama_perf_context_print:       total time =     143.49 ms /   129 tokens
0.00.821.503 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.078s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.049 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.620 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.627 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.628 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.628 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.628 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.634 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.604 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.606 I llama_model_loader: - type  f32:  194 tensors
0.00.023.606 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.607 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.678 I llm_load_vocab: special tokens cache size = 25
0.00.050.877 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.881 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.881 I llm_load_print_meta: arch             = gptneox
0.00.050.882 I llm_load_print_meta: vocab type       = BPE
0.00.050.882 I llm_load_print_meta: n_vocab          = 50304
0.00.050.882 I llm_load_print_meta: n_merges         = 50009
0.00.050.882 I llm_load_print_meta: vocab_only       = 0
0.00.050.883 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.883 I llm_load_print_meta: n_embd           = 2048
0.00.050.885 I llm_load_print_meta: n_layer          = 24
0.00.050.888 I llm_load_print_meta: n_head           = 16
0.00.050.889 I llm_load_print_meta: n_head_kv        = 16
0.00.050.889 I llm_load_print_meta: n_rot            = 32
0.00.050.889 I llm_load_print_meta: n_swa            = 0
0.00.050.890 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.890 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.891 I llm_load_print_meta: n_gqa            = 1
0.00.050.892 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.893 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.893 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.893 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.893 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.894 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.894 I llm_load_print_meta: n_ff             = 8192
0.00.050.894 I llm_load_print_meta: n_expert         = 0
0.00.050.895 I llm_load_print_meta: n_expert_used    = 0
0.00.050.895 I llm_load_print_meta: causal attn      = 1
0.00.050.895 I llm_load_print_meta: pooling type     = 0
0.00.050.895 I llm_load_print_meta: rope type        = 2
0.00.050.896 I llm_load_print_meta: rope scaling     = linear
0.00.050.897 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.897 I llm_load_print_meta: freq_scale_train = 1
0.00.050.897 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.897 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.898 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.898 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.898 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.898 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.898 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.910 I llm_load_print_meta: model type       = 1.4B
0.00.050.910 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.910 I llm_load_print_meta: model params     = 1.41 B
0.00.050.911 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.911 I llm_load_print_meta: general.name     = 1.4B
0.00.050.911 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.912 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.912 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.912 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.912 I llm_load_print_meta: max token length = 1024
0.00.052.989 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.989 I llm_load_tensors: offloading output layer to GPU
0.00.052.989 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.999 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.000 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.931 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.932 I llama_new_context_with_model: n_ctx         = 128
0.00.053.932 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.933 I llama_new_context_with_model: n_batch       = 128
0.00.053.933 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.933 I llama_new_context_with_model: flash_attn    = 0
0.00.053.933 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.933 I llama_new_context_with_model: freq_scale    = 1
0.00.053.934 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.934 I ggml_metal_init: allocating
0.00.053.937 I ggml_metal_init: found device: Apple M4
0.00.053.939 I ggml_metal_init: picking default device: Apple M4
0.00.054.484 I ggml_metal_init: using embedded metal library
0.00.056.475 I ggml_metal_init: GPU name:   Apple M4
0.00.056.477 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.477 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.478 I ggml_metal_init: simdgroup reduction   = true
0.00.056.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.478 I ggml_metal_init: has bfloat            = true
0.00.056.478 I ggml_metal_init: use bfloat            = true
0.00.056.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.016 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.018 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.032 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.032 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.033 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.033 I llama_new_context_with_model: graph nodes  = 967
0.00.067.033 I llama_new_context_with_model: graph splits = 2
0.00.067.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.325 I 
0.00.645.341 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.645.344 I perplexity: tokenizing the input ..
0.00.652.987 I perplexity: tokenization took 7.642 ms
0.00.652.998 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.057 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.789.241 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.789.264 I llama_perf_context_print:        load time =     636.27 ms
0.00.789.265 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.41 tokens per second)
0.00.789.268 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.268 I llama_perf_context_print:       total time =     143.94 ms /   129 tokens
0.00.789.793 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.078s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.150 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.583 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.588 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.590 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.591 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.593 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.594 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.284 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.285 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.285 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.286 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.286 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.286 I llama_model_loader: - type  f32:  194 tensors
0.00.023.287 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.287 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.287 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.443 I llm_load_vocab: special tokens cache size = 25
0.00.049.338 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.341 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.341 I llm_load_print_meta: arch             = gptneox
0.00.049.342 I llm_load_print_meta: vocab type       = BPE
0.00.049.342 I llm_load_print_meta: n_vocab          = 50304
0.00.049.342 I llm_load_print_meta: n_merges         = 50009
0.00.049.342 I llm_load_print_meta: vocab_only       = 0
0.00.049.342 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.343 I llm_load_print_meta: n_embd           = 2048
0.00.049.343 I llm_load_print_meta: n_layer          = 24
0.00.049.345 I llm_load_print_meta: n_head           = 16
0.00.049.346 I llm_load_print_meta: n_head_kv        = 16
0.00.049.346 I llm_load_print_meta: n_rot            = 32
0.00.049.346 I llm_load_print_meta: n_swa            = 0
0.00.049.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.347 I llm_load_print_meta: n_gqa            = 1
0.00.049.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.349 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.350 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.350 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.350 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.350 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.351 I llm_load_print_meta: n_ff             = 8192
0.00.049.351 I llm_load_print_meta: n_expert         = 0
0.00.049.351 I llm_load_print_meta: n_expert_used    = 0
0.00.049.352 I llm_load_print_meta: causal attn      = 1
0.00.049.352 I llm_load_print_meta: pooling type     = 0
0.00.049.352 I llm_load_print_meta: rope type        = 2
0.00.049.352 I llm_load_print_meta: rope scaling     = linear
0.00.049.352 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.353 I llm_load_print_meta: freq_scale_train = 1
0.00.049.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.353 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.353 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.354 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.365 I llm_load_print_meta: model type       = 1.4B
0.00.049.366 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.366 I llm_load_print_meta: model params     = 1.41 B
0.00.049.366 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.367 I llm_load_print_meta: general.name     = 1.4B
0.00.049.367 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.367 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.367 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.367 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.368 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.368 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.368 I llm_load_print_meta: max token length = 1024
0.00.050.912 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.912 I llm_load_tensors: offloading output layer to GPU
0.00.050.912 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.922 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.923 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.768 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.769 I llama_new_context_with_model: n_ctx         = 128
0.00.051.769 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.769 I llama_new_context_with_model: n_batch       = 128
0.00.051.769 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.770 I llama_new_context_with_model: flash_attn    = 0
0.00.051.770 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.770 I llama_new_context_with_model: freq_scale    = 1
0.00.051.771 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.771 I ggml_metal_init: allocating
0.00.051.774 I ggml_metal_init: found device: Apple M4
0.00.051.776 I ggml_metal_init: picking default device: Apple M4
0.00.052.300 I ggml_metal_init: using embedded metal library
0.00.054.200 I ggml_metal_init: GPU name:   Apple M4
0.00.054.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.203 I ggml_metal_init: simdgroup reduction   = true
0.00.054.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.203 I ggml_metal_init: has bfloat            = true
0.00.054.203 I ggml_metal_init: use bfloat            = true
0.00.054.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.204 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.479 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.492 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.406 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.407 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.407 I llama_new_context_with_model: graph nodes  = 967
0.00.064.408 I llama_new_context_with_model: graph splits = 2
0.00.064.420 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.930 I 
0.00.437.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.437.960 I perplexity: tokenizing the input ..
0.00.445.806 I perplexity: tokenization took 7.844 ms
0.00.445.815 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.618 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.578.758 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.578.770 I llama_perf_context_print:        load time =     428.78 ms
0.00.578.771 I llama_perf_context_print: prompt eval time =     131.58 ms /   128 tokens (    1.03 ms per token,   972.81 tokens per second)
0.00.578.772 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.578.772 I llama_perf_context_print:       total time =     140.84 ms /   129 tokens
0.00.579.132 I ggml_metal_free: deallocating

real	0m0.593s
user	0m0.077s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.546 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.395 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.400 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.402 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.402 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.403 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.403 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.404 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.404 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.404 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.405 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.405 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.405 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.406 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.408 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.408 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.408 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.133 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.134 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.135 I llama_model_loader: - type  f32:  194 tensors
0.00.023.135 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.135 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.136 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.136 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.731 I llm_load_vocab: special tokens cache size = 25
0.00.049.753 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.755 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.756 I llm_load_print_meta: arch             = gptneox
0.00.049.756 I llm_load_print_meta: vocab type       = BPE
0.00.049.756 I llm_load_print_meta: n_vocab          = 50304
0.00.049.757 I llm_load_print_meta: n_merges         = 50009
0.00.049.757 I llm_load_print_meta: vocab_only       = 0
0.00.049.757 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.757 I llm_load_print_meta: n_embd           = 2048
0.00.049.757 I llm_load_print_meta: n_layer          = 24
0.00.049.760 I llm_load_print_meta: n_head           = 16
0.00.049.761 I llm_load_print_meta: n_head_kv        = 16
0.00.049.761 I llm_load_print_meta: n_rot            = 32
0.00.049.761 I llm_load_print_meta: n_swa            = 0
0.00.049.761 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.761 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.762 I llm_load_print_meta: n_gqa            = 1
0.00.049.763 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.764 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.764 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.766 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.766 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.767 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.767 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.767 I llm_load_print_meta: n_ff             = 8192
0.00.049.768 I llm_load_print_meta: n_expert         = 0
0.00.049.768 I llm_load_print_meta: n_expert_used    = 0
0.00.049.768 I llm_load_print_meta: causal attn      = 1
0.00.049.768 I llm_load_print_meta: pooling type     = 0
0.00.049.768 I llm_load_print_meta: rope type        = 2
0.00.049.768 I llm_load_print_meta: rope scaling     = linear
0.00.049.769 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.769 I llm_load_print_meta: freq_scale_train = 1
0.00.049.769 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.769 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.770 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.770 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.770 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.770 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.770 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.782 I llm_load_print_meta: model type       = 1.4B
0.00.049.782 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.782 I llm_load_print_meta: model params     = 1.41 B
0.00.049.783 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.783 I llm_load_print_meta: general.name     = 1.4B
0.00.049.783 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.783 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.784 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.784 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.784 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.784 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.784 I llm_load_print_meta: max token length = 1024
0.00.051.340 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.340 I llm_load_tensors: offloading output layer to GPU
0.00.051.340 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.350 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.351 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.189 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.190 I llama_new_context_with_model: n_ctx         = 128
0.00.052.190 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.190 I llama_new_context_with_model: n_batch       = 128
0.00.052.190 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.190 I llama_new_context_with_model: flash_attn    = 0
0.00.052.191 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.191 I llama_new_context_with_model: freq_scale    = 1
0.00.052.191 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.192 I ggml_metal_init: allocating
0.00.052.195 I ggml_metal_init: found device: Apple M4
0.00.052.197 I ggml_metal_init: picking default device: Apple M4
0.00.052.723 I ggml_metal_init: using embedded metal library
0.00.054.645 I ggml_metal_init: GPU name:   Apple M4
0.00.054.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.648 I ggml_metal_init: simdgroup reduction   = true
0.00.054.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.648 I ggml_metal_init: has bfloat            = true
0.00.054.648 I ggml_metal_init: use bfloat            = true
0.00.054.648 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.046 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.049 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.063 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.962 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.963 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.963 I llama_new_context_with_model: graph nodes  = 967
0.00.064.964 I llama_new_context_with_model: graph splits = 2
0.00.064.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.406 I 
0.00.505.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.505.441 I perplexity: tokenizing the input ..
0.00.513.484 I perplexity: tokenization took 8.042 ms
0.00.513.495 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.118 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.646.274 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.646.286 I llama_perf_context_print:        load time =     496.86 ms
0.00.646.287 I llama_perf_context_print: prompt eval time =     131.40 ms /   128 tokens (    1.03 ms per token,   974.13 tokens per second)
0.00.646.288 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.646.288 I llama_perf_context_print:       total time =     140.88 ms /   129 tokens
0.00.646.716 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.078s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.574 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.585 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.586 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.588 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.588 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.589 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.594 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.594 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.394 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.254 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.255 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.256 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.256 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.257 I llama_model_loader: - type  f32:  194 tensors
0.00.025.257 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.257 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.258 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.721 I llm_load_vocab: special tokens cache size = 25
0.00.052.849 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.852 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.852 I llm_load_print_meta: arch             = gptneox
0.00.052.852 I llm_load_print_meta: vocab type       = BPE
0.00.052.852 I llm_load_print_meta: n_vocab          = 50304
0.00.052.853 I llm_load_print_meta: n_merges         = 50009
0.00.052.853 I llm_load_print_meta: vocab_only       = 0
0.00.052.853 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.853 I llm_load_print_meta: n_embd           = 2048
0.00.052.853 I llm_load_print_meta: n_layer          = 24
0.00.052.856 I llm_load_print_meta: n_head           = 16
0.00.052.856 I llm_load_print_meta: n_head_kv        = 16
0.00.052.857 I llm_load_print_meta: n_rot            = 32
0.00.052.857 I llm_load_print_meta: n_swa            = 0
0.00.052.857 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.857 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.858 I llm_load_print_meta: n_gqa            = 1
0.00.052.859 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.860 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.860 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.860 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.860 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.861 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.861 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.861 I llm_load_print_meta: n_ff             = 8192
0.00.052.862 I llm_load_print_meta: n_expert         = 0
0.00.052.862 I llm_load_print_meta: n_expert_used    = 0
0.00.052.862 I llm_load_print_meta: causal attn      = 1
0.00.052.862 I llm_load_print_meta: pooling type     = 0
0.00.052.862 I llm_load_print_meta: rope type        = 2
0.00.052.862 I llm_load_print_meta: rope scaling     = linear
0.00.052.863 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.863 I llm_load_print_meta: freq_scale_train = 1
0.00.052.863 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.863 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.864 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.864 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.866 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.866 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.866 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.878 I llm_load_print_meta: model type       = 1.4B
0.00.052.878 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.880 I llm_load_print_meta: model params     = 1.41 B
0.00.052.881 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.881 I llm_load_print_meta: general.name     = 1.4B
0.00.052.881 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.881 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.881 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.881 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.882 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.882 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.882 I llm_load_print_meta: max token length = 1024
0.00.054.908 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.908 I llm_load_tensors: offloading output layer to GPU
0.00.054.909 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.919 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.920 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.840 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.841 I llama_new_context_with_model: n_ctx         = 128
0.00.055.841 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.842 I llama_new_context_with_model: n_batch       = 128
0.00.055.842 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.842 I llama_new_context_with_model: flash_attn    = 0
0.00.055.842 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.843 I llama_new_context_with_model: freq_scale    = 1
0.00.055.843 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.843 I ggml_metal_init: allocating
0.00.055.847 I ggml_metal_init: found device: Apple M4
0.00.055.848 I ggml_metal_init: picking default device: Apple M4
0.00.056.398 I ggml_metal_init: using embedded metal library
0.00.058.362 I ggml_metal_init: GPU name:   Apple M4
0.00.058.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.364 I ggml_metal_init: simdgroup reduction   = true
0.00.058.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.364 I ggml_metal_init: has bfloat            = true
0.00.058.364 I ggml_metal_init: use bfloat            = true
0.00.058.365 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.365 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.099 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.105 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.119 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.039 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.041 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.041 I llama_new_context_with_model: graph nodes  = 967
0.00.069.041 I llama_new_context_with_model: graph splits = 2
0.00.069.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.127 I 
0.00.590.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.590.152 I perplexity: tokenizing the input ..
0.00.597.682 I perplexity: tokenization took 7.529 ms
0.00.597.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.732.143 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.733.378 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.733.401 I llama_perf_context_print:        load time =     579.55 ms
0.00.733.402 I llama_perf_context_print: prompt eval time =     134.23 ms /   128 tokens (    1.05 ms per token,   953.59 tokens per second)
0.00.733.403 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.403 I llama_perf_context_print:       total time =     143.27 ms /   129 tokens
0.00.733.838 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.079s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.856 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.515 I llama_model_loader: - type  f32:  194 tensors
0.00.023.515 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.515 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.051 I llm_load_vocab: special tokens cache size = 25
0.00.050.126 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.129 I llm_load_print_meta: arch             = gptneox
0.00.050.130 I llm_load_print_meta: vocab type       = BPE
0.00.050.130 I llm_load_print_meta: n_vocab          = 50304
0.00.050.130 I llm_load_print_meta: n_merges         = 50009
0.00.050.131 I llm_load_print_meta: vocab_only       = 0
0.00.050.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.131 I llm_load_print_meta: n_embd           = 2048
0.00.050.131 I llm_load_print_meta: n_layer          = 24
0.00.050.134 I llm_load_print_meta: n_head           = 16
0.00.050.134 I llm_load_print_meta: n_head_kv        = 16
0.00.050.134 I llm_load_print_meta: n_rot            = 32
0.00.050.135 I llm_load_print_meta: n_swa            = 0
0.00.050.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.138 I llm_load_print_meta: n_gqa            = 1
0.00.050.139 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.139 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.140 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.140 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.142 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.142 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.142 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.143 I llm_load_print_meta: n_ff             = 8192
0.00.050.143 I llm_load_print_meta: n_expert         = 0
0.00.050.143 I llm_load_print_meta: n_expert_used    = 0
0.00.050.143 I llm_load_print_meta: causal attn      = 1
0.00.050.143 I llm_load_print_meta: pooling type     = 0
0.00.050.143 I llm_load_print_meta: rope type        = 2
0.00.050.144 I llm_load_print_meta: rope scaling     = linear
0.00.050.144 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.144 I llm_load_print_meta: freq_scale_train = 1
0.00.050.144 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.145 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.146 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.146 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.146 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.146 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.146 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.158 I llm_load_print_meta: model type       = 1.4B
0.00.050.159 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.159 I llm_load_print_meta: model params     = 1.41 B
0.00.050.159 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.160 I llm_load_print_meta: general.name     = 1.4B
0.00.050.160 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.161 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.161 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.161 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.162 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.162 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.162 I llm_load_print_meta: max token length = 1024
0.00.052.154 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.155 I llm_load_tensors: offloading output layer to GPU
0.00.052.155 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.165 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.166 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.081 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.082 I llama_new_context_with_model: n_ctx         = 128
0.00.053.082 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.083 I llama_new_context_with_model: n_batch       = 128
0.00.053.083 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.083 I llama_new_context_with_model: flash_attn    = 0
0.00.053.083 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.084 I llama_new_context_with_model: freq_scale    = 1
0.00.053.084 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.085 I ggml_metal_init: allocating
0.00.053.090 I ggml_metal_init: found device: Apple M4
0.00.053.092 I ggml_metal_init: picking default device: Apple M4
0.00.053.630 I ggml_metal_init: using embedded metal library
0.00.055.546 I ggml_metal_init: GPU name:   Apple M4
0.00.055.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.548 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.548 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.548 I ggml_metal_init: simdgroup reduction   = true
0.00.055.549 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.549 I ggml_metal_init: has bfloat            = true
0.00.055.549 I ggml_metal_init: use bfloat            = true
0.00.055.549 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.550 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.843 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.845 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.858 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.752 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.753 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.754 I llama_new_context_with_model: graph nodes  = 967
0.00.065.754 I llama_new_context_with_model: graph splits = 2
0.00.065.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.769 I 
0.00.650.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.650.800 I perplexity: tokenizing the input ..
0.00.658.679 I perplexity: tokenization took 7.878 ms
0.00.658.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.266 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.800.455 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.800.474 I llama_perf_context_print:        load time =     641.91 ms
0.00.800.474 I llama_perf_context_print: prompt eval time =     140.34 ms /   128 tokens (    1.10 ms per token,   912.09 tokens per second)
0.00.800.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.476 I llama_perf_context_print:       total time =     149.71 ms /   129 tokens
0.00.800.953 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.569 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.204 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.205 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.206 I llama_model_loader: - type  f32:  194 tensors
0.00.027.206 I llama_model_loader: - type q6_K:   98 tensors
0.00.048.353 I llm_load_vocab: special tokens cache size = 25
0.00.054.443 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.445 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.445 I llm_load_print_meta: arch             = gptneox
0.00.054.446 I llm_load_print_meta: vocab type       = BPE
0.00.054.446 I llm_load_print_meta: n_vocab          = 50304
0.00.054.446 I llm_load_print_meta: n_merges         = 50009
0.00.054.446 I llm_load_print_meta: vocab_only       = 0
0.00.054.446 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.447 I llm_load_print_meta: n_embd           = 2048
0.00.054.447 I llm_load_print_meta: n_layer          = 24
0.00.054.450 I llm_load_print_meta: n_head           = 16
0.00.054.451 I llm_load_print_meta: n_head_kv        = 16
0.00.054.451 I llm_load_print_meta: n_rot            = 32
0.00.054.452 I llm_load_print_meta: n_swa            = 0
0.00.054.452 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.452 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.453 I llm_load_print_meta: n_gqa            = 1
0.00.054.453 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.454 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.455 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.455 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.455 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.456 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.456 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.457 I llm_load_print_meta: n_ff             = 8192
0.00.054.457 I llm_load_print_meta: n_expert         = 0
0.00.054.457 I llm_load_print_meta: n_expert_used    = 0
0.00.054.457 I llm_load_print_meta: causal attn      = 1
0.00.054.457 I llm_load_print_meta: pooling type     = 0
0.00.054.457 I llm_load_print_meta: rope type        = 2
0.00.054.458 I llm_load_print_meta: rope scaling     = linear
0.00.054.458 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.458 I llm_load_print_meta: freq_scale_train = 1
0.00.054.458 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.459 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.459 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.459 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.459 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.461 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.461 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.473 I llm_load_print_meta: model type       = 1.4B
0.00.054.473 I llm_load_print_meta: model ftype      = Q6_K
0.00.054.473 I llm_load_print_meta: model params     = 1.41 B
0.00.054.474 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.054.480 I llm_load_print_meta: general.name     = 1.4B
0.00.054.481 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.482 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.482 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.482 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.482 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.482 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.483 I llm_load_print_meta: max token length = 1024
0.00.056.032 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.032 I llm_load_tensors: offloading output layer to GPU
0.00.056.032 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.042 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.056.043 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.844 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.845 I llama_new_context_with_model: n_ctx         = 128
0.00.056.845 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.845 I llama_new_context_with_model: n_batch       = 128
0.00.056.845 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.846 I llama_new_context_with_model: flash_attn    = 0
0.00.056.846 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.846 I llama_new_context_with_model: freq_scale    = 1
0.00.056.847 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.847 I ggml_metal_init: allocating
0.00.056.851 I ggml_metal_init: found device: Apple M4
0.00.056.853 I ggml_metal_init: picking default device: Apple M4
0.00.057.398 I ggml_metal_init: using embedded metal library
0.00.059.330 I ggml_metal_init: GPU name:   Apple M4
0.00.059.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.332 I ggml_metal_init: simdgroup reduction   = true
0.00.059.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.332 I ggml_metal_init: has bfloat            = true
0.00.059.332 I ggml_metal_init: use bfloat            = true
0.00.059.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.265 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.267 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.292 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.163 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.164 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.165 I llama_new_context_with_model: graph nodes  = 967
0.00.069.165 I llama_new_context_with_model: graph splits = 2
0.00.069.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.177.847 I 
0.00.177.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.177.893 I perplexity: tokenizing the input ..
0.00.185.303 I perplexity: tokenization took 7.407 ms
0.00.185.321 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.326.148 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.327.373 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.327.391 I llama_perf_context_print:        load time =     165.27 ms
0.00.327.392 I llama_perf_context_print: prompt eval time =     140.59 ms /   128 tokens (    1.10 ms per token,   910.46 tokens per second)
0.00.327.393 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.327.393 I llama_perf_context_print:       total time =     149.55 ms /   129 tokens
0.00.327.843 I ggml_metal_free: deallocating

real	0m0.345s
user	0m0.077s
sys	0m0.046s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.273 I build: 4172 (2649e275) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.816 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.823 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.830 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.833 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.834 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.838 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.839 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.839 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.170 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.171 I llama_model_loader: - type  f32:  194 tensors
0.00.053.172 I llama_model_loader: - type  f16:   98 tensors
0.00.081.618 I llm_load_vocab: special tokens cache size = 25
0.00.088.388 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.390 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.391 I llm_load_print_meta: arch             = gptneox
0.00.088.391 I llm_load_print_meta: vocab type       = BPE
0.00.088.391 I llm_load_print_meta: n_vocab          = 50304
0.00.088.391 I llm_load_print_meta: n_merges         = 50009
0.00.088.392 I llm_load_print_meta: vocab_only       = 0
0.00.088.392 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.392 I llm_load_print_meta: n_embd           = 2048
0.00.088.392 I llm_load_print_meta: n_layer          = 24
0.00.088.394 I llm_load_print_meta: n_head           = 16
0.00.088.395 I llm_load_print_meta: n_head_kv        = 16
0.00.088.395 I llm_load_print_meta: n_rot            = 32
0.00.088.395 I llm_load_print_meta: n_swa            = 0
0.00.088.395 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.395 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.396 I llm_load_print_meta: n_gqa            = 1
0.00.088.397 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.397 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.400 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.400 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.401 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.401 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.401 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.402 I llm_load_print_meta: n_ff             = 8192
0.00.088.402 I llm_load_print_meta: n_expert         = 0
0.00.088.402 I llm_load_print_meta: n_expert_used    = 0
0.00.088.402 I llm_load_print_meta: causal attn      = 1
0.00.088.402 I llm_load_print_meta: pooling type     = 0
0.00.088.402 I llm_load_print_meta: rope type        = 2
0.00.088.403 I llm_load_print_meta: rope scaling     = linear
0.00.088.403 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.403 I llm_load_print_meta: freq_scale_train = 1
0.00.088.404 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.405 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.405 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.405 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.405 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.405 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.405 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.417 I llm_load_print_meta: model type       = 1.4B
0.00.088.418 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.418 I llm_load_print_meta: model params     = 1.41 B
0.00.088.419 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.419 I llm_load_print_meta: general.name     = 1.4B
0.00.088.419 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.419 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.419 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.419 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.420 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.421 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.421 I llm_load_print_meta: max token length = 1024
0.00.090.571 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.572 I llm_load_tensors: offloading output layer to GPU
0.00.090.572 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.576 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.576 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.559 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.560 I llama_new_context_with_model: n_ctx         = 128
0.00.091.561 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.561 I llama_new_context_with_model: n_batch       = 128
0.00.091.561 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.561 I llama_new_context_with_model: flash_attn    = 0
0.00.091.562 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.562 I llama_new_context_with_model: freq_scale    = 1
0.00.091.562 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.562 I ggml_metal_init: allocating
0.00.091.570 I ggml_metal_init: found device: Apple M4
0.00.091.572 I ggml_metal_init: picking default device: Apple M4
0.00.092.149 I ggml_metal_init: using embedded metal library
0.00.094.289 I ggml_metal_init: GPU name:   Apple M4
0.00.094.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.291 I ggml_metal_init: simdgroup reduction   = true
0.00.094.291 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.292 I ggml_metal_init: has bfloat            = true
0.00.094.292 I ggml_metal_init: use bfloat            = true
0.00.094.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.230 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.232 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.254 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.125 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.104.126 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.104.127 I llama_new_context_with_model: graph nodes  = 967
0.00.104.127 I llama_new_context_with_model: graph splits = 2
0.00.104.134 I 
0.00.104.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | 
0.00.104.156 I compute_imatrix: tokenizing the input ..
0.00.111.204 I compute_imatrix: tokenization took 7.047 ms
0.00.111.206 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.581.781 I compute_imatrix: 1.47 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.584.317 I llama_perf_context_print:        load time =    1558.78 ms
0.01.584.318 I llama_perf_context_print: prompt eval time =    1469.95 ms /   128 tokens (   11.48 ms per token,    87.08 tokens per second)
0.01.584.318 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.584.319 I llama_perf_context_print:       total time =    1561.31 ms /   129 tokens
0.01.584.840 I ggml_metal_free: deallocating

real	0m1.771s
user	0m0.166s
sys	0m0.245s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4172 (2649e275)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157a0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157a0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157a0c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157a0c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157a0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157a0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157a0d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157a0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157a0e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157a0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157a0edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157a0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157a0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157a105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157a10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157a114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157a11c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157a12330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157a12a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157a13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157a13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157a14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157a14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157a15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157a15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157a15a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157a16010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157a16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157a171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157a17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157a17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157a18470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157a189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157a18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157a19110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157a195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157a19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157a19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157a1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157a1a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157a1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157a1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157a1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157a1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157a1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157a1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157a1ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157a1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157a1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157a1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157a1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157a1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157a1f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157a1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157a1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157a203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157a20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157a20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157a21460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157a21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157a21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157a22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157a22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157a229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157a22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157a232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157a23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157a23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157a240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157a24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157a24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157a24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157a25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157a257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157a25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157a26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157a265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157a26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157a26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157a273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157a27840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157a27ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157a28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157a28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157a28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157a29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157a298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157a29d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157a2a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157a2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157a2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157a2afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157a2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157a2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157a2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157a1cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157a2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157a2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157a2cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157a2d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157a2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157a2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157a2dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157a2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157a2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157a2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157a2f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157a2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157a2fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157a30010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157a304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157a30950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157a30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157a31290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157a31730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157a31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157a32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157a32510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157a329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157a32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157a332f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157a33790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157a33c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157a340d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157a34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157a34a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157a34eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157a35350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157a357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157a35c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157a36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157a365d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157a36a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157a36f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157a373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157a37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157a37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157a38190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157a38630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157a38ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157a38f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157a39410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157a398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157a39d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157a3a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157a3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157a3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157a3afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157a3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157a3b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157a3bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157a3c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157a3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157a3cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157a3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157a3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157a3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157a3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157a3edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157a3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157a3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157a40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157a40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157a409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157a41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157a416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157a41c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157a42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157a426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157a42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157a43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157a436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157a43c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157a44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157a446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157a44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157a45140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157a45690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157a45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157a46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157a46680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157a46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157a47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157a47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157a47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157a48110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157a48660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157a48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157a49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157a49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157a49ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157a4a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157a4a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157a4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157a4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157a4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157a4bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157a4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157a4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157a4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157a4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157a4d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157a4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157a4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157a4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157a4eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157a4f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157a4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157a4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157a50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157a505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157a51080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157a515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157a51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157a52070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157a525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157a52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157a53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157a535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157a53b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157a53fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157a54440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157a548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157a54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157a55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157a556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157a55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157a56000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157a564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157a56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157a56de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157a57280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157a57720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157a57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157a58390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157a58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157a591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157a598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157a59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157a5a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157a5a7d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.132.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157b058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157b05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157b06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157b065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157b06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157b06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157b07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157b077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157b07c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157b08090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157b08500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157b08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157b096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157b09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157b0a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157b0adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157b0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157b0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157b0c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157b0caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157b0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157b0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157b0e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157b0e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157b0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157b0f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157b0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157b0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157b0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157b10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157b10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157b10b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157b10fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157b112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157b11710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157b11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157b120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157b125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157b12ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157b12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157b134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157b139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157b13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157b143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157b148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157b14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157b151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157b15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157b15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157b15f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157b16380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157b167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157b16c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157b170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157b17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157b17d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157b181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157b18470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157b18a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157b19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157b19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157b19bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157b1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157b1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157b1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157b1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157b1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157b1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157b1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157b1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157b1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157b1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157b1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157b1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157b1d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157b1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157b1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157b1e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157b1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157b1eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157b1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157b1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157b1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157b20170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157b20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157b20ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157b20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157b213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157b21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157b21d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157b221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157b22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157b22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157b22fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157b23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157b238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157b23d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157b24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157b246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157b24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157b25010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157b254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157b25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157b25df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157b26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157b26730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157b26bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157b27070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157b27510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157b279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157b27e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157b282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157b28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157b28c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157b290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157b29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157b29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157b29eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157b2a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157b2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157b2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157b2b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157b2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157b2ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157b2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157b2c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157b2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157b2ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157b2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157b2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157b2dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157b2df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157b2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157b2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157b2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157b2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157b2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157b2fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157b2ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157b30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157b30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157b30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157b31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157b316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157b31b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157b32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157b324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157b32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157b32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157b332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157b33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157b33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157b34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157b345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157b34b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157b35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157b355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157b35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157b35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157b364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157b36ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157b370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157b376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157b37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157b38370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157b38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157b38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157b39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157b399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157b39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157b3a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157b3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157b3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157b3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157b3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157b3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157b3c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157b3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157b3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157b3d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157b3d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157b3dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157b3e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157b3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157b3eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157b3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157b3f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157b3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157b403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157b40940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157b40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157b413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157b41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157b41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157b423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157b42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157b42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157b433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157b43910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157b43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157b443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157b44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157b44e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157b453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157b458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157b45e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157b46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157b468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157b46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157b47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157b478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157b47e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157b48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157b488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157b48e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157b49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157b498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157b49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157b4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157b4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157b4adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157b4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157b4b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157b4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157b4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157b4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157b4cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157b4d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157b4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157b4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157b4de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157b4e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157b4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157b4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157b4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157b4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157b4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157b4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157b50670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157b50d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157b514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157b51bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157b51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157b524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157b52ab0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157a0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157a0c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157a0d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157a0c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157a0d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157a0dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157a0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157a0aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157a0b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157a4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157a4aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157a4ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157a4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157a4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157a4c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157a4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157a4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157a4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157a4e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157a4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157a4f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157a4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157a500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157a507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157a50ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157a51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157a517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157a51c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157a52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157a52500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157a52970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157a52de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157a53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157a53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157a53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157a53df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157a54260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157a546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157a54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157a54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157a55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157a55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157a55d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157a56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157a565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157a56a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157a56ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157a57330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157a577a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157a57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157a58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157a584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157a58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157a58dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157a59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157a596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157a59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157a59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157a5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157a5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157a18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157a18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157a19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157a198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157a19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157a1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157a1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157a1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157a1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157a1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157a1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157a1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157a1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157a1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157a1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157b054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157b05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157b05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157b06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157b06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157b06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157b06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157b073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157b07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157b07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157b08110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157b08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157b089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157b08e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157b092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157b09740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157b09bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157b0a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157b0a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157b0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157b0ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157b0b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157b0b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157b0bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157b0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157b0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157b0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157b0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157b0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157b0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157b0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157b0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157b0e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157b0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157b0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157b0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157b0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157b0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157b0fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157b101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157b10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157b10aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157b10f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157b11380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157b117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157b11c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157b120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157b12540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157b129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157b12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157b13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157b13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157b13b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157b13fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157b14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157b148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157b14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157b151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157b15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157b15a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157b15ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157b16360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157b167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157b16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157b170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157b17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157b17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157b17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157b18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157b186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157b18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157b18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157b19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157b198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157b19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157b1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157b1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157b1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157b1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157b1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157b1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157b1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157b1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157b1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157b1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157b1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157b1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157b1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157b1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157b1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157b1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157b1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157b1f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157b1f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157b1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157b1fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157b20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157b207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157b20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157b21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157b214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157b21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157b21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157b22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157b226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157b22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157b22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157b23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157b23870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157b23ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157b24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157b245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157b24a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157b24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157b25310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157b25780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157b25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157b26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157b264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157b26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157b26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157b27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157b27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157b27b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157b27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157b283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157b28850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157b28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157b29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157b295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157b29a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157b29e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157b2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157b2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157b2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157b2b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157b2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157b2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157b2bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157b2c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157b2c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157b2cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157b2cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157b2d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157b2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157b2dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157b2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157b2e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157b2e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157b2ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157b2f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157b2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157b2fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157b30020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157b30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157b30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157b30d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157b311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157b31650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157b31ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157b31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157b323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157b32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157b32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157b335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157b33ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157b343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157b34840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157b34cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157b35120 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.800s
user	0m0.288s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4172 (2649e275)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15660b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15660b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15660bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15660c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15660cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15660d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15660d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15660dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15660e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15660e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15660eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15660f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15660fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156610360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156610b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1566119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1566120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1566127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156612fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1566136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156613e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156614520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156614dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1566154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1566157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156615db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156616a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156616f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1566176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156618210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156618a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156618eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156619350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1566197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156619c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15661a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15661a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15661aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15661af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15661b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15661b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15661bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15661c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15661cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15661d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15661d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15661dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15661e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15661ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15661f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15661f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15661fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156620400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156620a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1566214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156621960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156621e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1566222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156622740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156622be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156623080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156623520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1566239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156623e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156624300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1566247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156624c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1566250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156625580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156625a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156625ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156626800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156626ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1566275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156627a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156627f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1566283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156628860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156628d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1566291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156629640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156629ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156629f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15662a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15662a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15662ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15662b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15662b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15662bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15661c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15662c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15662c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15662cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15662cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15662d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15662d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15662dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15662e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15662e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15662eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15662efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15662f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15662f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15662fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1566306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156631030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1566314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156631970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156631e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1566322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156632750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156632bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156633090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156633530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1566339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156633e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156634310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1566347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156634c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1566350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156635590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156635a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156635ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156636370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156636810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156636cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156637150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1566375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156637a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156637f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1566383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156638870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156638d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1566391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156639650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156639af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156639f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15663a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15663a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15663ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15663b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15663b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15663bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15663c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15663c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15663cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15663d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15663d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15663d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15663df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15663e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15663eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15663f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15663f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15663fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1566402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156640770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156641470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1566419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156641f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156642460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1566429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156642f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1566439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156643ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156644440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156644990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156644ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156645430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156645980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156645ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156646420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156646970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156646ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156647410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156647960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156647eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156648950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1566493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156649940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15664a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15664a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15664ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15664b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15664b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15664be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15664c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15664c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15664ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15664d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15664d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15664de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15664e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15664e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15664ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15664f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15664f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15664fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156650380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1566508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156650e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156651370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1566518c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156651e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156652360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1566528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156652e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156653350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1566538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156653d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1566541e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156654680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156654b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156654fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156656240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1566566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156656b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156657020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1566574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156657a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156658130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156658850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156658f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156659690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156659950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156659f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15665a570 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.054 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146704ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146704f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1467053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146705830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146705ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146706110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146706580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1467069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146706e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1467073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146707850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146707ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1467089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1467091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1467099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14670a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14670a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14670af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14670b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14670be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14670c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14670cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14670d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14670da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14670e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14670e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14670e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14670eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14670f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14670f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14670f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14670fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146710280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146710540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1467109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146710e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146711290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146711700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146711b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146711fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146712450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1467128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1467131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146713610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146713a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146713ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1467147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146714c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1467150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146715520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146715990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146715e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146716270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1467166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146716c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146717150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1467175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146717ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146718310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146719060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1467194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146719940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146719db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14671a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14671a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14671ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14671af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14671b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14671b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14671bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14671c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14671c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14671ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14671ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14671d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14671d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14671dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14671e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14671e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14671e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14671ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14671f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14671f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14671fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14671ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1467203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146720830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146720ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146721110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146721580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1467219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146721e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1467222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146722740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146722bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146723020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146723490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146723900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146723d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1467241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146724650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146724ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146724f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1467253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146725810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146725c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1467260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146726560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1467269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146726e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1467272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146727720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146727b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146728000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146728470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1467288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146728d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1467291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146729630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146729aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14672a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14672a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14672ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14672b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14672b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14672b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14672be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14672c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14672c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14672cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14672cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14672d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14672d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14672dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14672e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14672e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14672ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14672eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14672f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14672f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14672fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1467300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146730520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146730990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146730e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146731270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1467316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146731b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146731fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146732430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1467328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146732d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146733180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1467335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146733a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146733ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146734340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1467347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146734c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146735090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146735500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146736350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146736610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146736a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146736ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146737360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1467377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146737c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1467380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146738520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146738990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1467396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146739b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146739fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14673a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14673a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14673ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14673b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14673b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14673ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14673bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14673c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14673c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14673cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14673d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14673d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14673d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14673dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14673e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14673e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14673eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14673efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14673f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14673f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14673fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146740160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1467405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146741320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146741790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146741c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146742070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1467424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146742950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146742dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146743230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1467436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146743b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146743f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1467443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146744860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146744cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146745140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1467455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146745a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146745e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146746300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146746770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146746be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146747050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1467474c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146747930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146747da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146748210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146748680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146748af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146748f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1467493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146749f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14674a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14674ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14674b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14674b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14674b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14674be60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158005aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158005f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158006380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1580067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158006c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1580070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158007540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1580079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158008290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158008700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158008df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158009910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15800a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15800a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15800aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15800b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15800be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15800c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15800cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15800d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15800dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15800e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15800e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15800f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15800f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15800f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15800fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15800fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1580102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158010c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158011100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1580113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158011830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158011ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158012110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158012580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1580129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158012e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1580132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158013740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158013bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158014020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158014490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158014900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158014d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1580151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158015650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158015ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158015f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1580163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158016810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158016c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1580170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158017560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158017ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158017fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158018440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1580188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158018d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158019190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158019600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158019a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158019ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15801a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15801a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15801ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15801b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15801b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15801b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15801bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15801c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15801c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15801cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15801cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15801d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15801d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15801dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15801e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15801e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15801ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15801eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15801f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15801f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15801fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158020080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1580204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158020960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158020dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158021240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1580216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158021b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158021f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158022400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158022870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158022ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158023150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1580235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158023a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158023ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158024310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158024780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158024bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158025060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1580254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158025940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158025db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158026220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158026690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158026b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158026f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1580273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158027850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158027cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158028130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1580285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158028a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158028e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1580292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158029760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158029bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15802a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15802a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15802a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15802ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15802b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15802b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15802bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15802bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15802c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15802c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15802cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15802d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15802d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15802d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15802de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15802e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15802e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15802ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15802f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15802f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15802f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15802fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1580301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158030650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158030ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158030f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1580313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158031810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158031c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1580320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158032560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1580329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158032e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1580332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158033720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158033b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158034000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158034470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1580348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158034d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1580351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158035630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158035aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158035f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158036380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158036f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1580371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158037490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158037900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158037d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1580381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158038650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158038ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158038f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1580393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158039810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158039c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15803a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15803a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15803a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15803ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15803b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15803b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15803bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15803c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15803c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15803c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15803cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15803d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15803d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15803daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15803df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15803e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15803e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15803ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15803f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15803f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15803f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15803fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158040290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158040700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158040b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158040fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158041450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1580418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158041d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1580421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158042610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158042a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158042ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158043360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1580437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158043c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1580440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158044520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158044990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158044e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158045270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1580456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158045b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158045fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158046430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1580468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158046d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158047180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1580475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158047a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158047ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158048340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1580487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158048c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158049090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158049500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158049970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158049de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15804a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15804ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15804b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15804bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15804c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15804c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15804c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15804cce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.910s
user	0m0.241s
sys	0m0.125s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
