Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.682s
user	0m0.703s
sys	0m1.012s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Built target sha1
[  7%] Built target build_info
[  7%] Built target sha256
[  7%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 13%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Linking CXX shared library libllama.dylib
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 24%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 28%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target llava
[ 31%] Built target llama-simple-chat
[ 31%] Built target test-c
[ 31%] Built target llama-quantize-stats
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-simple
[ 32%] Built target common
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 32%] Built target llava_static
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-grammar-integration
[ 42%] Linking CXX executable ../bin/test-grammar-parser
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-llama-grammar
[ 43%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 45%] Built target test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-bpe
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Built target test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Linking CXX executable ../bin/test-chat-template
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 55%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Linking CXX executable ../bin/test-quantize-fns
[ 58%] Built target test-chat-template
[ 59%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Built target test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Linking CXX executable ../../bin/llama-batched
[ 61%] Built target test-autorelease
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 61%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 61%] Built target test-quantize-fns
[ 61%] Built target test-barrier
[ 62%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-embedding
[ 64%] Linking CXX executable ../../bin/llama-eval-callback
[ 65%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 65%] Built target test-rope
[ 65%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Built target llama-batched
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 67%] Built target llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-imatrix
[ 67%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-embedding
[ 70%] Built target llama-eval-callback
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Built target llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Built target llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-lookup
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Built target llama-bench
[ 76%] Built target llama-infill
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-cli
[ 76%] Built target llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-parallel
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Generating loading.html.hpp
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 78%] Built target llama-lookup-merge
[ 78%] Built target llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Generating index.html.gz.hpp
[ 80%] Built target llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-cli
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-quantize
[ 85%] Built target llama-passkey
[ 85%] Built target llama-perplexity
[ 85%] Built target llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-run
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-speculative
[ 90%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Built target llama-tokenize
[ 95%] Built target llama-tts
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.855s
user	0m5.748s
sys	0m8.677s

main: quantize time =  4738.82 ms
main:    total time =  4738.82 ms

main: quantize time =  1846.31 ms
main:    total time =  1846.31 ms

main: quantize time =  1728.50 ms
main:    total time =  1728.50 ms

main: quantize time =  2645.32 ms
main:    total time =  2645.32 ms

main: quantize time =  2645.85 ms
main:    total time =  2645.85 ms

main: quantize time =  5559.49 ms
main:    total time =  5559.49 ms

main: quantize time =  5746.35 ms
main:    total time =  5746.35 ms

main: quantize time =  6800.52 ms
main:    total time =  6800.52 ms

main: quantize time =  5888.91 ms
main:    total time =  5888.91 ms

main: quantize time =  4535.23 ms
main:    total time =  4535.23 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.220 I main: llama backend init
0.00.000.226 I main: load the model and apply lora adapter, if any
0.00.029.771 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.433 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.446 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.459 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.469 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.302 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.633 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.634 I llama_model_loader: - type  f32:  194 tensors
0.00.059.635 I llama_model_loader: - type  f16:   98 tensors
0.00.088.999 I llm_load_vocab: special tokens cache size = 25
0.00.095.856 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.859 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.859 I llm_load_print_meta: arch             = gptneox
0.00.095.859 I llm_load_print_meta: vocab type       = BPE
0.00.095.859 I llm_load_print_meta: n_vocab          = 50304
0.00.095.860 I llm_load_print_meta: n_merges         = 50009
0.00.095.860 I llm_load_print_meta: vocab_only       = 0
0.00.095.860 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.860 I llm_load_print_meta: n_embd           = 2048
0.00.095.860 I llm_load_print_meta: n_layer          = 24
0.00.095.863 I llm_load_print_meta: n_head           = 16
0.00.095.864 I llm_load_print_meta: n_head_kv        = 16
0.00.095.864 I llm_load_print_meta: n_rot            = 32
0.00.095.864 I llm_load_print_meta: n_swa            = 0
0.00.095.867 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.867 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.867 I llm_load_print_meta: n_gqa            = 1
0.00.095.868 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.868 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.869 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.869 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.870 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.870 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.870 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.872 I llm_load_print_meta: n_ff             = 8192
0.00.095.872 I llm_load_print_meta: n_expert         = 0
0.00.095.872 I llm_load_print_meta: n_expert_used    = 0
0.00.095.872 I llm_load_print_meta: causal attn      = 1
0.00.095.872 I llm_load_print_meta: pooling type     = 0
0.00.095.873 I llm_load_print_meta: rope type        = 2
0.00.095.873 I llm_load_print_meta: rope scaling     = linear
0.00.095.873 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.873 I llm_load_print_meta: freq_scale_train = 1
0.00.095.873 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.874 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.874 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.874 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.874 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.874 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.874 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.874 I llm_load_print_meta: model type       = 1.4B
0.00.095.875 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.875 I llm_load_print_meta: model params     = 1.41 B
0.00.095.875 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.876 I llm_load_print_meta: general.name     = 1.4B
0.00.095.878 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.878 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.878 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.878 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.878 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.095.878 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.879 I llm_load_print_meta: max token length = 1024
0.00.098.378 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.378 I llm_load_tensors: offloading output layer to GPU
0.00.098.379 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.396 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.397 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.326 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.327 I llama_new_context_with_model: n_ctx         = 2048
0.00.099.327 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.099.327 I llama_new_context_with_model: n_batch       = 2048
0.00.099.327 I llama_new_context_with_model: n_ubatch      = 512
0.00.099.328 I llama_new_context_with_model: flash_attn    = 0
0.00.099.328 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.329 I llama_new_context_with_model: freq_scale    = 1
0.00.099.329 I ggml_metal_init: allocating
0.00.099.340 I ggml_metal_init: found device: Apple M4
0.00.099.342 I ggml_metal_init: picking default device: Apple M4
0.00.100.014 I ggml_metal_init: using embedded metal library
0.00.109.292 I ggml_metal_init: GPU name:   Apple M4
0.00.109.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.295 I ggml_metal_init: simdgroup reduction   = true
0.00.109.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.295 I ggml_metal_init: has bfloat            = true
0.00.109.295 I ggml_metal_init: use bfloat            = true
0.00.109.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.132.838 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.152.217 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.152.223 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.152.242 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.153.223 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.153.225 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.153.225 I llama_new_context_with_model: graph nodes  = 967
0.00.153.225 I llama_new_context_with_model: graph splits = 2
0.00.153.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.153.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.153.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.236.526 I main: llama threadpool init, n_threads = 4
0.00.236.567 I 
0.00.236.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.236.609 I 
0.00.236.681 I sampler seed: 1234
0.00.236.686 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.236.720 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.236.722 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.236.722 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.078.579 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.02.078.580 I llama_perf_context_print:        load time =     206.74 ms
0.02.078.581 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.00 tokens per second)
0.02.078.582 I llama_perf_context_print:        eval time =    1795.18 ms /    63 runs   (   28.49 ms per token,    35.09 tokens per second)
0.02.078.582 I llama_perf_context_print:       total time =    1842.06 ms /    70 tokens
0.02.078.785 I ggml_metal_free: deallocating

real	0m2.392s
user	0m0.142s
sys	0m0.103s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.775 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.295 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.171 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.116 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.119 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.120 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.120 I llama_model_loader: - type  f32:  194 tensors
0.00.037.120 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.640 I llm_load_vocab: special tokens cache size = 25
0.00.068.853 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.857 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.858 I llm_load_print_meta: arch             = gptneox
0.00.068.858 I llm_load_print_meta: vocab type       = BPE
0.00.068.858 I llm_load_print_meta: n_vocab          = 50304
0.00.068.858 I llm_load_print_meta: n_merges         = 50009
0.00.068.860 I llm_load_print_meta: vocab_only       = 0
0.00.068.860 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.860 I llm_load_print_meta: n_embd           = 2048
0.00.068.861 I llm_load_print_meta: n_layer          = 24
0.00.068.865 I llm_load_print_meta: n_head           = 16
0.00.068.865 I llm_load_print_meta: n_head_kv        = 16
0.00.068.866 I llm_load_print_meta: n_rot            = 32
0.00.068.866 I llm_load_print_meta: n_swa            = 0
0.00.068.866 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.866 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.867 I llm_load_print_meta: n_gqa            = 1
0.00.068.869 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.870 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.870 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.871 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.871 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.871 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.872 I llm_load_print_meta: n_ff             = 8192
0.00.068.872 I llm_load_print_meta: n_expert         = 0
0.00.068.872 I llm_load_print_meta: n_expert_used    = 0
0.00.068.873 I llm_load_print_meta: causal attn      = 1
0.00.068.873 I llm_load_print_meta: pooling type     = 0
0.00.068.873 I llm_load_print_meta: rope type        = 2
0.00.068.873 I llm_load_print_meta: rope scaling     = linear
0.00.068.874 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.874 I llm_load_print_meta: freq_scale_train = 1
0.00.068.874 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.874 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.875 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.875 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.875 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.875 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.875 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.875 I llm_load_print_meta: model type       = 1.4B
0.00.068.876 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.876 I llm_load_print_meta: model params     = 1.41 B
0.00.068.877 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.877 I llm_load_print_meta: general.name     = 1.4B
0.00.068.877 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.877 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.877 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.878 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.878 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.878 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.878 I llm_load_print_meta: max token length = 1024
0.00.071.503 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.504 I llm_load_tensors: offloading output layer to GPU
0.00.071.505 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.516 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.517 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.539 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.540 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.540 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.540 I llama_new_context_with_model: n_batch       = 2048
0.00.072.540 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.541 I llama_new_context_with_model: flash_attn    = 0
0.00.072.541 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.541 I llama_new_context_with_model: freq_scale    = 1
0.00.072.542 I ggml_metal_init: allocating
0.00.072.549 I ggml_metal_init: found device: Apple M4
0.00.072.551 I ggml_metal_init: picking default device: Apple M4
0.00.073.371 I ggml_metal_init: using embedded metal library
0.00.076.323 I ggml_metal_init: GPU name:   Apple M4
0.00.076.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.326 I ggml_metal_init: simdgroup reduction   = true
0.00.076.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.326 I ggml_metal_init: has bfloat            = true
0.00.076.326 I ggml_metal_init: use bfloat            = true
0.00.076.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.048 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.112.404 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.414 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.440 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.480 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.483 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.483 I llama_new_context_with_model: graph nodes  = 967
0.00.113.483 I llama_new_context_with_model: graph splits = 2
0.00.113.497 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.113.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.625 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.214.444 I main: llama threadpool init, n_threads = 4
0.01.214.480 I 
0.01.214.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.214.510 I 
0.01.214.740 I sampler seed: 1234
0.01.214.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.214.793 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.214.794 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.214.794 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.310.337 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.02.310.337 I llama_perf_context_print:        load time =    1204.66 ms
0.02.310.338 I llama_perf_context_print: prompt eval time =      44.28 ms /     7 tokens (    6.33 ms per token,   158.08 tokens per second)
0.02.310.339 I llama_perf_context_print:        eval time =    1048.47 ms /    63 runs   (   16.64 ms per token,    60.09 tokens per second)
0.02.310.339 I llama_perf_context_print:       total time =    1095.89 ms /    70 tokens
0.02.310.530 I ggml_metal_free: deallocating

real	0m2.329s
user	0m0.118s
sys	0m0.211s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.017.302 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.382 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.034.387 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.389 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.391 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.392 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.392 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.392 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.394 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.394 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.395 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.395 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.396 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.398 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.400 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.400 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.617 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.359 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.359 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.359 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.360 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.360 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.361 I llama_model_loader: - type  f32:  194 tensors
0.00.044.361 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.361 I llama_model_loader: - type q6_K:    1 tensors
0.00.071.618 I llm_load_vocab: special tokens cache size = 25
0.00.080.906 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.910 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.910 I llm_load_print_meta: arch             = gptneox
0.00.080.911 I llm_load_print_meta: vocab type       = BPE
0.00.080.911 I llm_load_print_meta: n_vocab          = 50304
0.00.080.911 I llm_load_print_meta: n_merges         = 50009
0.00.080.912 I llm_load_print_meta: vocab_only       = 0
0.00.080.912 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.912 I llm_load_print_meta: n_embd           = 2048
0.00.080.912 I llm_load_print_meta: n_layer          = 24
0.00.080.918 I llm_load_print_meta: n_head           = 16
0.00.080.919 I llm_load_print_meta: n_head_kv        = 16
0.00.080.919 I llm_load_print_meta: n_rot            = 32
0.00.080.919 I llm_load_print_meta: n_swa            = 0
0.00.080.919 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.920 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.920 I llm_load_print_meta: n_gqa            = 1
0.00.080.924 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.924 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.927 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.927 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.927 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.928 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.928 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.929 I llm_load_print_meta: n_ff             = 8192
0.00.080.929 I llm_load_print_meta: n_expert         = 0
0.00.080.929 I llm_load_print_meta: n_expert_used    = 0
0.00.080.929 I llm_load_print_meta: causal attn      = 1
0.00.080.929 I llm_load_print_meta: pooling type     = 0
0.00.080.929 I llm_load_print_meta: rope type        = 2
0.00.080.930 I llm_load_print_meta: rope scaling     = linear
0.00.080.930 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.930 I llm_load_print_meta: freq_scale_train = 1
0.00.080.931 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.931 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.931 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.931 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.932 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.932 I llm_load_print_meta: model type       = 1.4B
0.00.080.938 I llm_load_print_meta: model ftype      = Q4_0
0.00.080.938 I llm_load_print_meta: model params     = 1.41 B
0.00.080.939 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.080.939 I llm_load_print_meta: general.name     = 1.4B
0.00.080.939 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.939 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.940 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.940 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.942 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.080.942 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.943 I llm_load_print_meta: max token length = 1024
0.00.083.839 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.083.840 I llm_load_tensors: offloading output layer to GPU
0.00.083.840 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.083.852 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.083.854 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.085.271 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.272 I llama_new_context_with_model: n_ctx         = 2048
0.00.085.273 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.085.273 I llama_new_context_with_model: n_batch       = 2048
0.00.085.273 I llama_new_context_with_model: n_ubatch      = 512
0.00.085.274 I llama_new_context_with_model: flash_attn    = 0
0.00.085.274 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.275 I llama_new_context_with_model: freq_scale    = 1
0.00.085.275 I ggml_metal_init: allocating
0.00.085.284 I ggml_metal_init: found device: Apple M4
0.00.085.288 I ggml_metal_init: picking default device: Apple M4
0.00.086.226 I ggml_metal_init: using embedded metal library
0.00.090.043 I ggml_metal_init: GPU name:   Apple M4
0.00.090.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.046 I ggml_metal_init: simdgroup reduction   = true
0.00.090.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.047 I ggml_metal_init: has bfloat            = true
0.00.090.047 I ggml_metal_init: use bfloat            = true
0.00.090.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.216 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.128.560 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.128.567 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.128.591 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.129.603 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.129.604 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.129.605 I llama_new_context_with_model: graph nodes  = 967
0.00.129.605 I llama_new_context_with_model: graph splits = 2
0.00.129.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.129.742 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.129.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.622 I main: llama threadpool init, n_threads = 4
0.00.731.667 I 
0.00.731.702 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.702 I 
0.00.731.940 I sampler seed: 1234
0.00.731.947 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.993 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.997 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.422.618 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.422.619 I llama_perf_context_print:        load time =     714.31 ms
0.01.422.620 I llama_perf_context_print: prompt eval time =      44.51 ms /     7 tokens (    6.36 ms per token,   157.28 tokens per second)
0.01.422.620 I llama_perf_context_print:        eval time =     643.14 ms /    63 runs   (   10.21 ms per token,    97.96 tokens per second)
0.01.422.621 I llama_perf_context_print:       total time =     691.00 ms /    70 tokens
0.01.422.837 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.127s
sys	0m0.166s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.034 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.045 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.045 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.046 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.051 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.216 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.291 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.425 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.427 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.427 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.428 I llama_model_loader: - type  f32:  194 tensors
0.00.025.429 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.429 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.345 I llm_load_vocab: special tokens cache size = 25
0.00.053.400 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.404 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.404 I llm_load_print_meta: arch             = gptneox
0.00.053.405 I llm_load_print_meta: vocab type       = BPE
0.00.053.405 I llm_load_print_meta: n_vocab          = 50304
0.00.053.405 I llm_load_print_meta: n_merges         = 50009
0.00.053.405 I llm_load_print_meta: vocab_only       = 0
0.00.053.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.409 I llm_load_print_meta: n_embd           = 2048
0.00.053.409 I llm_load_print_meta: n_layer          = 24
0.00.053.413 I llm_load_print_meta: n_head           = 16
0.00.053.419 I llm_load_print_meta: n_head_kv        = 16
0.00.053.420 I llm_load_print_meta: n_rot            = 32
0.00.053.420 I llm_load_print_meta: n_swa            = 0
0.00.053.420 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.420 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.432 I llm_load_print_meta: n_gqa            = 1
0.00.053.433 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.434 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.435 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.435 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.435 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.435 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.435 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.436 I llm_load_print_meta: n_ff             = 8192
0.00.053.436 I llm_load_print_meta: n_expert         = 0
0.00.053.436 I llm_load_print_meta: n_expert_used    = 0
0.00.053.436 I llm_load_print_meta: causal attn      = 1
0.00.053.436 I llm_load_print_meta: pooling type     = 0
0.00.053.437 I llm_load_print_meta: rope type        = 2
0.00.053.437 I llm_load_print_meta: rope scaling     = linear
0.00.053.437 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.437 I llm_load_print_meta: freq_scale_train = 1
0.00.053.437 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.438 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.438 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.438 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.439 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.439 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.439 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.439 I llm_load_print_meta: model type       = 1.4B
0.00.053.439 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.440 I llm_load_print_meta: model params     = 1.41 B
0.00.053.440 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.441 I llm_load_print_meta: general.name     = 1.4B
0.00.053.441 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.441 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.441 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.445 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: max token length = 1024
0.00.055.455 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.456 I llm_load_tensors: offloading output layer to GPU
0.00.055.456 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.467 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.468 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.056.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.386 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.386 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.386 I llama_new_context_with_model: n_batch       = 2048
0.00.056.386 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.386 I llama_new_context_with_model: flash_attn    = 0
0.00.056.387 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.387 I llama_new_context_with_model: freq_scale    = 1
0.00.056.388 I ggml_metal_init: allocating
0.00.056.393 I ggml_metal_init: found device: Apple M4
0.00.056.396 I ggml_metal_init: picking default device: Apple M4
0.00.057.058 I ggml_metal_init: using embedded metal library
0.00.059.438 I ggml_metal_init: GPU name:   Apple M4
0.00.059.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.441 I ggml_metal_init: simdgroup reduction   = true
0.00.059.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.441 I ggml_metal_init: has bfloat            = true
0.00.059.441 I ggml_metal_init: use bfloat            = true
0.00.059.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.688 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.705 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.712 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.730 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.714 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.716 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.716 I llama_new_context_with_model: graph nodes  = 967
0.00.091.716 I llama_new_context_with_model: graph splits = 2
0.00.091.730 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.872 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.849 I main: llama threadpool init, n_threads = 4
0.00.698.892 I 
0.00.698.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.925 I 
0.00.699.159 I sampler seed: 1234
0.00.699.172 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.212 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.216 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.216 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.423.508 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.423.508 I llama_perf_context_print:        load time =     689.53 ms
0.01.423.509 I llama_perf_context_print: prompt eval time =      46.24 ms /     7 tokens (    6.61 ms per token,   151.39 tokens per second)
0.01.423.510 I llama_perf_context_print:        eval time =     675.61 ms /    63 runs   (   10.72 ms per token,    93.25 tokens per second)
0.01.423.510 I llama_perf_context_print:       total time =     724.66 ms /    70 tokens
0.01.423.718 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.112s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.523 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.960 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.975 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.975 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.975 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.913 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.732 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.733 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.733 I llama_model_loader: - type  f32:  194 tensors
0.00.026.734 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.734 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.835 I llm_load_vocab: special tokens cache size = 25
0.00.053.786 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.788 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.789 I llm_load_print_meta: arch             = gptneox
0.00.053.789 I llm_load_print_meta: vocab type       = BPE
0.00.053.789 I llm_load_print_meta: n_vocab          = 50304
0.00.053.790 I llm_load_print_meta: n_merges         = 50009
0.00.053.790 I llm_load_print_meta: vocab_only       = 0
0.00.053.790 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.790 I llm_load_print_meta: n_embd           = 2048
0.00.053.790 I llm_load_print_meta: n_layer          = 24
0.00.053.793 I llm_load_print_meta: n_head           = 16
0.00.053.796 I llm_load_print_meta: n_head_kv        = 16
0.00.053.796 I llm_load_print_meta: n_rot            = 32
0.00.053.796 I llm_load_print_meta: n_swa            = 0
0.00.053.796 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.797 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.797 I llm_load_print_meta: n_gqa            = 1
0.00.053.798 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.799 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.799 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.800 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.800 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.800 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.800 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.801 I llm_load_print_meta: n_ff             = 8192
0.00.053.801 I llm_load_print_meta: n_expert         = 0
0.00.053.802 I llm_load_print_meta: n_expert_used    = 0
0.00.053.802 I llm_load_print_meta: causal attn      = 1
0.00.053.802 I llm_load_print_meta: pooling type     = 0
0.00.053.802 I llm_load_print_meta: rope type        = 2
0.00.053.802 I llm_load_print_meta: rope scaling     = linear
0.00.053.803 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.803 I llm_load_print_meta: freq_scale_train = 1
0.00.053.803 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.803 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.803 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.804 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.805 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.806 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.806 I llm_load_print_meta: model type       = 1.4B
0.00.053.806 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.807 I llm_load_print_meta: model params     = 1.41 B
0.00.053.807 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.807 I llm_load_print_meta: general.name     = 1.4B
0.00.053.808 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.808 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.808 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.808 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.808 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.810 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.810 I llm_load_print_meta: max token length = 1024
0.00.055.936 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.937 I llm_load_tensors: offloading output layer to GPU
0.00.055.937 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.948 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.949 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.917 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.918 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.918 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.918 I llama_new_context_with_model: n_batch       = 2048
0.00.056.919 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.919 I llama_new_context_with_model: flash_attn    = 0
0.00.056.919 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.920 I llama_new_context_with_model: freq_scale    = 1
0.00.056.920 I ggml_metal_init: allocating
0.00.056.924 I ggml_metal_init: found device: Apple M4
0.00.056.926 I ggml_metal_init: picking default device: Apple M4
0.00.057.557 I ggml_metal_init: using embedded metal library
0.00.059.962 I ggml_metal_init: GPU name:   Apple M4
0.00.059.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.965 I ggml_metal_init: simdgroup reduction   = true
0.00.059.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.965 I ggml_metal_init: has bfloat            = true
0.00.059.965 I ggml_metal_init: use bfloat            = true
0.00.059.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.959 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.484 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.489 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.506 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.589 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.590 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.591 I llama_new_context_with_model: graph nodes  = 967
0.00.090.591 I llama_new_context_with_model: graph splits = 2
0.00.090.607 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.729 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.924 I main: llama threadpool init, n_threads = 4
0.00.729.968 I 
0.00.730.006 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.008 I 
0.00.730.238 I sampler seed: 1234
0.00.730.243 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.285 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.289 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.289 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.519.575 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.01.519.575 I llama_perf_context_print:        load time =     719.39 ms
0.01.519.579 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.39 tokens per second)
0.01.519.579 I llama_perf_context_print:        eval time =     743.20 ms /    63 runs   (   11.80 ms per token,    84.77 tokens per second)
0.01.519.580 I llama_perf_context_print:       total time =     789.66 ms /    70 tokens
0.01.519.772 I ggml_metal_free: deallocating

real	0m1.537s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.578 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.579 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.579 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.580 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.582 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.582 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.582 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.583 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.586 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.365 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.415 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.137 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.139 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.139 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.139 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.139 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.140 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.140 I llama_model_loader: - type  f32:  194 tensors
0.00.024.141 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.141 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.566 I llm_load_vocab: special tokens cache size = 25
0.00.050.608 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.610 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.611 I llm_load_print_meta: arch             = gptneox
0.00.050.611 I llm_load_print_meta: vocab type       = BPE
0.00.050.611 I llm_load_print_meta: n_vocab          = 50304
0.00.050.612 I llm_load_print_meta: n_merges         = 50009
0.00.050.612 I llm_load_print_meta: vocab_only       = 0
0.00.050.612 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.612 I llm_load_print_meta: n_embd           = 2048
0.00.050.612 I llm_load_print_meta: n_layer          = 24
0.00.050.615 I llm_load_print_meta: n_head           = 16
0.00.050.616 I llm_load_print_meta: n_head_kv        = 16
0.00.050.616 I llm_load_print_meta: n_rot            = 32
0.00.050.616 I llm_load_print_meta: n_swa            = 0
0.00.050.616 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.617 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.617 I llm_load_print_meta: n_gqa            = 1
0.00.050.618 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.619 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.620 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.621 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.621 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.622 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.622 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.622 I llm_load_print_meta: n_ff             = 8192
0.00.050.622 I llm_load_print_meta: n_expert         = 0
0.00.050.623 I llm_load_print_meta: n_expert_used    = 0
0.00.050.624 I llm_load_print_meta: causal attn      = 1
0.00.050.626 I llm_load_print_meta: pooling type     = 0
0.00.050.626 I llm_load_print_meta: rope type        = 2
0.00.050.627 I llm_load_print_meta: rope scaling     = linear
0.00.050.627 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.627 I llm_load_print_meta: freq_scale_train = 1
0.00.050.628 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.628 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.628 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.628 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.628 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.628 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.628 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.629 I llm_load_print_meta: model type       = 1.4B
0.00.050.629 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.629 I llm_load_print_meta: model params     = 1.41 B
0.00.050.630 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.630 I llm_load_print_meta: general.name     = 1.4B
0.00.050.630 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.632 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.633 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.633 I llm_load_print_meta: max token length = 1024
0.00.052.639 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.640 I llm_load_tensors: offloading output layer to GPU
0.00.052.640 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.650 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.651 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.560 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.561 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.561 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.561 I llama_new_context_with_model: n_batch       = 2048
0.00.053.562 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.562 I llama_new_context_with_model: flash_attn    = 0
0.00.053.562 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.562 I llama_new_context_with_model: freq_scale    = 1
0.00.053.563 I ggml_metal_init: allocating
0.00.053.566 I ggml_metal_init: found device: Apple M4
0.00.053.568 I ggml_metal_init: picking default device: Apple M4
0.00.054.173 I ggml_metal_init: using embedded metal library
0.00.056.517 I ggml_metal_init: GPU name:   Apple M4
0.00.056.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.519 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.520 I ggml_metal_init: simdgroup reduction   = true
0.00.056.521 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.521 I ggml_metal_init: has bfloat            = true
0.00.056.522 I ggml_metal_init: use bfloat            = true
0.00.056.522 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.322 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.936 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.964 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.007 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.008 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.009 I llama_new_context_with_model: graph nodes  = 967
0.00.087.009 I llama_new_context_with_model: graph splits = 2
0.00.087.024 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.603 I main: llama threadpool init, n_threads = 4
0.00.814.644 I 
0.00.814.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.696 I 
0.00.814.937 I sampler seed: 1234
0.00.814.943 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.959 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.959 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.959 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.657.372 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.657.373 I llama_perf_context_print:        load time =     805.97 ms
0.01.657.374 I llama_perf_context_print: prompt eval time =      45.24 ms /     7 tokens (    6.46 ms per token,   154.72 tokens per second)
0.01.657.374 I llama_perf_context_print:        eval time =     794.19 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.657.376 I llama_perf_context_print:       total time =     842.78 ms /    70 tokens
0.01.657.587 I ggml_metal_free: deallocating

real	0m1.674s
user	0m0.109s
sys	0m0.178s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.151 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.704 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.710 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.711 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.717 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.578 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.666 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.533 I llama_model_loader: - type  f32:  194 tensors
0.00.023.533 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.533 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.533 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.789 I llm_load_vocab: special tokens cache size = 25
0.00.050.803 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.806 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.806 I llm_load_print_meta: arch             = gptneox
0.00.050.806 I llm_load_print_meta: vocab type       = BPE
0.00.050.806 I llm_load_print_meta: n_vocab          = 50304
0.00.050.807 I llm_load_print_meta: n_merges         = 50009
0.00.050.807 I llm_load_print_meta: vocab_only       = 0
0.00.050.807 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.807 I llm_load_print_meta: n_embd           = 2048
0.00.050.807 I llm_load_print_meta: n_layer          = 24
0.00.050.810 I llm_load_print_meta: n_head           = 16
0.00.050.811 I llm_load_print_meta: n_head_kv        = 16
0.00.050.811 I llm_load_print_meta: n_rot            = 32
0.00.050.811 I llm_load_print_meta: n_swa            = 0
0.00.050.811 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.811 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.812 I llm_load_print_meta: n_gqa            = 1
0.00.050.813 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.814 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.814 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.815 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.815 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.818 I llm_load_print_meta: n_ff             = 8192
0.00.050.819 I llm_load_print_meta: n_expert         = 0
0.00.050.819 I llm_load_print_meta: n_expert_used    = 0
0.00.050.819 I llm_load_print_meta: causal attn      = 1
0.00.050.819 I llm_load_print_meta: pooling type     = 0
0.00.050.819 I llm_load_print_meta: rope type        = 2
0.00.050.820 I llm_load_print_meta: rope scaling     = linear
0.00.050.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.820 I llm_load_print_meta: freq_scale_train = 1
0.00.050.820 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.821 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.822 I llm_load_print_meta: model type       = 1.4B
0.00.050.822 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.824 I llm_load_print_meta: model params     = 1.41 B
0.00.050.824 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.824 I llm_load_print_meta: general.name     = 1.4B
0.00.050.825 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.825 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.827 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.827 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.828 I llm_load_print_meta: max token length = 1024
0.00.052.748 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.749 I llm_load_tensors: offloading output layer to GPU
0.00.052.749 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.760 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.761 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.646 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.647 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.647 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.648 I llama_new_context_with_model: n_batch       = 2048
0.00.053.648 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.648 I llama_new_context_with_model: flash_attn    = 0
0.00.053.648 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.649 I llama_new_context_with_model: freq_scale    = 1
0.00.053.649 I ggml_metal_init: allocating
0.00.053.656 I ggml_metal_init: found device: Apple M4
0.00.053.658 I ggml_metal_init: picking default device: Apple M4
0.00.054.248 I ggml_metal_init: using embedded metal library
0.00.056.628 I ggml_metal_init: GPU name:   Apple M4
0.00.056.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.630 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.631 I ggml_metal_init: simdgroup reduction   = true
0.00.056.631 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.631 I ggml_metal_init: has bfloat            = true
0.00.056.631 I ggml_metal_init: use bfloat            = true
0.00.056.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.448 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.186 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.204 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.257 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.258 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.259 I llama_new_context_with_model: graph nodes  = 967
0.00.087.259 I llama_new_context_with_model: graph splits = 2
0.00.087.275 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.402 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.772 I main: llama threadpool init, n_threads = 4
0.00.512.810 I 
0.00.512.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.512.843 I 
0.00.512.987 I sampler seed: 1234
0.00.512.992 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.005 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.007 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.007 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.193.537 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60735.67 tokens per second)
0.01.193.538 I llama_perf_context_print:        load time =     503.62 ms
0.01.193.539 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.72 tokens per second)
0.01.193.539 I llama_perf_context_print:        eval time =     641.77 ms /    63 runs   (   10.19 ms per token,    98.17 tokens per second)
0.01.193.540 I llama_perf_context_print:       total time =     680.77 ms /    70 tokens
0.01.193.736 I ggml_metal_free: deallocating

real	0m1.212s
user	0m0.110s
sys	0m0.115s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.621 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.104 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.111 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.111 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.112 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.113 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.114 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.115 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.115 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.115 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.116 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.118 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.118 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.021 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.022 I llama_model_loader: - type  f32:  194 tensors
0.00.023.022 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.022 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.023 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.148 I llm_load_vocab: special tokens cache size = 25
0.00.050.138 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.141 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.141 I llm_load_print_meta: arch             = gptneox
0.00.050.141 I llm_load_print_meta: vocab type       = BPE
0.00.050.142 I llm_load_print_meta: n_vocab          = 50304
0.00.050.142 I llm_load_print_meta: n_merges         = 50009
0.00.050.142 I llm_load_print_meta: vocab_only       = 0
0.00.050.142 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.142 I llm_load_print_meta: n_embd           = 2048
0.00.050.143 I llm_load_print_meta: n_layer          = 24
0.00.050.145 I llm_load_print_meta: n_head           = 16
0.00.050.146 I llm_load_print_meta: n_head_kv        = 16
0.00.050.146 I llm_load_print_meta: n_rot            = 32
0.00.050.146 I llm_load_print_meta: n_swa            = 0
0.00.050.147 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.147 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.147 I llm_load_print_meta: n_gqa            = 1
0.00.050.148 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.149 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.150 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.150 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.150 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.150 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.151 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.151 I llm_load_print_meta: n_ff             = 8192
0.00.050.151 I llm_load_print_meta: n_expert         = 0
0.00.050.153 I llm_load_print_meta: n_expert_used    = 0
0.00.050.154 I llm_load_print_meta: causal attn      = 1
0.00.050.154 I llm_load_print_meta: pooling type     = 0
0.00.050.154 I llm_load_print_meta: rope type        = 2
0.00.050.154 I llm_load_print_meta: rope scaling     = linear
0.00.050.154 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.155 I llm_load_print_meta: freq_scale_train = 1
0.00.050.156 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.157 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.157 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.158 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.158 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.158 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.158 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.158 I llm_load_print_meta: model type       = 1.4B
0.00.050.159 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.159 I llm_load_print_meta: model params     = 1.41 B
0.00.050.160 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.160 I llm_load_print_meta: general.name     = 1.4B
0.00.050.160 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.160 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.160 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.161 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.161 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.161 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.161 I llm_load_print_meta: max token length = 1024
0.00.051.750 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.750 I llm_load_tensors: offloading output layer to GPU
0.00.051.751 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.761 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.762 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.579 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.580 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.581 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.581 I llama_new_context_with_model: n_batch       = 2048
0.00.052.581 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.581 I llama_new_context_with_model: flash_attn    = 0
0.00.052.582 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.582 I llama_new_context_with_model: freq_scale    = 1
0.00.052.582 I ggml_metal_init: allocating
0.00.052.586 I ggml_metal_init: found device: Apple M4
0.00.052.588 I ggml_metal_init: picking default device: Apple M4
0.00.053.144 I ggml_metal_init: using embedded metal library
0.00.055.445 I ggml_metal_init: GPU name:   Apple M4
0.00.055.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.448 I ggml_metal_init: simdgroup reduction   = true
0.00.055.448 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.448 I ggml_metal_init: has bfloat            = true
0.00.055.448 I ggml_metal_init: use bfloat            = true
0.00.055.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.691 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.698 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.720 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.767 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.768 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.768 I llama_new_context_with_model: graph nodes  = 967
0.00.086.769 I llama_new_context_with_model: graph splits = 2
0.00.086.784 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.776 I main: llama threadpool init, n_threads = 4
0.00.587.825 I 
0.00.587.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.864 I 
0.00.588.108 I sampler seed: 1234
0.00.588.112 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.588.189 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.588.192 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.588.192 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.332.399 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.332.399 I llama_perf_context_print:        load time =     579.15 ms
0.01.332.401 I llama_perf_context_print: prompt eval time =      40.55 ms /     7 tokens (    5.79 ms per token,   172.65 tokens per second)
0.01.332.401 I llama_perf_context_print:        eval time =     700.87 ms /    63 runs   (   11.12 ms per token,    89.89 tokens per second)
0.01.332.402 I llama_perf_context_print:       total time =     744.62 ms /    70 tokens
0.01.332.651 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.112s
sys	0m0.137s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.010.009 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.440 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.445 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.364 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.188 I llama_model_loader: - type  f32:  194 tensors
0.00.024.189 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.189 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.189 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.525 I llm_load_vocab: special tokens cache size = 25
0.00.050.651 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.653 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.654 I llm_load_print_meta: arch             = gptneox
0.00.050.654 I llm_load_print_meta: vocab type       = BPE
0.00.050.654 I llm_load_print_meta: n_vocab          = 50304
0.00.050.654 I llm_load_print_meta: n_merges         = 50009
0.00.050.655 I llm_load_print_meta: vocab_only       = 0
0.00.050.655 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.655 I llm_load_print_meta: n_embd           = 2048
0.00.050.655 I llm_load_print_meta: n_layer          = 24
0.00.050.658 I llm_load_print_meta: n_head           = 16
0.00.050.659 I llm_load_print_meta: n_head_kv        = 16
0.00.050.659 I llm_load_print_meta: n_rot            = 32
0.00.050.659 I llm_load_print_meta: n_swa            = 0
0.00.050.659 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.660 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.661 I llm_load_print_meta: n_gqa            = 1
0.00.050.662 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.663 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.664 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.664 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.665 I llm_load_print_meta: n_ff             = 8192
0.00.050.665 I llm_load_print_meta: n_expert         = 0
0.00.050.665 I llm_load_print_meta: n_expert_used    = 0
0.00.050.665 I llm_load_print_meta: causal attn      = 1
0.00.050.665 I llm_load_print_meta: pooling type     = 0
0.00.050.665 I llm_load_print_meta: rope type        = 2
0.00.050.666 I llm_load_print_meta: rope scaling     = linear
0.00.050.666 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.666 I llm_load_print_meta: freq_scale_train = 1
0.00.050.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.667 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.667 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.667 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.667 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.667 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.668 I llm_load_print_meta: model type       = 1.4B
0.00.050.668 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.669 I llm_load_print_meta: model params     = 1.41 B
0.00.050.669 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.669 I llm_load_print_meta: general.name     = 1.4B
0.00.050.670 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.670 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.670 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.670 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.670 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.671 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.671 I llm_load_print_meta: max token length = 1024
0.00.052.682 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.683 I llm_load_tensors: offloading output layer to GPU
0.00.052.683 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.693 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.694 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.581 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.582 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.582 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.582 I llama_new_context_with_model: n_batch       = 2048
0.00.053.582 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.582 I llama_new_context_with_model: flash_attn    = 0
0.00.053.583 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.583 I llama_new_context_with_model: freq_scale    = 1
0.00.053.584 I ggml_metal_init: allocating
0.00.053.590 I ggml_metal_init: found device: Apple M4
0.00.053.593 I ggml_metal_init: picking default device: Apple M4
0.00.054.163 I ggml_metal_init: using embedded metal library
0.00.056.489 I ggml_metal_init: GPU name:   Apple M4
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.492 I ggml_metal_init: simdgroup reduction   = true
0.00.056.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.492 I ggml_metal_init: has bfloat            = true
0.00.056.492 I ggml_metal_init: use bfloat            = true
0.00.056.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.288 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.229 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.237 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.254 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.280 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.281 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.282 I llama_new_context_with_model: graph nodes  = 967
0.00.087.282 I llama_new_context_with_model: graph splits = 2
0.00.087.298 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.438 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.563 I main: llama threadpool init, n_threads = 4
0.00.613.609 I 
0.00.613.641 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.643 I 
0.00.613.885 I sampler seed: 1234
0.00.613.891 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.906 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.907 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.907 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.713 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.376.714 I llama_perf_context_print:        load time =     603.55 ms
0.01.376.714 I llama_perf_context_print: prompt eval time =      49.48 ms /     7 tokens (    7.07 ms per token,   141.48 tokens per second)
0.01.376.715 I llama_perf_context_print:        eval time =     710.33 ms /    63 runs   (   11.28 ms per token,    88.69 tokens per second)
0.01.376.715 I llama_perf_context_print:       total time =     763.16 ms /    70 tokens
0.01.376.907 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.110s
sys	0m0.138s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.604 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.209 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.209 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.214 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.726 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.727 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.728 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.022.729 I llama_model_loader: - type  f32:  194 tensors
0.00.022.729 I llama_model_loader: - type q5_K:   61 tensors
0.00.022.730 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.121 I llm_load_vocab: special tokens cache size = 25
0.00.048.975 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.978 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.978 I llm_load_print_meta: arch             = gptneox
0.00.048.979 I llm_load_print_meta: vocab type       = BPE
0.00.048.979 I llm_load_print_meta: n_vocab          = 50304
0.00.048.979 I llm_load_print_meta: n_merges         = 50009
0.00.048.979 I llm_load_print_meta: vocab_only       = 0
0.00.048.979 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.980 I llm_load_print_meta: n_embd           = 2048
0.00.048.980 I llm_load_print_meta: n_layer          = 24
0.00.048.982 I llm_load_print_meta: n_head           = 16
0.00.048.983 I llm_load_print_meta: n_head_kv        = 16
0.00.048.983 I llm_load_print_meta: n_rot            = 32
0.00.048.984 I llm_load_print_meta: n_swa            = 0
0.00.048.984 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.984 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.985 I llm_load_print_meta: n_gqa            = 1
0.00.048.987 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.988 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.988 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.989 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.989 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.989 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.990 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.990 I llm_load_print_meta: n_ff             = 8192
0.00.048.990 I llm_load_print_meta: n_expert         = 0
0.00.048.991 I llm_load_print_meta: n_expert_used    = 0
0.00.048.992 I llm_load_print_meta: causal attn      = 1
0.00.048.992 I llm_load_print_meta: pooling type     = 0
0.00.048.992 I llm_load_print_meta: rope type        = 2
0.00.048.993 I llm_load_print_meta: rope scaling     = linear
0.00.048.993 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.993 I llm_load_print_meta: freq_scale_train = 1
0.00.048.993 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.994 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.994 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.994 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.994 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.994 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.994 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.995 I llm_load_print_meta: model type       = 1.4B
0.00.048.995 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.048.995 I llm_load_print_meta: model params     = 1.41 B
0.00.048.996 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.048.997 I llm_load_print_meta: general.name     = 1.4B
0.00.049.001 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.001 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.002 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.002 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.002 I llm_load_print_meta: max token length = 1024
0.00.050.590 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.591 I llm_load_tensors: offloading output layer to GPU
0.00.050.591 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.600 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.601 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.438 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.438 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.438 I llama_new_context_with_model: n_batch       = 2048
0.00.051.438 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.438 I llama_new_context_with_model: flash_attn    = 0
0.00.051.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.439 I llama_new_context_with_model: freq_scale    = 1
0.00.051.440 I ggml_metal_init: allocating
0.00.051.446 I ggml_metal_init: found device: Apple M4
0.00.051.449 I ggml_metal_init: picking default device: Apple M4
0.00.052.004 I ggml_metal_init: using embedded metal library
0.00.054.338 I ggml_metal_init: GPU name:   Apple M4
0.00.054.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.340 I ggml_metal_init: simdgroup reduction   = true
0.00.054.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.340 I ggml_metal_init: has bfloat            = true
0.00.054.340 I ggml_metal_init: use bfloat            = true
0.00.054.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.085 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.588 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.593 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.673 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.674 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.675 I llama_new_context_with_model: graph nodes  = 967
0.00.084.675 I llama_new_context_with_model: graph splits = 2
0.00.084.691 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.629 I main: llama threadpool init, n_threads = 4
0.00.707.669 I 
0.00.707.716 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.717 I 
0.00.707.943 I sampler seed: 1234
0.00.707.949 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.964 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.964 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.964 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.347 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.557.348 I llama_perf_context_print:        load time =     699.02 ms
0.01.557.349 I llama_perf_context_print: prompt eval time =      51.57 ms /     7 tokens (    7.37 ms per token,   135.74 tokens per second)
0.01.557.349 I llama_perf_context_print:        eval time =     794.72 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.557.350 I llama_perf_context_print:       total time =     849.72 ms /    70 tokens
0.01.557.540 I ggml_metal_free: deallocating

real	0m1.575s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.303 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.669 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.669 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.670 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.671 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.672 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.672 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.672 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.675 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.678 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.678 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.389 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.390 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.390 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.391 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.391 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.392 I llama_model_loader: - type  f32:  194 tensors
0.00.024.392 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.788 I llm_load_vocab: special tokens cache size = 25
0.00.050.725 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.728 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.728 I llm_load_print_meta: arch             = gptneox
0.00.050.728 I llm_load_print_meta: vocab type       = BPE
0.00.050.729 I llm_load_print_meta: n_vocab          = 50304
0.00.050.729 I llm_load_print_meta: n_merges         = 50009
0.00.050.729 I llm_load_print_meta: vocab_only       = 0
0.00.050.729 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.729 I llm_load_print_meta: n_embd           = 2048
0.00.050.729 I llm_load_print_meta: n_layer          = 24
0.00.050.733 I llm_load_print_meta: n_head           = 16
0.00.050.734 I llm_load_print_meta: n_head_kv        = 16
0.00.050.734 I llm_load_print_meta: n_rot            = 32
0.00.050.734 I llm_load_print_meta: n_swa            = 0
0.00.050.735 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.735 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.736 I llm_load_print_meta: n_gqa            = 1
0.00.050.736 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.737 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.738 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.738 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.738 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.738 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.741 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.742 I llm_load_print_meta: n_ff             = 8192
0.00.050.742 I llm_load_print_meta: n_expert         = 0
0.00.050.742 I llm_load_print_meta: n_expert_used    = 0
0.00.050.742 I llm_load_print_meta: causal attn      = 1
0.00.050.744 I llm_load_print_meta: pooling type     = 0
0.00.050.744 I llm_load_print_meta: rope type        = 2
0.00.050.744 I llm_load_print_meta: rope scaling     = linear
0.00.050.745 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.745 I llm_load_print_meta: freq_scale_train = 1
0.00.050.745 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.745 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.745 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.746 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.746 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.746 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.747 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.751 I llm_load_print_meta: model type       = 1.4B
0.00.050.751 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.751 I llm_load_print_meta: model params     = 1.41 B
0.00.050.752 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.752 I llm_load_print_meta: general.name     = 1.4B
0.00.050.752 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.752 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.753 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.753 I llm_load_print_meta: max token length = 1024
0.00.052.816 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.816 I llm_load_tensors: offloading output layer to GPU
0.00.052.817 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.827 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.828 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.737 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.738 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.738 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.738 I llama_new_context_with_model: n_batch       = 2048
0.00.053.739 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.739 I llama_new_context_with_model: flash_attn    = 0
0.00.053.739 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.740 I llama_new_context_with_model: freq_scale    = 1
0.00.053.740 I ggml_metal_init: allocating
0.00.053.743 I ggml_metal_init: found device: Apple M4
0.00.053.745 I ggml_metal_init: picking default device: Apple M4
0.00.054.341 I ggml_metal_init: using embedded metal library
0.00.056.655 I ggml_metal_init: GPU name:   Apple M4
0.00.056.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.657 I ggml_metal_init: simdgroup reduction   = true
0.00.056.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.659 I ggml_metal_init: has bfloat            = true
0.00.056.659 I ggml_metal_init: use bfloat            = true
0.00.056.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.416 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.025 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.032 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.053 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.118 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.120 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.120 I llama_new_context_with_model: graph nodes  = 967
0.00.087.120 I llama_new_context_with_model: graph splits = 2
0.00.087.136 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.353 I main: llama threadpool init, n_threads = 4
0.00.768.391 I 
0.00.768.427 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.427 I 
0.00.768.665 I sampler seed: 1234
0.00.768.669 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.722 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.723 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.723 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.649.470 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.649.471 I llama_perf_context_print:        load time =     759.04 ms
0.01.649.471 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.56 tokens per second)
0.01.649.472 I llama_perf_context_print:        eval time =     823.31 ms /    63 runs   (   13.07 ms per token,    76.52 tokens per second)
0.01.649.472 I llama_perf_context_print:       total time =     881.12 ms /    70 tokens
0.01.649.688 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.109s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.568 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.773 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.521 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.533 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.534 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.534 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.534 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.535 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.535 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.538 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.538 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.728 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.392 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.396 I llama_model_loader: - type  f32:  194 tensors
0.00.052.397 I llama_model_loader: - type  f16:   98 tensors
0.00.082.047 I llm_load_vocab: special tokens cache size = 25
0.00.088.857 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.859 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.860 I llm_load_print_meta: arch             = gptneox
0.00.088.860 I llm_load_print_meta: vocab type       = BPE
0.00.088.860 I llm_load_print_meta: n_vocab          = 50304
0.00.088.860 I llm_load_print_meta: n_merges         = 50009
0.00.088.860 I llm_load_print_meta: vocab_only       = 0
0.00.088.861 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.861 I llm_load_print_meta: n_embd           = 2048
0.00.088.861 I llm_load_print_meta: n_layer          = 24
0.00.088.864 I llm_load_print_meta: n_head           = 16
0.00.088.864 I llm_load_print_meta: n_head_kv        = 16
0.00.088.865 I llm_load_print_meta: n_rot            = 32
0.00.088.865 I llm_load_print_meta: n_swa            = 0
0.00.088.865 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.865 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.866 I llm_load_print_meta: n_gqa            = 1
0.00.088.867 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.868 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.868 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.872 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.873 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.873 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.873 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.874 I llm_load_print_meta: n_ff             = 8192
0.00.088.874 I llm_load_print_meta: n_expert         = 0
0.00.088.874 I llm_load_print_meta: n_expert_used    = 0
0.00.088.875 I llm_load_print_meta: causal attn      = 1
0.00.088.877 I llm_load_print_meta: pooling type     = 0
0.00.088.877 I llm_load_print_meta: rope type        = 2
0.00.088.877 I llm_load_print_meta: rope scaling     = linear
0.00.088.877 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.878 I llm_load_print_meta: freq_scale_train = 1
0.00.088.878 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.878 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.878 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.878 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.878 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.879 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.879 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.879 I llm_load_print_meta: model type       = 1.4B
0.00.088.879 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.880 I llm_load_print_meta: model params     = 1.41 B
0.00.088.880 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.881 I llm_load_print_meta: general.name     = 1.4B
0.00.088.882 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.882 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.882 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.882 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.882 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.088.882 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.883 I llm_load_print_meta: max token length = 1024
0.00.091.419 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.420 I llm_load_tensors: offloading output layer to GPU
0.00.091.420 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.430 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.431 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.379 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.380 I llama_new_context_with_model: n_ctx         = 128
0.00.092.381 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.381 I llama_new_context_with_model: n_batch       = 128
0.00.092.381 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.381 I llama_new_context_with_model: flash_attn    = 0
0.00.092.382 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.382 I llama_new_context_with_model: freq_scale    = 1
0.00.092.382 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.383 I ggml_metal_init: allocating
0.00.092.393 I ggml_metal_init: found device: Apple M4
0.00.092.397 I ggml_metal_init: picking default device: Apple M4
0.00.093.023 I ggml_metal_init: using embedded metal library
0.00.095.584 I ggml_metal_init: GPU name:   Apple M4
0.00.095.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.587 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.589 I ggml_metal_init: simdgroup reduction   = true
0.00.095.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.589 I ggml_metal_init: has bfloat            = true
0.00.095.589 I ggml_metal_init: use bfloat            = true
0.00.095.589 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.481 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.748 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.750 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.764 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.583 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.584 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.585 I llama_new_context_with_model: graph nodes  = 967
0.00.107.585 I llama_new_context_with_model: graph splits = 2
0.00.107.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.882.979 I 
0.00.883.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.883.082 I perplexity: tokenizing the input ..
0.00.897.288 I perplexity: tokenization took 14.2 ms
0.00.897.297 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.017.975 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.019.308 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.019.321 I llama_perf_context_print:        load time =     860.19 ms
0.01.019.322 I llama_perf_context_print: prompt eval time =     119.70 ms /   128 tokens (    0.94 ms per token,  1069.32 tokens per second)
0.01.019.322 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.019.323 I llama_perf_context_print:       total time =     136.35 ms /   129 tokens
0.01.019.714 I ggml_metal_free: deallocating

real	0m1.213s
user	0m0.119s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.195 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.158 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.217 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.264 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.215 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.217 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.218 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.220 I llama_model_loader: - type  f32:  194 tensors
0.00.024.220 I llama_model_loader: - type q8_0:   98 tensors
0.00.045.044 I llm_load_vocab: special tokens cache size = 25
0.00.051.199 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.203 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.203 I llm_load_print_meta: arch             = gptneox
0.00.051.204 I llm_load_print_meta: vocab type       = BPE
0.00.051.204 I llm_load_print_meta: n_vocab          = 50304
0.00.051.204 I llm_load_print_meta: n_merges         = 50009
0.00.051.204 I llm_load_print_meta: vocab_only       = 0
0.00.051.205 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.205 I llm_load_print_meta: n_embd           = 2048
0.00.051.205 I llm_load_print_meta: n_layer          = 24
0.00.051.210 I llm_load_print_meta: n_head           = 16
0.00.051.211 I llm_load_print_meta: n_head_kv        = 16
0.00.051.211 I llm_load_print_meta: n_rot            = 32
0.00.051.211 I llm_load_print_meta: n_swa            = 0
0.00.051.211 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.211 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.212 I llm_load_print_meta: n_gqa            = 1
0.00.051.213 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.214 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.214 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.215 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.215 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.215 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.215 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.216 I llm_load_print_meta: n_ff             = 8192
0.00.051.216 I llm_load_print_meta: n_expert         = 0
0.00.051.216 I llm_load_print_meta: n_expert_used    = 0
0.00.051.216 I llm_load_print_meta: causal attn      = 1
0.00.051.216 I llm_load_print_meta: pooling type     = 0
0.00.051.216 I llm_load_print_meta: rope type        = 2
0.00.051.216 I llm_load_print_meta: rope scaling     = linear
0.00.051.217 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.217 I llm_load_print_meta: freq_scale_train = 1
0.00.051.217 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.219 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.219 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.219 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.220 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.220 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.220 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.220 I llm_load_print_meta: model type       = 1.4B
0.00.051.221 I llm_load_print_meta: model ftype      = Q8_0
0.00.051.221 I llm_load_print_meta: model params     = 1.41 B
0.00.051.221 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.051.221 I llm_load_print_meta: general.name     = 1.4B
0.00.051.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.223 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.225 I llm_load_print_meta: max token length = 1024
0.00.053.324 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.324 I llm_load_tensors: offloading output layer to GPU
0.00.053.325 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.335 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.053.336 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.054.212 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.213 I llama_new_context_with_model: n_ctx         = 128
0.00.054.213 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.214 I llama_new_context_with_model: n_batch       = 128
0.00.054.214 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.214 I llama_new_context_with_model: flash_attn    = 0
0.00.054.215 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.215 I llama_new_context_with_model: freq_scale    = 1
0.00.054.215 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.216 I ggml_metal_init: allocating
0.00.054.223 I ggml_metal_init: found device: Apple M4
0.00.054.225 I ggml_metal_init: picking default device: Apple M4
0.00.054.839 I ggml_metal_init: using embedded metal library
0.00.057.189 I ggml_metal_init: GPU name:   Apple M4
0.00.057.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.192 I ggml_metal_init: simdgroup reduction   = true
0.00.057.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.192 I ggml_metal_init: has bfloat            = true
0.00.057.192 I ggml_metal_init: use bfloat            = true
0.00.057.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.194 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.761 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.766 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.780 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.792 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.793 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.793 I llama_new_context_with_model: graph nodes  = 967
0.00.069.794 I llama_new_context_with_model: graph splits = 2
0.00.069.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.908 I 
0.00.861.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.862.013 I perplexity: tokenizing the input ..
0.00.869.685 I perplexity: tokenization took 7.67 ms
0.00.869.690 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.993.416 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.995.103 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.995.119 I llama_perf_context_print:        load time =     852.71 ms
0.00.995.120 I llama_perf_context_print: prompt eval time =     123.46 ms /   128 tokens (    0.96 ms per token,  1036.76 tokens per second)
0.00.995.121 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.995.121 I llama_perf_context_print:       total time =     133.22 ms /   129 tokens
0.00.995.497 I ggml_metal_free: deallocating

real	0m1.013s
user	0m0.082s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.750 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.735 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.738 I llama_model_loader: - type  f32:  194 tensors
0.00.024.739 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.120 I llm_load_vocab: special tokens cache size = 25
0.00.052.235 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.238 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.239 I llm_load_print_meta: arch             = gptneox
0.00.052.239 I llm_load_print_meta: vocab type       = BPE
0.00.052.239 I llm_load_print_meta: n_vocab          = 50304
0.00.052.240 I llm_load_print_meta: n_merges         = 50009
0.00.052.240 I llm_load_print_meta: vocab_only       = 0
0.00.052.240 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.240 I llm_load_print_meta: n_embd           = 2048
0.00.052.240 I llm_load_print_meta: n_layer          = 24
0.00.052.244 I llm_load_print_meta: n_head           = 16
0.00.052.245 I llm_load_print_meta: n_head_kv        = 16
0.00.052.245 I llm_load_print_meta: n_rot            = 32
0.00.052.245 I llm_load_print_meta: n_swa            = 0
0.00.052.245 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.248 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.249 I llm_load_print_meta: n_gqa            = 1
0.00.052.250 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.251 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.251 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.251 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.252 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.252 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.252 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.253 I llm_load_print_meta: n_ff             = 8192
0.00.052.254 I llm_load_print_meta: n_expert         = 0
0.00.052.254 I llm_load_print_meta: n_expert_used    = 0
0.00.052.254 I llm_load_print_meta: causal attn      = 1
0.00.052.254 I llm_load_print_meta: pooling type     = 0
0.00.052.254 I llm_load_print_meta: rope type        = 2
0.00.052.254 I llm_load_print_meta: rope scaling     = linear
0.00.052.256 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.257 I llm_load_print_meta: freq_scale_train = 1
0.00.052.257 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.257 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.257 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.257 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.259 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.259 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.259 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.259 I llm_load_print_meta: model type       = 1.4B
0.00.052.260 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.265 I llm_load_print_meta: model params     = 1.41 B
0.00.052.265 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.266 I llm_load_print_meta: general.name     = 1.4B
0.00.052.266 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.266 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.266 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.266 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.267 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.267 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.267 I llm_load_print_meta: max token length = 1024
0.00.054.285 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.286 I llm_load_tensors: offloading output layer to GPU
0.00.054.286 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.297 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.298 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.191 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.192 I llama_new_context_with_model: n_ctx         = 128
0.00.055.192 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.192 I llama_new_context_with_model: n_batch       = 128
0.00.055.192 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.193 I llama_new_context_with_model: flash_attn    = 0
0.00.055.193 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.193 I llama_new_context_with_model: freq_scale    = 1
0.00.055.194 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.194 I ggml_metal_init: allocating
0.00.055.203 I ggml_metal_init: found device: Apple M4
0.00.055.205 I ggml_metal_init: picking default device: Apple M4
0.00.055.824 I ggml_metal_init: using embedded metal library
0.00.058.211 I ggml_metal_init: GPU name:   Apple M4
0.00.058.212 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.213 I ggml_metal_init: simdgroup reduction   = true
0.00.058.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.214 I ggml_metal_init: has bfloat            = true
0.00.058.214 I ggml_metal_init: use bfloat            = true
0.00.058.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.679 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.023 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.026 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.040 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.866 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.867 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.868 I llama_new_context_with_model: graph nodes  = 967
0.00.070.868 I llama_new_context_with_model: graph splits = 2
0.00.070.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.245 I 
0.00.606.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.303 I perplexity: tokenizing the input ..
0.00.614.319 I perplexity: tokenization took 8.015 ms
0.00.614.322 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.736.983 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.738.235 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.738.263 I llama_perf_context_print:        load time =     596.49 ms
0.00.738.264 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.44 tokens per second)
0.00.738.265 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.738.265 I llama_perf_context_print:       total time =     132.02 ms /   129 tokens
0.00.738.714 I ggml_metal_free: deallocating

real	0m0.754s
user	0m0.080s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.968 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.905 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.905 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.906 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.906 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.906 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.907 I llama_model_loader: - type  f32:  194 tensors
0.00.025.907 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.908 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.161 I llm_load_vocab: special tokens cache size = 25
0.00.053.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.153 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.154 I llm_load_print_meta: arch             = gptneox
0.00.053.154 I llm_load_print_meta: vocab type       = BPE
0.00.053.154 I llm_load_print_meta: n_vocab          = 50304
0.00.053.155 I llm_load_print_meta: n_merges         = 50009
0.00.053.155 I llm_load_print_meta: vocab_only       = 0
0.00.053.155 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.155 I llm_load_print_meta: n_embd           = 2048
0.00.053.155 I llm_load_print_meta: n_layer          = 24
0.00.053.158 I llm_load_print_meta: n_head           = 16
0.00.053.159 I llm_load_print_meta: n_head_kv        = 16
0.00.053.159 I llm_load_print_meta: n_rot            = 32
0.00.053.159 I llm_load_print_meta: n_swa            = 0
0.00.053.162 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.162 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.163 I llm_load_print_meta: n_gqa            = 1
0.00.053.164 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.164 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.165 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.165 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.165 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.166 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.166 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.166 I llm_load_print_meta: n_ff             = 8192
0.00.053.167 I llm_load_print_meta: n_expert         = 0
0.00.053.167 I llm_load_print_meta: n_expert_used    = 0
0.00.053.167 I llm_load_print_meta: causal attn      = 1
0.00.053.167 I llm_load_print_meta: pooling type     = 0
0.00.053.167 I llm_load_print_meta: rope type        = 2
0.00.053.169 I llm_load_print_meta: rope scaling     = linear
0.00.053.169 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.169 I llm_load_print_meta: freq_scale_train = 1
0.00.053.169 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.170 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.170 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.170 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.170 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.170 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.170 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.170 I llm_load_print_meta: model type       = 1.4B
0.00.053.171 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.171 I llm_load_print_meta: model params     = 1.41 B
0.00.053.172 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.172 I llm_load_print_meta: general.name     = 1.4B
0.00.053.172 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.173 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.173 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.173 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.173 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.174 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.174 I llm_load_print_meta: max token length = 1024
0.00.055.222 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.223 I llm_load_tensors: offloading output layer to GPU
0.00.055.223 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.234 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.235 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.056.154 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.155 I llama_new_context_with_model: n_ctx         = 128
0.00.056.155 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.155 I llama_new_context_with_model: n_batch       = 128
0.00.056.155 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.155 I llama_new_context_with_model: flash_attn    = 0
0.00.056.156 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.156 I llama_new_context_with_model: freq_scale    = 1
0.00.056.156 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.157 I ggml_metal_init: allocating
0.00.056.163 I ggml_metal_init: found device: Apple M4
0.00.056.165 I ggml_metal_init: picking default device: Apple M4
0.00.056.745 I ggml_metal_init: using embedded metal library
0.00.059.115 I ggml_metal_init: GPU name:   Apple M4
0.00.059.117 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.117 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.117 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.118 I ggml_metal_init: simdgroup reduction   = true
0.00.059.118 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.118 I ggml_metal_init: has bfloat            = true
0.00.059.118 I ggml_metal_init: use bfloat            = true
0.00.059.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.973 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.259 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.263 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.278 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.116 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.117 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.118 I llama_new_context_with_model: graph nodes  = 967
0.00.071.118 I llama_new_context_with_model: graph splits = 2
0.00.071.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.409 I 
0.00.746.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.466 I perplexity: tokenizing the input ..
0.00.754.618 I perplexity: tokenization took 8.148 ms
0.00.754.621 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.185 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.878.373 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.878.390 I llama_perf_context_print:        load time =     737.43 ms
0.00.878.391 I llama_perf_context_print: prompt eval time =     122.33 ms /   128 tokens (    0.96 ms per token,  1046.33 tokens per second)
0.00.878.391 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.393 I llama_perf_context_print:       total time =     131.98 ms /   129 tokens
0.00.878.805 I ggml_metal_free: deallocating

real	0m0.893s
user	0m0.079s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.356 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.626 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.626 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.629 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.630 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.631 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.631 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.633 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.606 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.912 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.914 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.914 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.915 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.915 I llama_model_loader: - type  f32:  194 tensors
0.00.039.916 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.916 I llama_model_loader: - type q6_K:    1 tensors
0.00.074.465 I llm_load_vocab: special tokens cache size = 25
0.00.084.842 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.845 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.846 I llm_load_print_meta: arch             = gptneox
0.00.084.846 I llm_load_print_meta: vocab type       = BPE
0.00.084.846 I llm_load_print_meta: n_vocab          = 50304
0.00.084.847 I llm_load_print_meta: n_merges         = 50009
0.00.084.847 I llm_load_print_meta: vocab_only       = 0
0.00.084.847 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.847 I llm_load_print_meta: n_embd           = 2048
0.00.084.848 I llm_load_print_meta: n_layer          = 24
0.00.084.851 I llm_load_print_meta: n_head           = 16
0.00.084.852 I llm_load_print_meta: n_head_kv        = 16
0.00.084.852 I llm_load_print_meta: n_rot            = 32
0.00.084.852 I llm_load_print_meta: n_swa            = 0
0.00.084.853 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.853 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.854 I llm_load_print_meta: n_gqa            = 1
0.00.084.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.857 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.857 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.858 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.858 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.858 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.859 I llm_load_print_meta: n_ff             = 8192
0.00.084.859 I llm_load_print_meta: n_expert         = 0
0.00.084.859 I llm_load_print_meta: n_expert_used    = 0
0.00.084.860 I llm_load_print_meta: causal attn      = 1
0.00.084.860 I llm_load_print_meta: pooling type     = 0
0.00.084.860 I llm_load_print_meta: rope type        = 2
0.00.084.860 I llm_load_print_meta: rope scaling     = linear
0.00.084.861 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.861 I llm_load_print_meta: freq_scale_train = 1
0.00.084.861 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.862 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.862 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.862 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.862 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.863 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.863 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.863 I llm_load_print_meta: model type       = 1.4B
0.00.084.864 I llm_load_print_meta: model ftype      = Q5_0
0.00.084.864 I llm_load_print_meta: model params     = 1.41 B
0.00.084.865 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.084.865 I llm_load_print_meta: general.name     = 1.4B
0.00.084.866 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.866 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.869 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.869 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.869 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.869 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.870 I llm_load_print_meta: max token length = 1024
0.00.087.182 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.183 I llm_load_tensors: offloading output layer to GPU
0.00.087.184 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.194 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.087.194 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.088.446 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.447 I llama_new_context_with_model: n_ctx         = 128
0.00.088.448 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.448 I llama_new_context_with_model: n_batch       = 128
0.00.088.448 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.448 I llama_new_context_with_model: flash_attn    = 0
0.00.088.449 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.449 I llama_new_context_with_model: freq_scale    = 1
0.00.088.450 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.451 I ggml_metal_init: allocating
0.00.088.458 I ggml_metal_init: found device: Apple M4
0.00.088.461 I ggml_metal_init: picking default device: Apple M4
0.00.089.254 I ggml_metal_init: using embedded metal library
0.00.092.799 I ggml_metal_init: GPU name:   Apple M4
0.00.092.801 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.802 I ggml_metal_init: simdgroup reduction   = true
0.00.092.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.803 I ggml_metal_init: has bfloat            = true
0.00.092.803 I ggml_metal_init: use bfloat            = true
0.00.092.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.100 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.105.582 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.105.585 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.105.602 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.602 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.604 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.604 I llama_new_context_with_model: graph nodes  = 967
0.00.106.604 I llama_new_context_with_model: graph splits = 2
0.00.106.617 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.539 I 
0.00.727.669 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.702 I perplexity: tokenizing the input ..
0.00.746.517 I perplexity: tokenization took 18.812 ms
0.00.746.525 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.898.066 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.899.297 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.899.319 I llama_perf_context_print:        load time =     714.17 ms
0.00.899.320 I llama_perf_context_print: prompt eval time =     150.69 ms /   128 tokens (    1.18 ms per token,   849.43 tokens per second)
0.00.899.321 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.899.321 I llama_perf_context_print:       total time =     171.79 ms /   129 tokens
0.00.900.090 I ggml_metal_free: deallocating

real	0m0.931s
user	0m0.117s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.168 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.023.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.495 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.496 I llama_model_loader: - type  f32:  194 tensors
0.00.032.496 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.496 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.112 I llm_load_vocab: special tokens cache size = 25
0.00.059.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.201 I llm_load_print_meta: arch             = gptneox
0.00.059.201 I llm_load_print_meta: vocab type       = BPE
0.00.059.202 I llm_load_print_meta: n_vocab          = 50304
0.00.059.202 I llm_load_print_meta: n_merges         = 50009
0.00.059.202 I llm_load_print_meta: vocab_only       = 0
0.00.059.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.202 I llm_load_print_meta: n_embd           = 2048
0.00.059.202 I llm_load_print_meta: n_layer          = 24
0.00.059.205 I llm_load_print_meta: n_head           = 16
0.00.059.206 I llm_load_print_meta: n_head_kv        = 16
0.00.059.206 I llm_load_print_meta: n_rot            = 32
0.00.059.206 I llm_load_print_meta: n_swa            = 0
0.00.059.207 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.207 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.207 I llm_load_print_meta: n_gqa            = 1
0.00.059.208 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.209 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.210 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.210 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.210 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.210 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.212 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.214 I llm_load_print_meta: n_ff             = 8192
0.00.059.214 I llm_load_print_meta: n_expert         = 0
0.00.059.214 I llm_load_print_meta: n_expert_used    = 0
0.00.059.215 I llm_load_print_meta: causal attn      = 1
0.00.059.215 I llm_load_print_meta: pooling type     = 0
0.00.059.215 I llm_load_print_meta: rope type        = 2
0.00.059.215 I llm_load_print_meta: rope scaling     = linear
0.00.059.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.216 I llm_load_print_meta: freq_scale_train = 1
0.00.059.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.217 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.217 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.217 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.217 I llm_load_print_meta: model type       = 1.4B
0.00.059.217 I llm_load_print_meta: model ftype      = Q5_1
0.00.059.218 I llm_load_print_meta: model params     = 1.41 B
0.00.059.223 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.059.223 I llm_load_print_meta: general.name     = 1.4B
0.00.059.223 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.223 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.224 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.224 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.225 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.226 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.226 I llm_load_print_meta: max token length = 1024
0.00.061.189 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.189 I llm_load_tensors: offloading output layer to GPU
0.00.061.190 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.200 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.061.201 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.062.108 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.109 I llama_new_context_with_model: n_ctx         = 128
0.00.062.109 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.110 I llama_new_context_with_model: n_batch       = 128
0.00.062.110 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.110 I llama_new_context_with_model: flash_attn    = 0
0.00.062.110 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.110 I llama_new_context_with_model: freq_scale    = 1
0.00.062.111 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.111 I ggml_metal_init: allocating
0.00.062.114 I ggml_metal_init: found device: Apple M4
0.00.062.116 I ggml_metal_init: picking default device: Apple M4
0.00.062.698 I ggml_metal_init: using embedded metal library
0.00.065.019 I ggml_metal_init: GPU name:   Apple M4
0.00.065.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.022 I ggml_metal_init: simdgroup reduction   = true
0.00.065.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.022 I ggml_metal_init: has bfloat            = true
0.00.065.022 I ggml_metal_init: use bfloat            = true
0.00.065.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.689 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.075.969 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.972 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.988 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.897 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.898 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.899 I llama_new_context_with_model: graph nodes  = 967
0.00.076.899 I llama_new_context_with_model: graph splits = 2
0.00.076.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.076.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.862.774 I 
0.00.862.813 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.862.825 I perplexity: tokenizing the input ..
0.00.870.912 I perplexity: tokenization took 8.086 ms
0.00.870.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.005.997 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.01.007.168 I Final estimate: PPL = 10.1971 +/- 3.18866

0.01.007.185 I llama_perf_context_print:        load time =     846.60 ms
0.01.007.186 I llama_perf_context_print: prompt eval time =     134.85 ms /   128 tokens (    1.05 ms per token,   949.17 tokens per second)
0.01.007.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.007.187 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.01.007.509 I ggml_metal_free: deallocating

real	0m1.021s
user	0m0.079s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.933 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.503 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.022.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.511 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.512 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.513 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.513 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.513 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.516 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.521 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.262 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.264 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.031.265 I llama_model_loader: - type  f32:  194 tensors
0.00.031.265 I llama_model_loader: - type q2_K:   49 tensors
0.00.031.265 I llama_model_loader: - type q3_K:   48 tensors
0.00.031.266 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.753 I llm_load_vocab: special tokens cache size = 25
0.00.059.861 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.864 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.865 I llm_load_print_meta: arch             = gptneox
0.00.059.865 I llm_load_print_meta: vocab type       = BPE
0.00.059.865 I llm_load_print_meta: n_vocab          = 50304
0.00.059.865 I llm_load_print_meta: n_merges         = 50009
0.00.059.866 I llm_load_print_meta: vocab_only       = 0
0.00.059.866 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.866 I llm_load_print_meta: n_embd           = 2048
0.00.059.866 I llm_load_print_meta: n_layer          = 24
0.00.059.869 I llm_load_print_meta: n_head           = 16
0.00.059.870 I llm_load_print_meta: n_head_kv        = 16
0.00.059.870 I llm_load_print_meta: n_rot            = 32
0.00.059.870 I llm_load_print_meta: n_swa            = 0
0.00.059.870 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.870 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.871 I llm_load_print_meta: n_gqa            = 1
0.00.059.872 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.875 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.875 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.876 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.876 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.876 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.876 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.877 I llm_load_print_meta: n_ff             = 8192
0.00.059.877 I llm_load_print_meta: n_expert         = 0
0.00.059.880 I llm_load_print_meta: n_expert_used    = 0
0.00.059.880 I llm_load_print_meta: causal attn      = 1
0.00.059.880 I llm_load_print_meta: pooling type     = 0
0.00.059.880 I llm_load_print_meta: rope type        = 2
0.00.059.881 I llm_load_print_meta: rope scaling     = linear
0.00.059.881 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.881 I llm_load_print_meta: freq_scale_train = 1
0.00.059.881 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.882 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.882 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.882 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.882 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.886 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.886 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.887 I llm_load_print_meta: model type       = 1.4B
0.00.059.888 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.059.888 I llm_load_print_meta: model params     = 1.41 B
0.00.059.889 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.059.889 I llm_load_print_meta: general.name     = 1.4B
0.00.059.890 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.890 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.890 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.890 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.891 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.891 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.891 I llm_load_print_meta: max token length = 1024
0.00.061.804 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.804 I llm_load_tensors: offloading output layer to GPU
0.00.061.805 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.815 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.061.816 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.062.686 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.687 I llama_new_context_with_model: n_ctx         = 128
0.00.062.687 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.687 I llama_new_context_with_model: n_batch       = 128
0.00.062.687 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.687 I llama_new_context_with_model: flash_attn    = 0
0.00.062.688 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.688 I llama_new_context_with_model: freq_scale    = 1
0.00.062.688 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.689 I ggml_metal_init: allocating
0.00.062.694 I ggml_metal_init: found device: Apple M4
0.00.062.696 I ggml_metal_init: picking default device: Apple M4
0.00.063.263 I ggml_metal_init: using embedded metal library
0.00.065.582 I ggml_metal_init: GPU name:   Apple M4
0.00.065.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.584 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.585 I ggml_metal_init: simdgroup reduction   = true
0.00.065.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.585 I ggml_metal_init: has bfloat            = true
0.00.065.585 I ggml_metal_init: use bfloat            = true
0.00.065.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.973 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.076.197 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.202 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.215 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.074 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.075 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.075 I llama_new_context_with_model: graph nodes  = 967
0.00.077.076 I llama_new_context_with_model: graph splits = 2
0.00.077.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.077.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.604 I 
0.00.538.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.659 I perplexity: tokenizing the input ..
0.00.546.724 I perplexity: tokenization took 8.062 ms
0.00.546.727 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.679.414 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.680.748 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.680.787 I llama_perf_context_print:        load time =     523.66 ms
0.00.680.789 I llama_perf_context_print: prompt eval time =     132.46 ms /   128 tokens (    1.03 ms per token,   966.37 tokens per second)
0.00.680.789 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.680.791 I llama_perf_context_print:       total time =     142.18 ms /   129 tokens
0.00.681.378 I ggml_metal_free: deallocating

real	0m0.697s
user	0m0.080s
sys	0m0.080s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.696 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.303 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.304 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.305 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.307 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.308 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.195 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.277 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.200 I llama_model_loader: - type  f32:  194 tensors
0.00.026.200 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.201 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.201 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.201 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.364 I llm_load_vocab: special tokens cache size = 25
0.00.053.309 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.311 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.312 I llm_load_print_meta: arch             = gptneox
0.00.053.312 I llm_load_print_meta: vocab type       = BPE
0.00.053.312 I llm_load_print_meta: n_vocab          = 50304
0.00.053.312 I llm_load_print_meta: n_merges         = 50009
0.00.053.313 I llm_load_print_meta: vocab_only       = 0
0.00.053.313 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.313 I llm_load_print_meta: n_embd           = 2048
0.00.053.313 I llm_load_print_meta: n_layer          = 24
0.00.053.316 I llm_load_print_meta: n_head           = 16
0.00.053.316 I llm_load_print_meta: n_head_kv        = 16
0.00.053.317 I llm_load_print_meta: n_rot            = 32
0.00.053.317 I llm_load_print_meta: n_swa            = 0
0.00.053.317 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.317 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.319 I llm_load_print_meta: n_gqa            = 1
0.00.053.320 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.321 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.321 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.321 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.322 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.322 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.322 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.323 I llm_load_print_meta: n_ff             = 8192
0.00.053.323 I llm_load_print_meta: n_expert         = 0
0.00.053.323 I llm_load_print_meta: n_expert_used    = 0
0.00.053.323 I llm_load_print_meta: causal attn      = 1
0.00.053.323 I llm_load_print_meta: pooling type     = 0
0.00.053.325 I llm_load_print_meta: rope type        = 2
0.00.053.326 I llm_load_print_meta: rope scaling     = linear
0.00.053.327 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.327 I llm_load_print_meta: freq_scale_train = 1
0.00.053.327 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.327 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.327 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.328 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.328 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.328 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.328 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.328 I llm_load_print_meta: model type       = 1.4B
0.00.053.328 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.333 I llm_load_print_meta: model params     = 1.41 B
0.00.053.334 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.334 I llm_load_print_meta: general.name     = 1.4B
0.00.053.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.335 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.335 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.336 I llm_load_print_meta: max token length = 1024
0.00.055.339 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.340 I llm_load_tensors: offloading output layer to GPU
0.00.055.340 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.351 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.352 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.290 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.291 I llama_new_context_with_model: n_ctx         = 128
0.00.056.291 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.291 I llama_new_context_with_model: n_batch       = 128
0.00.056.291 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.291 I llama_new_context_with_model: flash_attn    = 0
0.00.056.292 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.292 I llama_new_context_with_model: freq_scale    = 1
0.00.056.292 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.293 I ggml_metal_init: allocating
0.00.056.296 I ggml_metal_init: found device: Apple M4
0.00.056.298 I ggml_metal_init: picking default device: Apple M4
0.00.056.879 I ggml_metal_init: using embedded metal library
0.00.059.246 I ggml_metal_init: GPU name:   Apple M4
0.00.059.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.250 I ggml_metal_init: simdgroup reduction   = true
0.00.059.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.250 I ggml_metal_init: has bfloat            = true
0.00.059.250 I ggml_metal_init: use bfloat            = true
0.00.059.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.139 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.600 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.602 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.623 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.516 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.517 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.517 I llama_new_context_with_model: graph nodes  = 967
0.00.071.517 I llama_new_context_with_model: graph splits = 2
0.00.071.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.555.279 I 
0.00.555.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.555.343 I perplexity: tokenizing the input ..
0.00.563.257 I perplexity: tokenization took 7.913 ms
0.00.563.260 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.695.178 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.696.381 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.696.399 I llama_perf_context_print:        load time =     545.58 ms
0.00.696.400 I llama_perf_context_print: prompt eval time =     131.68 ms /   128 tokens (    1.03 ms per token,   972.03 tokens per second)
0.00.696.401 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.696.401 I llama_perf_context_print:       total time =     141.12 ms /   129 tokens
0.00.696.905 I ggml_metal_free: deallocating

real	0m0.710s
user	0m0.079s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.734 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.885 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.886 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.888 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.888 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.888 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.575 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.576 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.576 I llama_model_loader: - type  f32:  194 tensors
0.00.024.577 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.577 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.577 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.985 I llm_load_vocab: special tokens cache size = 25
0.00.050.863 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.865 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.866 I llm_load_print_meta: arch             = gptneox
0.00.050.866 I llm_load_print_meta: vocab type       = BPE
0.00.050.866 I llm_load_print_meta: n_vocab          = 50304
0.00.050.866 I llm_load_print_meta: n_merges         = 50009
0.00.050.867 I llm_load_print_meta: vocab_only       = 0
0.00.050.867 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.867 I llm_load_print_meta: n_embd           = 2048
0.00.050.867 I llm_load_print_meta: n_layer          = 24
0.00.050.870 I llm_load_print_meta: n_head           = 16
0.00.050.871 I llm_load_print_meta: n_head_kv        = 16
0.00.050.871 I llm_load_print_meta: n_rot            = 32
0.00.050.871 I llm_load_print_meta: n_swa            = 0
0.00.050.871 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.871 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.872 I llm_load_print_meta: n_gqa            = 1
0.00.050.873 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.874 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.874 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.875 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.875 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.875 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.875 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.876 I llm_load_print_meta: n_ff             = 8192
0.00.050.876 I llm_load_print_meta: n_expert         = 0
0.00.050.876 I llm_load_print_meta: n_expert_used    = 0
0.00.050.876 I llm_load_print_meta: causal attn      = 1
0.00.050.876 I llm_load_print_meta: pooling type     = 0
0.00.050.877 I llm_load_print_meta: rope type        = 2
0.00.050.877 I llm_load_print_meta: rope scaling     = linear
0.00.050.877 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.878 I llm_load_print_meta: freq_scale_train = 1
0.00.050.878 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.878 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.880 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.881 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.881 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.881 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.881 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.881 I llm_load_print_meta: model type       = 1.4B
0.00.050.882 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.882 I llm_load_print_meta: model params     = 1.41 B
0.00.050.883 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.883 I llm_load_print_meta: general.name     = 1.4B
0.00.050.883 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.884 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.888 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.888 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.889 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.890 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: max token length = 1024
0.00.052.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.827 I llm_load_tensors: offloading output layer to GPU
0.00.052.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.837 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.838 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.782 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.783 I llama_new_context_with_model: n_ctx         = 128
0.00.053.783 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.783 I llama_new_context_with_model: n_batch       = 128
0.00.053.783 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.783 I llama_new_context_with_model: flash_attn    = 0
0.00.053.784 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.784 I llama_new_context_with_model: freq_scale    = 1
0.00.053.784 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.785 I ggml_metal_init: allocating
0.00.053.788 I ggml_metal_init: found device: Apple M4
0.00.053.790 I ggml_metal_init: picking default device: Apple M4
0.00.054.346 I ggml_metal_init: using embedded metal library
0.00.056.658 I ggml_metal_init: GPU name:   Apple M4
0.00.056.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.660 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.660 I ggml_metal_init: simdgroup reduction   = true
0.00.056.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.660 I ggml_metal_init: has bfloat            = true
0.00.056.660 I ggml_metal_init: use bfloat            = true
0.00.056.661 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.325 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.557 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.559 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.572 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.468 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.469 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.469 I llama_new_context_with_model: graph nodes  = 967
0.00.068.469 I llama_new_context_with_model: graph splits = 2
0.00.068.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.987 I 
0.00.571.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.030 I perplexity: tokenizing the input ..
0.00.579.244 I perplexity: tokenization took 8.213 ms
0.00.579.248 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.713.809 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.714.993 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.715.003 I llama_perf_context_print:        load time =     561.25 ms
0.00.715.004 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.85 tokens per second)
0.00.715.004 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.006 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.715.326 I ggml_metal_free: deallocating

real	0m0.731s
user	0m0.078s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.977 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.459 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.138 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.139 I llama_model_loader: - type  f32:  194 tensors
0.00.024.139 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.139 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.395 I llm_load_vocab: special tokens cache size = 25
0.00.050.305 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.307 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.308 I llm_load_print_meta: arch             = gptneox
0.00.050.308 I llm_load_print_meta: vocab type       = BPE
0.00.050.308 I llm_load_print_meta: n_vocab          = 50304
0.00.050.308 I llm_load_print_meta: n_merges         = 50009
0.00.050.308 I llm_load_print_meta: vocab_only       = 0
0.00.050.309 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.309 I llm_load_print_meta: n_embd           = 2048
0.00.050.309 I llm_load_print_meta: n_layer          = 24
0.00.050.312 I llm_load_print_meta: n_head           = 16
0.00.050.313 I llm_load_print_meta: n_head_kv        = 16
0.00.050.313 I llm_load_print_meta: n_rot            = 32
0.00.050.313 I llm_load_print_meta: n_swa            = 0
0.00.050.313 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.314 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.314 I llm_load_print_meta: n_gqa            = 1
0.00.050.315 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.316 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.316 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.319 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.319 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.319 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.321 I llm_load_print_meta: n_ff             = 8192
0.00.050.321 I llm_load_print_meta: n_expert         = 0
0.00.050.322 I llm_load_print_meta: n_expert_used    = 0
0.00.050.322 I llm_load_print_meta: causal attn      = 1
0.00.050.322 I llm_load_print_meta: pooling type     = 0
0.00.050.322 I llm_load_print_meta: rope type        = 2
0.00.050.322 I llm_load_print_meta: rope scaling     = linear
0.00.050.323 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.323 I llm_load_print_meta: freq_scale_train = 1
0.00.050.323 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.324 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.324 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.324 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.326 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.326 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.326 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.326 I llm_load_print_meta: model type       = 1.4B
0.00.050.326 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.327 I llm_load_print_meta: model params     = 1.41 B
0.00.050.327 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.328 I llm_load_print_meta: general.name     = 1.4B
0.00.050.328 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.328 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.328 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.328 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.329 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.332 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.332 I llm_load_print_meta: max token length = 1024
0.00.052.339 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.340 I llm_load_tensors: offloading output layer to GPU
0.00.052.340 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.350 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.351 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.220 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.221 I llama_new_context_with_model: n_ctx         = 128
0.00.053.221 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.222 I llama_new_context_with_model: n_batch       = 128
0.00.053.222 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.222 I llama_new_context_with_model: flash_attn    = 0
0.00.053.222 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.223 I llama_new_context_with_model: freq_scale    = 1
0.00.053.223 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.223 I ggml_metal_init: allocating
0.00.053.229 I ggml_metal_init: found device: Apple M4
0.00.053.231 I ggml_metal_init: picking default device: Apple M4
0.00.053.814 I ggml_metal_init: using embedded metal library
0.00.056.130 I ggml_metal_init: GPU name:   Apple M4
0.00.056.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.132 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.133 I ggml_metal_init: simdgroup reduction   = true
0.00.056.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.133 I ggml_metal_init: has bfloat            = true
0.00.056.133 I ggml_metal_init: use bfloat            = true
0.00.056.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.924 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.161 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.169 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.187 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.057 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.058 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.058 I llama_new_context_with_model: graph nodes  = 967
0.00.068.059 I llama_new_context_with_model: graph splits = 2
0.00.068.071 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.072 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.876 I 
0.00.646.904 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.918 I perplexity: tokenizing the input ..
0.00.654.720 I perplexity: tokenization took 7.8 ms
0.00.654.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.998 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.248 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.274 I llama_perf_context_print:        load time =     636.89 ms
0.00.797.275 I llama_perf_context_print: prompt eval time =     141.04 ms /   128 tokens (    1.10 ms per token,   907.52 tokens per second)
0.00.797.276 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.276 I llama_perf_context_print:       total time =     150.40 ms /   129 tokens
0.00.797.735 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.233 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.024 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.027 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.031 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.031 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.958 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.958 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.959 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.959 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.959 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.960 I llama_model_loader: - type  f32:  194 tensors
0.00.025.960 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.125 I llm_load_vocab: special tokens cache size = 25
0.00.053.130 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.132 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.133 I llm_load_print_meta: arch             = gptneox
0.00.053.133 I llm_load_print_meta: vocab type       = BPE
0.00.053.133 I llm_load_print_meta: n_vocab          = 50304
0.00.053.134 I llm_load_print_meta: n_merges         = 50009
0.00.053.134 I llm_load_print_meta: vocab_only       = 0
0.00.053.134 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.134 I llm_load_print_meta: n_embd           = 2048
0.00.053.134 I llm_load_print_meta: n_layer          = 24
0.00.053.137 I llm_load_print_meta: n_head           = 16
0.00.053.137 I llm_load_print_meta: n_head_kv        = 16
0.00.053.138 I llm_load_print_meta: n_rot            = 32
0.00.053.138 I llm_load_print_meta: n_swa            = 0
0.00.053.138 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.138 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.139 I llm_load_print_meta: n_gqa            = 1
0.00.053.140 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.141 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.141 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.141 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.142 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.142 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.142 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.143 I llm_load_print_meta: n_ff             = 8192
0.00.053.143 I llm_load_print_meta: n_expert         = 0
0.00.053.143 I llm_load_print_meta: n_expert_used    = 0
0.00.053.143 I llm_load_print_meta: causal attn      = 1
0.00.053.143 I llm_load_print_meta: pooling type     = 0
0.00.053.143 I llm_load_print_meta: rope type        = 2
0.00.053.145 I llm_load_print_meta: rope scaling     = linear
0.00.053.146 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.146 I llm_load_print_meta: freq_scale_train = 1
0.00.053.146 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.147 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.147 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.147 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.147 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.149 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.149 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.149 I llm_load_print_meta: model type       = 1.4B
0.00.053.149 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.150 I llm_load_print_meta: model params     = 1.41 B
0.00.053.150 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.152 I llm_load_print_meta: general.name     = 1.4B
0.00.053.152 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.152 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.152 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.152 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.153 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.153 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.153 I llm_load_print_meta: max token length = 1024
0.00.055.240 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.241 I llm_load_tensors: offloading output layer to GPU
0.00.055.241 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.251 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.252 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.204 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.205 I llama_new_context_with_model: n_ctx         = 128
0.00.056.206 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.206 I llama_new_context_with_model: n_batch       = 128
0.00.056.206 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.206 I llama_new_context_with_model: flash_attn    = 0
0.00.056.207 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.207 I llama_new_context_with_model: freq_scale    = 1
0.00.056.207 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.208 I ggml_metal_init: allocating
0.00.056.211 I ggml_metal_init: found device: Apple M4
0.00.056.213 I ggml_metal_init: picking default device: Apple M4
0.00.056.773 I ggml_metal_init: using embedded metal library
0.00.059.113 I ggml_metal_init: GPU name:   Apple M4
0.00.059.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.115 I ggml_metal_init: simdgroup reduction   = true
0.00.059.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.115 I ggml_metal_init: has bfloat            = true
0.00.059.116 I ggml_metal_init: use bfloat            = true
0.00.059.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.074 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.070.364 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.368 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.382 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.311 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.312 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.313 I llama_new_context_with_model: graph nodes  = 967
0.00.071.313 I llama_new_context_with_model: graph splits = 2
0.00.071.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.242.564 I 
0.00.242.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.242.610 I perplexity: tokenizing the input ..
0.00.250.398 I perplexity: tokenization took 7.786 ms
0.00.250.405 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.389.916 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.391.334 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.391.348 I llama_perf_context_print:        load time =     231.33 ms
0.00.391.349 I llama_perf_context_print: prompt eval time =     139.24 ms /   128 tokens (    1.09 ms per token,   919.27 tokens per second)
0.00.391.350 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.391.350 I llama_perf_context_print:       total time =     148.78 ms /   129 tokens
0.00.391.651 I ggml_metal_free: deallocating

real	0m0.406s
user	0m0.079s
sys	0m0.050s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.225 I build: 4382 (265a5eac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.257 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.830 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.839 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.841 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.842 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.842 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.843 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.843 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.844 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.845 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.845 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.846 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.846 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.847 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.850 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.850 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.851 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.093 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.907 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.908 I llama_model_loader: - type  f32:  194 tensors
0.00.053.908 I llama_model_loader: - type  f16:   98 tensors
0.00.083.421 I llm_load_vocab: special tokens cache size = 25
0.00.090.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.127 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.127 I llm_load_print_meta: arch             = gptneox
0.00.090.128 I llm_load_print_meta: vocab type       = BPE
0.00.090.128 I llm_load_print_meta: n_vocab          = 50304
0.00.090.128 I llm_load_print_meta: n_merges         = 50009
0.00.090.128 I llm_load_print_meta: vocab_only       = 0
0.00.090.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.128 I llm_load_print_meta: n_embd           = 2048
0.00.090.128 I llm_load_print_meta: n_layer          = 24
0.00.090.131 I llm_load_print_meta: n_head           = 16
0.00.090.132 I llm_load_print_meta: n_head_kv        = 16
0.00.090.132 I llm_load_print_meta: n_rot            = 32
0.00.090.132 I llm_load_print_meta: n_swa            = 0
0.00.090.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.136 I llm_load_print_meta: n_gqa            = 1
0.00.090.136 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.137 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.138 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.138 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.138 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.139 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.139 I llm_load_print_meta: n_ff             = 8192
0.00.090.139 I llm_load_print_meta: n_expert         = 0
0.00.090.140 I llm_load_print_meta: n_expert_used    = 0
0.00.090.140 I llm_load_print_meta: causal attn      = 1
0.00.090.140 I llm_load_print_meta: pooling type     = 0
0.00.090.140 I llm_load_print_meta: rope type        = 2
0.00.090.147 I llm_load_print_meta: rope scaling     = linear
0.00.090.152 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.152 I llm_load_print_meta: freq_scale_train = 1
0.00.090.154 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.155 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.155 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.155 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.155 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.155 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.155 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.156 I llm_load_print_meta: model type       = 1.4B
0.00.090.156 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.157 I llm_load_print_meta: model params     = 1.41 B
0.00.090.157 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.157 I llm_load_print_meta: general.name     = 1.4B
0.00.090.158 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.158 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.158 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.160 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.161 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.161 I llm_load_print_meta: max token length = 1024
0.00.092.201 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.201 I llm_load_tensors: offloading output layer to GPU
0.00.092.202 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.207 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.207 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.198 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.199 I llama_new_context_with_model: n_ctx         = 128
0.00.093.199 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.199 I llama_new_context_with_model: n_batch       = 128
0.00.093.199 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.200 I llama_new_context_with_model: flash_attn    = 0
0.00.093.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.200 I llama_new_context_with_model: freq_scale    = 1
0.00.093.201 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.201 I ggml_metal_init: allocating
0.00.093.207 I ggml_metal_init: found device: Apple M4
0.00.093.209 I ggml_metal_init: picking default device: Apple M4
0.00.093.857 I ggml_metal_init: using embedded metal library
0.00.096.433 I ggml_metal_init: GPU name:   Apple M4
0.00.096.435 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.436 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.436 I ggml_metal_init: simdgroup reduction   = true
0.00.096.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.436 I ggml_metal_init: has bfloat            = true
0.00.096.436 I ggml_metal_init: use bfloat            = true
0.00.096.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.512 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.790 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.795 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.809 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.744 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.745 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.745 I llama_new_context_with_model: graph nodes  = 967
0.00.107.745 I llama_new_context_with_model: graph splits = 2
0.00.107.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.760 I 
0.00.107.792 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.107.793 I compute_imatrix: tokenizing the input ..
0.00.115.554 I compute_imatrix: tokenization took 7.76 ms
0.00.115.557 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.590.389 I compute_imatrix: 1.47 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.593.218 I llama_perf_context_print:        load time =    1567.13 ms
0.01.593.218 I llama_perf_context_print: prompt eval time =    1474.07 ms /   128 tokens (   11.52 ms per token,    86.83 tokens per second)
0.01.593.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.593.220 I llama_perf_context_print:       total time =    1569.95 ms /   129 tokens
0.01.593.742 I ggml_metal_free: deallocating

real	0m1.783s
user	0m0.167s
sys	0m0.240s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4382 (265a5eac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e30b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e30bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e30c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e30c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e30cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e30d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e30d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e30dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e30e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e30e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e30ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e30f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e30fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e3104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e310cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e3113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e311af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e312210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e312930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e313100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e313820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e313f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e314660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e314f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e315620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e3158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e315ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e316b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e3170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e317360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e317800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e317ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e318350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e318890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e318b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e318ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e319490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e319930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e319dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e31a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e31a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e31abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e31b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e31b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e31b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e31bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e31c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e31ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e31d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e31d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e31df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e31e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e31eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e31f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e31f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e31fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e320280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e320540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e320b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e321340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e321600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e321aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e321f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e3223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e322880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e322d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e3231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e323660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e323b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e323fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e324440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e3248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e324d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e3252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e325820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e325d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e3262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e326810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e326d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e3272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e327800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e327d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e3282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e3287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e328d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e329290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e3297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e329d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e32a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e32a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e32ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e32b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e32b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e32bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e32c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e32c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e32cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e31c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e32d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e32d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e32de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e32e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e32e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e32ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e32f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e32f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e32fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e3303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e3308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e330e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e331390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e3318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e331e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e3322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e332770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e3330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e333550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e3339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e333e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e334330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e3347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e334c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e335110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e3355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e335a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e335ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e336390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e336830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e336cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e337170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e337610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e337ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e337f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e3383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e338890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e338d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e3391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e339670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e339b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e339fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e33a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e33a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e33ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e33b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e33b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e33bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e33c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e33c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e33c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e33cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e33d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e33d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e33dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e33e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e33e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e33e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e33ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e33f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e33f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e33fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e3400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e340570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e340a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e340eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e341350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e3417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e341c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e342130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e3425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e342a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e342f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e3433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e343850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e343cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e344190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e344630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e344ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e344f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e345410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e3458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e345d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e3461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e346690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e346b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e346fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e347470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e347910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e347db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e348250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e3486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e348b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e349030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e349580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e349ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e34a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e34a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e34a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e34ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e34b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e34ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e34c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e34c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e34c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e34cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e34d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e34ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e34e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e34e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e34eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e34f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e34f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e34fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e350340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e350890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e350de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e351330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e351880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e351dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e352320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e352870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e352dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e353310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e353860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e353db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e354300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e354850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e354da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e3552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e355840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e355d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e3562e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e356830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e356d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e3572d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e357820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e357d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e3582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e358810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e358d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e3592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e359800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e359d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e35a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e35a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e35ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e35b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e35b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e35bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e35c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e35c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e35cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e35d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e35d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e35dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e35e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e35e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e35ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e35f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e35f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e35fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e360240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e360790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e360ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e361230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e361780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e361cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e362170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e362610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e362ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e362f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e3633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e363890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e363d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e3641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e364670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e364b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e364fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e365450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e3658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e365d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e366230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e366780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e366ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e3675c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e367ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e368400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e3686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e368eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e369170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e369780 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.134.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.134.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d7056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d7075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d7093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d709c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d70a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d70aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d70b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d70b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d70bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d70c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d70cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d70d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d70dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d70e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d70e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d70e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d70ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d70f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d70f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d70fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d70ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d7106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d7118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d712190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d712a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d713350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d7137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d7140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d7156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d717300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d717be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d718050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d7184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d718da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d719680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d719f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d71a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d71a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d71acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d71b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d71b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d71ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d71be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d71c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d71c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d71cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d71d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d71d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d71d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d71dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d71e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d71e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d71ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d71ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d71f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d71f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d71fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d720100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d7209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d720e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d7212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d721730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d7228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d7231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d723640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d723f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d724c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d7250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d7259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d7262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d726b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d726ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d727460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d7278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d727d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d7281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d728620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d728a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d728f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d729370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d7297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d72a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d72a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d72a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d72ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d72b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d72b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d72bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d72bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d72c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d72c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d72cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d72d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d72d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d72da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d72dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d72e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d72e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d72ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d72f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d72f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d72f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d72fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d7306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d730b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d730fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d731890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d731d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d732170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d7325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d732a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d732ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d733330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d7337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d733c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d734080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d7344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d734960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d735240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d7356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d735b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d736400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d736870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d736ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d737150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d7375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d737a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d737ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d738310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d738780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d738bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d739060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d7394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d739940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c304230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c3046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c304b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c304f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c3053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c305860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c305cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c306140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c3065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c306a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c306e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c307300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c307770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c307be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c308050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c3084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c308930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c308da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c309210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c309680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c309af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c309f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c30a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c30a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c30acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c30b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c30b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c30ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c30c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c30c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c30cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c30cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c30d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c30d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c30dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c30e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c30e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c30ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c30ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c30f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c30f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c30fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c310040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c3104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c310920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c310d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c311200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c311670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c311ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c311f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c3123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c312830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c312ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c313110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c313580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c3139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c313e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c3142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c314740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c314bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c315020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c315490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c315900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c315d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c3161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c316650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c316ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c316f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c3173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c317810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c317c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c3180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c318560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c3189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c318e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c3192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c319720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c319b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c31a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c31a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c31a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c31ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c31b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c31b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c31baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c31bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c31c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c31c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c31cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c31d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c31d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c31d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c31de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c31e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c31e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c31eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c31efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c31f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c31f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c31fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c3201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c320c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c321330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c321a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c322170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c322430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c3228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c322ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c3234b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d704ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d705150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d7055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d705a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d705ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d706310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d706780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d706bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d707060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d7074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d707940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d707f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d708810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d708f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d709770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d709e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d70a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d70ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d70b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d70bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d70c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d70ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d70d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d70d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d70df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d70e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d70e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d70ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d70f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d70f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d70fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d70fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d7102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d7105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d710a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d710e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d7112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d711760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d711bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d712040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d7124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d712920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d712d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d713200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d713670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d713f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d7143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d714830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d714ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d715110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d715580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d7159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d715e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d7162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d716bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d717020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d717490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d717900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d717d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d7181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d718650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d718ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d7193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d719810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d719c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d71a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d71a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d71a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d71ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d71b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10d71b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10d71bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10d71c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10d71c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10d71c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10d71cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10d71d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10d71d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10d71daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10d71df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10d71e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10d71e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10d71ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10d71f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10d71f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10d71f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10d71fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10d720290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10d720700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10d720b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10d720fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10d721450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10d7218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10d721d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10d7221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10d722610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10d722a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10d722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10d723360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10d7237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10d723c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10d7240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10d724520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10d724990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10d724e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10d725270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10d7256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10d725b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10d725fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10d726430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d7268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d726d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d727180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d7275f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d727a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d727ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d728340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d7287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d728c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d729090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d729500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d729970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d729de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d72a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d72a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d72ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d72afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d72b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d72b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d72bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d72c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d72c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d72ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d72ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d72d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d72d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d72dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d72e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d72e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d72e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d72edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d72f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d72f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d72fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d72ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d7303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d730860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d730cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d731140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d7315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d731a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d732300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d732770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d732be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d733050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d7334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d733930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d733da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d734210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d734680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d734af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d734f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d7353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d735840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d735cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d736120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d736590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d736a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d736e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d7372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d737750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d737bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d738030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d7384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d738910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d738d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d7391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d739660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d739ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d73a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d73aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d73af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d73b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d73b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d73be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d73c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d73c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d73cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d73d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d73da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d73dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d73e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d73e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d73edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d73f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d73f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10d7401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10d740680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d740940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10d741560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d741d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d7421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d742b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d7432e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d743830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d743d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d7442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d744820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d7452c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d745810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d745d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d7462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d746800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d746d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d7472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d7477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d747d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d748290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d7487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d748d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d749280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d7497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d749d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d74a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d74a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d74ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d74b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d74b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d74bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d74c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d74c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d74ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d74d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d74d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d74dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d74e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d74e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d74ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d74f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d74f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d74fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d750210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d750760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d750cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d751200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d751750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d751ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d7521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d752740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d7531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d753730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d753c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d7541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d754720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d754c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d7551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d755710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d755c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10d756100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10d7565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d756a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d757380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d757820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d757cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d758160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d758600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d758aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d7593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d759880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d759d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d75a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d75a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d75ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d75b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d75bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d75c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d75c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10d75ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d75d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d75d710 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.809s
user	0m0.290s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4382 (265a5eac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141e0f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141e0f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141e0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141e102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141e10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141e10e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141e113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141e11960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141e11f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141e12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141e12910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141e12e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141e13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141e140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141e148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141e15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141e15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141e15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141e16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141e16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141e17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141e17b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141e182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141e18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141e19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141e19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141e19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141e1a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141e1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141e1afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141e1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141e1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141e1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141e1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141e1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141e1cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141e1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141e1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141e1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141e1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141e1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141e1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141e1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141e1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141e1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141e1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141e20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141e20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141e20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141e21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141e21b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141e22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141e22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141e22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141e23580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141e23a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141e23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141e24180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141e24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141e24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141e25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141e256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141e25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141e26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141e264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141e26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141e26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141e272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141e27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141e27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141e28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141e28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141e289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141e28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141e29460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141e299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141e29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141e2a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141e2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141e2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141e2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141e2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141e2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141e2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141e2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141e2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141e2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141e2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141e2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141e2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141e2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141e2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141e2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141e2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141e303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141e30940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141e20620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141e30db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141e31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141e31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141e32000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141e32550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141e32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141e32ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141e33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141e33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141e33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141e34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141e34a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141e34fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141e35520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141e35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141e35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141e363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141e36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141e36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141e37190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141e37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141e37ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141e37f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141e388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141e38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141e391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141e39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141e39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141e39fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141e3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141e3a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141e3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141e3b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141e3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141e3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141e3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141e3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141e3c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141e3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141e3d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141e3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141e3dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141e3e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141e3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141e3e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141e3ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141e3f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141e3f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141e3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141e400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141e40590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141e40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141e40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141e41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141e41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141e41cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141e42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141e425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141e42a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141e42f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141e433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141e43870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141e43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141e441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141e44650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141e44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141e44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141e45430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141e458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141e45d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141e46210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141e466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141e46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141e46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141e47490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141e47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141e47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141e48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141e48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141e48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141e49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141e494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141e49990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141e49e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141e4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141e4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141e4ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141e4b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141e4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141e4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141e4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141e4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141e4c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141e4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141e4d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141e4d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141e4dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141e4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141e4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141e4ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141e4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141e4f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141e4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141e50330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141e505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141e50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141e51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141e51a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141e51ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141e52340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141e527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141e52f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141e534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141e53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141e53f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141e544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141e54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141e54f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141e554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141e55a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141e55f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141e564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141e56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141e56f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141e574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141e579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141e57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141e58490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141e589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141e58f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141e59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141e599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141e59f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141e5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141e5a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141e5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141e5b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141e5b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141e5bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141e5c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141e5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141e5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141e5d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141e5d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141e5dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141e5e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141e5e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141e5eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141e5f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141e5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141e5fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141e60410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141e60960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141e60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141e61400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141e61950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141e61ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141e623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141e62940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141e62e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141e633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141e63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141e643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141e64920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141e64e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141e653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141e65910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141e65db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141e66250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141e666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141e66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141e67030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141e674d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141e67970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141e67e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141e682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141e68750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141e68bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141e69090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141e69530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141e699d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141e69e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141e6a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141e6aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141e6b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141e6b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141e6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141e6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141e6caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141e6cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141e6d3c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.749 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141f05670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141f05ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141f05f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141f063c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141f06830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141f06ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141f07110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141f07580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141f079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141f07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141f082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141f094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141f0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141f0ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141f0b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141f0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141f0c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141f0c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141f0cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141f0d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141f0de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141f0e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141f0ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141f0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141f0f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141f0fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141f0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141f108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141f10d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141f11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141f11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141f118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141f11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141f121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141f12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141f12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141f12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141f13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141f137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141f13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141f140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141f14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141f149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141f14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141f15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141f15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141f15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141f15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141f16450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141f168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141f16d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141f171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141f17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141f17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141f18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141f184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141f18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141f18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141f19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141f196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141f19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141f19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141f1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141f1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141f1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141f1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141f1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141f1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141f1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141f1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141f1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141f1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141f1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141f1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141f1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141f1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141f1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141f1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141f1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141f1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141f1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141f1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141f1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141f20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141f205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141f20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141f20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141f212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141f21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141f21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141f22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141f224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141f22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141f22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141f23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141f23670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141f23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141f23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141f243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141f24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141f24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141f25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141f25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141f259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141f25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141f262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141f26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141f26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141f27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141f27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141f27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141f27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141f281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141f28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141f28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141f28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141f293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141f29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141f29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141f2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141f2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141f2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141f2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141f2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141f2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141f2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141f2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141f2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141f2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141f2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141f2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141f2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141f2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141f2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141f2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141f2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141f2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141f2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141f2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141f2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141f2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141f30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141f30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141f30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141f30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141f31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141f318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141f31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141f321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141f32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141f32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141f32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141f33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141f337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141f33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141f340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141f34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141f34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141f34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141f35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141f356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141f35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141f35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141f36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141f368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141f36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141f37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141f375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141f37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141f37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141f38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141f387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141f38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141f39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141f39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141f39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141f39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141f3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141f3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141f3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141f3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141f3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141f3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141f3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141f3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141f3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141f3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141f3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141f3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141f3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141f3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141f3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141f3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141f3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141f3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141f3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141f3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141f3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141f3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141f403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141f40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141f40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141f41140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141f416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141f41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141f41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141f42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141f42dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141f43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141f434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141f43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141f43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141f44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141f446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141f44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141f44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141f45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141f45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141f45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141f46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141f465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141f46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141f46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141f47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141f47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141f47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141f48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141f484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141f48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141f48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141f49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141f49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141f49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141f49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141f4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141f4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141f4acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141f4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141f4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141f4ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141f4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141f4c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141f4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141f4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141f4d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141f4d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141f4d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141f4dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141f4e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141f4e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141f4eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141f4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141f4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141f4f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141f4fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141f50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141f509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141f50e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141f512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141f51740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141f51bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141f52020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141f52900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141f52d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141f531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141f53650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141f53ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141f53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141f543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141f54810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141f54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141f550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141f55560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141f559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141f55e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141f562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141f56720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141f57190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141f578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141f57fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141f586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141f589b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141f58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141f59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141f59a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143805c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1438060c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143806530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1438069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143806e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143807280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1438076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143807b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143807fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143808440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1438088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143808f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143809a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14380a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14380a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14380b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14380b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14380bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14380c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14380ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14380d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14380dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14380e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14380eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14380f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14380f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14380f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14380fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143810040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1438104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143810920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143810e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1438112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143811580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1438119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143811e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1438122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143812740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143812bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143813020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143813490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143813900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143813d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1438141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143814650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143814ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143814f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1438153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143815810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143815c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1438160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143816560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1438169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143816e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1438172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143817720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143817c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143818190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143818600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143818a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143818ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143819350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1438197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143819c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14381a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14381a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14381a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14381adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14381b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14381b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14381bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14381bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14381c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14381c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14381cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14381d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14381d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14381da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14381dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14381e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14381e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14381ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14381f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14381f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14381f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14381fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143820240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1438206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143820b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143820f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143821400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143821870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143821ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143822150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1438225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143822a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143822ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143823310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143823780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143823bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143824060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1438244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143824940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143824db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143825220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143825690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143825b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143825f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1438263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143826850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143826cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143827130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1438275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143827a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143827e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1438282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143828760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143828bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143829040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1438294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143829920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143829d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14382a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14382a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14382aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14382af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14382b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14382b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14382bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14382c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14382c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14382c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14382ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14382d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14382d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14382dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14382e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14382e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14382e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14382ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14382f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14382f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14382fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14382ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1438303a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143830810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143830c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1438310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143831560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1438319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143831e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1438322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143832720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143832b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143833000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143833470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1438338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143833d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1438341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143834630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143834aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143834f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143835380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1438357f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143835c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1438360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143836540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1438369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143836e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143837290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143837700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143837b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143837fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143838450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1438388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143838d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1438391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143839610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143839a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143839ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14383a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14383a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14383ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14383b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14383b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14383b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14383be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14383c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14383c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14383cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14383cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14383d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14383d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14383dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14383e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14383e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14383ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14383eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14383f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14383f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14383fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143840090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143840500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143840970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143840de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143841250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1438416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143841c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1438420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143842530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143843080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143843340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143843600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143843a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143843ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143844350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1438447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143844c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1438450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143845510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143845980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143845df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143846260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1438466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143846b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143846fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143847420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143847890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143847d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143848170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1438485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143848ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143849330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1438497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143849c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14384a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14384a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14384a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14384add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14384b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14384b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14384bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14384bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14384c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14384cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14384d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14384d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14384d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14384dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14384e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14384e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14384ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14384ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14384f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14384f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14384fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143850100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143850570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1438509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143850e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1438512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143851730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143852010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143852480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1438528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143852d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1438531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143853640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143853ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143853f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143854390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143854800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143854c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1438550e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143855550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1438559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143855e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1438562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143856710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143856b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143856ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143857a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143858180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1438588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143858fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143859280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1438596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143859cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14385a300 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.931s
user	0m0.243s
sys	0m0.147s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
