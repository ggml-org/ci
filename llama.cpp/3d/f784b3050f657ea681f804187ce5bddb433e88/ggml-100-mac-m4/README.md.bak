### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.38 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.00 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.26 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.16 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.19 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.19 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  179.29 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.89 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.59 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.32 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 221.31 sec*proc (27 tests)

Total Test time (real) = 221.32 sec

real	3m41.495s
user	7m42.461s
sys	0m6.122s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.22 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.39 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.01 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.75 sec*proc (27 tests)

Total Test time (real) =  50.76 sec

real	0m50.770s
user	1m11.007s
sys	0m5.638s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.115 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.739 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.753 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.768 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.770 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.770 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.771 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.773 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.773 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.774 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.774 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.775 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.781 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.782 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.783 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.783 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.784 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.785 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.785 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.344 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.804 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.808 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.809 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.809 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.810 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.033.810 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.811 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.033.811 I llama_model_loader: - type  f32:  124 tensors
0.00.033.812 I llama_model_loader: - type  f16:   73 tensors
0.00.038.982 I llm_load_vocab: special tokens cache size = 5
0.00.041.450 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.454 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.455 I llm_load_print_meta: arch             = bert
0.00.041.455 I llm_load_print_meta: vocab type       = WPM
0.00.041.456 I llm_load_print_meta: n_vocab          = 30522
0.00.041.456 I llm_load_print_meta: n_merges         = 0
0.00.041.456 I llm_load_print_meta: vocab_only       = 0
0.00.041.456 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.460 I llm_load_print_meta: n_embd           = 384
0.00.041.460 I llm_load_print_meta: n_layer          = 12
0.00.041.464 I llm_load_print_meta: n_head           = 12
0.00.041.465 I llm_load_print_meta: n_head_kv        = 12
0.00.041.486 I llm_load_print_meta: n_rot            = 32
0.00.041.487 I llm_load_print_meta: n_swa            = 0
0.00.041.487 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.487 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.488 I llm_load_print_meta: n_gqa            = 1
0.00.041.489 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.490 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.491 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.492 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.492 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.492 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.493 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.494 I llm_load_print_meta: n_ff             = 1536
0.00.041.494 I llm_load_print_meta: n_expert         = 0
0.00.041.494 I llm_load_print_meta: n_expert_used    = 0
0.00.041.494 I llm_load_print_meta: causal attn      = 0
0.00.041.495 I llm_load_print_meta: pooling type     = 2
0.00.041.495 I llm_load_print_meta: rope type        = 2
0.00.041.497 I llm_load_print_meta: rope scaling     = linear
0.00.041.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.498 I llm_load_print_meta: freq_scale_train = 1
0.00.041.498 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.499 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.499 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.499 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.499 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.499 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.500 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.506 I llm_load_print_meta: model type       = 33M
0.00.041.507 I llm_load_print_meta: model ftype      = F16
0.00.041.507 I llm_load_print_meta: model params     = 33.21 M
0.00.041.508 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.509 I llm_load_print_meta: general.name     = Bge Small
0.00.041.509 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.509 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.510 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.510 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.510 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.511 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.511 I llm_load_print_meta: max token length = 21
0.00.043.734 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.043.736 I llm_load_tensors: offloading output layer to GPU
0.00.043.737 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.043.759 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.761 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.416 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.418 I llama_new_context_with_model: n_ctx         = 512
0.00.044.418 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.419 I llama_new_context_with_model: n_batch       = 2048
0.00.044.419 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.419 I llama_new_context_with_model: flash_attn    = 0
0.00.044.420 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.420 I llama_new_context_with_model: freq_scale    = 1
0.00.044.421 I ggml_metal_init: allocating
0.00.044.429 I ggml_metal_init: found device: Apple M4
0.00.044.433 I ggml_metal_init: picking default device: Apple M4
0.00.045.423 I ggml_metal_init: using embedded metal library
0.00.050.257 I ggml_metal_init: GPU name:   Apple M4
0.00.050.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.050.261 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.050.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.050.262 I ggml_metal_init: simdgroup reduction   = true
0.00.050.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.050.262 I ggml_metal_init: has bfloat            = true
0.00.050.262 I ggml_metal_init: use bfloat            = true
0.00.050.263 I ggml_metal_init: hasUnifiedMemory      = true
0.00.050.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.521 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.064.524 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.064.526 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.065.379 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.065.380 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.065.381 I llama_new_context_with_model: graph nodes  = 429
0.00.065.381 I llama_new_context_with_model: graph splits = 2
0.00.065.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.071.759 I 
0.00.071.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.072.491 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.077.255 I llama_perf_context_print:        load time =      50.12 ms
0.00.077.256 I llama_perf_context_print: prompt eval time =       4.62 ms /     9 tokens (    0.51 ms per token,  1947.21 tokens per second)
0.00.077.257 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.077.257 I llama_perf_context_print:       total time =       5.50 ms /    10 tokens
0.00.077.412 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.054s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.493 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.729 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.734 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.735 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.735 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.736 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.737 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.737 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.737 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.738 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.738 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.740 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.740 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.743 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.743 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.744 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.744 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.744 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.183 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.184 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.185 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.185 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.185 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.186 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.186 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.187 I llama_model_loader: - type  f32:  124 tensors
0.00.015.187 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.804 I llm_load_vocab: special tokens cache size = 5
0.00.019.147 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.149 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.150 I llm_load_print_meta: arch             = bert
0.00.019.150 I llm_load_print_meta: vocab type       = WPM
0.00.019.150 I llm_load_print_meta: n_vocab          = 30522
0.00.019.150 I llm_load_print_meta: n_merges         = 0
0.00.019.151 I llm_load_print_meta: vocab_only       = 0
0.00.019.151 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.151 I llm_load_print_meta: n_embd           = 384
0.00.019.151 I llm_load_print_meta: n_layer          = 12
0.00.019.154 I llm_load_print_meta: n_head           = 12
0.00.019.154 I llm_load_print_meta: n_head_kv        = 12
0.00.019.161 I llm_load_print_meta: n_rot            = 32
0.00.019.161 I llm_load_print_meta: n_swa            = 0
0.00.019.162 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.162 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.162 I llm_load_print_meta: n_gqa            = 1
0.00.019.163 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.164 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.164 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.164 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.165 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.165 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.165 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.165 I llm_load_print_meta: n_ff             = 1536
0.00.019.166 I llm_load_print_meta: n_expert         = 0
0.00.019.166 I llm_load_print_meta: n_expert_used    = 0
0.00.019.166 I llm_load_print_meta: causal attn      = 0
0.00.019.166 I llm_load_print_meta: pooling type     = 2
0.00.019.166 I llm_load_print_meta: rope type        = 2
0.00.019.166 I llm_load_print_meta: rope scaling     = linear
0.00.019.167 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.167 I llm_load_print_meta: freq_scale_train = 1
0.00.019.167 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.167 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.168 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.168 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.168 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.168 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.168 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.173 I llm_load_print_meta: model type       = 33M
0.00.019.173 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.173 I llm_load_print_meta: model params     = 33.21 M
0.00.019.174 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.174 I llm_load_print_meta: general.name     = Bge Small
0.00.019.174 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.174 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.175 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.175 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.175 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.175 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.175 I llm_load_print_meta: max token length = 21
0.00.020.472 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.473 I llm_load_tensors: offloading output layer to GPU
0.00.020.473 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.480 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.481 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.860 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.861 I llama_new_context_with_model: n_ctx         = 512
0.00.020.861 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.861 I llama_new_context_with_model: n_batch       = 2048
0.00.020.861 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.861 I llama_new_context_with_model: flash_attn    = 0
0.00.020.862 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.862 I llama_new_context_with_model: freq_scale    = 1
0.00.020.863 I ggml_metal_init: allocating
0.00.020.868 I ggml_metal_init: found device: Apple M4
0.00.020.871 I ggml_metal_init: picking default device: Apple M4
0.00.021.503 I ggml_metal_init: using embedded metal library
0.00.024.135 I ggml_metal_init: GPU name:   Apple M4
0.00.024.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.138 I ggml_metal_init: simdgroup reduction   = true
0.00.024.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.138 I ggml_metal_init: has bfloat            = true
0.00.024.138 I ggml_metal_init: use bfloat            = true
0.00.024.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.934 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.937 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.940 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.646 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.647 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.647 I llama_new_context_with_model: graph nodes  = 429
0.00.035.648 I llama_new_context_with_model: graph splits = 2
0.00.035.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.938 I 
0.00.040.962 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.481 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.957 I llama_perf_context_print:        load time =      31.44 ms
0.00.044.958 I llama_perf_context_print: prompt eval time =       3.35 ms /     9 tokens (    0.37 ms per token,  2687.37 tokens per second)
0.00.044.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.959 I llama_perf_context_print:       total time =       4.02 ms /    10 tokens
0.00.045.118 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.153 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.425 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.056 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.061 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.064 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.069 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.070 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.071 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.072 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.073 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.073 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.074 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.075 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.079 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.079 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.080 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.474 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.474 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.474 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.475 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.475 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.475 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.476 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.476 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.477 I llama_model_loader: - type  f32:   41 tensors
0.00.050.477 I llama_model_loader: - type  f16:   29 tensors
0.00.068.612 W llm_load_vocab: empty token at index 5
0.00.073.299 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.631 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.658 I llm_load_vocab: special tokens cache size = 5
0.00.335.593 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.335.601 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.335.602 I llm_load_print_meta: arch             = jina-bert-v2
0.00.335.603 I llm_load_print_meta: vocab type       = BPE
0.00.335.605 I llm_load_print_meta: n_vocab          = 61056
0.00.335.605 I llm_load_print_meta: n_merges         = 39382
0.00.335.605 I llm_load_print_meta: vocab_only       = 0
0.00.335.605 I llm_load_print_meta: n_ctx_train      = 8192
0.00.335.606 I llm_load_print_meta: n_embd           = 384
0.00.335.606 I llm_load_print_meta: n_layer          = 4
0.00.335.614 I llm_load_print_meta: n_head           = 12
0.00.335.615 I llm_load_print_meta: n_head_kv        = 12
0.00.335.645 I llm_load_print_meta: n_rot            = 32
0.00.335.645 I llm_load_print_meta: n_swa            = 0
0.00.335.646 I llm_load_print_meta: n_embd_head_k    = 32
0.00.335.646 I llm_load_print_meta: n_embd_head_v    = 32
0.00.335.646 I llm_load_print_meta: n_gqa            = 1
0.00.335.647 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.335.647 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.335.648 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.335.649 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.335.649 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.335.649 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.335.649 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.335.650 I llm_load_print_meta: n_ff             = 1536
0.00.335.650 I llm_load_print_meta: n_expert         = 0
0.00.335.650 I llm_load_print_meta: n_expert_used    = 0
0.00.335.651 I llm_load_print_meta: causal attn      = 0
0.00.335.651 I llm_load_print_meta: pooling type     = -1
0.00.335.651 I llm_load_print_meta: rope type        = -1
0.00.335.651 I llm_load_print_meta: rope scaling     = linear
0.00.335.652 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.335.652 I llm_load_print_meta: freq_scale_train = 1
0.00.335.652 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.335.652 I llm_load_print_meta: rope_finetuned   = unknown
0.00.335.653 I llm_load_print_meta: ssm_d_conv       = 0
0.00.335.653 I llm_load_print_meta: ssm_d_inner      = 0
0.00.335.653 I llm_load_print_meta: ssm_d_state      = 0
0.00.335.653 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.335.653 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.335.674 I llm_load_print_meta: model type       = 33M
0.00.335.674 I llm_load_print_meta: model ftype      = F16
0.00.335.675 I llm_load_print_meta: model params     = 32.90 M
0.00.335.675 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.335.675 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.335.675 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.335.676 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.335.676 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.335.676 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.335.676 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.335.676 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.335.677 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.335.677 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.335.677 I llm_load_print_meta: max token length = 45
0.00.336.799 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.336.799 I llm_load_tensors: offloading output layer to GPU
0.00.336.799 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.336.829 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.830 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.743 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.744 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.744 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.744 I llama_new_context_with_model: n_batch       = 2048
0.00.337.745 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.745 I llama_new_context_with_model: flash_attn    = 0
0.00.337.745 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.745 I llama_new_context_with_model: freq_scale    = 1
0.00.337.746 I ggml_metal_init: allocating
0.00.337.749 I ggml_metal_init: found device: Apple M4
0.00.337.751 I ggml_metal_init: picking default device: Apple M4
0.00.338.532 I ggml_metal_init: using embedded metal library
0.00.341.358 I ggml_metal_init: GPU name:   Apple M4
0.00.341.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.361 I ggml_metal_init: simdgroup reduction   = true
0.00.341.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.361 I ggml_metal_init: has bfloat            = true
0.00.341.362 I ggml_metal_init: use bfloat            = true
0.00.341.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.331 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.353.333 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.353.336 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.353.945 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.353.946 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.353.946 I llama_new_context_with_model: graph nodes  = 154
0.00.353.946 I llama_new_context_with_model: graph splits = 2
0.00.353.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.761 I 
0.00.366.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.951 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.366.952 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.366.961 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.366.961 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.366.965 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.366.965 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.507 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.083 I llama_perf_context_print:        load time =     343.33 ms
0.00.371.087 I llama_perf_context_print: prompt eval time =       3.57 ms /    62 tokens (    0.06 ms per token, 17376.68 tokens per second)
0.00.371.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.089 I llama_perf_context_print:       total time =       4.32 ms /    63 tokens
0.00.371.306 I ggml_metal_free: deallocating

real	0m1.059s
user	0m0.342s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.101 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.207 I main: llama backend init
0.00.000.214 I main: load the model and apply lora adapter, if any
0.00.047.948 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.059.241 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.059.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.059.277 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.059.277 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.059.278 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.059.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.059.280 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.059.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.059.283 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.059.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.059.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.059.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.059.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.059.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.059.295 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.059.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.059.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.041 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.078.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.045 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.045 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.046 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.047 I llama_model_loader: - type  f32:  194 tensors
0.00.078.047 I llama_model_loader: - type  f16:   98 tensors
0.00.111.698 I llm_load_vocab: special tokens cache size = 25
0.00.118.908 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.118.910 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.118.911 I llm_load_print_meta: arch             = gptneox
0.00.118.911 I llm_load_print_meta: vocab type       = BPE
0.00.118.911 I llm_load_print_meta: n_vocab          = 50304
0.00.118.911 I llm_load_print_meta: n_merges         = 50009
0.00.118.912 I llm_load_print_meta: vocab_only       = 0
0.00.118.912 I llm_load_print_meta: n_ctx_train      = 2048
0.00.118.912 I llm_load_print_meta: n_embd           = 2048
0.00.118.912 I llm_load_print_meta: n_layer          = 24
0.00.118.916 I llm_load_print_meta: n_head           = 16
0.00.118.917 I llm_load_print_meta: n_head_kv        = 16
0.00.118.938 I llm_load_print_meta: n_rot            = 32
0.00.118.938 I llm_load_print_meta: n_swa            = 0
0.00.118.939 I llm_load_print_meta: n_embd_head_k    = 128
0.00.118.939 I llm_load_print_meta: n_embd_head_v    = 128
0.00.118.939 I llm_load_print_meta: n_gqa            = 1
0.00.118.940 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.118.941 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.118.941 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.118.941 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.118.942 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.118.942 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.118.942 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.118.943 I llm_load_print_meta: n_ff             = 8192
0.00.118.943 I llm_load_print_meta: n_expert         = 0
0.00.118.944 I llm_load_print_meta: n_expert_used    = 0
0.00.118.947 I llm_load_print_meta: causal attn      = 1
0.00.118.948 I llm_load_print_meta: pooling type     = 0
0.00.118.948 I llm_load_print_meta: rope type        = 2
0.00.118.948 I llm_load_print_meta: rope scaling     = linear
0.00.118.948 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.118.949 I llm_load_print_meta: freq_scale_train = 1
0.00.118.949 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.118.949 I llm_load_print_meta: rope_finetuned   = unknown
0.00.118.949 I llm_load_print_meta: ssm_d_conv       = 0
0.00.118.949 I llm_load_print_meta: ssm_d_inner      = 0
0.00.118.949 I llm_load_print_meta: ssm_d_state      = 0
0.00.118.949 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.118.951 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.118.960 I llm_load_print_meta: model type       = 1.4B
0.00.118.961 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.118.961 I llm_load_print_meta: model params     = 1.41 B
0.00.118.962 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.118.962 I llm_load_print_meta: general.name     = 1.4B
0.00.118.962 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.118.962 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.118.963 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.118.963 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.118.963 I llm_load_print_meta: LF token         = 128 ''
0.00.118.963 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.118.963 I llm_load_print_meta: max token length = 1024
0.00.121.529 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.121.530 I llm_load_tensors: offloading output layer to GPU
0.00.121.530 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.121.548 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.121.549 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.122.556 I llama_new_context_with_model: n_seq_max     = 1
0.00.122.557 I llama_new_context_with_model: n_ctx         = 2048
0.00.122.557 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.122.557 I llama_new_context_with_model: n_batch       = 2048
0.00.122.557 I llama_new_context_with_model: n_ubatch      = 512
0.00.122.558 I llama_new_context_with_model: flash_attn    = 0
0.00.122.558 I llama_new_context_with_model: freq_base     = 10000.0
0.00.122.558 I llama_new_context_with_model: freq_scale    = 1
0.00.122.559 I ggml_metal_init: allocating
0.00.122.568 I ggml_metal_init: found device: Apple M4
0.00.122.571 I ggml_metal_init: picking default device: Apple M4
0.00.123.282 I ggml_metal_init: using embedded metal library
0.00.132.570 I ggml_metal_init: GPU name:   Apple M4
0.00.132.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.132.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.132.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.132.573 I ggml_metal_init: simdgroup reduction   = true
0.00.132.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.132.573 I ggml_metal_init: has bfloat            = true
0.00.132.573 I ggml_metal_init: use bfloat            = true
0.00.132.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.132.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.176.502 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.176.507 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.176.527 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.177.442 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.177.444 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.177.444 I llama_new_context_with_model: graph nodes  = 967
0.00.177.444 I llama_new_context_with_model: graph splits = 2
0.00.177.467 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.253.938 I main: llama threadpool init, n_threads = 4
0.00.253.971 I 
0.00.254.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.254.060 I 
0.00.254.146 I sampler seed: 1234
0.00.254.151 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.254.175 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.254.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.254.177 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.095.269 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.095.270 I llama_perf_context_print:        load time =     205.98 ms
0.02.095.271 I llama_perf_context_print: prompt eval time =      43.94 ms /     7 tokens (    6.28 ms per token,   159.30 tokens per second)
0.02.095.271 I llama_perf_context_print:        eval time =    1794.25 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.095.273 I llama_perf_context_print:       total time =    1841.33 ms /    70 tokens
0.02.095.477 I ggml_metal_free: deallocating

real	0m2.490s
user	0m0.147s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.804 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.062 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.916 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.950 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.285 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.263 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.266 I llama_model_loader: - type  f32:  194 tensors
0.00.062.266 I llama_model_loader: - type  f16:   98 tensors
0.00.094.864 I llm_load_vocab: special tokens cache size = 25
0.00.101.961 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.101.964 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.101.965 I llm_load_print_meta: arch             = gptneox
0.00.101.965 I llm_load_print_meta: vocab type       = BPE
0.00.101.965 I llm_load_print_meta: n_vocab          = 50304
0.00.101.965 I llm_load_print_meta: n_merges         = 50009
0.00.101.965 I llm_load_print_meta: vocab_only       = 0
0.00.101.966 I llm_load_print_meta: n_ctx_train      = 2048
0.00.101.966 I llm_load_print_meta: n_embd           = 2048
0.00.101.966 I llm_load_print_meta: n_layer          = 24
0.00.101.968 I llm_load_print_meta: n_head           = 16
0.00.101.969 I llm_load_print_meta: n_head_kv        = 16
0.00.101.982 I llm_load_print_meta: n_rot            = 32
0.00.101.982 I llm_load_print_meta: n_swa            = 0
0.00.101.982 I llm_load_print_meta: n_embd_head_k    = 128
0.00.101.982 I llm_load_print_meta: n_embd_head_v    = 128
0.00.101.983 I llm_load_print_meta: n_gqa            = 1
0.00.101.983 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.101.984 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.101.985 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.101.985 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.101.985 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.101.985 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.101.986 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.101.986 I llm_load_print_meta: n_ff             = 8192
0.00.101.986 I llm_load_print_meta: n_expert         = 0
0.00.101.988 I llm_load_print_meta: n_expert_used    = 0
0.00.101.990 I llm_load_print_meta: causal attn      = 1
0.00.101.990 I llm_load_print_meta: pooling type     = 0
0.00.101.990 I llm_load_print_meta: rope type        = 2
0.00.101.991 I llm_load_print_meta: rope scaling     = linear
0.00.101.991 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.101.991 I llm_load_print_meta: freq_scale_train = 1
0.00.101.992 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.101.992 I llm_load_print_meta: rope_finetuned   = unknown
0.00.101.992 I llm_load_print_meta: ssm_d_conv       = 0
0.00.101.992 I llm_load_print_meta: ssm_d_inner      = 0
0.00.101.992 I llm_load_print_meta: ssm_d_state      = 0
0.00.101.992 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.101.993 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.102.002 I llm_load_print_meta: model type       = 1.4B
0.00.102.003 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.102.003 I llm_load_print_meta: model params     = 1.41 B
0.00.102.004 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.102.004 I llm_load_print_meta: general.name     = 1.4B
0.00.102.005 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.102.006 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.102.006 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.102.006 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.102.006 I llm_load_print_meta: LF token         = 128 ''
0.00.102.007 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.102.008 I llm_load_print_meta: max token length = 1024
0.00.104.580 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.104.580 I llm_load_tensors: offloading output layer to GPU
0.00.104.580 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.104.591 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.592 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.105.556 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.557 I llama_new_context_with_model: n_ctx         = 128
0.00.105.557 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.105.557 I llama_new_context_with_model: n_batch       = 128
0.00.105.557 I llama_new_context_with_model: n_ubatch      = 128
0.00.105.557 I llama_new_context_with_model: flash_attn    = 0
0.00.105.558 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.558 I llama_new_context_with_model: freq_scale    = 1
0.00.105.559 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.105.559 I ggml_metal_init: allocating
0.00.105.565 I ggml_metal_init: found device: Apple M4
0.00.105.568 I ggml_metal_init: picking default device: Apple M4
0.00.106.190 I ggml_metal_init: using embedded metal library
0.00.108.875 I ggml_metal_init: GPU name:   Apple M4
0.00.108.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.108.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.108.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.108.879 I ggml_metal_init: simdgroup reduction   = true
0.00.108.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.108.879 I ggml_metal_init: has bfloat            = true
0.00.108.879 I ggml_metal_init: use bfloat            = true
0.00.108.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.108.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.120.405 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.120.409 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.120.423 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.316 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.121.317 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.121.317 I llama_new_context_with_model: graph nodes  = 967
0.00.121.317 I llama_new_context_with_model: graph splits = 2
0.00.121.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.000.439 I 
0.01.000.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.000.514 I perplexity: tokenizing the input ..
0.01.012.477 I perplexity: tokenization took 11.96 ms
0.01.012.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.132.673 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.134.538 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.134.563 I llama_perf_context_print:        load time =     968.36 ms
0.01.134.565 I llama_perf_context_print: prompt eval time =     119.79 ms /   128 tokens (    0.94 ms per token,  1068.51 tokens per second)
0.01.134.567 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.134.567 I llama_perf_context_print:       total time =     134.13 ms /   129 tokens
0.01.135.366 I ggml_metal_free: deallocating

real	0m1.324s
user	0m0.127s
sys	0m0.206s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.745 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.639 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.643 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.650 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.652 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.498 I llama_model_loader: - type  f32:  194 tensors
0.00.025.499 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.460 I llm_load_vocab: special tokens cache size = 25
0.00.052.460 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.464 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.464 I llm_load_print_meta: arch             = gptneox
0.00.052.465 I llm_load_print_meta: vocab type       = BPE
0.00.052.465 I llm_load_print_meta: n_vocab          = 50304
0.00.052.465 I llm_load_print_meta: n_merges         = 50009
0.00.052.467 I llm_load_print_meta: vocab_only       = 0
0.00.052.468 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.468 I llm_load_print_meta: n_embd           = 2048
0.00.052.468 I llm_load_print_meta: n_layer          = 24
0.00.052.473 I llm_load_print_meta: n_head           = 16
0.00.052.474 I llm_load_print_meta: n_head_kv        = 16
0.00.052.486 I llm_load_print_meta: n_rot            = 32
0.00.052.487 I llm_load_print_meta: n_swa            = 0
0.00.052.487 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.487 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.488 I llm_load_print_meta: n_gqa            = 1
0.00.052.488 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.489 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.489 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.490 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.490 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.490 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.490 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.492 I llm_load_print_meta: n_ff             = 8192
0.00.052.492 I llm_load_print_meta: n_expert         = 0
0.00.052.492 I llm_load_print_meta: n_expert_used    = 0
0.00.052.492 I llm_load_print_meta: causal attn      = 1
0.00.052.492 I llm_load_print_meta: pooling type     = 0
0.00.052.492 I llm_load_print_meta: rope type        = 2
0.00.052.493 I llm_load_print_meta: rope scaling     = linear
0.00.052.493 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.493 I llm_load_print_meta: freq_scale_train = 1
0.00.052.493 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.494 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.494 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.494 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.494 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.494 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.494 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.505 I llm_load_print_meta: model type       = 1.4B
0.00.052.505 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.505 I llm_load_print_meta: model params     = 1.41 B
0.00.052.506 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.506 I llm_load_print_meta: general.name     = 1.4B
0.00.052.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.506 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.506 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.506 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.507 I llm_load_print_meta: LF token         = 128 ''
0.00.052.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.507 I llm_load_print_meta: max token length = 1024
0.00.054.980 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.980 I llm_load_tensors: offloading output layer to GPU
0.00.054.980 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.992 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.993 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.964 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.965 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.965 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.965 I llama_new_context_with_model: n_batch       = 2048
0.00.055.966 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.966 I llama_new_context_with_model: flash_attn    = 0
0.00.055.966 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.966 I llama_new_context_with_model: freq_scale    = 1
0.00.055.967 I ggml_metal_init: allocating
0.00.055.970 I ggml_metal_init: found device: Apple M4
0.00.055.972 I ggml_metal_init: picking default device: Apple M4
0.00.056.671 I ggml_metal_init: using embedded metal library
0.00.059.159 I ggml_metal_init: GPU name:   Apple M4
0.00.059.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.162 I ggml_metal_init: simdgroup reduction   = true
0.00.059.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.162 I ggml_metal_init: has bfloat            = true
0.00.059.162 I ggml_metal_init: use bfloat            = true
0.00.059.163 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.724 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.732 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.754 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.947 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.948 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.948 I llama_new_context_with_model: graph nodes  = 967
0.00.093.949 I llama_new_context_with_model: graph splits = 2
0.00.093.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.035.152 I main: llama threadpool init, n_threads = 4
0.01.035.194 I 
0.01.035.219 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.035.222 I 
0.01.035.394 I sampler seed: 1234
0.01.035.400 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.035.434 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.035.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.035.438 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.134.358 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.02.134.359 I llama_perf_context_print:        load time =    1025.40 ms
0.02.134.361 I llama_perf_context_print: prompt eval time =      39.87 ms /     7 tokens (    5.70 ms per token,   175.57 tokens per second)
0.02.134.362 I llama_perf_context_print:        eval time =    1056.35 ms /    63 runs   (   16.77 ms per token,    59.64 tokens per second)
0.02.134.362 I llama_perf_context_print:       total time =    1099.21 ms /    70 tokens
0.02.134.561 I ggml_metal_free: deallocating

real	0m2.153s
user	0m0.111s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.136 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.803 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.640 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.640 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.641 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.642 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.642 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.643 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.643 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.644 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.644 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.647 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.647 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.648 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.335 I llama_model_loader: - type  f32:  194 tensors
0.00.040.336 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.776 I llm_load_vocab: special tokens cache size = 25
0.00.075.361 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.365 I llm_load_print_meta: arch             = gptneox
0.00.075.365 I llm_load_print_meta: vocab type       = BPE
0.00.075.365 I llm_load_print_meta: n_vocab          = 50304
0.00.075.365 I llm_load_print_meta: n_merges         = 50009
0.00.075.366 I llm_load_print_meta: vocab_only       = 0
0.00.075.366 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.366 I llm_load_print_meta: n_embd           = 2048
0.00.075.366 I llm_load_print_meta: n_layer          = 24
0.00.075.371 I llm_load_print_meta: n_head           = 16
0.00.075.372 I llm_load_print_meta: n_head_kv        = 16
0.00.075.382 I llm_load_print_meta: n_rot            = 32
0.00.075.382 I llm_load_print_meta: n_swa            = 0
0.00.075.382 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.382 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.383 I llm_load_print_meta: n_gqa            = 1
0.00.075.384 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.384 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.385 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.385 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.385 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.385 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.386 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.386 I llm_load_print_meta: n_ff             = 8192
0.00.075.386 I llm_load_print_meta: n_expert         = 0
0.00.075.387 I llm_load_print_meta: n_expert_used    = 0
0.00.075.387 I llm_load_print_meta: causal attn      = 1
0.00.075.387 I llm_load_print_meta: pooling type     = 0
0.00.075.387 I llm_load_print_meta: rope type        = 2
0.00.075.387 I llm_load_print_meta: rope scaling     = linear
0.00.075.388 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.388 I llm_load_print_meta: freq_scale_train = 1
0.00.075.388 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.388 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.388 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.388 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.388 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.389 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.389 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.399 I llm_load_print_meta: model type       = 1.4B
0.00.075.399 I llm_load_print_meta: model ftype      = Q8_0
0.00.075.399 I llm_load_print_meta: model params     = 1.41 B
0.00.075.400 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.075.400 I llm_load_print_meta: general.name     = 1.4B
0.00.075.400 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.400 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.400 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.401 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.401 I llm_load_print_meta: LF token         = 128 ''
0.00.075.401 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.401 I llm_load_print_meta: max token length = 1024
0.00.077.736 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.736 I llm_load_tensors: offloading output layer to GPU
0.00.077.736 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.748 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.077.749 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.078.718 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.718 I llama_new_context_with_model: n_ctx         = 128
0.00.078.719 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.078.719 I llama_new_context_with_model: n_batch       = 128
0.00.078.719 I llama_new_context_with_model: n_ubatch      = 128
0.00.078.719 I llama_new_context_with_model: flash_attn    = 0
0.00.078.720 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.720 I llama_new_context_with_model: freq_scale    = 1
0.00.078.720 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.078.721 I ggml_metal_init: allocating
0.00.078.727 I ggml_metal_init: found device: Apple M4
0.00.078.730 I ggml_metal_init: picking default device: Apple M4
0.00.079.409 I ggml_metal_init: using embedded metal library
0.00.082.022 I ggml_metal_init: GPU name:   Apple M4
0.00.082.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.025 I ggml_metal_init: simdgroup reduction   = true
0.00.082.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.025 I ggml_metal_init: has bfloat            = true
0.00.082.025 I ggml_metal_init: use bfloat            = true
0.00.082.026 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.240 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.092.247 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.092.271 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.224 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.225 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.225 I llama_new_context_with_model: graph nodes  = 967
0.00.093.225 I llama_new_context_with_model: graph splits = 2
0.00.093.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.003.048 I 
0.01.003.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.003.102 I perplexity: tokenizing the input ..
0.01.012.416 I perplexity: tokenization took 9.313 ms
0.01.012.426 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.137.986 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.139.140 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.139.150 I llama_perf_context_print:        load time =     991.24 ms
0.01.139.151 I llama_perf_context_print: prompt eval time =     125.34 ms /   128 tokens (    0.98 ms per token,  1021.21 tokens per second)
0.01.139.152 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.139.153 I llama_perf_context_print:       total time =     136.10 ms /   129 tokens
0.01.139.630 I ggml_metal_free: deallocating

real	0m1.163s
user	0m0.103s
sys	0m0.152s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.798 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.220 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.227 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.228 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.231 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.231 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.232 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.235 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.235 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.235 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.056 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.059 I llama_model_loader: - type  f32:  194 tensors
0.00.026.059 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.059 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.555 I llm_load_vocab: special tokens cache size = 25
0.00.052.504 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.507 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.507 I llm_load_print_meta: arch             = gptneox
0.00.052.507 I llm_load_print_meta: vocab type       = BPE
0.00.052.508 I llm_load_print_meta: n_vocab          = 50304
0.00.052.508 I llm_load_print_meta: n_merges         = 50009
0.00.052.508 I llm_load_print_meta: vocab_only       = 0
0.00.052.508 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.508 I llm_load_print_meta: n_embd           = 2048
0.00.052.508 I llm_load_print_meta: n_layer          = 24
0.00.052.513 I llm_load_print_meta: n_head           = 16
0.00.052.513 I llm_load_print_meta: n_head_kv        = 16
0.00.052.528 I llm_load_print_meta: n_rot            = 32
0.00.052.529 I llm_load_print_meta: n_swa            = 0
0.00.052.529 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.530 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.530 I llm_load_print_meta: n_gqa            = 1
0.00.052.531 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.532 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.532 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.533 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.533 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.533 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.533 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.534 I llm_load_print_meta: n_ff             = 8192
0.00.052.539 I llm_load_print_meta: n_expert         = 0
0.00.052.540 I llm_load_print_meta: n_expert_used    = 0
0.00.052.540 I llm_load_print_meta: causal attn      = 1
0.00.052.540 I llm_load_print_meta: pooling type     = 0
0.00.052.542 I llm_load_print_meta: rope type        = 2
0.00.052.542 I llm_load_print_meta: rope scaling     = linear
0.00.052.542 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.543 I llm_load_print_meta: freq_scale_train = 1
0.00.052.543 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.543 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.543 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.545 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.545 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.545 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.545 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.556 I llm_load_print_meta: model type       = 1.4B
0.00.052.556 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.556 I llm_load_print_meta: model params     = 1.41 B
0.00.052.557 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.557 I llm_load_print_meta: general.name     = 1.4B
0.00.052.557 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.557 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: LF token         = 128 ''
0.00.052.558 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.558 I llm_load_print_meta: max token length = 1024
0.00.054.852 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.852 I llm_load_tensors: offloading output layer to GPU
0.00.054.853 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.864 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.865 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.286 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.287 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.287 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.287 I llama_new_context_with_model: n_batch       = 2048
0.00.056.287 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.287 I llama_new_context_with_model: flash_attn    = 0
0.00.056.288 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.288 I llama_new_context_with_model: freq_scale    = 1
0.00.056.288 I ggml_metal_init: allocating
0.00.056.297 I ggml_metal_init: found device: Apple M4
0.00.056.299 I ggml_metal_init: picking default device: Apple M4
0.00.057.079 I ggml_metal_init: using embedded metal library
0.00.059.613 I ggml_metal_init: GPU name:   Apple M4
0.00.059.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.615 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.615 I ggml_metal_init: simdgroup reduction   = true
0.00.059.616 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.616 I ggml_metal_init: has bfloat            = true
0.00.059.616 I ggml_metal_init: use bfloat            = true
0.00.059.616 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.331 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.354 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.472 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.474 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.474 I llama_new_context_with_model: graph nodes  = 967
0.00.094.474 I llama_new_context_with_model: graph splits = 2
0.00.094.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.861 I main: llama threadpool init, n_threads = 4
0.00.690.898 I 
0.00.690.931 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.932 I 
0.00.691.201 I sampler seed: 1234
0.00.691.206 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.226 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.226 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.226 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.373.475 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.373.475 I llama_perf_context_print:        load time =     680.06 ms
0.01.373.476 I llama_perf_context_print: prompt eval time =      46.22 ms /     7 tokens (    6.60 ms per token,   151.45 tokens per second)
0.01.373.477 I llama_perf_context_print:        eval time =     632.97 ms /    63 runs   (   10.05 ms per token,    99.53 tokens per second)
0.01.373.477 I llama_perf_context_print:       total time =     682.62 ms /    70 tokens
0.01.373.661 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.347 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.951 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.952 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.952 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.955 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.956 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.956 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.958 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.958 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.555 I llama_model_loader: - type  f32:  194 tensors
0.00.027.555 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.555 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.633 I llm_load_vocab: special tokens cache size = 25
0.00.053.517 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.519 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.519 I llm_load_print_meta: arch             = gptneox
0.00.053.520 I llm_load_print_meta: vocab type       = BPE
0.00.053.520 I llm_load_print_meta: n_vocab          = 50304
0.00.053.520 I llm_load_print_meta: n_merges         = 50009
0.00.053.520 I llm_load_print_meta: vocab_only       = 0
0.00.053.521 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.521 I llm_load_print_meta: n_embd           = 2048
0.00.053.521 I llm_load_print_meta: n_layer          = 24
0.00.053.523 I llm_load_print_meta: n_head           = 16
0.00.053.524 I llm_load_print_meta: n_head_kv        = 16
0.00.053.533 I llm_load_print_meta: n_rot            = 32
0.00.053.533 I llm_load_print_meta: n_swa            = 0
0.00.053.534 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.534 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.534 I llm_load_print_meta: n_gqa            = 1
0.00.053.535 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.536 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.536 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.537 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.537 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.537 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.537 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.539 I llm_load_print_meta: n_ff             = 8192
0.00.053.539 I llm_load_print_meta: n_expert         = 0
0.00.053.539 I llm_load_print_meta: n_expert_used    = 0
0.00.053.539 I llm_load_print_meta: causal attn      = 1
0.00.053.540 I llm_load_print_meta: pooling type     = 0
0.00.053.540 I llm_load_print_meta: rope type        = 2
0.00.053.540 I llm_load_print_meta: rope scaling     = linear
0.00.053.541 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.541 I llm_load_print_meta: freq_scale_train = 1
0.00.053.541 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.541 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.541 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.541 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.542 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.542 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.542 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.546 I llm_load_print_meta: model type       = 1.4B
0.00.053.547 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.547 I llm_load_print_meta: model params     = 1.41 B
0.00.053.547 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.548 I llm_load_print_meta: general.name     = 1.4B
0.00.053.548 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.548 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.549 I llm_load_print_meta: LF token         = 128 ''
0.00.053.549 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.549 I llm_load_print_meta: max token length = 1024
0.00.055.261 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.261 I llm_load_tensors: offloading output layer to GPU
0.00.055.261 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.266 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.267 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.141 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.141 I llama_new_context_with_model: n_ctx         = 128
0.00.056.142 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.142 I llama_new_context_with_model: n_batch       = 128
0.00.056.142 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.142 I llama_new_context_with_model: flash_attn    = 0
0.00.056.142 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.143 I llama_new_context_with_model: freq_scale    = 1
0.00.056.143 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.143 I ggml_metal_init: allocating
0.00.056.146 I ggml_metal_init: found device: Apple M4
0.00.056.148 I ggml_metal_init: picking default device: Apple M4
0.00.056.731 I ggml_metal_init: using embedded metal library
0.00.059.053 I ggml_metal_init: GPU name:   Apple M4
0.00.059.054 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.055 I ggml_metal_init: simdgroup reduction   = true
0.00.059.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.056 I ggml_metal_init: has bfloat            = true
0.00.059.056 I ggml_metal_init: use bfloat            = true
0.00.059.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.453 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.455 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.469 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.072.342 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.072.343 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.072.344 I llama_new_context_with_model: graph nodes  = 967
0.00.072.344 I llama_new_context_with_model: graph splits = 2
0.00.072.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.883 I 
0.00.626.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.992 I perplexity: tokenizing the input ..
0.00.634.800 I perplexity: tokenization took 7.806 ms
0.00.634.811 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.364 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.758.536 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.758.548 I llama_perf_context_print:        load time =     615.53 ms
0.00.758.549 I llama_perf_context_print: prompt eval time =     122.33 ms /   128 tokens (    0.96 ms per token,  1046.38 tokens per second)
0.00.758.550 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.551 I llama_perf_context_print:       total time =     131.67 ms /   129 tokens
0.00.759.054 I ggml_metal_free: deallocating

real	0m0.778s
user	0m0.077s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.274 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.608 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.621 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.628 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.628 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.875 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.849 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.851 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.852 I llama_model_loader: - type  f32:  194 tensors
0.00.025.852 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.852 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.741 I llm_load_vocab: special tokens cache size = 25
0.00.051.651 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.654 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.654 I llm_load_print_meta: arch             = gptneox
0.00.051.654 I llm_load_print_meta: vocab type       = BPE
0.00.051.655 I llm_load_print_meta: n_vocab          = 50304
0.00.051.655 I llm_load_print_meta: n_merges         = 50009
0.00.051.655 I llm_load_print_meta: vocab_only       = 0
0.00.051.655 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.655 I llm_load_print_meta: n_embd           = 2048
0.00.051.655 I llm_load_print_meta: n_layer          = 24
0.00.051.658 I llm_load_print_meta: n_head           = 16
0.00.051.659 I llm_load_print_meta: n_head_kv        = 16
0.00.051.665 I llm_load_print_meta: n_rot            = 32
0.00.051.665 I llm_load_print_meta: n_swa            = 0
0.00.051.666 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.668 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.668 I llm_load_print_meta: n_gqa            = 1
0.00.051.669 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.670 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.670 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.671 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.673 I llm_load_print_meta: n_ff             = 8192
0.00.051.674 I llm_load_print_meta: n_expert         = 0
0.00.051.674 I llm_load_print_meta: n_expert_used    = 0
0.00.051.676 I llm_load_print_meta: causal attn      = 1
0.00.051.676 I llm_load_print_meta: pooling type     = 0
0.00.051.677 I llm_load_print_meta: rope type        = 2
0.00.051.677 I llm_load_print_meta: rope scaling     = linear
0.00.051.677 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.678 I llm_load_print_meta: freq_scale_train = 1
0.00.051.678 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.678 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.678 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.678 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.678 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.678 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.678 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.682 I llm_load_print_meta: model type       = 1.4B
0.00.051.683 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.683 I llm_load_print_meta: model params     = 1.41 B
0.00.051.684 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.684 I llm_load_print_meta: general.name     = 1.4B
0.00.051.684 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.684 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.684 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.685 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.685 I llm_load_print_meta: LF token         = 128 ''
0.00.051.685 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.685 I llm_load_print_meta: max token length = 1024
0.00.053.457 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.457 I llm_load_tensors: offloading output layer to GPU
0.00.053.458 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.463 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.464 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.811 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.812 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.812 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.813 I llama_new_context_with_model: n_batch       = 2048
0.00.054.813 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.813 I llama_new_context_with_model: flash_attn    = 0
0.00.054.813 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.814 I llama_new_context_with_model: freq_scale    = 1
0.00.054.814 I ggml_metal_init: allocating
0.00.054.822 I ggml_metal_init: found device: Apple M4
0.00.054.824 I ggml_metal_init: picking default device: Apple M4
0.00.055.431 I ggml_metal_init: using embedded metal library
0.00.057.778 I ggml_metal_init: GPU name:   Apple M4
0.00.057.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.780 I ggml_metal_init: simdgroup reduction   = true
0.00.057.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.780 I ggml_metal_init: has bfloat            = true
0.00.057.782 I ggml_metal_init: use bfloat            = true
0.00.057.782 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.783 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.525 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.531 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.556 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.580 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.581 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.582 I llama_new_context_with_model: graph nodes  = 967
0.00.087.582 I llama_new_context_with_model: graph splits = 2
0.00.087.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.203 I main: llama threadpool init, n_threads = 4
0.00.714.241 I 
0.00.714.270 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.270 I 
0.00.714.520 I sampler seed: 1234
0.00.714.525 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.569 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.570 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.440.702 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.440.703 I llama_perf_context_print:        load time =     703.92 ms
0.01.440.703 I llama_perf_context_print: prompt eval time =      42.04 ms /     7 tokens (    6.01 ms per token,   166.50 tokens per second)
0.01.440.704 I llama_perf_context_print:        eval time =     681.04 ms /    63 runs   (   10.81 ms per token,    92.51 tokens per second)
0.01.440.705 I llama_perf_context_print:       total time =     726.50 ms /    70 tokens
0.01.440.897 I ggml_metal_free: deallocating

real	0m1.459s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.729 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.151 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.152 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.906 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.710 I llama_model_loader: - type  f32:  194 tensors
0.00.022.710 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.640 I llm_load_vocab: special tokens cache size = 25
0.00.048.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.482 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.483 I llm_load_print_meta: arch             = gptneox
0.00.048.483 I llm_load_print_meta: vocab type       = BPE
0.00.048.483 I llm_load_print_meta: n_vocab          = 50304
0.00.048.483 I llm_load_print_meta: n_merges         = 50009
0.00.048.484 I llm_load_print_meta: vocab_only       = 0
0.00.048.484 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.484 I llm_load_print_meta: n_embd           = 2048
0.00.048.484 I llm_load_print_meta: n_layer          = 24
0.00.048.486 I llm_load_print_meta: n_head           = 16
0.00.048.487 I llm_load_print_meta: n_head_kv        = 16
0.00.048.498 I llm_load_print_meta: n_rot            = 32
0.00.048.499 I llm_load_print_meta: n_swa            = 0
0.00.048.499 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.499 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.500 I llm_load_print_meta: n_gqa            = 1
0.00.048.500 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.501 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.501 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.502 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.502 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.502 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.504 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.505 I llm_load_print_meta: n_ff             = 8192
0.00.048.505 I llm_load_print_meta: n_expert         = 0
0.00.048.505 I llm_load_print_meta: n_expert_used    = 0
0.00.048.505 I llm_load_print_meta: causal attn      = 1
0.00.048.505 I llm_load_print_meta: pooling type     = 0
0.00.048.505 I llm_load_print_meta: rope type        = 2
0.00.048.507 I llm_load_print_meta: rope scaling     = linear
0.00.048.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.508 I llm_load_print_meta: freq_scale_train = 1
0.00.048.508 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.508 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.508 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.508 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.508 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.508 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.517 I llm_load_print_meta: model type       = 1.4B
0.00.048.518 I llm_load_print_meta: model ftype      = Q4_1
0.00.048.518 I llm_load_print_meta: model params     = 1.41 B
0.00.048.519 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.048.520 I llm_load_print_meta: general.name     = 1.4B
0.00.048.520 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.520 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.520 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.520 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.521 I llm_load_print_meta: LF token         = 128 ''
0.00.048.521 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.521 I llm_load_print_meta: max token length = 1024
0.00.050.082 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.082 I llm_load_tensors: offloading output layer to GPU
0.00.050.082 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.092 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.093 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.050.945 I llama_new_context_with_model: n_seq_max     = 1
0.00.050.946 I llama_new_context_with_model: n_ctx         = 128
0.00.050.946 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.050.947 I llama_new_context_with_model: n_batch       = 128
0.00.050.947 I llama_new_context_with_model: n_ubatch      = 128
0.00.050.947 I llama_new_context_with_model: flash_attn    = 0
0.00.050.947 I llama_new_context_with_model: freq_base     = 10000.0
0.00.050.948 I llama_new_context_with_model: freq_scale    = 1
0.00.050.948 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.948 I ggml_metal_init: allocating
0.00.050.951 I ggml_metal_init: found device: Apple M4
0.00.050.953 I ggml_metal_init: picking default device: Apple M4
0.00.051.514 I ggml_metal_init: using embedded metal library
0.00.053.994 I ggml_metal_init: GPU name:   Apple M4
0.00.053.995 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.996 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.996 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.996 I ggml_metal_init: simdgroup reduction   = true
0.00.053.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.997 I ggml_metal_init: has bfloat            = true
0.00.053.997 I ggml_metal_init: use bfloat            = true
0.00.053.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.998 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.021 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.023 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.037 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.897 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.899 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.899 I llama_new_context_with_model: graph nodes  = 967
0.00.065.899 I llama_new_context_with_model: graph splits = 2
0.00.065.911 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.768 I 
0.00.657.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.812 I perplexity: tokenizing the input ..
0.00.666.240 I perplexity: tokenization took 8.427 ms
0.00.666.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.151 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.790.272 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.790.288 I llama_perf_context_print:        load time =     649.04 ms
0.00.790.289 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.59 tokens per second)
0.00.790.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.292 I llama_perf_context_print:       total time =     132.52 ms /   129 tokens
0.00.790.606 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.077s
sys	0m0.112s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.961 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.966 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.971 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.971 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.972 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.977 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.977 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.856 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.712 I llama_model_loader: - type  f32:  194 tensors
0.00.024.712 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.712 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.841 I llm_load_vocab: special tokens cache size = 25
0.00.050.854 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.856 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.856 I llm_load_print_meta: arch             = gptneox
0.00.050.857 I llm_load_print_meta: vocab type       = BPE
0.00.050.857 I llm_load_print_meta: n_vocab          = 50304
0.00.050.857 I llm_load_print_meta: n_merges         = 50009
0.00.050.857 I llm_load_print_meta: vocab_only       = 0
0.00.050.857 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.858 I llm_load_print_meta: n_embd           = 2048
0.00.050.858 I llm_load_print_meta: n_layer          = 24
0.00.050.860 I llm_load_print_meta: n_head           = 16
0.00.050.861 I llm_load_print_meta: n_head_kv        = 16
0.00.050.873 I llm_load_print_meta: n_rot            = 32
0.00.050.874 I llm_load_print_meta: n_swa            = 0
0.00.050.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.875 I llm_load_print_meta: n_gqa            = 1
0.00.050.875 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.878 I llm_load_print_meta: n_ff             = 8192
0.00.050.878 I llm_load_print_meta: n_expert         = 0
0.00.050.879 I llm_load_print_meta: n_expert_used    = 0
0.00.050.880 I llm_load_print_meta: causal attn      = 1
0.00.050.882 I llm_load_print_meta: pooling type     = 0
0.00.050.882 I llm_load_print_meta: rope type        = 2
0.00.050.882 I llm_load_print_meta: rope scaling     = linear
0.00.050.882 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.883 I llm_load_print_meta: freq_scale_train = 1
0.00.050.884 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.885 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.885 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.885 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.895 I llm_load_print_meta: model type       = 1.4B
0.00.050.895 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.895 I llm_load_print_meta: model params     = 1.41 B
0.00.050.896 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.897 I llm_load_print_meta: general.name     = 1.4B
0.00.050.897 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.897 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.897 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.898 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.898 I llm_load_print_meta: LF token         = 128 ''
0.00.050.898 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.898 I llm_load_print_meta: max token length = 1024
0.00.052.869 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.869 I llm_load_tensors: offloading output layer to GPU
0.00.052.869 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.880 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.881 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.782 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.783 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.783 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.783 I llama_new_context_with_model: n_batch       = 2048
0.00.053.783 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.783 I llama_new_context_with_model: flash_attn    = 0
0.00.053.784 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.784 I llama_new_context_with_model: freq_scale    = 1
0.00.053.785 I ggml_metal_init: allocating
0.00.053.788 I ggml_metal_init: found device: Apple M4
0.00.053.789 I ggml_metal_init: picking default device: Apple M4
0.00.054.377 I ggml_metal_init: using embedded metal library
0.00.056.673 I ggml_metal_init: GPU name:   Apple M4
0.00.056.674 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.675 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.675 I ggml_metal_init: simdgroup reduction   = true
0.00.056.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.676 I ggml_metal_init: has bfloat            = true
0.00.056.676 I ggml_metal_init: use bfloat            = true
0.00.056.676 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.241 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.247 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.238 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.239 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.240 I llama_new_context_with_model: graph nodes  = 967
0.00.085.240 I llama_new_context_with_model: graph splits = 2
0.00.085.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.364 I main: llama threadpool init, n_threads = 4
0.00.752.405 I 
0.00.752.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.437 I 
0.00.752.685 I sampler seed: 1234
0.00.752.689 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.729 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.734 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.734 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.559.697 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.559.698 I llama_perf_context_print:        load time =     743.73 ms
0.01.559.699 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.55 tokens per second)
0.01.559.700 I llama_perf_context_print:        eval time =     757.12 ms /    63 runs   (   12.02 ms per token,    83.21 tokens per second)
0.01.559.701 I llama_perf_context_print:       total time =     807.34 ms /    70 tokens
0.01.559.929 I ggml_metal_free: deallocating

real	0m1.577s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.147 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.806 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.817 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.817 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.627 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.587 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.589 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.590 I llama_model_loader: - type  f32:  194 tensors
0.00.023.590 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.591 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.126 I llm_load_vocab: special tokens cache size = 25
0.00.050.087 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.089 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.090 I llm_load_print_meta: arch             = gptneox
0.00.050.090 I llm_load_print_meta: vocab type       = BPE
0.00.050.090 I llm_load_print_meta: n_vocab          = 50304
0.00.050.090 I llm_load_print_meta: n_merges         = 50009
0.00.050.090 I llm_load_print_meta: vocab_only       = 0
0.00.050.091 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.091 I llm_load_print_meta: n_embd           = 2048
0.00.050.091 I llm_load_print_meta: n_layer          = 24
0.00.050.094 I llm_load_print_meta: n_head           = 16
0.00.050.095 I llm_load_print_meta: n_head_kv        = 16
0.00.050.107 I llm_load_print_meta: n_rot            = 32
0.00.050.107 I llm_load_print_meta: n_swa            = 0
0.00.050.107 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.107 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.108 I llm_load_print_meta: n_gqa            = 1
0.00.050.109 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.112 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.112 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.113 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.113 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.113 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.113 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.114 I llm_load_print_meta: n_ff             = 8192
0.00.050.114 I llm_load_print_meta: n_expert         = 0
0.00.050.114 I llm_load_print_meta: n_expert_used    = 0
0.00.050.114 I llm_load_print_meta: causal attn      = 1
0.00.050.114 I llm_load_print_meta: pooling type     = 0
0.00.050.114 I llm_load_print_meta: rope type        = 2
0.00.050.116 I llm_load_print_meta: rope scaling     = linear
0.00.050.116 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.116 I llm_load_print_meta: freq_scale_train = 1
0.00.050.116 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.116 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.117 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.117 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.117 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.117 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.117 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.127 I llm_load_print_meta: model type       = 1.4B
0.00.050.127 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.127 I llm_load_print_meta: model params     = 1.41 B
0.00.050.128 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.128 I llm_load_print_meta: general.name     = 1.4B
0.00.050.128 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.128 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.129 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.129 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.129 I llm_load_print_meta: LF token         = 128 ''
0.00.050.129 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.129 I llm_load_print_meta: max token length = 1024
0.00.052.061 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.061 I llm_load_tensors: offloading output layer to GPU
0.00.052.061 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.072 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.073 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.967 I llama_new_context_with_model: n_ctx         = 128
0.00.052.967 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.967 I llama_new_context_with_model: n_batch       = 128
0.00.052.967 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.968 I llama_new_context_with_model: flash_attn    = 0
0.00.052.968 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.968 I llama_new_context_with_model: freq_scale    = 1
0.00.052.969 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.969 I ggml_metal_init: allocating
0.00.052.975 I ggml_metal_init: found device: Apple M4
0.00.052.977 I ggml_metal_init: picking default device: Apple M4
0.00.053.547 I ggml_metal_init: using embedded metal library
0.00.055.904 I ggml_metal_init: GPU name:   Apple M4
0.00.055.906 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.906 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.907 I ggml_metal_init: simdgroup reduction   = true
0.00.055.907 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.908 I ggml_metal_init: has bfloat            = true
0.00.055.908 I ggml_metal_init: use bfloat            = true
0.00.055.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.777 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.787 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.806 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.655 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.656 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.657 I llama_new_context_with_model: graph nodes  = 967
0.00.068.657 I llama_new_context_with_model: graph splits = 2
0.00.068.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.718.239 I 
0.00.718.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.293 I perplexity: tokenizing the input ..
0.00.726.257 I perplexity: tokenization took 7.962 ms
0.00.726.267 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.426 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.862.570 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.862.584 I llama_perf_context_print:        load time =     709.09 ms
0.00.862.585 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.62 tokens per second)
0.00.862.587 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.587 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.863.137 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.079s
sys	0m0.121s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.010.351 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.468 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.481 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.307 I llama_model_loader: - type  f32:  194 tensors
0.00.026.307 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.307 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.143 I llm_load_vocab: special tokens cache size = 25
0.00.053.121 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.124 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.124 I llm_load_print_meta: arch             = gptneox
0.00.053.125 I llm_load_print_meta: vocab type       = BPE
0.00.053.125 I llm_load_print_meta: n_vocab          = 50304
0.00.053.125 I llm_load_print_meta: n_merges         = 50009
0.00.053.125 I llm_load_print_meta: vocab_only       = 0
0.00.053.126 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.126 I llm_load_print_meta: n_embd           = 2048
0.00.053.126 I llm_load_print_meta: n_layer          = 24
0.00.053.129 I llm_load_print_meta: n_head           = 16
0.00.053.132 I llm_load_print_meta: n_head_kv        = 16
0.00.053.148 I llm_load_print_meta: n_rot            = 32
0.00.053.148 I llm_load_print_meta: n_swa            = 0
0.00.053.148 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.148 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.149 I llm_load_print_meta: n_gqa            = 1
0.00.053.151 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.152 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.152 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.152 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.153 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.153 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.153 I llm_load_print_meta: n_ff             = 8192
0.00.053.153 I llm_load_print_meta: n_expert         = 0
0.00.053.154 I llm_load_print_meta: n_expert_used    = 0
0.00.053.154 I llm_load_print_meta: causal attn      = 1
0.00.053.155 I llm_load_print_meta: pooling type     = 0
0.00.053.156 I llm_load_print_meta: rope type        = 2
0.00.053.156 I llm_load_print_meta: rope scaling     = linear
0.00.053.156 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.157 I llm_load_print_meta: freq_scale_train = 1
0.00.053.157 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.157 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.157 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.157 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.157 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.157 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.158 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.167 I llm_load_print_meta: model type       = 1.4B
0.00.053.168 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.168 I llm_load_print_meta: model params     = 1.41 B
0.00.053.169 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.169 I llm_load_print_meta: general.name     = 1.4B
0.00.053.170 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.170 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.170 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: LF token         = 128 ''
0.00.053.171 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: max token length = 1024
0.00.054.788 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.788 I llm_load_tensors: offloading output layer to GPU
0.00.054.788 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.798 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.799 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.670 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.670 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.671 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.671 I llama_new_context_with_model: n_batch       = 2048
0.00.055.671 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.671 I llama_new_context_with_model: flash_attn    = 0
0.00.055.672 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.672 I llama_new_context_with_model: freq_scale    = 1
0.00.055.672 I ggml_metal_init: allocating
0.00.055.676 I ggml_metal_init: found device: Apple M4
0.00.055.677 I ggml_metal_init: picking default device: Apple M4
0.00.056.288 I ggml_metal_init: using embedded metal library
0.00.058.617 I ggml_metal_init: GPU name:   Apple M4
0.00.058.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.619 I ggml_metal_init: simdgroup reduction   = true
0.00.058.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.621 I ggml_metal_init: has bfloat            = true
0.00.058.621 I ggml_metal_init: use bfloat            = true
0.00.058.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.997 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.004 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.025 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.132 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.134 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.134 I llama_new_context_with_model: graph nodes  = 967
0.00.090.134 I llama_new_context_with_model: graph splits = 2
0.00.090.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.266 I main: llama threadpool init, n_threads = 4
0.00.793.308 I 
0.00.793.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.338 I 
0.00.793.504 I sampler seed: 1234
0.00.793.509 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.559 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.561 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.561 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.671.476 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.671.477 I llama_perf_context_print:        load time =     782.91 ms
0.01.671.478 I llama_perf_context_print: prompt eval time =      42.34 ms /     7 tokens (    6.05 ms per token,   165.34 tokens per second)
0.01.671.478 I llama_perf_context_print:        eval time =     832.56 ms /    63 runs   (   13.22 ms per token,    75.67 tokens per second)
0.01.671.479 I llama_perf_context_print:       total time =     878.21 ms /    70 tokens
0.01.671.658 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.468 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.501 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.361 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.362 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.363 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.363 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.364 I llama_model_loader: - type  f32:  194 tensors
0.00.023.364 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.365 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.159 I llm_load_vocab: special tokens cache size = 25
0.00.050.165 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.168 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.168 I llm_load_print_meta: arch             = gptneox
0.00.050.168 I llm_load_print_meta: vocab type       = BPE
0.00.050.169 I llm_load_print_meta: n_vocab          = 50304
0.00.050.169 I llm_load_print_meta: n_merges         = 50009
0.00.050.169 I llm_load_print_meta: vocab_only       = 0
0.00.050.169 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.169 I llm_load_print_meta: n_embd           = 2048
0.00.050.169 I llm_load_print_meta: n_layer          = 24
0.00.050.172 I llm_load_print_meta: n_head           = 16
0.00.050.173 I llm_load_print_meta: n_head_kv        = 16
0.00.050.185 I llm_load_print_meta: n_rot            = 32
0.00.050.185 I llm_load_print_meta: n_swa            = 0
0.00.050.186 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.186 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.187 I llm_load_print_meta: n_gqa            = 1
0.00.050.187 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.188 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.189 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.189 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.189 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.189 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.190 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.190 I llm_load_print_meta: n_ff             = 8192
0.00.050.190 I llm_load_print_meta: n_expert         = 0
0.00.050.191 I llm_load_print_meta: n_expert_used    = 0
0.00.050.191 I llm_load_print_meta: causal attn      = 1
0.00.050.191 I llm_load_print_meta: pooling type     = 0
0.00.050.191 I llm_load_print_meta: rope type        = 2
0.00.050.191 I llm_load_print_meta: rope scaling     = linear
0.00.050.191 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.192 I llm_load_print_meta: freq_scale_train = 1
0.00.050.192 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.192 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.192 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.192 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.192 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.193 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.193 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.202 I llm_load_print_meta: model type       = 1.4B
0.00.050.202 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.203 I llm_load_print_meta: model params     = 1.41 B
0.00.050.204 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.204 I llm_load_print_meta: general.name     = 1.4B
0.00.050.204 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.204 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: LF token         = 128 ''
0.00.050.205 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.206 I llm_load_print_meta: max token length = 1024
0.00.052.214 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.215 I llm_load_tensors: offloading output layer to GPU
0.00.052.215 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.225 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.227 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.134 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.135 I llama_new_context_with_model: n_ctx         = 128
0.00.053.135 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.136 I llama_new_context_with_model: n_batch       = 128
0.00.053.136 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.136 I llama_new_context_with_model: flash_attn    = 0
0.00.053.136 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.137 I llama_new_context_with_model: freq_scale    = 1
0.00.053.137 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.137 I ggml_metal_init: allocating
0.00.053.140 I ggml_metal_init: found device: Apple M4
0.00.053.142 I ggml_metal_init: picking default device: Apple M4
0.00.053.708 I ggml_metal_init: using embedded metal library
0.00.056.027 I ggml_metal_init: GPU name:   Apple M4
0.00.056.029 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.030 I ggml_metal_init: simdgroup reduction   = true
0.00.056.030 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.030 I ggml_metal_init: has bfloat            = true
0.00.056.030 I ggml_metal_init: use bfloat            = true
0.00.056.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.049 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.051 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.066 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.991 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.992 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.993 I llama_new_context_with_model: graph nodes  = 967
0.00.067.993 I llama_new_context_with_model: graph splits = 2
0.00.068.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.395 I 
0.00.604.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.442 I perplexity: tokenizing the input ..
0.00.612.295 I perplexity: tokenization took 7.852 ms
0.00.612.306 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.500 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.748.691 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.748.709 I llama_perf_context_print:        load time =     595.57 ms
0.00.748.710 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.36 tokens per second)
0.00.748.711 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.714 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.749.244 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.079s
sys	0m0.110s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.658 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.085 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.086 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.086 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.087 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.088 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.089 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.925 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.748 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.749 I llama_model_loader: - type  f32:  194 tensors
0.00.023.749 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.749 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.750 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.838 I llm_load_vocab: special tokens cache size = 25
0.00.049.857 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.860 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.860 I llm_load_print_meta: arch             = gptneox
0.00.049.861 I llm_load_print_meta: vocab type       = BPE
0.00.049.861 I llm_load_print_meta: n_vocab          = 50304
0.00.049.861 I llm_load_print_meta: n_merges         = 50009
0.00.049.861 I llm_load_print_meta: vocab_only       = 0
0.00.049.861 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.861 I llm_load_print_meta: n_embd           = 2048
0.00.049.862 I llm_load_print_meta: n_layer          = 24
0.00.049.864 I llm_load_print_meta: n_head           = 16
0.00.049.865 I llm_load_print_meta: n_head_kv        = 16
0.00.049.876 I llm_load_print_meta: n_rot            = 32
0.00.049.876 I llm_load_print_meta: n_swa            = 0
0.00.049.877 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.877 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.877 I llm_load_print_meta: n_gqa            = 1
0.00.049.878 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.879 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.880 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.880 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.880 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.881 I llm_load_print_meta: n_ff             = 8192
0.00.049.881 I llm_load_print_meta: n_expert         = 0
0.00.049.881 I llm_load_print_meta: n_expert_used    = 0
0.00.049.881 I llm_load_print_meta: causal attn      = 1
0.00.049.881 I llm_load_print_meta: pooling type     = 0
0.00.049.882 I llm_load_print_meta: rope type        = 2
0.00.049.882 I llm_load_print_meta: rope scaling     = linear
0.00.049.882 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.883 I llm_load_print_meta: freq_scale_train = 1
0.00.049.883 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.883 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.883 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.883 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.883 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.883 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.884 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.893 I llm_load_print_meta: model type       = 1.4B
0.00.049.893 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.893 I llm_load_print_meta: model params     = 1.41 B
0.00.049.894 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.894 I llm_load_print_meta: general.name     = 1.4B
0.00.049.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.894 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.894 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: LF token         = 128 ''
0.00.049.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: max token length = 1024
0.00.051.465 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.465 I llm_load_tensors: offloading output layer to GPU
0.00.051.465 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.476 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.477 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.331 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.332 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.332 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.332 I llama_new_context_with_model: n_batch       = 2048
0.00.052.333 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.333 I llama_new_context_with_model: flash_attn    = 0
0.00.052.333 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.333 I llama_new_context_with_model: freq_scale    = 1
0.00.052.334 I ggml_metal_init: allocating
0.00.052.340 I ggml_metal_init: found device: Apple M4
0.00.052.342 I ggml_metal_init: picking default device: Apple M4
0.00.052.939 I ggml_metal_init: using embedded metal library
0.00.055.262 I ggml_metal_init: GPU name:   Apple M4
0.00.055.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.263 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.264 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.264 I ggml_metal_init: simdgroup reduction   = true
0.00.055.264 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.264 I ggml_metal_init: has bfloat            = true
0.00.055.264 I ggml_metal_init: use bfloat            = true
0.00.055.265 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.267 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.060 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.069 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.117 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.118 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.119 I llama_new_context_with_model: graph nodes  = 967
0.00.085.119 I llama_new_context_with_model: graph splits = 2
0.00.085.133 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.508.548 I main: llama threadpool init, n_threads = 4
0.00.508.589 I 
0.00.508.620 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.620 I 
0.00.508.870 I sampler seed: 1234
0.00.508.876 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.508.911 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.508.923 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.508.926 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.191.548 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.191.549 I llama_perf_context_print:        load time =     498.88 ms
0.01.191.552 I llama_perf_context_print: prompt eval time =      39.66 ms /     7 tokens (    5.67 ms per token,   176.49 tokens per second)
0.01.191.554 I llama_perf_context_print:        eval time =     639.93 ms /    63 runs   (   10.16 ms per token,    98.45 tokens per second)
0.01.191.554 I llama_perf_context_print:       total time =     683.01 ms /    70 tokens
0.01.191.741 I ggml_metal_free: deallocating

real	0m1.210s
user	0m0.109s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.131 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.640 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.646 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.647 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.385 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.386 I llama_model_loader: - type  f32:  194 tensors
0.00.026.386 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.387 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.387 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.074 I llm_load_vocab: special tokens cache size = 25
0.00.052.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.154 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.154 I llm_load_print_meta: arch             = gptneox
0.00.052.155 I llm_load_print_meta: vocab type       = BPE
0.00.052.155 I llm_load_print_meta: n_vocab          = 50304
0.00.052.155 I llm_load_print_meta: n_merges         = 50009
0.00.052.155 I llm_load_print_meta: vocab_only       = 0
0.00.052.156 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.156 I llm_load_print_meta: n_embd           = 2048
0.00.052.156 I llm_load_print_meta: n_layer          = 24
0.00.052.158 I llm_load_print_meta: n_head           = 16
0.00.052.159 I llm_load_print_meta: n_head_kv        = 16
0.00.052.171 I llm_load_print_meta: n_rot            = 32
0.00.052.171 I llm_load_print_meta: n_swa            = 0
0.00.052.171 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.171 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.172 I llm_load_print_meta: n_gqa            = 1
0.00.052.173 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.173 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.174 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.174 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.175 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.175 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.175 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.175 I llm_load_print_meta: n_ff             = 8192
0.00.052.176 I llm_load_print_meta: n_expert         = 0
0.00.052.176 I llm_load_print_meta: n_expert_used    = 0
0.00.052.176 I llm_load_print_meta: causal attn      = 1
0.00.052.176 I llm_load_print_meta: pooling type     = 0
0.00.052.176 I llm_load_print_meta: rope type        = 2
0.00.052.176 I llm_load_print_meta: rope scaling     = linear
0.00.052.179 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.179 I llm_load_print_meta: freq_scale_train = 1
0.00.052.180 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.180 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.180 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.180 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.180 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.180 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.190 I llm_load_print_meta: model type       = 1.4B
0.00.052.191 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.191 I llm_load_print_meta: model params     = 1.41 B
0.00.052.192 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.192 I llm_load_print_meta: general.name     = 1.4B
0.00.052.192 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.192 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.193 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.193 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.193 I llm_load_print_meta: LF token         = 128 ''
0.00.052.194 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.194 I llm_load_print_meta: max token length = 1024
0.00.054.028 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.028 I llm_load_tensors: offloading output layer to GPU
0.00.054.028 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.039 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.040 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.910 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.911 I llama_new_context_with_model: n_ctx         = 128
0.00.054.911 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.911 I llama_new_context_with_model: n_batch       = 128
0.00.054.911 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.912 I llama_new_context_with_model: flash_attn    = 0
0.00.054.912 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.912 I llama_new_context_with_model: freq_scale    = 1
0.00.054.913 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.913 I ggml_metal_init: allocating
0.00.054.916 I ggml_metal_init: found device: Apple M4
0.00.054.918 I ggml_metal_init: picking default device: Apple M4
0.00.055.489 I ggml_metal_init: using embedded metal library
0.00.057.772 I ggml_metal_init: GPU name:   Apple M4
0.00.057.774 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.774 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.774 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.775 I ggml_metal_init: simdgroup reduction   = true
0.00.057.775 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.775 I ggml_metal_init: has bfloat            = true
0.00.057.775 I ggml_metal_init: use bfloat            = true
0.00.057.775 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.435 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.438 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.451 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.354 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.355 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.355 I llama_new_context_with_model: graph nodes  = 967
0.00.069.356 I llama_new_context_with_model: graph splits = 2
0.00.069.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.166 I 
0.00.437.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.208 I perplexity: tokenizing the input ..
0.00.444.884 I perplexity: tokenization took 7.674 ms
0.00.444.894 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.481 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.578.768 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.578.794 I llama_perf_context_print:        load time =     425.03 ms
0.00.578.796 I llama_perf_context_print: prompt eval time =     132.35 ms /   128 tokens (    1.03 ms per token,   967.16 tokens per second)
0.00.578.797 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.578.797 I llama_perf_context_print:       total time =     141.63 ms /   129 tokens
0.00.579.318 I ggml_metal_free: deallocating

real	0m0.594s
user	0m0.076s
sys	0m0.076s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.621 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.812 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.822 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.823 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.823 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.824 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.824 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.825 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.826 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.827 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.678 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.575 I llama_model_loader: - type  f32:  194 tensors
0.00.025.576 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.576 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.576 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.576 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.693 I llm_load_vocab: special tokens cache size = 25
0.00.051.693 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.695 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.696 I llm_load_print_meta: arch             = gptneox
0.00.051.696 I llm_load_print_meta: vocab type       = BPE
0.00.051.696 I llm_load_print_meta: n_vocab          = 50304
0.00.051.696 I llm_load_print_meta: n_merges         = 50009
0.00.051.697 I llm_load_print_meta: vocab_only       = 0
0.00.051.697 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.697 I llm_load_print_meta: n_embd           = 2048
0.00.051.697 I llm_load_print_meta: n_layer          = 24
0.00.051.700 I llm_load_print_meta: n_head           = 16
0.00.051.701 I llm_load_print_meta: n_head_kv        = 16
0.00.051.712 I llm_load_print_meta: n_rot            = 32
0.00.051.714 I llm_load_print_meta: n_swa            = 0
0.00.051.714 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.715 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.715 I llm_load_print_meta: n_gqa            = 1
0.00.051.716 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.717 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.717 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.718 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.718 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.718 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.718 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.718 I llm_load_print_meta: n_ff             = 8192
0.00.051.719 I llm_load_print_meta: n_expert         = 0
0.00.051.719 I llm_load_print_meta: n_expert_used    = 0
0.00.051.719 I llm_load_print_meta: causal attn      = 1
0.00.051.719 I llm_load_print_meta: pooling type     = 0
0.00.051.719 I llm_load_print_meta: rope type        = 2
0.00.051.719 I llm_load_print_meta: rope scaling     = linear
0.00.051.720 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.720 I llm_load_print_meta: freq_scale_train = 1
0.00.051.720 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.720 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.720 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.721 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.721 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.721 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.721 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.730 I llm_load_print_meta: model type       = 1.4B
0.00.051.731 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.731 I llm_load_print_meta: model params     = 1.41 B
0.00.051.732 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.732 I llm_load_print_meta: general.name     = 1.4B
0.00.051.732 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.732 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.732 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.732 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.733 I llm_load_print_meta: LF token         = 128 ''
0.00.051.733 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.733 I llm_load_print_meta: max token length = 1024
0.00.053.668 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.669 I llm_load_tensors: offloading output layer to GPU
0.00.053.669 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.679 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.680 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.612 I llama_new_context_with_model: n_batch       = 2048
0.00.054.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.613 I llama_new_context_with_model: flash_attn    = 0
0.00.054.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.614 I llama_new_context_with_model: freq_scale    = 1
0.00.054.614 I ggml_metal_init: allocating
0.00.054.620 I ggml_metal_init: found device: Apple M4
0.00.054.623 I ggml_metal_init: picking default device: Apple M4
0.00.055.190 I ggml_metal_init: using embedded metal library
0.00.057.531 I ggml_metal_init: GPU name:   Apple M4
0.00.057.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.533 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.533 I ggml_metal_init: simdgroup reduction   = true
0.00.057.534 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.534 I ggml_metal_init: has bfloat            = true
0.00.057.534 I ggml_metal_init: use bfloat            = true
0.00.057.534 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.535 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.021 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.027 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.046 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.021 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.022 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.023 I llama_new_context_with_model: graph nodes  = 967
0.00.087.023 I llama_new_context_with_model: graph splits = 2
0.00.087.037 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.046 I main: llama threadpool init, n_threads = 4
0.00.532.135 I 
0.00.532.164 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.165 I 
0.00.532.391 I sampler seed: 1234
0.00.532.395 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.415 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.416 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.416 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.280.222 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.280.222 I llama_perf_context_print:        load time =     521.42 ms
0.01.280.223 I llama_perf_context_print: prompt eval time =      40.43 ms /     7 tokens (    5.78 ms per token,   173.15 tokens per second)
0.01.280.224 I llama_perf_context_print:        eval time =     704.39 ms /    63 runs   (   11.18 ms per token,    89.44 tokens per second)
0.01.280.224 I llama_perf_context_print:       total time =     748.18 ms /    70 tokens
0.01.280.413 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.108s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.259 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.264 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.266 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.266 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.268 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.268 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.271 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.274 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.972 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.998 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.722 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.724 I llama_model_loader: - type  f32:  194 tensors
0.00.022.724 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.725 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.725 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.608 I llm_load_vocab: special tokens cache size = 25
0.00.048.420 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.423 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.423 I llm_load_print_meta: arch             = gptneox
0.00.048.424 I llm_load_print_meta: vocab type       = BPE
0.00.048.424 I llm_load_print_meta: n_vocab          = 50304
0.00.048.424 I llm_load_print_meta: n_merges         = 50009
0.00.048.425 I llm_load_print_meta: vocab_only       = 0
0.00.048.425 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.425 I llm_load_print_meta: n_embd           = 2048
0.00.048.425 I llm_load_print_meta: n_layer          = 24
0.00.048.427 I llm_load_print_meta: n_head           = 16
0.00.048.428 I llm_load_print_meta: n_head_kv        = 16
0.00.048.440 I llm_load_print_meta: n_rot            = 32
0.00.048.440 I llm_load_print_meta: n_swa            = 0
0.00.048.440 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.441 I llm_load_print_meta: n_gqa            = 1
0.00.048.442 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.443 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.444 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.444 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.444 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.445 I llm_load_print_meta: n_ff             = 8192
0.00.048.445 I llm_load_print_meta: n_expert         = 0
0.00.048.445 I llm_load_print_meta: n_expert_used    = 0
0.00.048.445 I llm_load_print_meta: causal attn      = 1
0.00.048.445 I llm_load_print_meta: pooling type     = 0
0.00.048.446 I llm_load_print_meta: rope type        = 2
0.00.048.446 I llm_load_print_meta: rope scaling     = linear
0.00.048.446 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.446 I llm_load_print_meta: freq_scale_train = 1
0.00.048.447 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.447 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.447 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.447 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.447 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.447 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.447 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.457 I llm_load_print_meta: model type       = 1.4B
0.00.048.457 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.457 I llm_load_print_meta: model params     = 1.41 B
0.00.048.458 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.458 I llm_load_print_meta: general.name     = 1.4B
0.00.048.458 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.458 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.458 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.459 I llm_load_print_meta: LF token         = 128 ''
0.00.048.459 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.459 I llm_load_print_meta: max token length = 1024
0.00.050.347 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.348 I llm_load_tensors: offloading output layer to GPU
0.00.050.348 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.359 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.360 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.275 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.276 I llama_new_context_with_model: n_ctx         = 128
0.00.051.276 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.276 I llama_new_context_with_model: n_batch       = 128
0.00.051.276 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.276 I llama_new_context_with_model: flash_attn    = 0
0.00.051.277 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.277 I llama_new_context_with_model: freq_scale    = 1
0.00.051.277 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.278 I ggml_metal_init: allocating
0.00.051.281 I ggml_metal_init: found device: Apple M4
0.00.051.283 I ggml_metal_init: picking default device: Apple M4
0.00.051.878 I ggml_metal_init: using embedded metal library
0.00.054.174 I ggml_metal_init: GPU name:   Apple M4
0.00.054.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.176 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.177 I ggml_metal_init: simdgroup reduction   = true
0.00.054.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.177 I ggml_metal_init: has bfloat            = true
0.00.054.177 I ggml_metal_init: use bfloat            = true
0.00.054.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.932 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.936 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.949 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.869 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.870 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.870 I llama_new_context_with_model: graph nodes  = 967
0.00.065.871 I llama_new_context_with_model: graph splits = 2
0.00.065.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.821 I 
0.00.480.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.868 I perplexity: tokenizing the input ..
0.00.489.008 I perplexity: tokenization took 8.138 ms
0.00.489.019 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.621.291 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.622.605 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.622.619 I llama_perf_context_print:        load time =     472.14 ms
0.00.622.620 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.36 tokens per second)
0.00.622.621 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.622.621 I llama_perf_context_print:       total time =     141.80 ms /   129 tokens
0.00.623.008 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.076s
sys	0m0.085s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.011.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.248 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.253 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.255 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.255 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.256 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.256 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.259 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.262 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.262 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.140 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.164 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.969 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.970 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.971 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.971 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.972 I llama_model_loader: - type  f32:  194 tensors
0.00.026.972 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.972 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.972 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.088 I llm_load_vocab: special tokens cache size = 25
0.00.053.128 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.130 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.131 I llm_load_print_meta: arch             = gptneox
0.00.053.131 I llm_load_print_meta: vocab type       = BPE
0.00.053.131 I llm_load_print_meta: n_vocab          = 50304
0.00.053.131 I llm_load_print_meta: n_merges         = 50009
0.00.053.132 I llm_load_print_meta: vocab_only       = 0
0.00.053.132 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.132 I llm_load_print_meta: n_embd           = 2048
0.00.053.132 I llm_load_print_meta: n_layer          = 24
0.00.053.135 I llm_load_print_meta: n_head           = 16
0.00.053.136 I llm_load_print_meta: n_head_kv        = 16
0.00.053.147 I llm_load_print_meta: n_rot            = 32
0.00.053.148 I llm_load_print_meta: n_swa            = 0
0.00.053.149 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.149 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.150 I llm_load_print_meta: n_gqa            = 1
0.00.053.151 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.152 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.152 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.152 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.152 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.153 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.153 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.153 I llm_load_print_meta: n_ff             = 8192
0.00.053.155 I llm_load_print_meta: n_expert         = 0
0.00.053.156 I llm_load_print_meta: n_expert_used    = 0
0.00.053.158 I llm_load_print_meta: causal attn      = 1
0.00.053.158 I llm_load_print_meta: pooling type     = 0
0.00.053.158 I llm_load_print_meta: rope type        = 2
0.00.053.158 I llm_load_print_meta: rope scaling     = linear
0.00.053.158 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.159 I llm_load_print_meta: freq_scale_train = 1
0.00.053.159 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.159 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.159 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.159 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.159 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.159 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.160 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.169 I llm_load_print_meta: model type       = 1.4B
0.00.053.169 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.170 I llm_load_print_meta: model params     = 1.41 B
0.00.053.170 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.170 I llm_load_print_meta: general.name     = 1.4B
0.00.053.170 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.171 I llm_load_print_meta: LF token         = 128 ''
0.00.053.172 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.172 I llm_load_print_meta: max token length = 1024
0.00.055.170 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.170 I llm_load_tensors: offloading output layer to GPU
0.00.055.170 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.181 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.182 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.148 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.148 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.148 I llama_new_context_with_model: n_batch       = 2048
0.00.056.148 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.148 I llama_new_context_with_model: flash_attn    = 0
0.00.056.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.149 I llama_new_context_with_model: freq_scale    = 1
0.00.056.150 I ggml_metal_init: allocating
0.00.056.153 I ggml_metal_init: found device: Apple M4
0.00.056.155 I ggml_metal_init: picking default device: Apple M4
0.00.056.733 I ggml_metal_init: using embedded metal library
0.00.059.073 I ggml_metal_init: GPU name:   Apple M4
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.077 I ggml_metal_init: simdgroup reduction   = true
0.00.059.077 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.077 I ggml_metal_init: has bfloat            = true
0.00.059.078 I ggml_metal_init: use bfloat            = true
0.00.059.078 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.027 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.032 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.049 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.097 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.098 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.099 I llama_new_context_with_model: graph nodes  = 967
0.00.089.099 I llama_new_context_with_model: graph splits = 2
0.00.089.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.080 I main: llama threadpool init, n_threads = 4
0.00.631.117 I 
0.00.631.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.162 I 
0.00.631.384 I sampler seed: 1234
0.00.631.389 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.631.440 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.631.442 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.631.442 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.391.971 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.391.971 I llama_perf_context_print:        load time =     619.33 ms
0.01.391.972 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.54 tokens per second)
0.01.391.974 I llama_perf_context_print:        eval time =     710.35 ms /    63 runs   (   11.28 ms per token,    88.69 tokens per second)
0.01.391.974 I llama_perf_context_print:       total time =     760.89 ms /    70 tokens
0.01.392.199 I ggml_metal_free: deallocating

real	0m1.410s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.877 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.502 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.504 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.505 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.505 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.506 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.508 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.177 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.179 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.179 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.180 I llama_model_loader: - type  f32:  194 tensors
0.00.024.180 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.180 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.180 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.675 I llm_load_vocab: special tokens cache size = 25
0.00.050.676 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.679 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.679 I llm_load_print_meta: arch             = gptneox
0.00.050.680 I llm_load_print_meta: vocab type       = BPE
0.00.050.680 I llm_load_print_meta: n_vocab          = 50304
0.00.050.680 I llm_load_print_meta: n_merges         = 50009
0.00.050.680 I llm_load_print_meta: vocab_only       = 0
0.00.050.681 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.681 I llm_load_print_meta: n_embd           = 2048
0.00.050.681 I llm_load_print_meta: n_layer          = 24
0.00.050.684 I llm_load_print_meta: n_head           = 16
0.00.050.685 I llm_load_print_meta: n_head_kv        = 16
0.00.050.697 I llm_load_print_meta: n_rot            = 32
0.00.050.697 I llm_load_print_meta: n_swa            = 0
0.00.050.697 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.697 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.698 I llm_load_print_meta: n_gqa            = 1
0.00.050.699 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.699 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.700 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.701 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.701 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.701 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.701 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.702 I llm_load_print_meta: n_ff             = 8192
0.00.050.702 I llm_load_print_meta: n_expert         = 0
0.00.050.702 I llm_load_print_meta: n_expert_used    = 0
0.00.050.702 I llm_load_print_meta: causal attn      = 1
0.00.050.702 I llm_load_print_meta: pooling type     = 0
0.00.050.702 I llm_load_print_meta: rope type        = 2
0.00.050.703 I llm_load_print_meta: rope scaling     = linear
0.00.050.703 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.703 I llm_load_print_meta: freq_scale_train = 1
0.00.050.703 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.704 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.704 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.704 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.704 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.704 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.704 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.714 I llm_load_print_meta: model type       = 1.4B
0.00.050.714 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.714 I llm_load_print_meta: model params     = 1.41 B
0.00.050.715 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.715 I llm_load_print_meta: general.name     = 1.4B
0.00.050.715 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.715 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.715 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.715 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.716 I llm_load_print_meta: LF token         = 128 ''
0.00.050.716 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.716 I llm_load_print_meta: max token length = 1024
0.00.052.682 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.682 I llm_load_tensors: offloading output layer to GPU
0.00.052.682 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.692 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.694 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.635 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.636 I llama_new_context_with_model: n_ctx         = 128
0.00.053.636 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.636 I llama_new_context_with_model: n_batch       = 128
0.00.053.636 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.637 I llama_new_context_with_model: flash_attn    = 0
0.00.053.637 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.637 I llama_new_context_with_model: freq_scale    = 1
0.00.053.638 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.638 I ggml_metal_init: allocating
0.00.053.644 I ggml_metal_init: found device: Apple M4
0.00.053.646 I ggml_metal_init: picking default device: Apple M4
0.00.054.189 I ggml_metal_init: using embedded metal library
0.00.056.537 I ggml_metal_init: GPU name:   Apple M4
0.00.056.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.539 I ggml_metal_init: simdgroup reduction   = true
0.00.056.539 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.539 I ggml_metal_init: has bfloat            = true
0.00.056.540 I ggml_metal_init: use bfloat            = true
0.00.056.540 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.480 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.494 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.366 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.367 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.368 I llama_new_context_with_model: graph nodes  = 967
0.00.068.368 I llama_new_context_with_model: graph splits = 2
0.00.068.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.569.920 I 
0.00.569.952 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.569.960 I perplexity: tokenizing the input ..
0.00.577.755 I perplexity: tokenization took 7.793 ms
0.00.577.769 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.704 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.712.917 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.712.938 I llama_perf_context_print:        load time =     560.04 ms
0.00.712.939 I llama_perf_context_print: prompt eval time =     133.71 ms /   128 tokens (    1.04 ms per token,   957.30 tokens per second)
0.00.712.940 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.712.941 I llama_perf_context_print:       total time =     143.02 ms /   129 tokens
0.00.713.398 I ggml_metal_free: deallocating

real	0m0.728s
user	0m0.078s
sys	0m0.108s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.012.427 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.652 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.665 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.535 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.535 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.535 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.536 I llama_model_loader: - type  f32:  194 tensors
0.00.027.536 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.537 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.579 I llm_load_vocab: special tokens cache size = 25
0.00.053.500 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.503 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.504 I llm_load_print_meta: arch             = gptneox
0.00.053.504 I llm_load_print_meta: vocab type       = BPE
0.00.053.504 I llm_load_print_meta: n_vocab          = 50304
0.00.053.504 I llm_load_print_meta: n_merges         = 50009
0.00.053.505 I llm_load_print_meta: vocab_only       = 0
0.00.053.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.505 I llm_load_print_meta: n_embd           = 2048
0.00.053.505 I llm_load_print_meta: n_layer          = 24
0.00.053.508 I llm_load_print_meta: n_head           = 16
0.00.053.508 I llm_load_print_meta: n_head_kv        = 16
0.00.053.520 I llm_load_print_meta: n_rot            = 32
0.00.053.520 I llm_load_print_meta: n_swa            = 0
0.00.053.521 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.521 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.521 I llm_load_print_meta: n_gqa            = 1
0.00.053.522 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.523 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.523 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.524 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.524 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.524 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.524 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.525 I llm_load_print_meta: n_ff             = 8192
0.00.053.525 I llm_load_print_meta: n_expert         = 0
0.00.053.525 I llm_load_print_meta: n_expert_used    = 0
0.00.053.530 I llm_load_print_meta: causal attn      = 1
0.00.053.530 I llm_load_print_meta: pooling type     = 0
0.00.053.530 I llm_load_print_meta: rope type        = 2
0.00.053.530 I llm_load_print_meta: rope scaling     = linear
0.00.053.531 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.531 I llm_load_print_meta: freq_scale_train = 1
0.00.053.531 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.531 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.532 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.532 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.532 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.532 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.533 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.543 I llm_load_print_meta: model type       = 1.4B
0.00.053.543 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.543 I llm_load_print_meta: model params     = 1.41 B
0.00.053.544 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.544 I llm_load_print_meta: general.name     = 1.4B
0.00.053.544 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.544 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.545 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.545 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.545 I llm_load_print_meta: LF token         = 128 ''
0.00.053.545 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.545 I llm_load_print_meta: max token length = 1024
0.00.055.513 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.514 I llm_load_tensors: offloading output layer to GPU
0.00.055.514 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.524 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.525 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.481 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.482 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.482 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.482 I llama_new_context_with_model: n_batch       = 2048
0.00.056.482 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.483 I llama_new_context_with_model: flash_attn    = 0
0.00.056.483 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.483 I llama_new_context_with_model: freq_scale    = 1
0.00.056.484 I ggml_metal_init: allocating
0.00.056.487 I ggml_metal_init: found device: Apple M4
0.00.056.489 I ggml_metal_init: picking default device: Apple M4
0.00.057.068 I ggml_metal_init: using embedded metal library
0.00.059.551 I ggml_metal_init: GPU name:   Apple M4
0.00.059.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.553 I ggml_metal_init: simdgroup reduction   = true
0.00.059.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.554 I ggml_metal_init: has bfloat            = true
0.00.059.554 I ggml_metal_init: use bfloat            = true
0.00.059.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.549 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.554 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.571 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.649 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.650 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.650 I llama_new_context_with_model: graph nodes  = 967
0.00.091.651 I llama_new_context_with_model: graph splits = 2
0.00.091.664 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.278 I main: llama threadpool init, n_threads = 4
0.00.700.316 I 
0.00.700.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.346 I 
0.00.700.567 I sampler seed: 1234
0.00.700.571 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.603 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.604 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.604 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.557.223 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53787.88 tokens per second)
0.01.557.223 I llama_perf_context_print:        load time =     687.85 ms
0.01.557.224 I llama_perf_context_print: prompt eval time =      55.54 ms /     7 tokens (    7.93 ms per token,   126.04 tokens per second)
0.01.557.225 I llama_perf_context_print:        eval time =     798.64 ms /    63 runs   (   12.68 ms per token,    78.88 tokens per second)
0.01.557.226 I llama_perf_context_print:       total time =     856.95 ms /    70 tokens
0.01.557.429 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.108s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.877 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.511 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.516 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.518 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.519 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.289 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.126 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.128 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.129 I llama_model_loader: - type  f32:  194 tensors
0.00.023.130 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.130 I llama_model_loader: - type q6_K:   37 tensors
0.00.042.821 I llm_load_vocab: special tokens cache size = 25
0.00.048.725 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.728 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.728 I llm_load_print_meta: arch             = gptneox
0.00.048.728 I llm_load_print_meta: vocab type       = BPE
0.00.048.729 I llm_load_print_meta: n_vocab          = 50304
0.00.048.729 I llm_load_print_meta: n_merges         = 50009
0.00.048.729 I llm_load_print_meta: vocab_only       = 0
0.00.048.729 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.729 I llm_load_print_meta: n_embd           = 2048
0.00.048.729 I llm_load_print_meta: n_layer          = 24
0.00.048.732 I llm_load_print_meta: n_head           = 16
0.00.048.733 I llm_load_print_meta: n_head_kv        = 16
0.00.048.739 I llm_load_print_meta: n_rot            = 32
0.00.048.739 I llm_load_print_meta: n_swa            = 0
0.00.048.739 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.739 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.740 I llm_load_print_meta: n_gqa            = 1
0.00.048.741 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.741 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.742 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.743 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.743 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.743 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.745 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.746 I llm_load_print_meta: n_ff             = 8192
0.00.048.746 I llm_load_print_meta: n_expert         = 0
0.00.048.746 I llm_load_print_meta: n_expert_used    = 0
0.00.048.746 I llm_load_print_meta: causal attn      = 1
0.00.048.746 I llm_load_print_meta: pooling type     = 0
0.00.048.747 I llm_load_print_meta: rope type        = 2
0.00.048.748 I llm_load_print_meta: rope scaling     = linear
0.00.048.749 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.749 I llm_load_print_meta: freq_scale_train = 1
0.00.048.749 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.749 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.749 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.749 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.750 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.751 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.751 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.756 I llm_load_print_meta: model type       = 1.4B
0.00.048.756 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.048.756 I llm_load_print_meta: model params     = 1.41 B
0.00.048.757 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.048.757 I llm_load_print_meta: general.name     = 1.4B
0.00.048.757 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.758 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.758 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.758 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.758 I llm_load_print_meta: LF token         = 128 ''
0.00.048.759 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.759 I llm_load_print_meta: max token length = 1024
0.00.050.518 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.518 I llm_load_tensors: offloading output layer to GPU
0.00.050.519 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.524 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.524 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.412 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.413 I llama_new_context_with_model: n_ctx         = 128
0.00.051.413 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.413 I llama_new_context_with_model: n_batch       = 128
0.00.051.413 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.413 I llama_new_context_with_model: flash_attn    = 0
0.00.051.414 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.414 I llama_new_context_with_model: freq_scale    = 1
0.00.051.414 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.415 I ggml_metal_init: allocating
0.00.051.418 I ggml_metal_init: found device: Apple M4
0.00.051.420 I ggml_metal_init: picking default device: Apple M4
0.00.051.989 I ggml_metal_init: using embedded metal library
0.00.054.339 I ggml_metal_init: GPU name:   Apple M4
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.342 I ggml_metal_init: simdgroup reduction   = true
0.00.054.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.342 I ggml_metal_init: has bfloat            = true
0.00.054.342 I ggml_metal_init: use bfloat            = true
0.00.054.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.346 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.203 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.211 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.107 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.108 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.108 I llama_new_context_with_model: graph nodes  = 967
0.00.066.108 I llama_new_context_with_model: graph splits = 2
0.00.066.116 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.649.567 I 
0.00.649.595 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.649.604 I perplexity: tokenizing the input ..
0.00.657.318 I perplexity: tokenization took 7.712 ms
0.00.657.328 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.094 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.799.252 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.799.260 I llama_perf_context_print:        load time =     640.69 ms
0.00.799.261 I llama_perf_context_print: prompt eval time =     140.54 ms /   128 tokens (    1.10 ms per token,   910.77 tokens per second)
0.00.799.262 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.262 I llama_perf_context_print:       total time =     149.69 ms /   129 tokens
0.00.799.744 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.118s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.020.545 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.295 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.028.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.301 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.302 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.303 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.303 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.304 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.305 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.305 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.306 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.309 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.310 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.937 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.919 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.921 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.040.923 I llama_model_loader: - type  f32:  194 tensors
0.00.040.924 I llama_model_loader: - type q6_K:   98 tensors
0.00.076.589 I llm_load_vocab: special tokens cache size = 25
0.00.085.438 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.441 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.441 I llm_load_print_meta: arch             = gptneox
0.00.085.442 I llm_load_print_meta: vocab type       = BPE
0.00.085.442 I llm_load_print_meta: n_vocab          = 50304
0.00.085.442 I llm_load_print_meta: n_merges         = 50009
0.00.085.442 I llm_load_print_meta: vocab_only       = 0
0.00.085.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.443 I llm_load_print_meta: n_embd           = 2048
0.00.085.443 I llm_load_print_meta: n_layer          = 24
0.00.085.446 I llm_load_print_meta: n_head           = 16
0.00.085.447 I llm_load_print_meta: n_head_kv        = 16
0.00.085.461 I llm_load_print_meta: n_rot            = 32
0.00.085.462 I llm_load_print_meta: n_swa            = 0
0.00.085.462 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.462 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.465 I llm_load_print_meta: n_gqa            = 1
0.00.085.466 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.466 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.467 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.467 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.468 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.468 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.468 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.469 I llm_load_print_meta: n_ff             = 8192
0.00.085.469 I llm_load_print_meta: n_expert         = 0
0.00.085.469 I llm_load_print_meta: n_expert_used    = 0
0.00.085.469 I llm_load_print_meta: causal attn      = 1
0.00.085.469 I llm_load_print_meta: pooling type     = 0
0.00.085.469 I llm_load_print_meta: rope type        = 2
0.00.085.470 I llm_load_print_meta: rope scaling     = linear
0.00.085.470 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.472 I llm_load_print_meta: freq_scale_train = 1
0.00.085.472 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.473 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.473 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.473 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.473 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.473 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.474 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.484 I llm_load_print_meta: model type       = 1.4B
0.00.085.484 I llm_load_print_meta: model ftype      = Q6_K
0.00.085.485 I llm_load_print_meta: model params     = 1.41 B
0.00.085.485 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.085.485 I llm_load_print_meta: general.name     = 1.4B
0.00.085.485 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.486 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.486 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.486 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.486 I llm_load_print_meta: LF token         = 128 ''
0.00.085.487 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.487 I llm_load_print_meta: max token length = 1024
0.00.088.023 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.023 I llm_load_tensors: offloading output layer to GPU
0.00.088.023 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.035 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.088.036 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.089.236 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.237 I llama_new_context_with_model: n_ctx         = 2048
0.00.089.237 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.089.238 I llama_new_context_with_model: n_batch       = 2048
0.00.089.238 I llama_new_context_with_model: n_ubatch      = 512
0.00.089.238 I llama_new_context_with_model: flash_attn    = 0
0.00.089.239 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.239 I llama_new_context_with_model: freq_scale    = 1
0.00.089.240 I ggml_metal_init: allocating
0.00.089.247 I ggml_metal_init: found device: Apple M4
0.00.089.249 I ggml_metal_init: picking default device: Apple M4
0.00.090.018 I ggml_metal_init: using embedded metal library
0.00.093.122 I ggml_metal_init: GPU name:   Apple M4
0.00.093.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.125 I ggml_metal_init: simdgroup reduction   = true
0.00.093.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.126 I ggml_metal_init: has bfloat            = true
0.00.093.126 I ggml_metal_init: use bfloat            = true
0.00.093.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.123.840 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.846 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.123.866 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.124.735 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.124.737 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.124.737 I llama_new_context_with_model: graph nodes  = 967
0.00.124.737 I llama_new_context_with_model: graph splits = 2
0.00.124.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.393 I main: llama threadpool init, n_threads = 4
0.00.879.487 I 
0.00.879.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.572 I 
0.00.880.094 I sampler seed: 1234
0.00.880.099 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.880.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.880.172 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.880.172 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.772.418 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.772.419 I llama_perf_context_print:        load time =     858.84 ms
0.01.772.420 I llama_perf_context_print: prompt eval time =      55.27 ms /     7 tokens (    7.90 ms per token,   126.65 tokens per second)
0.01.772.420 I llama_perf_context_print:        eval time =     834.01 ms /    63 runs   (   13.24 ms per token,    75.54 tokens per second)
0.01.772.421 I llama_perf_context_print:       total time =     893.03 ms /    70 tokens
0.01.772.613 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.144s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4280 (3df784b3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.946 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.566 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.566 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.568 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.569 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.571 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.571 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.413 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.266 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.267 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.267 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.268 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.268 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.269 I llama_model_loader: - type  f32:  194 tensors
0.00.024.269 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.951 I llm_load_vocab: special tokens cache size = 25
0.00.051.046 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.049 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.049 I llm_load_print_meta: arch             = gptneox
0.00.051.049 I llm_load_print_meta: vocab type       = BPE
0.00.051.050 I llm_load_print_meta: n_vocab          = 50304
0.00.051.050 I llm_load_print_meta: n_merges         = 50009
0.00.051.050 I llm_load_print_meta: vocab_only       = 0
0.00.051.050 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.050 I llm_load_print_meta: n_embd           = 2048
0.00.051.051 I llm_load_print_meta: n_layer          = 24
0.00.051.053 I llm_load_print_meta: n_head           = 16
0.00.051.054 I llm_load_print_meta: n_head_kv        = 16
0.00.051.065 I llm_load_print_meta: n_rot            = 32
0.00.051.066 I llm_load_print_meta: n_swa            = 0
0.00.051.066 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.066 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.067 I llm_load_print_meta: n_gqa            = 1
0.00.051.068 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.068 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.069 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.069 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.069 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.069 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.070 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.070 I llm_load_print_meta: n_ff             = 8192
0.00.051.071 I llm_load_print_meta: n_expert         = 0
0.00.051.072 I llm_load_print_meta: n_expert_used    = 0
0.00.051.072 I llm_load_print_meta: causal attn      = 1
0.00.051.072 I llm_load_print_meta: pooling type     = 0
0.00.051.072 I llm_load_print_meta: rope type        = 2
0.00.051.072 I llm_load_print_meta: rope scaling     = linear
0.00.051.073 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.073 I llm_load_print_meta: freq_scale_train = 1
0.00.051.073 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.074 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.074 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.074 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.084 I llm_load_print_meta: model type       = 1.4B
0.00.051.084 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.084 I llm_load_print_meta: model params     = 1.41 B
0.00.051.085 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.085 I llm_load_print_meta: general.name     = 1.4B
0.00.051.085 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.086 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.086 I llm_load_print_meta: LF token         = 128 ''
0.00.051.086 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.086 I llm_load_print_meta: max token length = 1024
0.00.053.142 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.142 I llm_load_tensors: offloading output layer to GPU
0.00.053.142 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.153 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.154 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.101 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.102 I llama_new_context_with_model: n_ctx         = 128
0.00.054.103 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.103 I llama_new_context_with_model: n_batch       = 128
0.00.054.103 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.103 I llama_new_context_with_model: flash_attn    = 0
0.00.054.104 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.104 I llama_new_context_with_model: freq_scale    = 1
0.00.054.104 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.105 I ggml_metal_init: allocating
0.00.054.108 I ggml_metal_init: found device: Apple M4
0.00.054.110 I ggml_metal_init: picking default device: Apple M4
0.00.054.670 I ggml_metal_init: using embedded metal library
0.00.057.010 I ggml_metal_init: GPU name:   Apple M4
0.00.057.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.013 I ggml_metal_init: simdgroup reduction   = true
0.00.057.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.013 I ggml_metal_init: has bfloat            = true
0.00.057.013 I ggml_metal_init: use bfloat            = true
0.00.057.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.049 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.053 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.067 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.984 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.985 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.985 I llama_new_context_with_model: graph nodes  = 967
0.00.068.985 I llama_new_context_with_model: graph splits = 2
0.00.068.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.254.080 I 
0.00.254.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.254.121 I perplexity: tokenizing the input ..
0.00.262.196 I perplexity: tokenization took 8.074 ms
0.00.262.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.402.502 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.403.803 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.403.821 I llama_perf_context_print:        load time =     244.13 ms
0.00.403.822 I llama_perf_context_print: prompt eval time =     140.06 ms /   128 tokens (    1.09 ms per token,   913.91 tokens per second)
0.00.403.823 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.403.824 I llama_perf_context_print:       total time =     149.74 ms /   129 tokens
0.00.404.264 I ggml_metal_free: deallocating

real	0m0.419s
user	0m0.078s
sys	0m0.057s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4280 (3df784b3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f60a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f60a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f60ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f60b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f60b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f60bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f60c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f60cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f60d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f60d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f60dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f60dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f60ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f60f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f60fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f6101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f6108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f610ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f611710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f611ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f612600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f612d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f613440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f613ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f614400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f6146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f614cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f615940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f615e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f616140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f6165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f6168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f617130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f617670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f617dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f618270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f618710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f618bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f619050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f6194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f619990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f619e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f61a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f61a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f61aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f61b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f61bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f61c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f61c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f61cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f61d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f61d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f61df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f61e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f61ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f61f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f61f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f61f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f620120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f6203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f620880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f620d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f6211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f621660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f621b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f621fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f622440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f6228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f622d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f623220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f6236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f623b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f6240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f624600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f624b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f6250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f6255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f625b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f626090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f6265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f626b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f627080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f6275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f627b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f628070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f6285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f628b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f629060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f6295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f62a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f62a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f62aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f62b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f62b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f62bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f61b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f62bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f62c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f62cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f62d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f62d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f62dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f62e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f62e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f62ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f62f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f62f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f62fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f630170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f6306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f630c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f6310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f631550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f6319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f632330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f6327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f632c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f633110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f6335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f633a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f633ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f634390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f634830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f634cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f635170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f635610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f635ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f635f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f6363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f636890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f636d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f6371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f637670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f637b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f637fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f638450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f6388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f639230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f6396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f639b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f63a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f63a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f63a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f63adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f63b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f63b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f63bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f63c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f63c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f63c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f63ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f63d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f63d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f63dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f63e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f63e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f63ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f63eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f63f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f63f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f63fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f640130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f6405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f640a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f640f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f6413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f641850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f641cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f642190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f642630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f642ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f642f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f643410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f6438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f643d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f6441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f644690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f644b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f644fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f645470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f645910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f645db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f646250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f6466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f646b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f647030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f6474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f647970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f647e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f648360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f6488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f648e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f649350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f649610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f649c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f64a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f64a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f64b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f64b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f64b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f64bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f64c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f64cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f64d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f64d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f64d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f64e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f64e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f64ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f64f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f64f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f64fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f650110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f650660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f650bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f651100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f651650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f651ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f6520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f652640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f652b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f6530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f653630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f653b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f6540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f654620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f654b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f6550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f655610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f655b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f6560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f656600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f656b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f6570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f6575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f657b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f658090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f6585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f658b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f659080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f6595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f659b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f65a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f65a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f65ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f65b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f65b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f65bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f65c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f65c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f65caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f65d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f65d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f65dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f65e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f65e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f65ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f65f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f65f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f65fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f660010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f660560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f660ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f660f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f6613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f661890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f661d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f6621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f662670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f662b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f662fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f663450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f6638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f663d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f664230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f6646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f664b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f665010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f665560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f665c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f6663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f666ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f6671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f6674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f667c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f667f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f668560 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.140.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f60bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f60c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f60c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f60cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f60d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f60d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f60d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f60ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f60e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f60e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f60eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f60f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f60f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f610150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f611020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f611710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f611e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f6124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f613560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f615590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f615e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f6162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f616750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f616bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f617030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f6174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f617bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f618040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f6184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f618920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f619200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f61a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f61a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f61aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f61b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f61b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f61b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f61be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f61c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f61c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f61cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f61d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f61d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f61d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f61dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f61e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f61e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f61eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f61ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f61f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f61f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f61fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f6200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f620560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f6209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f620e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f6212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f622470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f6228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f6231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f623f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f6247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f6250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f625540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f6259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f626700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f626fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f627450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f6278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f6281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f6297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f62a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f62a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f62a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f62ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f62b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f62b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f62bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f62bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f62c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f62c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f62cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f62d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f62d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f62da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f62ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f62e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f62e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f62ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f62f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f62f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f62f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f62fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f6306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f630b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f630fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f631410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f631880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f631cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f632160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f6325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f632a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f632eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f633c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f6344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f634950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f634dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f6356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f635f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f6363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f6375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f637a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f638300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f638be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f6394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f63a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f63a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f63aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f63af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f63b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f63b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f63bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f63c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f63c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f63ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f63ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f63d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f63d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f63dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f63e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f63e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f63e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f63ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f63f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f63f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f63fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f63ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f6403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f640820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f640c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f641100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f641570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f6419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f6422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f643010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f643480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f643d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f6441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f644640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f644f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f645390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f645800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f6460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f646550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f6469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f646e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f6472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f647b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f648460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f648be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f649050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f6494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f649930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f649da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f64a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f64a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f64aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f64af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f64b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f64b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f64bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f64c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f64c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f64ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f64ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f64d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f64d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f64dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f64e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f64e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f64e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f64ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f64f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f64f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f64fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f64ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f6503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f650820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f650c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f651100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f651570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f6519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f651e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f6522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f652730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f652ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f653010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f653480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f6538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f6541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f654640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f654ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f654f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f655390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f655800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f655c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f6560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f656550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f6569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f656e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f6572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f657710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f657ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f658460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f6588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f658d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f6591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f659620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f659a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f659f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f65a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f65a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f65ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f65b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f65b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f65b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f65be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f65c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f65c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f65cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f65d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f65d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f65e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f65e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f65eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f65f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f65f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f65f8e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f60bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f60c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f60c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f60cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f60d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f60d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f60d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f60ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f60e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f60e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f60eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f60f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f60f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f610150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f611020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f611710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f611e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f6124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f612e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f613560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f615590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f615e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f6162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f616750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f616bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f617030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f6174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f617bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f618040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f6184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f618920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f619200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f61a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f61a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f61aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f61b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f61b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f61b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f61be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f61c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f61c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f61cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f61d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f61d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f61d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f61dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f61e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f61e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f61eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f61ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f61f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f61f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f61fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f6200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f620560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f6209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f620e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f6212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f622470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f6228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f6231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f623f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f6247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f6250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f625540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f6259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f626700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f626fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f627450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f6278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f6281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f6297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f62a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f62a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f62a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f62ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f62b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f62b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f62bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f62bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f62c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f62c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f62cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f62d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f62d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f62da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f62ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f62e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f62e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f62ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f62f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f62f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f62f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f62fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f6306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f630b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f630fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f631410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f631880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f631cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f632160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f6325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f632a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f632eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f633c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f6344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f634950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f634dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f6356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f635f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f6363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f6375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f637a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f638300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f638be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f6394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f63a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f63a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f63aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f63af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f63b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f63b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f63bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f63c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f63c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f63ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f63ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f63d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f63d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f63dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f63e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f63e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f63e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f63ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f63f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f63f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f63fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f63ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f6403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f640820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f640c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f641100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f641570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f6419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f6422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f642730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f643010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f643480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f643d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f6441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f644640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f644f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f645390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f645800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f645c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f6460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f646550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f6469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f646e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f6472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f647b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f647ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f648460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f648be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f649050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f6494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f649930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f649da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f64a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f64a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f64aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f64af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f64b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f64b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f64bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f64c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f64c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f64ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f64ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f64d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f64d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f64dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f64e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f64e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f64e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f64ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f64f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f64f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f64fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f64ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f6503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f650820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f650c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f651100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f651570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f6519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f651e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f6522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f652730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f652ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f653010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f653480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f6538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f653d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f6541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f654640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f654ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f654f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f655390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f655800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f655c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f6560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f656550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f6569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f656e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f6572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f657710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f657ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f658460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f6588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f658d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f6591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f659620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f659a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f659f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f65a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f65a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f65ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f65b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f65b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f65b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f65be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f65c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f65c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f65cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f65d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f65dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f65e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f65e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f65ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f65f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f65f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f65fa50 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.804s
user	0m0.292s
sys	0m0.300s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4280 (3df784b3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c7103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c710af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c7110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c711650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c711c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c7121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c712d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c7132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c7137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c713cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c7141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c715490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c715ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c7163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c716ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c717200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c717920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c7180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c719ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c71a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c71a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c71aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c71bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c71c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c71c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c71c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c71cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c71d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c71d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c71db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c71dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c71e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c71e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c71edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c71f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c71f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c71fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c720040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c7204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c7207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c720db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c721ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c7222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c722900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c722f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c723520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c723b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c724140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c724930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c724dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c725270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c725b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c726330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c7265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c726a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c726f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c7273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c727870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c727d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c7281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c728650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c728af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c728f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c729430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c7298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c729d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c72a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c72a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c72ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c72b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c72b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c72bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c72c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c72c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c72cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c72d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c72d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c72dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c72e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c72e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c72ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c72f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c72f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c72fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c7307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c731250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c7317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c731cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c7219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c732160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c732e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c7333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c733900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c733e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c7343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c7348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c735390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c7358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c735e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c736380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c7368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c736e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c7372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c737760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c737c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c7380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c738540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c7389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c738e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c739320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c7397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c739c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c73a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c73a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c73aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c73aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c73b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c73b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c73bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c73c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c73c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c73caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c73cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c73d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c73d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c73dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c73e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c73e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c73eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c73efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c73f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c73f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c73fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c740220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c7406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c740b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c741000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c7414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c741940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c741de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c742280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c742720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c743060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c743500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c7439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c743e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c7442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c744c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c7450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c745560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c745a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c745ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c746340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c7467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c747120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c7475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c747a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c747f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c7483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c748840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c748ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c749180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c749ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c749f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c74a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c74a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c74ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c74b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c74b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c74bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c74bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c74c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c74c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c74cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c74d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c74d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c74db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c74e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c74e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c74eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c74f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c74f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c74f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c74fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c750440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c750a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c751240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c7516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c7519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c751fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c7525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c753250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c7536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c753b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c754340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c754890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c755330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c755880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c756320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c756870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c756dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c757310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c757860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c757db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c758300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c758da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c7592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c759840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c759d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c75a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c75a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c75ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c75b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c75b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c75bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c75c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c75c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c75cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c75d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c75d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c75dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c75e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c75e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c75ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c75f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c75f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c75fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c760280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c7607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c760d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c761270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c7617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c761d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c762260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c7627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c762d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c763250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c7637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c763cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c764240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c764790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c764ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c765230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c765780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c765cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c766220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c766770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c766cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c767160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c767600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c767aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c767f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c7683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c768880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c768d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c7691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c769660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c769b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c769fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c76a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c76a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c76ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c76b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c76b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c76be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c76c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c76ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c76d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c76d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c76dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c76e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c76e770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.502 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c605bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c606020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c606490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c606900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c606d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c6071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c607650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c607ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c607f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c6083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c608e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c609990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c60a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c60a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c60b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c60b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c60beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c60c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c60cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c60d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c60dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c60e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c60ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c60f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c60f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c60f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c60fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c60ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c610410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c610db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c611220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c6114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c611950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c6126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c612b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c612f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c6133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c613860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c613cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c6145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c614a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c615770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c6164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c6180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c618560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c6189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c6192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c619b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c61a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c61a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c61a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c61ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c61b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c61b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c61baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c61bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c61c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c61c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c61cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c61d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c61d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c61d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c61de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c61e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c61e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c61eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c61efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c61f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c61f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c61fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c6201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c6217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c6220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c6236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c6248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c6255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c6267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c6286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c629cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c62a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c62a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c62aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c62aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c62b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c62b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c62bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c62c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c62c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c62c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c62cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c62d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c62d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c62db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c62df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c62e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c62e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c62ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c62f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c62f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c62fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c62fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c6314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c631930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c631da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c632210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c6333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c634e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c6352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c6364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c636910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c636d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c6371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c637660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c637f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c6383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c638820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c638c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c639100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c639570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c6399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c639e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c63a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c63a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c63aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c63b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c63b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c63b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c63bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c63c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c63cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c63cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c63d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c63d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c63e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c63e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c63e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c63f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c63f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c63fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c63fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c6408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c640d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c6411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c641620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c641bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c642490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c642fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c6432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c643560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c6439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c643e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c6442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c644720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c645000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c645470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c6458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c6461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c646630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c646aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c646f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c647380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c6477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c647c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c6480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c6489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c648e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c649290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c649700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c649b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c649fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c64a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c64a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c64ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c64b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c64b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c64ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c64bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c64c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c64c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c64cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c64d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c64d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c64de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c64e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c64e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c64eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c64efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c64f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c64f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c64fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c650180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c6505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c650a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c650ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c6517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c651c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c652090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c652500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c652970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c652de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c653250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c6536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c653b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c653fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c654410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c654880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c654cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c655160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c6555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c656790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c657670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c657d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c6584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c658bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c659300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c659900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c659f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d8044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d8056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d8063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d8077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d808300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d808ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d8092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d8099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d80a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d80a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d80af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d80b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d80be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d80c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d80cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d80d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d80dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d80dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d80e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d80e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d80e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d80ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d80f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d80f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d80fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d80fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d8102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d810730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d810ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d811010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d811480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d8118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d811d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d8121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d812640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d812ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d812f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d813390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d813800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d813c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d8140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d814550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d8149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d814e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d8152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d815710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d815b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d815ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d816560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d816a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d816ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d817340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d8177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d817c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d818090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d818500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d818970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d818de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d819250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d8196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d819b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d819fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d81a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d81a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d81acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d81b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d81b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d81ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d81beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d81c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d81c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d81cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d81d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d81d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d81d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d81ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d81e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d81e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d81eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d81ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d81f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d81f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d81fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d820140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d8205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d820a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d820e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d821300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d821770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d821be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d822050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d8224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d822930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d822da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d823210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d823680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d823af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d823f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d8243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d824840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d824cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d825120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d825590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d825a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d825e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d8262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d826750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d826bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d827030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d8274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d827910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d827d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d8281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d828660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d828ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d828f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d8293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d829820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d829c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d82a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d82a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d82a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d82ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d82b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d82b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d82bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d82c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d82c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d82c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d82cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d82d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d82d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d82dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d82df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d82e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d82e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d82ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d82f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d82f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d82f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d82fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d8302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d830710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d830b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d830ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d831460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d8318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d831d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d8321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d832620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d832a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d832f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d833370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d8337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d833c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d8340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d834530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d8349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d834e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d835280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d8356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d835b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d835fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d836440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d8368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d836d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d837190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d837600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d837a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d837ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d838350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d8387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d838c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d8390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d839510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d839980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d839df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d83a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d83a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d83ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d83afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d83b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d83b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d83bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d83c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d83c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d83ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d83cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d83d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d83d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d83dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d83e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d83e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d83e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d83edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d83f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d83f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d83fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d83ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d840520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d840990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d840e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d841950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d841c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d841ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d842340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d8427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d842c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d843090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d843500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d843970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d843de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d844250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d8446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d844fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d845410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d845880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d845cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d846160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d8465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d846a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d846eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d847320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d847790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d847c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d848070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d8484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d848950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d848dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d849230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d8496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d849b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d849f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d84a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d84a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d84b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d84b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d84b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d84bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d84c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d84c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d84cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d84cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d84d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d84d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d84dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d84e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d84e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d84e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d84ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d84f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d84f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d84fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d850000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d850470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d8508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d850d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d8511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d851630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d851aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d851f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d852380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d8527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d852c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d8530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d853540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d8539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d853e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d854290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d854700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d854b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d854fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d855450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d8558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d856330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d856a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d857170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d857890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d857b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d857fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d8585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d858bd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.946s
user	0m0.242s
sys	0m0.147s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.55 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.17 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
