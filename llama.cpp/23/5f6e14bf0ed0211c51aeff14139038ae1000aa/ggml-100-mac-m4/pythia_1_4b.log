Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:303 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.658s
user	0m0.690s
sys	0m0.990s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Built target build_info
[  6%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  7%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 13%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 15%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama
[ 22%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-run
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Built target llama-quantize-stats
[ 33%] Built target test-c
[ 33%] Built target llama-run
[ 33%] Built target llama-simple-chat
[ 33%] Linking CXX shared library libllava_shared.dylib
[ 33%] Built target llama-simple
[ 34%] Linking CXX static library libcommon.a
[ 34%] Built target llava_static
[ 34%] Built target common
[ 34%] Built target llava_shared
[ 34%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-chat-template
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-arg-parser
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-sampling
[ 49%] Built target test-chat-template
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../../bin/llama-batched
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-barrier
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Built target test-autorelease
[ 64%] Built target llama-batched-bench
[ 64%] Built target test-backend-ops
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Built target llama-batched
[ 64%] Built target test-json-schema-to-grammar
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-rope
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 69%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-embedding
[ 74%] Built target llama-eval-callback
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-gritlm
[ 74%] Built target llama-lookahead
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-infill
[ 74%] Built target llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Built target llama-lookup
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 78%] Generating loading.html.hpp
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Generating index.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-lookup-stats
[ 84%] Built target llama-lookup-merge
[ 84%] Built target llama-cli
[ 84%] Built target llama-quantize
[ 84%] Built target llama-parallel
[ 84%] Built target llama-lookup-create
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-retrieval
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 89%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 90%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-tokenize
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-save-load-state
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-speculative
[ 96%] Built target llama-speculative-simple
[ 96%] Built target llama-tokenize
[ 96%] Built target llama-cvector-generator
[ 96%] Built target llama-minicpmv-cli
[ 96%] Built target llama-export-lora
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-llava-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Built target llama-q8dot
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-vdot
[100%] Built target llama-server

real	0m2.512s
user	0m5.102s
sys	0m8.831s

main: quantize time =  4455.45 ms
main:    total time =  4455.45 ms

main: quantize time =  1369.90 ms
main:    total time =  1369.90 ms

main: quantize time =  1339.15 ms
main:    total time =  1339.15 ms

main: quantize time =  1636.73 ms
main:    total time =  1636.73 ms

main: quantize time =  3053.44 ms
main:    total time =  3053.44 ms

main: quantize time =  5056.87 ms
main:    total time =  5056.87 ms

main: quantize time =  5619.59 ms
main:    total time =  5619.59 ms

main: quantize time =  6970.70 ms
main:    total time =  6970.70 ms

main: quantize time =  5998.52 ms
main:    total time =  5998.52 ms

main: quantize time =  4536.94 ms
main:    total time =  4536.94 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.135 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.287 I main: llama backend init
0.00.000.298 I main: load the model and apply lora adapter, if any
0.00.043.747 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.055.525 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.055.538 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.055.551 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.055.552 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.055.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.055.553 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.055.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.055.557 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.055.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.055.558 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.055.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.055.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.055.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.055.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.055.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.055.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.055.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.005 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.074.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.074.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.074.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.074.238 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.074.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.074.239 I llama_model_loader: - type  f32:  194 tensors
0.00.074.240 I llama_model_loader: - type  f16:   98 tensors
0.00.104.040 I llm_load_vocab: special tokens cache size = 25
0.00.110.738 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.110.741 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.110.742 I llm_load_print_meta: arch             = gptneox
0.00.110.742 I llm_load_print_meta: vocab type       = BPE
0.00.110.742 I llm_load_print_meta: n_vocab          = 50304
0.00.110.742 I llm_load_print_meta: n_merges         = 50009
0.00.110.742 I llm_load_print_meta: vocab_only       = 0
0.00.110.742 I llm_load_print_meta: n_ctx_train      = 2048
0.00.110.743 I llm_load_print_meta: n_embd           = 2048
0.00.110.743 I llm_load_print_meta: n_layer          = 24
0.00.110.764 I llm_load_print_meta: n_head           = 16
0.00.110.765 I llm_load_print_meta: n_head_kv        = 16
0.00.110.766 I llm_load_print_meta: n_rot            = 32
0.00.110.766 I llm_load_print_meta: n_swa            = 0
0.00.110.766 I llm_load_print_meta: n_embd_head_k    = 128
0.00.110.766 I llm_load_print_meta: n_embd_head_v    = 128
0.00.110.767 I llm_load_print_meta: n_gqa            = 1
0.00.110.767 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.110.768 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.110.768 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.110.769 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.110.769 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.110.769 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.110.769 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.110.770 I llm_load_print_meta: n_ff             = 8192
0.00.110.770 I llm_load_print_meta: n_expert         = 0
0.00.110.770 I llm_load_print_meta: n_expert_used    = 0
0.00.110.770 I llm_load_print_meta: causal attn      = 1
0.00.110.770 I llm_load_print_meta: pooling type     = 0
0.00.110.770 I llm_load_print_meta: rope type        = 2
0.00.110.771 I llm_load_print_meta: rope scaling     = linear
0.00.110.771 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.110.771 I llm_load_print_meta: freq_scale_train = 1
0.00.110.771 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.110.771 I llm_load_print_meta: rope_finetuned   = unknown
0.00.110.772 I llm_load_print_meta: ssm_d_conv       = 0
0.00.110.772 I llm_load_print_meta: ssm_d_inner      = 0
0.00.110.772 I llm_load_print_meta: ssm_d_state      = 0
0.00.110.772 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.110.772 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.110.782 I llm_load_print_meta: model type       = 1.4B
0.00.110.782 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.110.783 I llm_load_print_meta: model params     = 1.41 B
0.00.110.783 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.110.783 I llm_load_print_meta: general.name     = 1.4B
0.00.110.783 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.110.783 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.110.785 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.110.785 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.110.785 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.110.785 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.110.786 I llm_load_print_meta: max token length = 1024
0.00.113.315 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.113.315 I llm_load_tensors: offloading output layer to GPU
0.00.113.315 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.113.334 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.113.335 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.114.292 I llama_new_context_with_model: n_seq_max     = 1
0.00.114.293 I llama_new_context_with_model: n_ctx         = 2048
0.00.114.293 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.114.293 I llama_new_context_with_model: n_batch       = 2048
0.00.114.293 I llama_new_context_with_model: n_ubatch      = 512
0.00.114.293 I llama_new_context_with_model: flash_attn    = 0
0.00.114.294 I llama_new_context_with_model: freq_base     = 10000.0
0.00.114.294 I llama_new_context_with_model: freq_scale    = 1
0.00.114.295 I ggml_metal_init: allocating
0.00.114.302 I ggml_metal_init: found device: Apple M4
0.00.114.308 I ggml_metal_init: picking default device: Apple M4
0.00.114.960 I ggml_metal_init: using embedded metal library
0.00.124.249 I ggml_metal_init: GPU name:   Apple M4
0.00.124.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.124.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.124.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.124.252 I ggml_metal_init: simdgroup reduction   = true
0.00.124.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.124.253 I ggml_metal_init: has bfloat            = true
0.00.124.253 I ggml_metal_init: use bfloat            = true
0.00.124.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.124.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.167.068 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.167.074 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.167.095 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.168.038 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.168.040 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.168.041 I llama_new_context_with_model: graph nodes  = 967
0.00.168.041 I llama_new_context_with_model: graph splits = 2
0.00.168.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.662 I main: llama threadpool init, n_threads = 4
0.00.249.705 I 
0.00.249.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.744 I 
0.00.249.827 I sampler seed: 1234
0.00.249.832 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.249.856 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.249.858 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.249.858 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.091.137 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.02.091.137 I llama_perf_context_print:        load time =     205.90 ms
0.02.091.138 I llama_perf_context_print: prompt eval time =      43.78 ms /     7 tokens (    6.25 ms per token,   159.90 tokens per second)
0.02.091.139 I llama_perf_context_print:        eval time =    1794.55 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.091.139 I llama_perf_context_print:       total time =    1841.48 ms /    70 tokens
0.02.091.345 I ggml_metal_free: deallocating

real	0m2.407s
user	0m0.143s
sys	0m0.104s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.027 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.572 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.573 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.573 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.577 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.579 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.670 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.673 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.674 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.675 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.675 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.675 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.676 I llama_model_loader: - type  f32:  194 tensors
0.00.034.676 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.378 I llm_load_vocab: special tokens cache size = 25
0.00.064.594 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.598 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.598 I llm_load_print_meta: arch             = gptneox
0.00.064.598 I llm_load_print_meta: vocab type       = BPE
0.00.064.599 I llm_load_print_meta: n_vocab          = 50304
0.00.064.599 I llm_load_print_meta: n_merges         = 50009
0.00.064.599 I llm_load_print_meta: vocab_only       = 0
0.00.064.599 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.599 I llm_load_print_meta: n_embd           = 2048
0.00.064.599 I llm_load_print_meta: n_layer          = 24
0.00.064.618 I llm_load_print_meta: n_head           = 16
0.00.064.619 I llm_load_print_meta: n_head_kv        = 16
0.00.064.619 I llm_load_print_meta: n_rot            = 32
0.00.064.619 I llm_load_print_meta: n_swa            = 0
0.00.064.619 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.620 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.620 I llm_load_print_meta: n_gqa            = 1
0.00.064.621 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.621 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.622 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.622 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.623 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.623 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.623 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.624 I llm_load_print_meta: n_ff             = 8192
0.00.064.624 I llm_load_print_meta: n_expert         = 0
0.00.064.624 I llm_load_print_meta: n_expert_used    = 0
0.00.064.624 I llm_load_print_meta: causal attn      = 1
0.00.064.624 I llm_load_print_meta: pooling type     = 0
0.00.064.624 I llm_load_print_meta: rope type        = 2
0.00.064.625 I llm_load_print_meta: rope scaling     = linear
0.00.064.625 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.625 I llm_load_print_meta: freq_scale_train = 1
0.00.064.625 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.628 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.628 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.629 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.629 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.629 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.629 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.639 I llm_load_print_meta: model type       = 1.4B
0.00.064.640 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.640 I llm_load_print_meta: model params     = 1.41 B
0.00.064.641 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.641 I llm_load_print_meta: general.name     = 1.4B
0.00.064.641 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.641 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.641 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.642 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.644 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.644 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.644 I llm_load_print_meta: max token length = 1024
0.00.067.123 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.123 I llm_load_tensors: offloading output layer to GPU
0.00.067.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.134 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.067.135 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.068.124 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.124 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.125 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.125 I llama_new_context_with_model: n_batch       = 2048
0.00.068.125 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.125 I llama_new_context_with_model: flash_attn    = 0
0.00.068.126 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.126 I llama_new_context_with_model: freq_scale    = 1
0.00.068.126 I ggml_metal_init: allocating
0.00.068.129 I ggml_metal_init: found device: Apple M4
0.00.068.131 I ggml_metal_init: picking default device: Apple M4
0.00.068.858 I ggml_metal_init: using embedded metal library
0.00.071.415 I ggml_metal_init: GPU name:   Apple M4
0.00.071.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.418 I ggml_metal_init: simdgroup reduction   = true
0.00.071.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.418 I ggml_metal_init: has bfloat            = true
0.00.071.418 I ggml_metal_init: use bfloat            = true
0.00.071.419 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.780 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.790 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.823 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.050 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.109.052 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.109.053 I llama_new_context_with_model: graph nodes  = 967
0.00.109.053 I llama_new_context_with_model: graph splits = 2
0.00.109.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.472.775 I main: llama threadpool init, n_threads = 4
0.01.472.813 I 
0.01.472.865 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.472.867 I 
0.01.473.104 I sampler seed: 1234
0.01.473.108 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.473.131 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.473.132 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.473.133 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.581.970 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49719.89 tokens per second)
0.02.581.970 I llama_perf_context_print:        load time =    1462.74 ms
0.02.581.972 I llama_perf_context_print: prompt eval time =      49.31 ms /     7 tokens (    7.04 ms per token,   141.94 tokens per second)
0.02.581.973 I llama_perf_context_print:        eval time =    1056.82 ms /    63 runs   (   16.77 ms per token,    59.61 tokens per second)
0.02.581.973 I llama_perf_context_print:       total time =    1109.20 ms /    70 tokens
0.02.582.181 I ggml_metal_free: deallocating

real	0m2.602s
user	0m0.121s
sys	0m0.237s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.014.418 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.400 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.402 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.402 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.402 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.403 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.778 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.779 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.779 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.780 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.780 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.054.781 I llama_model_loader: - type  f32:  194 tensors
0.00.054.781 I llama_model_loader: - type q4_0:   97 tensors
0.00.054.782 I llama_model_loader: - type q6_K:    1 tensors
0.00.096.512 I llm_load_vocab: special tokens cache size = 25
0.00.105.274 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.105.277 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.105.277 I llm_load_print_meta: arch             = gptneox
0.00.105.278 I llm_load_print_meta: vocab type       = BPE
0.00.105.278 I llm_load_print_meta: n_vocab          = 50304
0.00.105.278 I llm_load_print_meta: n_merges         = 50009
0.00.105.279 I llm_load_print_meta: vocab_only       = 0
0.00.105.279 I llm_load_print_meta: n_ctx_train      = 2048
0.00.105.279 I llm_load_print_meta: n_embd           = 2048
0.00.105.279 I llm_load_print_meta: n_layer          = 24
0.00.105.295 I llm_load_print_meta: n_head           = 16
0.00.105.299 I llm_load_print_meta: n_head_kv        = 16
0.00.105.299 I llm_load_print_meta: n_rot            = 32
0.00.105.299 I llm_load_print_meta: n_swa            = 0
0.00.105.299 I llm_load_print_meta: n_embd_head_k    = 128
0.00.105.299 I llm_load_print_meta: n_embd_head_v    = 128
0.00.105.300 I llm_load_print_meta: n_gqa            = 1
0.00.105.301 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.105.303 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.105.303 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.105.304 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.105.304 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.105.304 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.105.304 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.105.305 I llm_load_print_meta: n_ff             = 8192
0.00.105.305 I llm_load_print_meta: n_expert         = 0
0.00.105.306 I llm_load_print_meta: n_expert_used    = 0
0.00.105.308 I llm_load_print_meta: causal attn      = 1
0.00.105.309 I llm_load_print_meta: pooling type     = 0
0.00.105.309 I llm_load_print_meta: rope type        = 2
0.00.105.309 I llm_load_print_meta: rope scaling     = linear
0.00.105.310 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.105.310 I llm_load_print_meta: freq_scale_train = 1
0.00.105.310 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.105.311 I llm_load_print_meta: rope_finetuned   = unknown
0.00.105.311 I llm_load_print_meta: ssm_d_conv       = 0
0.00.105.315 I llm_load_print_meta: ssm_d_inner      = 0
0.00.105.315 I llm_load_print_meta: ssm_d_state      = 0
0.00.105.315 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.105.316 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.105.326 I llm_load_print_meta: model type       = 1.4B
0.00.105.327 I llm_load_print_meta: model ftype      = Q4_0
0.00.105.327 I llm_load_print_meta: model params     = 1.41 B
0.00.105.328 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.105.328 I llm_load_print_meta: general.name     = 1.4B
0.00.105.328 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.105.328 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.105.329 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.105.329 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.105.329 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.105.330 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.105.330 I llm_load_print_meta: max token length = 1024
0.00.107.936 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.936 I llm_load_tensors: offloading output layer to GPU
0.00.107.936 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.948 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.107.949 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.109.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.109.104 I llama_new_context_with_model: n_ctx         = 2048
0.00.109.104 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.109.104 I llama_new_context_with_model: n_batch       = 2048
0.00.109.105 I llama_new_context_with_model: n_ubatch      = 512
0.00.109.105 I llama_new_context_with_model: flash_attn    = 0
0.00.109.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.109.106 I llama_new_context_with_model: freq_scale    = 1
0.00.109.106 I ggml_metal_init: allocating
0.00.109.110 I ggml_metal_init: found device: Apple M4
0.00.109.112 I ggml_metal_init: picking default device: Apple M4
0.00.109.894 I ggml_metal_init: using embedded metal library
0.00.113.122 I ggml_metal_init: GPU name:   Apple M4
0.00.113.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.125 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.125 I ggml_metal_init: simdgroup reduction   = true
0.00.113.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.125 I ggml_metal_init: has bfloat            = true
0.00.113.127 I ggml_metal_init: use bfloat            = true
0.00.113.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.149.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.149.636 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.149.660 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.150.721 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.150.722 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.150.723 I llama_new_context_with_model: graph nodes  = 967
0.00.150.723 I llama_new_context_with_model: graph splits = 2
0.00.150.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.873.951 I main: llama threadpool init, n_threads = 4
0.00.874.042 I 
0.00.874.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.129 I 
0.00.874.590 I sampler seed: 1234
0.00.874.598 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.629 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.631 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.631 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.569.385 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50678.09 tokens per second)
0.01.569.386 I llama_perf_context_print:        load time =     859.52 ms
0.01.569.387 I llama_perf_context_print: prompt eval time =      50.97 ms /     7 tokens (    7.28 ms per token,   137.34 tokens per second)
0.01.569.388 I llama_perf_context_print:        eval time =     640.64 ms /    63 runs   (   10.17 ms per token,    98.34 tokens per second)
0.01.569.391 I llama_perf_context_print:       total time =     695.45 ms /    70 tokens
0.01.569.598 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.159s
sys	0m0.190s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.097 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.443 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.448 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.451 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.451 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.451 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.352 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.354 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.354 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.355 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.355 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.356 I llama_model_loader: - type  f32:  194 tensors
0.00.024.356 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.356 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.493 I llm_load_vocab: special tokens cache size = 25
0.00.051.343 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.345 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.346 I llm_load_print_meta: arch             = gptneox
0.00.051.346 I llm_load_print_meta: vocab type       = BPE
0.00.051.346 I llm_load_print_meta: n_vocab          = 50304
0.00.051.346 I llm_load_print_meta: n_merges         = 50009
0.00.051.347 I llm_load_print_meta: vocab_only       = 0
0.00.051.347 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.347 I llm_load_print_meta: n_embd           = 2048
0.00.051.347 I llm_load_print_meta: n_layer          = 24
0.00.051.356 I llm_load_print_meta: n_head           = 16
0.00.051.357 I llm_load_print_meta: n_head_kv        = 16
0.00.051.357 I llm_load_print_meta: n_rot            = 32
0.00.051.357 I llm_load_print_meta: n_swa            = 0
0.00.051.358 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.358 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.359 I llm_load_print_meta: n_gqa            = 1
0.00.051.359 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.361 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.363 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.363 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.363 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.364 I llm_load_print_meta: n_ff             = 8192
0.00.051.364 I llm_load_print_meta: n_expert         = 0
0.00.051.364 I llm_load_print_meta: n_expert_used    = 0
0.00.051.365 I llm_load_print_meta: causal attn      = 1
0.00.051.365 I llm_load_print_meta: pooling type     = 0
0.00.051.365 I llm_load_print_meta: rope type        = 2
0.00.051.365 I llm_load_print_meta: rope scaling     = linear
0.00.051.366 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.366 I llm_load_print_meta: freq_scale_train = 1
0.00.051.367 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.368 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.368 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.368 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.368 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.368 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.368 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.373 I llm_load_print_meta: model type       = 1.4B
0.00.051.373 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.373 I llm_load_print_meta: model params     = 1.41 B
0.00.051.374 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.374 I llm_load_print_meta: general.name     = 1.4B
0.00.051.374 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.374 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.375 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.375 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.375 I llm_load_print_meta: max token length = 1024
0.00.053.202 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.203 I llm_load_tensors: offloading output layer to GPU
0.00.053.203 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.209 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.209 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.509 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.510 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.510 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.510 I llama_new_context_with_model: n_batch       = 2048
0.00.054.511 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.511 I llama_new_context_with_model: flash_attn    = 0
0.00.054.511 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.512 I llama_new_context_with_model: freq_scale    = 1
0.00.054.512 I ggml_metal_init: allocating
0.00.054.518 I ggml_metal_init: found device: Apple M4
0.00.054.521 I ggml_metal_init: picking default device: Apple M4
0.00.055.155 I ggml_metal_init: using embedded metal library
0.00.057.469 I ggml_metal_init: GPU name:   Apple M4
0.00.057.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.472 I ggml_metal_init: simdgroup reduction   = true
0.00.057.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.472 I ggml_metal_init: has bfloat            = true
0.00.057.472 I ggml_metal_init: use bfloat            = true
0.00.057.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.720 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.727 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.747 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.661 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.662 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.662 I llama_new_context_with_model: graph nodes  = 967
0.00.086.663 I llama_new_context_with_model: graph splits = 2
0.00.086.673 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.830 I main: llama threadpool init, n_threads = 4
0.00.727.871 I 
0.00.727.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.900 I 
0.00.728.068 I sampler seed: 1234
0.00.728.072 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.082 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.082 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.458.058 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64663.02 tokens per second)
0.01.458.059 I llama_perf_context_print:        load time =     718.73 ms
0.01.458.060 I llama_perf_context_print: prompt eval time =      39.53 ms /     7 tokens (    5.65 ms per token,   177.10 tokens per second)
0.01.458.060 I llama_perf_context_print:        eval time =     687.66 ms /    63 runs   (   10.92 ms per token,    91.62 tokens per second)
0.01.458.061 I llama_perf_context_print:       total time =     730.23 ms /    70 tokens
0.01.458.253 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.669 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.967 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.980 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.980 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.981 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.938 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.895 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.895 I llama_model_loader: - type  f32:  194 tensors
0.00.024.896 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.896 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.943 I llm_load_vocab: special tokens cache size = 25
0.00.052.009 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.012 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.012 I llm_load_print_meta: arch             = gptneox
0.00.052.013 I llm_load_print_meta: vocab type       = BPE
0.00.052.013 I llm_load_print_meta: n_vocab          = 50304
0.00.052.013 I llm_load_print_meta: n_merges         = 50009
0.00.052.013 I llm_load_print_meta: vocab_only       = 0
0.00.052.013 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.014 I llm_load_print_meta: n_embd           = 2048
0.00.052.014 I llm_load_print_meta: n_layer          = 24
0.00.052.023 I llm_load_print_meta: n_head           = 16
0.00.052.024 I llm_load_print_meta: n_head_kv        = 16
0.00.052.024 I llm_load_print_meta: n_rot            = 32
0.00.052.024 I llm_load_print_meta: n_swa            = 0
0.00.052.025 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.025 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.025 I llm_load_print_meta: n_gqa            = 1
0.00.052.026 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.030 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.030 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.031 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.031 I llm_load_print_meta: n_ff             = 8192
0.00.052.031 I llm_load_print_meta: n_expert         = 0
0.00.052.032 I llm_load_print_meta: n_expert_used    = 0
0.00.052.035 I llm_load_print_meta: causal attn      = 1
0.00.052.036 I llm_load_print_meta: pooling type     = 0
0.00.052.036 I llm_load_print_meta: rope type        = 2
0.00.052.036 I llm_load_print_meta: rope scaling     = linear
0.00.052.036 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.037 I llm_load_print_meta: freq_scale_train = 1
0.00.052.037 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.038 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.038 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.038 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.038 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.039 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.039 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.043 I llm_load_print_meta: model type       = 1.4B
0.00.052.043 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.044 I llm_load_print_meta: model params     = 1.41 B
0.00.052.044 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.044 I llm_load_print_meta: general.name     = 1.4B
0.00.052.045 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.045 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.045 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.045 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.045 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.045 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.046 I llm_load_print_meta: max token length = 1024
0.00.053.840 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.840 I llm_load_tensors: offloading output layer to GPU
0.00.053.840 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.845 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.846 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.851 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.852 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.852 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.852 I llama_new_context_with_model: n_batch       = 2048
0.00.054.852 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.853 I llama_new_context_with_model: flash_attn    = 0
0.00.054.853 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.853 I llama_new_context_with_model: freq_scale    = 1
0.00.054.854 I ggml_metal_init: allocating
0.00.054.860 I ggml_metal_init: found device: Apple M4
0.00.054.863 I ggml_metal_init: picking default device: Apple M4
0.00.055.482 I ggml_metal_init: using embedded metal library
0.00.057.820 I ggml_metal_init: GPU name:   Apple M4
0.00.057.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.822 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.823 I ggml_metal_init: simdgroup reduction   = true
0.00.057.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.823 I ggml_metal_init: has bfloat            = true
0.00.057.823 I ggml_metal_init: use bfloat            = true
0.00.057.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.826 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.613 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.622 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.640 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.579 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.581 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.581 I llama_new_context_with_model: graph nodes  = 967
0.00.087.581 I llama_new_context_with_model: graph splits = 2
0.00.087.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.765.905 I main: llama threadpool init, n_threads = 4
0.00.765.945 I 
0.00.765.978 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.979 I 
0.00.766.190 I sampler seed: 1234
0.00.766.195 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.226 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.227 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.564.966 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.564.967 I llama_perf_context_print:        load time =     756.23 ms
0.01.564.972 I llama_perf_context_print: prompt eval time =      45.74 ms /     7 tokens (    6.53 ms per token,   153.03 tokens per second)
0.01.564.973 I llama_perf_context_print:        eval time =     750.29 ms /    63 runs   (   11.91 ms per token,    83.97 tokens per second)
0.01.564.973 I llama_perf_context_print:       total time =     799.06 ms /    70 tokens
0.01.565.199 I ggml_metal_free: deallocating

real	0m1.582s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.530 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.934 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.842 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.770 I llama_model_loader: - type  f32:  194 tensors
0.00.024.770 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.770 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.921 I llm_load_vocab: special tokens cache size = 25
0.00.051.980 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.983 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.983 I llm_load_print_meta: arch             = gptneox
0.00.051.984 I llm_load_print_meta: vocab type       = BPE
0.00.051.984 I llm_load_print_meta: n_vocab          = 50304
0.00.051.984 I llm_load_print_meta: n_merges         = 50009
0.00.051.984 I llm_load_print_meta: vocab_only       = 0
0.00.051.984 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.985 I llm_load_print_meta: n_embd           = 2048
0.00.051.985 I llm_load_print_meta: n_layer          = 24
0.00.051.999 I llm_load_print_meta: n_head           = 16
0.00.052.000 I llm_load_print_meta: n_head_kv        = 16
0.00.052.000 I llm_load_print_meta: n_rot            = 32
0.00.052.000 I llm_load_print_meta: n_swa            = 0
0.00.052.000 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.000 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.001 I llm_load_print_meta: n_gqa            = 1
0.00.052.002 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.002 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.003 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.004 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.004 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.005 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.005 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.006 I llm_load_print_meta: n_ff             = 8192
0.00.052.007 I llm_load_print_meta: n_expert         = 0
0.00.052.008 I llm_load_print_meta: n_expert_used    = 0
0.00.052.008 I llm_load_print_meta: causal attn      = 1
0.00.052.008 I llm_load_print_meta: pooling type     = 0
0.00.052.008 I llm_load_print_meta: rope type        = 2
0.00.052.008 I llm_load_print_meta: rope scaling     = linear
0.00.052.008 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.009 I llm_load_print_meta: freq_scale_train = 1
0.00.052.009 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.009 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.009 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.009 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.009 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.010 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.010 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.019 I llm_load_print_meta: model type       = 1.4B
0.00.052.019 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.019 I llm_load_print_meta: model params     = 1.41 B
0.00.052.020 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.020 I llm_load_print_meta: general.name     = 1.4B
0.00.052.020 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.020 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.020 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.021 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.021 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.022 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.023 I llm_load_print_meta: max token length = 1024
0.00.053.685 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.685 I llm_load_tensors: offloading output layer to GPU
0.00.053.686 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.696 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.697 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.558 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.560 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.560 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.560 I llama_new_context_with_model: n_batch       = 2048
0.00.054.560 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.560 I llama_new_context_with_model: flash_attn    = 0
0.00.054.561 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.561 I llama_new_context_with_model: freq_scale    = 1
0.00.054.562 I ggml_metal_init: allocating
0.00.054.571 I ggml_metal_init: found device: Apple M4
0.00.054.574 I ggml_metal_init: picking default device: Apple M4
0.00.055.177 I ggml_metal_init: using embedded metal library
0.00.057.487 I ggml_metal_init: GPU name:   Apple M4
0.00.057.489 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.490 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.490 I ggml_metal_init: simdgroup reduction   = true
0.00.057.490 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.490 I ggml_metal_init: has bfloat            = true
0.00.057.490 I ggml_metal_init: use bfloat            = true
0.00.057.491 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.492 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.998 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.004 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.025 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.019 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.020 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.020 I llama_new_context_with_model: graph nodes  = 967
0.00.088.021 I llama_new_context_with_model: graph splits = 2
0.00.088.035 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.013 I main: llama threadpool init, n_threads = 4
0.00.737.053 I 
0.00.737.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.097 I 
0.00.737.324 I sampler seed: 1234
0.00.737.329 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.357 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.359 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.359 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.575.528 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.575.529 I llama_perf_context_print:        load time =     727.48 ms
0.01.575.530 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.62 tokens per second)
0.01.575.531 I llama_perf_context_print:        eval time =     793.04 ms /    63 runs   (   12.59 ms per token,    79.44 tokens per second)
0.01.575.531 I llama_perf_context_print:       total time =     838.52 ms /    70 tokens
0.01.575.730 I ggml_metal_free: deallocating

real	0m1.593s
user	0m0.112s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.224 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.839 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.849 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.850 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.850 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.851 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.726 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.726 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.749 I llm_load_vocab: special tokens cache size = 25
0.00.051.829 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.831 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.832 I llm_load_print_meta: arch             = gptneox
0.00.051.832 I llm_load_print_meta: vocab type       = BPE
0.00.051.832 I llm_load_print_meta: n_vocab          = 50304
0.00.051.832 I llm_load_print_meta: n_merges         = 50009
0.00.051.832 I llm_load_print_meta: vocab_only       = 0
0.00.051.833 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.833 I llm_load_print_meta: n_embd           = 2048
0.00.051.833 I llm_load_print_meta: n_layer          = 24
0.00.051.848 I llm_load_print_meta: n_head           = 16
0.00.051.848 I llm_load_print_meta: n_head_kv        = 16
0.00.051.849 I llm_load_print_meta: n_rot            = 32
0.00.051.849 I llm_load_print_meta: n_swa            = 0
0.00.051.849 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.849 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.850 I llm_load_print_meta: n_gqa            = 1
0.00.051.851 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.854 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.855 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.855 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.855 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.855 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.855 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.856 I llm_load_print_meta: n_ff             = 8192
0.00.051.856 I llm_load_print_meta: n_expert         = 0
0.00.051.857 I llm_load_print_meta: n_expert_used    = 0
0.00.051.857 I llm_load_print_meta: causal attn      = 1
0.00.051.857 I llm_load_print_meta: pooling type     = 0
0.00.051.857 I llm_load_print_meta: rope type        = 2
0.00.051.857 I llm_load_print_meta: rope scaling     = linear
0.00.051.858 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.858 I llm_load_print_meta: freq_scale_train = 1
0.00.051.858 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.858 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.859 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.860 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.860 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.860 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.860 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.870 I llm_load_print_meta: model type       = 1.4B
0.00.051.870 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.870 I llm_load_print_meta: model params     = 1.41 B
0.00.051.871 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.871 I llm_load_print_meta: general.name     = 1.4B
0.00.051.871 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.871 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.871 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.872 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.872 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.872 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.872 I llm_load_print_meta: max token length = 1024
0.00.053.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.729 I llm_load_tensors: offloading output layer to GPU
0.00.053.730 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.740 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.741 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.658 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.659 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.659 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.659 I llama_new_context_with_model: n_batch       = 2048
0.00.054.659 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.660 I llama_new_context_with_model: flash_attn    = 0
0.00.054.660 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.660 I llama_new_context_with_model: freq_scale    = 1
0.00.054.661 I ggml_metal_init: allocating
0.00.054.664 I ggml_metal_init: found device: Apple M4
0.00.054.666 I ggml_metal_init: picking default device: Apple M4
0.00.055.264 I ggml_metal_init: using embedded metal library
0.00.057.629 I ggml_metal_init: GPU name:   Apple M4
0.00.057.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.632 I ggml_metal_init: simdgroup reduction   = true
0.00.057.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.632 I ggml_metal_init: has bfloat            = true
0.00.057.632 I ggml_metal_init: use bfloat            = true
0.00.057.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.818 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.824 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.846 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.870 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.872 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.872 I llama_new_context_with_model: graph nodes  = 967
0.00.088.872 I llama_new_context_with_model: graph splits = 2
0.00.088.887 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.450.424 I main: llama threadpool init, n_threads = 4
0.00.450.465 I 
0.00.450.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.450.504 I 
0.00.450.740 I sampler seed: 1234
0.00.450.745 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.450.778 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.450.780 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.450.780 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.133.015 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.133.015 I llama_perf_context_print:        load time =     440.19 ms
0.01.133.016 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.87 tokens per second)
0.01.133.017 I llama_perf_context_print:        eval time =     639.55 ms /    63 runs   (   10.15 ms per token,    98.51 tokens per second)
0.01.133.017 I llama_perf_context_print:       total time =     682.60 ms /    70 tokens
0.01.133.213 I ggml_metal_free: deallocating

real	0m1.151s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.136 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.703 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.533 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.449 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.451 I llama_model_loader: - type  f32:  194 tensors
0.00.025.451 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.452 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.452 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.452 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.877 I llm_load_vocab: special tokens cache size = 25
0.00.051.966 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.969 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.969 I llm_load_print_meta: arch             = gptneox
0.00.051.969 I llm_load_print_meta: vocab type       = BPE
0.00.051.970 I llm_load_print_meta: n_vocab          = 50304
0.00.051.970 I llm_load_print_meta: n_merges         = 50009
0.00.051.970 I llm_load_print_meta: vocab_only       = 0
0.00.051.970 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.970 I llm_load_print_meta: n_embd           = 2048
0.00.051.970 I llm_load_print_meta: n_layer          = 24
0.00.051.987 I llm_load_print_meta: n_head           = 16
0.00.051.988 I llm_load_print_meta: n_head_kv        = 16
0.00.051.989 I llm_load_print_meta: n_rot            = 32
0.00.051.989 I llm_load_print_meta: n_swa            = 0
0.00.051.989 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.989 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.990 I llm_load_print_meta: n_gqa            = 1
0.00.051.991 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.991 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.992 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.992 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.993 I llm_load_print_meta: n_ff             = 8192
0.00.051.993 I llm_load_print_meta: n_expert         = 0
0.00.051.994 I llm_load_print_meta: n_expert_used    = 0
0.00.051.994 I llm_load_print_meta: causal attn      = 1
0.00.051.994 I llm_load_print_meta: pooling type     = 0
0.00.051.994 I llm_load_print_meta: rope type        = 2
0.00.051.994 I llm_load_print_meta: rope scaling     = linear
0.00.051.994 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.996 I llm_load_print_meta: freq_scale_train = 1
0.00.051.996 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.997 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.997 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.997 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.997 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.997 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.997 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.007 I llm_load_print_meta: model type       = 1.4B
0.00.052.007 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.007 I llm_load_print_meta: model params     = 1.41 B
0.00.052.008 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.008 I llm_load_print_meta: general.name     = 1.4B
0.00.052.008 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.009 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.009 I llm_load_print_meta: max token length = 1024
0.00.053.901 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.901 I llm_load_tensors: offloading output layer to GPU
0.00.053.901 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.912 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.913 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.839 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.840 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.840 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.840 I llama_new_context_with_model: n_batch       = 2048
0.00.054.841 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.841 I llama_new_context_with_model: flash_attn    = 0
0.00.054.841 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.841 I llama_new_context_with_model: freq_scale    = 1
0.00.054.842 I ggml_metal_init: allocating
0.00.054.845 I ggml_metal_init: found device: Apple M4
0.00.054.847 I ggml_metal_init: picking default device: Apple M4
0.00.055.423 I ggml_metal_init: using embedded metal library
0.00.057.766 I ggml_metal_init: GPU name:   Apple M4
0.00.057.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.768 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.769 I ggml_metal_init: simdgroup reduction   = true
0.00.057.769 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.769 I ggml_metal_init: has bfloat            = true
0.00.057.769 I ggml_metal_init: use bfloat            = true
0.00.057.769 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.770 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.121 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.127 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.147 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.193 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.194 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.195 I llama_new_context_with_model: graph nodes  = 967
0.00.088.195 I llama_new_context_with_model: graph splits = 2
0.00.088.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.538.820 I main: llama threadpool init, n_threads = 4
0.00.538.869 I 
0.00.538.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.538.944 I 
0.00.539.164 I sampler seed: 1234
0.00.539.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.539.208 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.539.213 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.539.213 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.288.293 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.288.293 I llama_perf_context_print:        load time =     527.68 ms
0.01.288.296 I llama_perf_context_print: prompt eval time =      44.44 ms /     7 tokens (    6.35 ms per token,   157.53 tokens per second)
0.01.288.297 I llama_perf_context_print:        eval time =     701.47 ms /    63 runs   (   11.13 ms per token,    89.81 tokens per second)
0.01.288.297 I llama_perf_context_print:       total time =     749.48 ms /    70 tokens
0.01.288.498 I ggml_metal_free: deallocating

real	0m1.304s
user	0m0.109s
sys	0m0.125s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.010.322 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.870 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.870 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.871 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.873 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.799 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.855 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.706 I llama_model_loader: - type  f32:  194 tensors
0.00.024.707 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.707 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.707 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.939 I llm_load_vocab: special tokens cache size = 25
0.00.050.980 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.982 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.983 I llm_load_print_meta: arch             = gptneox
0.00.050.983 I llm_load_print_meta: vocab type       = BPE
0.00.050.983 I llm_load_print_meta: n_vocab          = 50304
0.00.050.983 I llm_load_print_meta: n_merges         = 50009
0.00.050.984 I llm_load_print_meta: vocab_only       = 0
0.00.050.984 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.984 I llm_load_print_meta: n_embd           = 2048
0.00.050.984 I llm_load_print_meta: n_layer          = 24
0.00.050.999 I llm_load_print_meta: n_head           = 16
0.00.051.000 I llm_load_print_meta: n_head_kv        = 16
0.00.051.000 I llm_load_print_meta: n_rot            = 32
0.00.051.000 I llm_load_print_meta: n_swa            = 0
0.00.051.000 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.003 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.004 I llm_load_print_meta: n_gqa            = 1
0.00.051.005 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.005 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.006 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.006 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.006 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.006 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.006 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.007 I llm_load_print_meta: n_ff             = 8192
0.00.051.007 I llm_load_print_meta: n_expert         = 0
0.00.051.007 I llm_load_print_meta: n_expert_used    = 0
0.00.051.008 I llm_load_print_meta: causal attn      = 1
0.00.051.008 I llm_load_print_meta: pooling type     = 0
0.00.051.008 I llm_load_print_meta: rope type        = 2
0.00.051.008 I llm_load_print_meta: rope scaling     = linear
0.00.051.008 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.009 I llm_load_print_meta: freq_scale_train = 1
0.00.051.009 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.009 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.009 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.009 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.009 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.010 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.010 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.019 I llm_load_print_meta: model type       = 1.4B
0.00.051.020 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.020 I llm_load_print_meta: model params     = 1.41 B
0.00.051.020 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.020 I llm_load_print_meta: general.name     = 1.4B
0.00.051.021 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.021 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.021 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.021 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.021 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.022 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.023 I llm_load_print_meta: max token length = 1024
0.00.052.963 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.963 I llm_load_tensors: offloading output layer to GPU
0.00.052.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.974 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.975 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.885 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.886 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.886 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.886 I llama_new_context_with_model: n_batch       = 2048
0.00.053.886 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.886 I llama_new_context_with_model: flash_attn    = 0
0.00.053.887 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.887 I llama_new_context_with_model: freq_scale    = 1
0.00.053.888 I ggml_metal_init: allocating
0.00.053.895 I ggml_metal_init: found device: Apple M4
0.00.053.898 I ggml_metal_init: picking default device: Apple M4
0.00.054.488 I ggml_metal_init: using embedded metal library
0.00.056.809 I ggml_metal_init: GPU name:   Apple M4
0.00.056.810 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.810 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.811 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.811 I ggml_metal_init: simdgroup reduction   = true
0.00.056.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.811 I ggml_metal_init: has bfloat            = true
0.00.056.812 I ggml_metal_init: use bfloat            = true
0.00.056.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.734 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.742 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.759 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.761 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.763 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.763 I llama_new_context_with_model: graph nodes  = 967
0.00.086.764 I llama_new_context_with_model: graph splits = 2
0.00.086.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.185 I main: llama threadpool init, n_threads = 4
0.00.629.227 I 
0.00.629.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.268 I 
0.00.629.506 I sampler seed: 1234
0.00.629.511 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.532 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.532 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.532 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.392.215 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.392.215 I llama_perf_context_print:        load time =     618.86 ms
0.01.392.216 I llama_perf_context_print: prompt eval time =      51.04 ms /     7 tokens (    7.29 ms per token,   137.15 tokens per second)
0.01.392.217 I llama_perf_context_print:        eval time =     708.54 ms /    63 runs   (   11.25 ms per token,    88.92 tokens per second)
0.01.392.217 I llama_perf_context_print:       total time =     763.04 ms /    70 tokens
0.01.392.409 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.604 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.096 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.098 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.099 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.976 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.022.818 I llama_model_loader: - type  f32:  194 tensors
0.00.022.818 I llama_model_loader: - type q5_K:   61 tensors
0.00.022.818 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.257 I llm_load_vocab: special tokens cache size = 25
0.00.049.194 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.196 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.197 I llm_load_print_meta: arch             = gptneox
0.00.049.197 I llm_load_print_meta: vocab type       = BPE
0.00.049.197 I llm_load_print_meta: n_vocab          = 50304
0.00.049.198 I llm_load_print_meta: n_merges         = 50009
0.00.049.198 I llm_load_print_meta: vocab_only       = 0
0.00.049.198 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.198 I llm_load_print_meta: n_embd           = 2048
0.00.049.198 I llm_load_print_meta: n_layer          = 24
0.00.049.213 I llm_load_print_meta: n_head           = 16
0.00.049.214 I llm_load_print_meta: n_head_kv        = 16
0.00.049.214 I llm_load_print_meta: n_rot            = 32
0.00.049.214 I llm_load_print_meta: n_swa            = 0
0.00.049.214 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.214 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.215 I llm_load_print_meta: n_gqa            = 1
0.00.049.216 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.216 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.217 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.217 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.218 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.218 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.218 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.219 I llm_load_print_meta: n_ff             = 8192
0.00.049.219 I llm_load_print_meta: n_expert         = 0
0.00.049.219 I llm_load_print_meta: n_expert_used    = 0
0.00.049.219 I llm_load_print_meta: causal attn      = 1
0.00.049.219 I llm_load_print_meta: pooling type     = 0
0.00.049.222 I llm_load_print_meta: rope type        = 2
0.00.049.222 I llm_load_print_meta: rope scaling     = linear
0.00.049.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.223 I llm_load_print_meta: freq_scale_train = 1
0.00.049.224 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.224 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.224 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.224 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.224 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.234 I llm_load_print_meta: model type       = 1.4B
0.00.049.234 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.235 I llm_load_print_meta: model params     = 1.41 B
0.00.049.235 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.236 I llm_load_print_meta: general.name     = 1.4B
0.00.049.236 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.236 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.236 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.236 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.237 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.237 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.237 I llm_load_print_meta: max token length = 1024
0.00.051.146 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.146 I llm_load_tensors: offloading output layer to GPU
0.00.051.146 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.156 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.158 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.039 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.040 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.040 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.040 I llama_new_context_with_model: n_batch       = 2048
0.00.052.040 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.040 I llama_new_context_with_model: flash_attn    = 0
0.00.052.041 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.041 I llama_new_context_with_model: freq_scale    = 1
0.00.052.042 I ggml_metal_init: allocating
0.00.052.045 I ggml_metal_init: found device: Apple M4
0.00.052.047 I ggml_metal_init: picking default device: Apple M4
0.00.052.618 I ggml_metal_init: using embedded metal library
0.00.054.964 I ggml_metal_init: GPU name:   Apple M4
0.00.054.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.966 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.966 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.966 I ggml_metal_init: simdgroup reduction   = true
0.00.054.967 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.967 I ggml_metal_init: has bfloat            = true
0.00.054.967 I ggml_metal_init: use bfloat            = true
0.00.054.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.968 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.520 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.526 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.543 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.553 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.554 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.554 I llama_new_context_with_model: graph nodes  = 967
0.00.084.554 I llama_new_context_with_model: graph splits = 2
0.00.084.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.877 I main: llama threadpool init, n_threads = 4
0.00.709.919 I 
0.00.709.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.976 I 
0.00.710.218 I sampler seed: 1234
0.00.710.222 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.265 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.265 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.265 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.559.427 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.559.428 I llama_perf_context_print:        load time =     701.27 ms
0.01.559.428 I llama_perf_context_print: prompt eval time =      51.58 ms /     7 tokens (    7.37 ms per token,   135.71 tokens per second)
0.01.559.429 I llama_perf_context_print:        eval time =     794.73 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.559.429 I llama_perf_context_print:       total time =     849.55 ms /    70 tokens
0.01.559.634 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.564 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.150 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.154 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.155 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.155 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.156 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.157 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.172 I llama_model_loader: - type  f32:  194 tensors
0.00.025.173 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.420 I llm_load_vocab: special tokens cache size = 25
0.00.052.464 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.467 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.467 I llm_load_print_meta: arch             = gptneox
0.00.052.468 I llm_load_print_meta: vocab type       = BPE
0.00.052.468 I llm_load_print_meta: n_vocab          = 50304
0.00.052.468 I llm_load_print_meta: n_merges         = 50009
0.00.052.468 I llm_load_print_meta: vocab_only       = 0
0.00.052.468 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.468 I llm_load_print_meta: n_embd           = 2048
0.00.052.469 I llm_load_print_meta: n_layer          = 24
0.00.052.483 I llm_load_print_meta: n_head           = 16
0.00.052.483 I llm_load_print_meta: n_head_kv        = 16
0.00.052.483 I llm_load_print_meta: n_rot            = 32
0.00.052.484 I llm_load_print_meta: n_swa            = 0
0.00.052.484 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.484 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.485 I llm_load_print_meta: n_gqa            = 1
0.00.052.486 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.486 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.487 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.487 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.488 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.488 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.488 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.489 I llm_load_print_meta: n_ff             = 8192
0.00.052.489 I llm_load_print_meta: n_expert         = 0
0.00.052.489 I llm_load_print_meta: n_expert_used    = 0
0.00.052.489 I llm_load_print_meta: causal attn      = 1
0.00.052.490 I llm_load_print_meta: pooling type     = 0
0.00.052.492 I llm_load_print_meta: rope type        = 2
0.00.052.492 I llm_load_print_meta: rope scaling     = linear
0.00.052.493 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.493 I llm_load_print_meta: freq_scale_train = 1
0.00.052.493 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.493 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.494 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.494 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.494 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.494 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.494 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.503 I llm_load_print_meta: model type       = 1.4B
0.00.052.505 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.505 I llm_load_print_meta: model params     = 1.41 B
0.00.052.506 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.506 I llm_load_print_meta: general.name     = 1.4B
0.00.052.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.506 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.506 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.506 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.507 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.507 I llm_load_print_meta: max token length = 1024
0.00.054.558 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.558 I llm_load_tensors: offloading output layer to GPU
0.00.054.558 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.569 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.570 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.496 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.497 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.497 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.497 I llama_new_context_with_model: n_batch       = 2048
0.00.055.497 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.497 I llama_new_context_with_model: flash_attn    = 0
0.00.055.498 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.498 I llama_new_context_with_model: freq_scale    = 1
0.00.055.499 I ggml_metal_init: allocating
0.00.055.505 I ggml_metal_init: found device: Apple M4
0.00.055.508 I ggml_metal_init: picking default device: Apple M4
0.00.056.109 I ggml_metal_init: using embedded metal library
0.00.058.441 I ggml_metal_init: GPU name:   Apple M4
0.00.058.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.443 I ggml_metal_init: simdgroup reduction   = true
0.00.058.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.445 I ggml_metal_init: has bfloat            = true
0.00.058.445 I ggml_metal_init: use bfloat            = true
0.00.058.446 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.193 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.206 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.233 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.211 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.212 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.212 I llama_new_context_with_model: graph nodes  = 967
0.00.088.212 I llama_new_context_with_model: graph splits = 2
0.00.088.226 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.867 I main: llama threadpool init, n_threads = 4
0.00.766.912 I 
0.00.766.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.942 I 
0.00.767.201 I sampler seed: 1234
0.00.767.206 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.243 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.244 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.245 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.648.422 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.648.423 I llama_perf_context_print:        load time =     757.30 ms
0.01.648.423 I llama_perf_context_print: prompt eval time =      54.45 ms /     7 tokens (    7.78 ms per token,   128.57 tokens per second)
0.01.648.424 I llama_perf_context_print:        eval time =     823.88 ms /    63 runs   (   13.08 ms per token,    76.47 tokens per second)
0.01.648.424 I llama_perf_context_print:       total time =     881.56 ms /    70 tokens
0.01.648.615 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.525 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.576 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.845 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.854 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.859 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.860 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.862 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.865 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.224 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.792 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.796 I llama_model_loader: - type  f32:  194 tensors
0.00.054.797 I llama_model_loader: - type  f16:   98 tensors
0.00.084.407 I llm_load_vocab: special tokens cache size = 25
0.00.091.162 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.164 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.165 I llm_load_print_meta: arch             = gptneox
0.00.091.165 I llm_load_print_meta: vocab type       = BPE
0.00.091.165 I llm_load_print_meta: n_vocab          = 50304
0.00.091.165 I llm_load_print_meta: n_merges         = 50009
0.00.091.166 I llm_load_print_meta: vocab_only       = 0
0.00.091.166 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.166 I llm_load_print_meta: n_embd           = 2048
0.00.091.166 I llm_load_print_meta: n_layer          = 24
0.00.091.180 I llm_load_print_meta: n_head           = 16
0.00.091.181 I llm_load_print_meta: n_head_kv        = 16
0.00.091.182 I llm_load_print_meta: n_rot            = 32
0.00.091.182 I llm_load_print_meta: n_swa            = 0
0.00.091.182 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.182 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.183 I llm_load_print_meta: n_gqa            = 1
0.00.091.183 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.184 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.184 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.185 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.185 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.185 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.185 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.186 I llm_load_print_meta: n_ff             = 8192
0.00.091.186 I llm_load_print_meta: n_expert         = 0
0.00.091.186 I llm_load_print_meta: n_expert_used    = 0
0.00.091.186 I llm_load_print_meta: causal attn      = 1
0.00.091.186 I llm_load_print_meta: pooling type     = 0
0.00.091.187 I llm_load_print_meta: rope type        = 2
0.00.091.187 I llm_load_print_meta: rope scaling     = linear
0.00.091.187 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.187 I llm_load_print_meta: freq_scale_train = 1
0.00.091.188 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.188 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.188 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.188 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.188 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.189 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.189 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.198 I llm_load_print_meta: model type       = 1.4B
0.00.091.198 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.199 I llm_load_print_meta: model params     = 1.41 B
0.00.091.199 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.200 I llm_load_print_meta: general.name     = 1.4B
0.00.091.200 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.200 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.202 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.202 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.202 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.202 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.203 I llm_load_print_meta: max token length = 1024
0.00.093.663 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.664 I llm_load_tensors: offloading output layer to GPU
0.00.093.664 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.674 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.675 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.604 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.605 I llama_new_context_with_model: n_ctx         = 128
0.00.094.605 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.605 I llama_new_context_with_model: n_batch       = 128
0.00.094.605 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.606 I llama_new_context_with_model: flash_attn    = 0
0.00.094.606 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.606 I llama_new_context_with_model: freq_scale    = 1
0.00.094.607 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.607 I ggml_metal_init: allocating
0.00.094.614 I ggml_metal_init: found device: Apple M4
0.00.094.616 I ggml_metal_init: picking default device: Apple M4
0.00.095.238 I ggml_metal_init: using embedded metal library
0.00.097.795 I ggml_metal_init: GPU name:   Apple M4
0.00.097.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.798 I ggml_metal_init: simdgroup reduction   = true
0.00.097.798 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.798 I ggml_metal_init: has bfloat            = true
0.00.097.798 I ggml_metal_init: use bfloat            = true
0.00.097.799 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.799 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.046 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.048 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.062 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.925 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.926 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.927 I llama_new_context_with_model: graph nodes  = 967
0.00.109.927 I llama_new_context_with_model: graph splits = 2
0.00.109.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.010.940 I 
0.01.010.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.011.002 I perplexity: tokenizing the input ..
0.01.023.962 I perplexity: tokenization took 12.954 ms
0.01.023.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.145.673 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.147.537 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.147.579 I llama_perf_context_print:        load time =     987.35 ms
0.01.147.581 I llama_perf_context_print: prompt eval time =     120.98 ms /   128 tokens (    0.95 ms per token,  1058.02 tokens per second)
0.01.147.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.147.587 I llama_perf_context_print:       total time =     136.64 ms /   129 tokens
0.01.148.450 I ggml_metal_free: deallocating

real	0m1.340s
user	0m0.127s
sys	0m0.214s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.126 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.642 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.506 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.512 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.514 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.514 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.515 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.515 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.516 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.516 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.517 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.520 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.521 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.521 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.582 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.239 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.240 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.240 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.241 I llama_model_loader: - type  f32:  194 tensors
0.00.034.242 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.741 I llm_load_vocab: special tokens cache size = 25
0.00.068.304 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.307 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.307 I llm_load_print_meta: arch             = gptneox
0.00.068.308 I llm_load_print_meta: vocab type       = BPE
0.00.068.308 I llm_load_print_meta: n_vocab          = 50304
0.00.068.308 I llm_load_print_meta: n_merges         = 50009
0.00.068.308 I llm_load_print_meta: vocab_only       = 0
0.00.068.309 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.309 I llm_load_print_meta: n_embd           = 2048
0.00.068.309 I llm_load_print_meta: n_layer          = 24
0.00.068.324 I llm_load_print_meta: n_head           = 16
0.00.068.324 I llm_load_print_meta: n_head_kv        = 16
0.00.068.324 I llm_load_print_meta: n_rot            = 32
0.00.068.325 I llm_load_print_meta: n_swa            = 0
0.00.068.325 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.325 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.328 I llm_load_print_meta: n_gqa            = 1
0.00.068.328 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.329 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.329 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.330 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.331 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.331 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.331 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.332 I llm_load_print_meta: n_ff             = 8192
0.00.068.332 I llm_load_print_meta: n_expert         = 0
0.00.068.332 I llm_load_print_meta: n_expert_used    = 0
0.00.068.332 I llm_load_print_meta: causal attn      = 1
0.00.068.333 I llm_load_print_meta: pooling type     = 0
0.00.068.333 I llm_load_print_meta: rope type        = 2
0.00.068.333 I llm_load_print_meta: rope scaling     = linear
0.00.068.333 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.333 I llm_load_print_meta: freq_scale_train = 1
0.00.068.334 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.334 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.334 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.334 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.334 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.334 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.334 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.345 I llm_load_print_meta: model type       = 1.4B
0.00.068.345 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.347 I llm_load_print_meta: model params     = 1.41 B
0.00.068.347 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.347 I llm_load_print_meta: general.name     = 1.4B
0.00.068.348 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.348 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.348 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.348 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.348 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.349 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.349 I llm_load_print_meta: max token length = 1024
0.00.070.704 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.704 I llm_load_tensors: offloading output layer to GPU
0.00.070.704 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.715 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.716 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.071.649 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.649 I llama_new_context_with_model: n_ctx         = 128
0.00.071.650 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.071.650 I llama_new_context_with_model: n_batch       = 128
0.00.071.650 I llama_new_context_with_model: n_ubatch      = 128
0.00.071.650 I llama_new_context_with_model: flash_attn    = 0
0.00.071.650 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.651 I llama_new_context_with_model: freq_scale    = 1
0.00.071.651 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.651 I ggml_metal_init: allocating
0.00.071.655 I ggml_metal_init: found device: Apple M4
0.00.071.657 I ggml_metal_init: picking default device: Apple M4
0.00.072.298 I ggml_metal_init: using embedded metal library
0.00.074.893 I ggml_metal_init: GPU name:   Apple M4
0.00.074.894 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.895 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.895 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.895 I ggml_metal_init: simdgroup reduction   = true
0.00.074.895 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.896 I ggml_metal_init: has bfloat            = true
0.00.074.896 I ggml_metal_init: use bfloat            = true
0.00.074.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.085.757 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.771 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.742 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.086.743 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.086.743 I llama_new_context_with_model: graph nodes  = 967
0.00.086.744 I llama_new_context_with_model: graph splits = 2
0.00.086.756 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.958.775 I 
0.00.958.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.958.850 I perplexity: tokenizing the input ..
0.00.977.189 I perplexity: tokenization took 18.338 ms
0.00.977.211 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.118.698 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.120.355 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.120.404 I llama_perf_context_print:        load time =     947.13 ms
0.01.120.405 I llama_perf_context_print: prompt eval time =     140.63 ms /   128 tokens (    1.10 ms per token,   910.20 tokens per second)
0.01.120.407 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.120.408 I llama_perf_context_print:       total time =     161.63 ms /   129 tokens
0.01.121.134 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.114s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.127 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.428 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.438 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.439 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.440 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.440 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.441 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.443 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.444 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.510 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.202 I llama_model_loader: - type  f32:  194 tensors
0.00.033.202 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.203 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.252 I llm_load_vocab: special tokens cache size = 25
0.00.064.122 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.125 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.125 I llm_load_print_meta: arch             = gptneox
0.00.064.126 I llm_load_print_meta: vocab type       = BPE
0.00.064.126 I llm_load_print_meta: n_vocab          = 50304
0.00.064.126 I llm_load_print_meta: n_merges         = 50009
0.00.064.126 I llm_load_print_meta: vocab_only       = 0
0.00.064.127 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.127 I llm_load_print_meta: n_embd           = 2048
0.00.064.127 I llm_load_print_meta: n_layer          = 24
0.00.064.141 I llm_load_print_meta: n_head           = 16
0.00.064.142 I llm_load_print_meta: n_head_kv        = 16
0.00.064.142 I llm_load_print_meta: n_rot            = 32
0.00.064.142 I llm_load_print_meta: n_swa            = 0
0.00.064.143 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.143 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.144 I llm_load_print_meta: n_gqa            = 1
0.00.064.144 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.145 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.146 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.146 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.146 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.146 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.147 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.147 I llm_load_print_meta: n_ff             = 8192
0.00.064.148 I llm_load_print_meta: n_expert         = 0
0.00.064.149 I llm_load_print_meta: n_expert_used    = 0
0.00.064.149 I llm_load_print_meta: causal attn      = 1
0.00.064.150 I llm_load_print_meta: pooling type     = 0
0.00.064.150 I llm_load_print_meta: rope type        = 2
0.00.064.150 I llm_load_print_meta: rope scaling     = linear
0.00.064.150 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.151 I llm_load_print_meta: freq_scale_train = 1
0.00.064.151 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.151 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.151 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.152 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.152 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.152 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.152 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.161 I llm_load_print_meta: model type       = 1.4B
0.00.064.162 I llm_load_print_meta: model ftype      = Q4_0
0.00.064.162 I llm_load_print_meta: model params     = 1.41 B
0.00.064.163 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.064.163 I llm_load_print_meta: general.name     = 1.4B
0.00.064.163 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.163 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.163 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.163 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.164 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.164 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.164 I llm_load_print_meta: max token length = 1024
0.00.066.138 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.138 I llm_load_tensors: offloading output layer to GPU
0.00.066.139 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.149 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.066.150 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.067.075 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.076 I llama_new_context_with_model: n_ctx         = 128
0.00.067.076 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.076 I llama_new_context_with_model: n_batch       = 128
0.00.067.076 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.077 I llama_new_context_with_model: flash_attn    = 0
0.00.067.077 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.077 I llama_new_context_with_model: freq_scale    = 1
0.00.067.078 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.078 I ggml_metal_init: allocating
0.00.067.082 I ggml_metal_init: found device: Apple M4
0.00.067.084 I ggml_metal_init: picking default device: Apple M4
0.00.067.654 I ggml_metal_init: using embedded metal library
0.00.070.155 I ggml_metal_init: GPU name:   Apple M4
0.00.070.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.157 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.157 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.157 I ggml_metal_init: simdgroup reduction   = true
0.00.070.157 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.158 I ggml_metal_init: has bfloat            = true
0.00.070.158 I ggml_metal_init: use bfloat            = true
0.00.070.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.159 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.747 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.749 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.762 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.732 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.733 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.733 I llama_new_context_with_model: graph nodes  = 967
0.00.081.733 I llama_new_context_with_model: graph splits = 2
0.00.081.745 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.457 I 
0.00.646.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.501 I perplexity: tokenizing the input ..
0.00.654.847 I perplexity: tokenization took 8.343 ms
0.00.654.859 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.906 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.778.064 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.778.079 I llama_perf_context_print:        load time =     634.76 ms
0.00.778.080 I llama_perf_context_print: prompt eval time =     121.82 ms /   128 tokens (    0.95 ms per token,  1050.71 tokens per second)
0.00.778.081 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.082 I llama_perf_context_print:       total time =     131.63 ms /   129 tokens
0.00.778.544 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.092s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.887 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.591 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.601 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.602 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.602 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.603 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.604 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.605 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.606 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.608 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.608 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.609 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.594 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.512 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.513 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.514 I llama_model_loader: - type  f32:  194 tensors
0.00.023.514 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.514 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.598 I llm_load_vocab: special tokens cache size = 25
0.00.050.697 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.699 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.700 I llm_load_print_meta: arch             = gptneox
0.00.050.700 I llm_load_print_meta: vocab type       = BPE
0.00.050.700 I llm_load_print_meta: n_vocab          = 50304
0.00.050.700 I llm_load_print_meta: n_merges         = 50009
0.00.050.700 I llm_load_print_meta: vocab_only       = 0
0.00.050.701 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.701 I llm_load_print_meta: n_embd           = 2048
0.00.050.701 I llm_load_print_meta: n_layer          = 24
0.00.050.716 I llm_load_print_meta: n_head           = 16
0.00.050.716 I llm_load_print_meta: n_head_kv        = 16
0.00.050.717 I llm_load_print_meta: n_rot            = 32
0.00.050.717 I llm_load_print_meta: n_swa            = 0
0.00.050.717 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.717 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.718 I llm_load_print_meta: n_gqa            = 1
0.00.050.719 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.722 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.722 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.723 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.724 I llm_load_print_meta: n_ff             = 8192
0.00.050.724 I llm_load_print_meta: n_expert         = 0
0.00.050.724 I llm_load_print_meta: n_expert_used    = 0
0.00.050.724 I llm_load_print_meta: causal attn      = 1
0.00.050.724 I llm_load_print_meta: pooling type     = 0
0.00.050.724 I llm_load_print_meta: rope type        = 2
0.00.050.724 I llm_load_print_meta: rope scaling     = linear
0.00.050.725 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.725 I llm_load_print_meta: freq_scale_train = 1
0.00.050.725 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.725 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.725 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.726 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.726 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.726 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.726 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.735 I llm_load_print_meta: model type       = 1.4B
0.00.050.736 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.736 I llm_load_print_meta: model params     = 1.41 B
0.00.050.737 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.737 I llm_load_print_meta: general.name     = 1.4B
0.00.050.737 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.737 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.737 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.737 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.739 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: max token length = 1024
0.00.052.738 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.738 I llm_load_tensors: offloading output layer to GPU
0.00.052.738 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.748 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.749 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.659 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.660 I llama_new_context_with_model: n_ctx         = 128
0.00.053.660 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.660 I llama_new_context_with_model: n_batch       = 128
0.00.053.660 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.660 I llama_new_context_with_model: flash_attn    = 0
0.00.053.661 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.661 I llama_new_context_with_model: freq_scale    = 1
0.00.053.661 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.662 I ggml_metal_init: allocating
0.00.053.668 I ggml_metal_init: found device: Apple M4
0.00.053.670 I ggml_metal_init: picking default device: Apple M4
0.00.054.243 I ggml_metal_init: using embedded metal library
0.00.056.557 I ggml_metal_init: GPU name:   Apple M4
0.00.056.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.559 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.559 I ggml_metal_init: simdgroup reduction   = true
0.00.056.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.560 I ggml_metal_init: has bfloat            = true
0.00.056.560 I ggml_metal_init: use bfloat            = true
0.00.056.560 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.110 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.120 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.138 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.985 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.986 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.986 I llama_new_context_with_model: graph nodes  = 967
0.00.067.986 I llama_new_context_with_model: graph splits = 2
0.00.067.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.588 I 
0.00.673.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.632 I perplexity: tokenizing the input ..
0.00.681.702 I perplexity: tokenization took 8.068 ms
0.00.681.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.568 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.805.830 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.805.844 I llama_perf_context_print:        load time =     664.70 ms
0.00.805.845 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.07 tokens per second)
0.00.805.846 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.847 I llama_perf_context_print:       total time =     132.26 ms /   129 tokens
0.00.806.237 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.013 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.629 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.634 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.635 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.635 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.636 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.636 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.637 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.637 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.637 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.638 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.638 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.639 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.640 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.795 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.795 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.796 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.796 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.797 I llama_model_loader: - type  f32:  194 tensors
0.00.024.797 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.797 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.674 I llm_load_vocab: special tokens cache size = 25
0.00.051.707 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.710 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.710 I llm_load_print_meta: arch             = gptneox
0.00.051.711 I llm_load_print_meta: vocab type       = BPE
0.00.051.711 I llm_load_print_meta: n_vocab          = 50304
0.00.051.711 I llm_load_print_meta: n_merges         = 50009
0.00.051.711 I llm_load_print_meta: vocab_only       = 0
0.00.051.711 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.712 I llm_load_print_meta: n_embd           = 2048
0.00.051.712 I llm_load_print_meta: n_layer          = 24
0.00.051.721 I llm_load_print_meta: n_head           = 16
0.00.051.722 I llm_load_print_meta: n_head_kv        = 16
0.00.051.722 I llm_load_print_meta: n_rot            = 32
0.00.051.723 I llm_load_print_meta: n_swa            = 0
0.00.051.723 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.723 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.724 I llm_load_print_meta: n_gqa            = 1
0.00.051.724 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.725 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.726 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.726 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.726 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.727 I llm_load_print_meta: n_ff             = 8192
0.00.051.727 I llm_load_print_meta: n_expert         = 0
0.00.051.728 I llm_load_print_meta: n_expert_used    = 0
0.00.051.728 I llm_load_print_meta: causal attn      = 1
0.00.051.728 I llm_load_print_meta: pooling type     = 0
0.00.051.728 I llm_load_print_meta: rope type        = 2
0.00.051.728 I llm_load_print_meta: rope scaling     = linear
0.00.051.729 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.729 I llm_load_print_meta: freq_scale_train = 1
0.00.051.729 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.729 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.729 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.730 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.730 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.730 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.730 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.735 I llm_load_print_meta: model type       = 1.4B
0.00.051.735 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.735 I llm_load_print_meta: model params     = 1.41 B
0.00.051.737 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.738 I llm_load_print_meta: general.name     = 1.4B
0.00.051.738 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.738 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.738 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.738 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.739 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.739 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.743 I llm_load_print_meta: max token length = 1024
0.00.053.481 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.481 I llm_load_tensors: offloading output layer to GPU
0.00.053.482 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.487 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.487 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.340 I llama_new_context_with_model: n_ctx         = 128
0.00.054.340 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.340 I llama_new_context_with_model: n_batch       = 128
0.00.054.341 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.341 I llama_new_context_with_model: flash_attn    = 0
0.00.054.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.341 I llama_new_context_with_model: freq_scale    = 1
0.00.054.342 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.342 I ggml_metal_init: allocating
0.00.054.348 I ggml_metal_init: found device: Apple M4
0.00.054.350 I ggml_metal_init: picking default device: Apple M4
0.00.054.891 I ggml_metal_init: using embedded metal library
0.00.057.198 I ggml_metal_init: GPU name:   Apple M4
0.00.057.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.201 I ggml_metal_init: simdgroup reduction   = true
0.00.057.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.201 I ggml_metal_init: has bfloat            = true
0.00.057.201 I ggml_metal_init: use bfloat            = true
0.00.057.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.728 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.736 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.752 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.571 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.572 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.572 I llama_new_context_with_model: graph nodes  = 967
0.00.068.573 I llama_new_context_with_model: graph splits = 2
0.00.068.580 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.436 I 
0.00.704.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.487 I perplexity: tokenizing the input ..
0.00.712.679 I perplexity: tokenization took 8.191 ms
0.00.712.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.270 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.848.606 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.848.620 I llama_perf_context_print:        load time =     694.42 ms
0.00.848.621 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.71 tokens per second)
0.00.848.622 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.622 I llama_perf_context_print:       total time =     144.19 ms /   129 tokens
0.00.849.148 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.078s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.066 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.902 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.906 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.914 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.916 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.916 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.917 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.919 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.919 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.919 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.921 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.922 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.922 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.756 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.817 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.818 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.819 I llama_model_loader: - type  f32:  194 tensors
0.00.023.819 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.541 I llm_load_vocab: special tokens cache size = 25
0.00.050.565 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.568 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.568 I llm_load_print_meta: arch             = gptneox
0.00.050.569 I llm_load_print_meta: vocab type       = BPE
0.00.050.569 I llm_load_print_meta: n_vocab          = 50304
0.00.050.569 I llm_load_print_meta: n_merges         = 50009
0.00.050.569 I llm_load_print_meta: vocab_only       = 0
0.00.050.569 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.570 I llm_load_print_meta: n_embd           = 2048
0.00.050.570 I llm_load_print_meta: n_layer          = 24
0.00.050.585 I llm_load_print_meta: n_head           = 16
0.00.050.587 I llm_load_print_meta: n_head_kv        = 16
0.00.050.587 I llm_load_print_meta: n_rot            = 32
0.00.050.587 I llm_load_print_meta: n_swa            = 0
0.00.050.587 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.588 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.588 I llm_load_print_meta: n_gqa            = 1
0.00.050.589 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.589 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.590 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.590 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.590 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.591 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.591 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.592 I llm_load_print_meta: n_ff             = 8192
0.00.050.592 I llm_load_print_meta: n_expert         = 0
0.00.050.592 I llm_load_print_meta: n_expert_used    = 0
0.00.050.592 I llm_load_print_meta: causal attn      = 1
0.00.050.592 I llm_load_print_meta: pooling type     = 0
0.00.050.592 I llm_load_print_meta: rope type        = 2
0.00.050.592 I llm_load_print_meta: rope scaling     = linear
0.00.050.593 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.593 I llm_load_print_meta: freq_scale_train = 1
0.00.050.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.593 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.593 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.593 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.603 I llm_load_print_meta: model type       = 1.4B
0.00.050.604 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.604 I llm_load_print_meta: model params     = 1.41 B
0.00.050.605 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.605 I llm_load_print_meta: general.name     = 1.4B
0.00.050.605 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.605 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.605 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.605 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.606 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.606 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.606 I llm_load_print_meta: max token length = 1024
0.00.052.621 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.621 I llm_load_tensors: offloading output layer to GPU
0.00.052.622 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.632 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.633 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.569 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.569 I llama_new_context_with_model: n_ctx         = 128
0.00.053.570 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.570 I llama_new_context_with_model: n_batch       = 128
0.00.053.570 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.570 I llama_new_context_with_model: flash_attn    = 0
0.00.053.571 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.571 I llama_new_context_with_model: freq_scale    = 1
0.00.053.571 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.572 I ggml_metal_init: allocating
0.00.053.575 I ggml_metal_init: found device: Apple M4
0.00.053.577 I ggml_metal_init: picking default device: Apple M4
0.00.054.195 I ggml_metal_init: using embedded metal library
0.00.056.526 I ggml_metal_init: GPU name:   Apple M4
0.00.056.527 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.528 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.528 I ggml_metal_init: simdgroup reduction   = true
0.00.056.529 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.529 I ggml_metal_init: has bfloat            = true
0.00.056.529 I ggml_metal_init: use bfloat            = true
0.00.056.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.063 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.079 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.008 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.009 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.009 I llama_new_context_with_model: graph nodes  = 967
0.00.069.009 I llama_new_context_with_model: graph splits = 2
0.00.069.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.645 I 
0.00.668.696 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.700 I perplexity: tokenizing the input ..
0.00.676.525 I perplexity: tokenization took 7.823 ms
0.00.676.536 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.615 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.812.799 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.812.821 I llama_perf_context_print:        load time =     659.57 ms
0.00.812.822 I llama_perf_context_print: prompt eval time =     134.85 ms /   128 tokens (    1.05 ms per token,   949.20 tokens per second)
0.00.812.827 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.828 I llama_perf_context_print:       total time =     144.18 ms /   129 tokens
0.00.813.345 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.608 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.303 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.303 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.304 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.304 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.304 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.305 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.306 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.045 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.063 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.883 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.884 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.885 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.885 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.885 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.885 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.886 I llama_model_loader: - type  f32:  194 tensors
0.00.023.886 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.886 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.886 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.821 I llm_load_vocab: special tokens cache size = 25
0.00.049.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.711 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.711 I llm_load_print_meta: arch             = gptneox
0.00.049.711 I llm_load_print_meta: vocab type       = BPE
0.00.049.712 I llm_load_print_meta: n_vocab          = 50304
0.00.049.712 I llm_load_print_meta: n_merges         = 50009
0.00.049.712 I llm_load_print_meta: vocab_only       = 0
0.00.049.712 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.712 I llm_load_print_meta: n_embd           = 2048
0.00.049.712 I llm_load_print_meta: n_layer          = 24
0.00.049.726 I llm_load_print_meta: n_head           = 16
0.00.049.727 I llm_load_print_meta: n_head_kv        = 16
0.00.049.727 I llm_load_print_meta: n_rot            = 32
0.00.049.728 I llm_load_print_meta: n_swa            = 0
0.00.049.728 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.729 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.729 I llm_load_print_meta: n_gqa            = 1
0.00.049.730 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.731 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.731 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.732 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.734 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.734 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.734 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.734 I llm_load_print_meta: n_ff             = 8192
0.00.049.735 I llm_load_print_meta: n_expert         = 0
0.00.049.735 I llm_load_print_meta: n_expert_used    = 0
0.00.049.735 I llm_load_print_meta: causal attn      = 1
0.00.049.735 I llm_load_print_meta: pooling type     = 0
0.00.049.735 I llm_load_print_meta: rope type        = 2
0.00.049.735 I llm_load_print_meta: rope scaling     = linear
0.00.049.736 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.736 I llm_load_print_meta: freq_scale_train = 1
0.00.049.737 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.737 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.737 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.737 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.738 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.738 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.738 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.746 I llm_load_print_meta: model type       = 1.4B
0.00.049.747 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.747 I llm_load_print_meta: model params     = 1.41 B
0.00.049.747 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.748 I llm_load_print_meta: general.name     = 1.4B
0.00.049.748 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.748 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.748 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.749 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.749 I llm_load_print_meta: max token length = 1024
0.00.051.275 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.275 I llm_load_tensors: offloading output layer to GPU
0.00.051.275 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.285 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.286 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.154 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.155 I llama_new_context_with_model: n_ctx         = 128
0.00.052.155 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.155 I llama_new_context_with_model: n_batch       = 128
0.00.052.155 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.156 I llama_new_context_with_model: flash_attn    = 0
0.00.052.156 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.156 I llama_new_context_with_model: freq_scale    = 1
0.00.052.157 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.157 I ggml_metal_init: allocating
0.00.052.160 I ggml_metal_init: found device: Apple M4
0.00.052.162 I ggml_metal_init: picking default device: Apple M4
0.00.052.699 I ggml_metal_init: using embedded metal library
0.00.055.000 I ggml_metal_init: GPU name:   Apple M4
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.002 I ggml_metal_init: simdgroup reduction   = true
0.00.055.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.003 I ggml_metal_init: has bfloat            = true
0.00.055.003 I ggml_metal_init: use bfloat            = true
0.00.055.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.737 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.739 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.751 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.670 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.671 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.671 I llama_new_context_with_model: graph nodes  = 967
0.00.066.671 I llama_new_context_with_model: graph splits = 2
0.00.066.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.393.369 I 
0.00.393.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.393.425 I perplexity: tokenizing the input ..
0.00.401.472 I perplexity: tokenization took 8.045 ms
0.00.401.483 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.533.884 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.535.062 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.535.081 I llama_perf_context_print:        load time =     383.75 ms
0.00.535.082 I llama_perf_context_print: prompt eval time =     132.16 ms /   128 tokens (    1.03 ms per token,   968.50 tokens per second)
0.00.535.083 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.535.086 I llama_perf_context_print:       total time =     141.72 ms /   129 tokens
0.00.535.674 I ggml_metal_free: deallocating

real	0m0.551s
user	0m0.078s
sys	0m0.074s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.740 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.274 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.285 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.289 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.290 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.169 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.075 I llama_model_loader: - type  f32:  194 tensors
0.00.023.076 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.076 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.076 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.076 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.136 I llm_load_vocab: special tokens cache size = 25
0.00.049.037 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.040 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.040 I llm_load_print_meta: arch             = gptneox
0.00.049.040 I llm_load_print_meta: vocab type       = BPE
0.00.049.041 I llm_load_print_meta: n_vocab          = 50304
0.00.049.041 I llm_load_print_meta: n_merges         = 50009
0.00.049.041 I llm_load_print_meta: vocab_only       = 0
0.00.049.041 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.041 I llm_load_print_meta: n_embd           = 2048
0.00.049.042 I llm_load_print_meta: n_layer          = 24
0.00.049.056 I llm_load_print_meta: n_head           = 16
0.00.049.059 I llm_load_print_meta: n_head_kv        = 16
0.00.049.059 I llm_load_print_meta: n_rot            = 32
0.00.049.060 I llm_load_print_meta: n_swa            = 0
0.00.049.060 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.060 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.061 I llm_load_print_meta: n_gqa            = 1
0.00.049.062 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.062 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.063 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.063 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.063 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.063 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.063 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.064 I llm_load_print_meta: n_ff             = 8192
0.00.049.064 I llm_load_print_meta: n_expert         = 0
0.00.049.064 I llm_load_print_meta: n_expert_used    = 0
0.00.049.064 I llm_load_print_meta: causal attn      = 1
0.00.049.065 I llm_load_print_meta: pooling type     = 0
0.00.049.065 I llm_load_print_meta: rope type        = 2
0.00.049.066 I llm_load_print_meta: rope scaling     = linear
0.00.049.066 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.066 I llm_load_print_meta: freq_scale_train = 1
0.00.049.067 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.067 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.067 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.068 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.068 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.068 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.068 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.078 I llm_load_print_meta: model type       = 1.4B
0.00.049.078 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.078 I llm_load_print_meta: model params     = 1.41 B
0.00.049.079 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.079 I llm_load_print_meta: general.name     = 1.4B
0.00.049.079 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.079 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.080 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.080 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.080 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.080 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.080 I llm_load_print_meta: max token length = 1024
0.00.051.000 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.000 I llm_load_tensors: offloading output layer to GPU
0.00.051.001 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.011 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.012 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.880 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.881 I llama_new_context_with_model: n_ctx         = 128
0.00.051.881 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.881 I llama_new_context_with_model: n_batch       = 128
0.00.051.881 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.881 I llama_new_context_with_model: flash_attn    = 0
0.00.051.882 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.882 I llama_new_context_with_model: freq_scale    = 1
0.00.051.882 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.883 I ggml_metal_init: allocating
0.00.051.889 I ggml_metal_init: found device: Apple M4
0.00.051.891 I ggml_metal_init: picking default device: Apple M4
0.00.052.485 I ggml_metal_init: using embedded metal library
0.00.054.828 I ggml_metal_init: GPU name:   Apple M4
0.00.054.830 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.831 I ggml_metal_init: simdgroup reduction   = true
0.00.054.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.831 I ggml_metal_init: has bfloat            = true
0.00.054.831 I ggml_metal_init: use bfloat            = true
0.00.054.832 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.832 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.558 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.420 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.421 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.422 I llama_new_context_with_model: graph nodes  = 967
0.00.066.422 I llama_new_context_with_model: graph splits = 2
0.00.066.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.481.358 I 
0.00.481.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.481.398 I perplexity: tokenizing the input ..
0.00.489.522 I perplexity: tokenization took 8.122 ms
0.00.489.536 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.622.053 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.623.330 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.623.349 I llama_perf_context_print:        load time =     472.61 ms
0.00.623.350 I llama_perf_context_print: prompt eval time =     132.29 ms /   128 tokens (    1.03 ms per token,   967.58 tokens per second)
0.00.623.351 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.351 I llama_perf_context_print:       total time =     141.99 ms /   129 tokens
0.00.623.824 I ggml_metal_free: deallocating

real	0m0.639s
user	0m0.077s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.218 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.774 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.778 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.781 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.781 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.783 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.783 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.783 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.784 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.784 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.787 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.712 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.768 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.768 I llama_model_loader: - type  f32:  194 tensors
0.00.023.769 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.769 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.769 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.043 I llm_load_vocab: special tokens cache size = 25
0.00.050.001 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.004 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.004 I llm_load_print_meta: arch             = gptneox
0.00.050.005 I llm_load_print_meta: vocab type       = BPE
0.00.050.005 I llm_load_print_meta: n_vocab          = 50304
0.00.050.005 I llm_load_print_meta: n_merges         = 50009
0.00.050.005 I llm_load_print_meta: vocab_only       = 0
0.00.050.006 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.006 I llm_load_print_meta: n_embd           = 2048
0.00.050.006 I llm_load_print_meta: n_layer          = 24
0.00.050.020 I llm_load_print_meta: n_head           = 16
0.00.050.021 I llm_load_print_meta: n_head_kv        = 16
0.00.050.021 I llm_load_print_meta: n_rot            = 32
0.00.050.021 I llm_load_print_meta: n_swa            = 0
0.00.050.021 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.021 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.022 I llm_load_print_meta: n_gqa            = 1
0.00.050.023 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.023 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.024 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.024 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.025 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.025 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.025 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.026 I llm_load_print_meta: n_ff             = 8192
0.00.050.026 I llm_load_print_meta: n_expert         = 0
0.00.050.027 I llm_load_print_meta: n_expert_used    = 0
0.00.050.027 I llm_load_print_meta: causal attn      = 1
0.00.050.027 I llm_load_print_meta: pooling type     = 0
0.00.050.028 I llm_load_print_meta: rope type        = 2
0.00.050.028 I llm_load_print_meta: rope scaling     = linear
0.00.050.028 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.028 I llm_load_print_meta: freq_scale_train = 1
0.00.050.028 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.031 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.031 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.031 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.040 I llm_load_print_meta: model type       = 1.4B
0.00.050.041 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.041 I llm_load_print_meta: model params     = 1.41 B
0.00.050.041 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.042 I llm_load_print_meta: general.name     = 1.4B
0.00.050.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.043 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.043 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.043 I llm_load_print_meta: max token length = 1024
0.00.051.988 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.988 I llm_load_tensors: offloading output layer to GPU
0.00.051.988 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.999 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.000 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.939 I llama_new_context_with_model: n_ctx         = 128
0.00.052.939 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.939 I llama_new_context_with_model: n_batch       = 128
0.00.052.939 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.940 I llama_new_context_with_model: flash_attn    = 0
0.00.052.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.940 I llama_new_context_with_model: freq_scale    = 1
0.00.052.941 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.941 I ggml_metal_init: allocating
0.00.052.944 I ggml_metal_init: found device: Apple M4
0.00.052.946 I ggml_metal_init: picking default device: Apple M4
0.00.053.482 I ggml_metal_init: using embedded metal library
0.00.055.782 I ggml_metal_init: GPU name:   Apple M4
0.00.055.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.784 I ggml_metal_init: simdgroup reduction   = true
0.00.055.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.784 I ggml_metal_init: has bfloat            = true
0.00.055.784 I ggml_metal_init: use bfloat            = true
0.00.055.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.785 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.527 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.531 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.546 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.444 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.445 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.445 I llama_new_context_with_model: graph nodes  = 967
0.00.067.446 I llama_new_context_with_model: graph splits = 2
0.00.067.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.566.552 I 
0.00.566.589 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.566.592 I perplexity: tokenizing the input ..
0.00.574.800 I perplexity: tokenization took 8.206 ms
0.00.574.811 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.708.990 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.710.171 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.710.187 I llama_perf_context_print:        load time =     557.33 ms
0.00.710.188 I llama_perf_context_print: prompt eval time =     133.95 ms /   128 tokens (    1.05 ms per token,   955.56 tokens per second)
0.00.710.189 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.710.190 I llama_perf_context_print:       total time =     143.64 ms /   129 tokens
0.00.710.722 I ggml_metal_free: deallocating

real	0m0.726s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.290 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.325 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.327 I llama_model_loader: - type  f32:  194 tensors
0.00.023.327 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.328 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.201 I llm_load_vocab: special tokens cache size = 25
0.00.050.116 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.119 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.119 I llm_load_print_meta: arch             = gptneox
0.00.050.120 I llm_load_print_meta: vocab type       = BPE
0.00.050.120 I llm_load_print_meta: n_vocab          = 50304
0.00.050.120 I llm_load_print_meta: n_merges         = 50009
0.00.050.120 I llm_load_print_meta: vocab_only       = 0
0.00.050.120 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.120 I llm_load_print_meta: n_embd           = 2048
0.00.050.121 I llm_load_print_meta: n_layer          = 24
0.00.050.129 I llm_load_print_meta: n_head           = 16
0.00.050.129 I llm_load_print_meta: n_head_kv        = 16
0.00.050.129 I llm_load_print_meta: n_rot            = 32
0.00.050.130 I llm_load_print_meta: n_swa            = 0
0.00.050.130 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.130 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.131 I llm_load_print_meta: n_gqa            = 1
0.00.050.131 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.132 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.133 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.133 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.133 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.134 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.134 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.135 I llm_load_print_meta: n_ff             = 8192
0.00.050.135 I llm_load_print_meta: n_expert         = 0
0.00.050.135 I llm_load_print_meta: n_expert_used    = 0
0.00.050.135 I llm_load_print_meta: causal attn      = 1
0.00.050.135 I llm_load_print_meta: pooling type     = 0
0.00.050.135 I llm_load_print_meta: rope type        = 2
0.00.050.135 I llm_load_print_meta: rope scaling     = linear
0.00.050.137 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.138 I llm_load_print_meta: freq_scale_train = 1
0.00.050.138 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.140 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.140 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.140 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.141 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.141 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.141 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.145 I llm_load_print_meta: model type       = 1.4B
0.00.050.146 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.146 I llm_load_print_meta: model params     = 1.41 B
0.00.050.146 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.147 I llm_load_print_meta: general.name     = 1.4B
0.00.050.147 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.147 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.148 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.149 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.149 I llm_load_print_meta: max token length = 1024
0.00.052.069 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.069 I llm_load_tensors: offloading output layer to GPU
0.00.052.069 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.075 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.075 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.144 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.145 I llama_new_context_with_model: n_ctx         = 128
0.00.053.145 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.146 I llama_new_context_with_model: n_batch       = 128
0.00.053.146 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.146 I llama_new_context_with_model: flash_attn    = 0
0.00.053.146 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.146 I llama_new_context_with_model: freq_scale    = 1
0.00.053.147 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.147 I ggml_metal_init: allocating
0.00.053.150 I ggml_metal_init: found device: Apple M4
0.00.053.152 I ggml_metal_init: picking default device: Apple M4
0.00.053.716 I ggml_metal_init: using embedded metal library
0.00.056.072 I ggml_metal_init: GPU name:   Apple M4
0.00.056.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.074 I ggml_metal_init: simdgroup reduction   = true
0.00.056.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.076 I ggml_metal_init: has bfloat            = true
0.00.056.076 I ggml_metal_init: use bfloat            = true
0.00.056.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.183 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.185 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.199 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.181 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.182 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.182 I llama_new_context_with_model: graph nodes  = 967
0.00.068.183 I llama_new_context_with_model: graph splits = 2
0.00.068.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.344 I 
0.00.643.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.376 I perplexity: tokenizing the input ..
0.00.651.451 I perplexity: tokenization took 8.074 ms
0.00.651.465 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.060 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.226 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.242 I llama_perf_context_print:        load time =     634.71 ms
0.00.793.245 I llama_perf_context_print: prompt eval time =     140.36 ms /   128 tokens (    1.10 ms per token,   911.94 tokens per second)
0.00.793.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.247 I llama_perf_context_print:       total time =     149.90 ms /   129 tokens
0.00.793.649 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.079s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.178 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.914 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.919 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.921 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.922 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.922 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.923 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.923 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.923 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.924 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.924 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.835 I llama_model_loader: - type  f32:  194 tensors
0.00.023.835 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.066 I llm_load_vocab: special tokens cache size = 25
0.00.050.118 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.120 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.121 I llm_load_print_meta: arch             = gptneox
0.00.050.121 I llm_load_print_meta: vocab type       = BPE
0.00.050.121 I llm_load_print_meta: n_vocab          = 50304
0.00.050.121 I llm_load_print_meta: n_merges         = 50009
0.00.050.122 I llm_load_print_meta: vocab_only       = 0
0.00.050.122 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.122 I llm_load_print_meta: n_embd           = 2048
0.00.050.122 I llm_load_print_meta: n_layer          = 24
0.00.050.137 I llm_load_print_meta: n_head           = 16
0.00.050.137 I llm_load_print_meta: n_head_kv        = 16
0.00.050.138 I llm_load_print_meta: n_rot            = 32
0.00.050.139 I llm_load_print_meta: n_swa            = 0
0.00.050.139 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.139 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.140 I llm_load_print_meta: n_gqa            = 1
0.00.050.141 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.141 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.142 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.142 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.142 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.143 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.143 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.144 I llm_load_print_meta: n_ff             = 8192
0.00.050.144 I llm_load_print_meta: n_expert         = 0
0.00.050.144 I llm_load_print_meta: n_expert_used    = 0
0.00.050.144 I llm_load_print_meta: causal attn      = 1
0.00.050.144 I llm_load_print_meta: pooling type     = 0
0.00.050.144 I llm_load_print_meta: rope type        = 2
0.00.050.145 I llm_load_print_meta: rope scaling     = linear
0.00.050.145 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.145 I llm_load_print_meta: freq_scale_train = 1
0.00.050.146 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.146 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.146 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.146 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.146 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.146 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.148 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.158 I llm_load_print_meta: model type       = 1.4B
0.00.050.158 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.158 I llm_load_print_meta: model params     = 1.41 B
0.00.050.159 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.159 I llm_load_print_meta: general.name     = 1.4B
0.00.050.159 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.159 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.159 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.159 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.160 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.160 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.160 I llm_load_print_meta: max token length = 1024
0.00.051.801 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.801 I llm_load_tensors: offloading output layer to GPU
0.00.051.801 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.811 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.812 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.697 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.698 I llama_new_context_with_model: n_ctx         = 128
0.00.052.698 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.698 I llama_new_context_with_model: n_batch       = 128
0.00.052.698 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.698 I llama_new_context_with_model: flash_attn    = 0
0.00.052.699 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.699 I llama_new_context_with_model: freq_scale    = 1
0.00.052.699 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.700 I ggml_metal_init: allocating
0.00.052.703 I ggml_metal_init: found device: Apple M4
0.00.052.705 I ggml_metal_init: picking default device: Apple M4
0.00.053.242 I ggml_metal_init: using embedded metal library
0.00.055.559 I ggml_metal_init: GPU name:   Apple M4
0.00.055.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.561 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.561 I ggml_metal_init: simdgroup reduction   = true
0.00.055.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.562 I ggml_metal_init: has bfloat            = true
0.00.055.562 I ggml_metal_init: use bfloat            = true
0.00.055.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.563 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.342 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.237 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.239 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.239 I llama_new_context_with_model: graph nodes  = 967
0.00.067.239 I llama_new_context_with_model: graph splits = 2
0.00.067.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.259.890 I 
0.00.259.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.259.976 I perplexity: tokenizing the input ..
0.00.268.275 I perplexity: tokenization took 8.298 ms
0.00.268.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.409.104 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.410.378 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.410.400 I llama_perf_context_print:        load time =     250.71 ms
0.00.410.401 I llama_perf_context_print: prompt eval time =     140.55 ms /   128 tokens (    1.10 ms per token,   910.73 tokens per second)
0.00.410.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.410.403 I llama_perf_context_print:       total time =     150.51 ms /   129 tokens
0.00.410.833 I ggml_metal_free: deallocating

real	0m0.426s
user	0m0.078s
sys	0m0.058s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.235 I build: 4307 (235f6e14) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.139 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.450 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.460 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.474 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.728 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.463 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.470 I llama_model_loader: - type  f32:  194 tensors
0.00.052.476 I llama_model_loader: - type  f16:   98 tensors
0.00.081.125 I llm_load_vocab: special tokens cache size = 25
0.00.087.688 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.691 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.691 I llm_load_print_meta: arch             = gptneox
0.00.087.692 I llm_load_print_meta: vocab type       = BPE
0.00.087.692 I llm_load_print_meta: n_vocab          = 50304
0.00.087.692 I llm_load_print_meta: n_merges         = 50009
0.00.087.692 I llm_load_print_meta: vocab_only       = 0
0.00.087.692 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.692 I llm_load_print_meta: n_embd           = 2048
0.00.087.693 I llm_load_print_meta: n_layer          = 24
0.00.087.707 I llm_load_print_meta: n_head           = 16
0.00.087.708 I llm_load_print_meta: n_head_kv        = 16
0.00.087.708 I llm_load_print_meta: n_rot            = 32
0.00.087.708 I llm_load_print_meta: n_swa            = 0
0.00.087.708 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.708 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.709 I llm_load_print_meta: n_gqa            = 1
0.00.087.710 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.710 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.711 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.711 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.711 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.711 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.712 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.712 I llm_load_print_meta: n_ff             = 8192
0.00.087.712 I llm_load_print_meta: n_expert         = 0
0.00.087.712 I llm_load_print_meta: n_expert_used    = 0
0.00.087.713 I llm_load_print_meta: causal attn      = 1
0.00.087.713 I llm_load_print_meta: pooling type     = 0
0.00.087.713 I llm_load_print_meta: rope type        = 2
0.00.087.715 I llm_load_print_meta: rope scaling     = linear
0.00.087.715 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.716 I llm_load_print_meta: freq_scale_train = 1
0.00.087.717 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.717 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.717 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.717 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.717 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.717 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.718 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.728 I llm_load_print_meta: model type       = 1.4B
0.00.087.728 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.087.729 I llm_load_print_meta: model params     = 1.41 B
0.00.087.729 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.087.729 I llm_load_print_meta: general.name     = 1.4B
0.00.087.730 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.730 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.730 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.730 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.731 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.731 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.731 I llm_load_print_meta: max token length = 1024
0.00.090.228 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.228 I llm_load_tensors: offloading output layer to GPU
0.00.090.228 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.238 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.239 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.091.141 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.142 I llama_new_context_with_model: n_ctx         = 128
0.00.091.142 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.091.142 I llama_new_context_with_model: n_batch       = 128
0.00.091.142 I llama_new_context_with_model: n_ubatch      = 128
0.00.091.142 I llama_new_context_with_model: flash_attn    = 0
0.00.091.143 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.143 I llama_new_context_with_model: freq_scale    = 1
0.00.091.143 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.144 I ggml_metal_init: allocating
0.00.091.147 I ggml_metal_init: found device: Apple M4
0.00.091.149 I ggml_metal_init: picking default device: Apple M4
0.00.091.745 I ggml_metal_init: using embedded metal library
0.00.094.189 I ggml_metal_init: GPU name:   Apple M4
0.00.094.191 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.191 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.192 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.192 I ggml_metal_init: simdgroup reduction   = true
0.00.094.192 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.192 I ggml_metal_init: has bfloat            = true
0.00.094.192 I ggml_metal_init: use bfloat            = true
0.00.094.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.193 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.487 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.489 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.502 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.437 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.438 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.439 I llama_new_context_with_model: graph nodes  = 967
0.00.105.439 I llama_new_context_with_model: graph splits = 2
0.00.105.451 I 
0.00.105.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.484 I compute_imatrix: tokenizing the input ..
0.00.112.475 I compute_imatrix: tokenization took 6.99 ms
0.00.112.477 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.664.354 I compute_imatrix: 1.55 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.666.989 I llama_perf_context_print:        load time =    1642.21 ms
0.01.666.991 I llama_perf_context_print: prompt eval time =    1551.22 ms /   128 tokens (   12.12 ms per token,    82.52 tokens per second)
0.01.666.992 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.666.994 I llama_perf_context_print:       total time =    1644.84 ms /   129 tokens
0.01.667.622 I ggml_metal_free: deallocating

real	0m1.855s
user	0m0.168s
sys	0m0.255s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4307 (235f6e14)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13660a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13660a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13660aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13660b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13660ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13660bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13660c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13660cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13660d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13660d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13660daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13660dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13660eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13660f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13660fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1366101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136611030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136611750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136611f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136613480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136613d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136614440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136614700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136615980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136615ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136616180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1366168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136617170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1366176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136617e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1366182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136618750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136619090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1366199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136619e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13661a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13661a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13661abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13661b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13661bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13661c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13661c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13661cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13661d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13661d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13661df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13661e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13661ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13661f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13661f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13661f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136620160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136620420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1366208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136620d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1366216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136621b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136622480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136622920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136623260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136623700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136623ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1366240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136624b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1366250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136625630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136625b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1366260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136626620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1366270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136627610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1366280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136628b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1366290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1366295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136629b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13662a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13662a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13662ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13662b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13662b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13662bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13661b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13662bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13662c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13662cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13662d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13662d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13662dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13662e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13662e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13662ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13662f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13662f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13662fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1366301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136630700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136630c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1366310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136631a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136631ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136632370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136632810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136632cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1366335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136633a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136633f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1366343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136634870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1366351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136635af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136635f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1366368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136636d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1366376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136637b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136638490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136638930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136638dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136639270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136639710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136639bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13663a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13663a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13663a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13663ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13663b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13663b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13663bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13663c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13663c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13663c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13663ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13663d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13663d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13663dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13663e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13663e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13663ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13663eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13663f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13663f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13663fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136640170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136640f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1366413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136641890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136641d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1366421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136642670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136642fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136643450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1366438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136643d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136644230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1366446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136644b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136645010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1366454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136645950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136645df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136646290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136646bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136647070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1366479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136647e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1366483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1366488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136648e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136649650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136649c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13664a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13664a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13664b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13664b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13664b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13664bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13664c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13664cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13664d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13664d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13664d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13664e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13664e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13664ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13664f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13664f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13664fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136650150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1366506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136650bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136651690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136651be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136652130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136652680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136652bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136653670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136653bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136654110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136654bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136655ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1366560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136656640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136656b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1366570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136657630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136657b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1366580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136658620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136658b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1366590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136659610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136659b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13665a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13665a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13665ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13665b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13665b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13665bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13665c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13665c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13665cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13665d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13665d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13665db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13665e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13665e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13665eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13665f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13665f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13665fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136660050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1366605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136661430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1366618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136661d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136662210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1366626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136662b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136662ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136663490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136663930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136663dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136664270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136664710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136664bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136665050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1366655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136665cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1366663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136666b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1366674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136667cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136667f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1366685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104d06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104d06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104d069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104d06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104d072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104d07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104d07ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104d04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104d044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104d04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104d08010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104d085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104d09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104d098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104d0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104d0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104d0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104d0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104d0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104d0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104d0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104d0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104d0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104d0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104d0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104d0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104d0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104d0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104d0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104d105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104d10a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104d10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104d115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104d11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104d11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104d12240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104d126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104d12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104d13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104d134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104d13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104d13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104d142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104d14740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104d14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104d15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104d15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104d15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104d16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104d16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104d16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104d17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104d17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104d18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104d18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104d18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104d191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104d19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104d19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104d1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104d1a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104d1abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104d1b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104d1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104d1b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104d1be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104d1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104d1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104d1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104d1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104d1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104d1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104d1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104d1e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104d1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104d1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104d1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104d1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104d1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104d203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104d20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104d20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104d213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104d21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104d21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104d223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104d22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104d22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104d233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104d238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104d23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104d24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104d248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104d24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104d25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104d258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104d25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104d26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104d268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104d26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104d27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104d278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104d27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104d28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104d288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104d28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104d29340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104d29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104d29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104d2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104d2a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104d2add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104d2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104d2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104d2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104d2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104d2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104d2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104d2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104d2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104d2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104d2dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104d2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104d2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104d2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104d2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104d2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104d2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104d2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104d301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104d30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104d30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104d30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104d31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104d318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104d31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104d32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104d326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104d32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104d33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104d334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104d33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104d33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104d34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104d34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104d34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104d35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104d35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104d359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104d35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104d362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104d36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104d36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104d370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104d37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104d37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104d37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104d38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104d387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104d38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104d39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104d395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104d39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104d39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104d3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104d3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104d3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104d3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104d3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104d3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104d3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104d3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104d3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104d3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104d3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104d3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104d3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104d3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104d3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104d3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104d3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104d3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104d3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104d3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104d40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104d404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104d40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104d40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104d412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104d41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104d41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104d42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104d42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104d42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104d42fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104d43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104d43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104d43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104d44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104d44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104d44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104d45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104d45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104d45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104d464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104d46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104d472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104d47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104d47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104d48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104d48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104d48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104d492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104d49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104d49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104d4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104d4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104d4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104d4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104d4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104d4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104d4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104d4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104d4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104d4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104d4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104d4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104d4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104d4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104d4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104d4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104d4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104d4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104d50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104d507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104d50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104d51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104d517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104d51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104d52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104d527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104d52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104d53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104d53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104d53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104d54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104d54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104d54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104d55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104d55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104d55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104d56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104d56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104d56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104d57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104d57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104d57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104d581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104d58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104d58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104d591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104d59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104d59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104d5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104d5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104d5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104d5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104d5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104d5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104d5bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104d5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104d5c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104d5cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104d5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104d5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104d5db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104d5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104d5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104d5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104d5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104d5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104d5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104d5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104d60390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104d60ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104d611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104d618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104d61bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104d623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104d62660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104d62c70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13660d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13660dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13660e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13660e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13660e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13660ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13660f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13660f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13660fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13660ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1366103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1366109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136611a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1366121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1366128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136612fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1366136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136613db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136614730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136614e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136615510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136615c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1366162f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1366169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136616e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1366172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136617730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136617ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136618010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1366188f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136618d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136619020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136619490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136619900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136619d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13661a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13661a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13661aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13661af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13661b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13661b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13661bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13661c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13661c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13661c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13661ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13661d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13661d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13661db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13661e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13661e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13661e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13661ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13661f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13661f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13661faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13661ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136620380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1366207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136620c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1366210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136621540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1366219b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136621e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136622290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136622700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136622b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136622fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1366238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136623d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1366241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136624610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136624a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136624ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136625360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1366257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1366260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136626520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136626e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136627270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1366276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136627b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136627fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136628430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1366288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136628d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136629180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1366295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136629a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136629ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13662a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13662a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13662ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13662b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13662b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13662b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13662bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13662c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13662c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13662cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13662cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13662d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13662d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13662dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13662e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13662e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13662ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13662eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13662f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13662f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13662fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136630070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1366304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136630950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136630dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136631230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1366316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136631b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136631f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1366323f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136633140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1366335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136633a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136633e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136634300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136634770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136634be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136635050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1366354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136635da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136636210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136636680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136636f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1366373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136637840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136637cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136638120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136638a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1366392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136639750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136639bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13663a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13663a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13663a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13663ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13663b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13663b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13663bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13663bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13663c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13663c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13663cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13663d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13663d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13663d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13663de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13663e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13663e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13663eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13663f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13663f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13663f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13663fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1366401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136640640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136640ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136640f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136641390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136641800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136641c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1366420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136642550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1366429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136642e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1366432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136643710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136643b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136643ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136644460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1366448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136644d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1366451b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136645620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136645a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136645f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136646370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1366467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1366470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1366479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136647e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136648280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1366486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136648b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136648fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136649440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1366498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136649d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13664a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13664a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13664ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13664b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13664b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13664bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13664bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13664c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13664c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13664cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13664d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13664d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13664d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13664de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13664e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13664e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13664eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13664f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13664f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13664f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13664fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1366501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136650640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136650ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136650f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136651390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136651800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136651c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1366520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136652550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1366529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136652e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1366532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136653710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136653b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136653ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136654460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1366548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136654d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1366551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136655620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136655a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136655f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136656370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1366567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136656c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1366570c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136657530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1366579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136657e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136658280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1366586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136658b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136658fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136659440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1366598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136659d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13665a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13665a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13665aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13665aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13665b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13665b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13665bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13665c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13665c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13665c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13665cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13665d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13665d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13665db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13665dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13665e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13665ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13665f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13665fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136660150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1366605c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136660a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136660ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136661310 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.830s
user	0m0.293s
sys	0m0.310s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4307 (235f6e14)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c60aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c60b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c60b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c60bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c60c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c60c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c60ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c60d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c60d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c60deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c60e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c60e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c60f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c60fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c610390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c610ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c6111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c6118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c612010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c6127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c612f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c613620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c613d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c6145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c614d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c6155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c616240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c616780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c616a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c616ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c6171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c618230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c6186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c618b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c619010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c6194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c619950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c619df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c61a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c61a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c61abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c61ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c61b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c61bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c61c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c61c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c61cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c61d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c61dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c61e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c61e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c61f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c61f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c61f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c61fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c620230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c620a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c620ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c621180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c621620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c621ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c621f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c622400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c6228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c622d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c6231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c623680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c623b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c624460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c6249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c624f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c625450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c6259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c625ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c626440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c626990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c626ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c627430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c627980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c627ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c628420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c628970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c628ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c629960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c629eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c62a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c62a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c62aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c62b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c62b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c62be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c62c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c61c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c62c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c62d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c62d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c62daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c62dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c62e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c62ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c62efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c62f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c62fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c62ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c630520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c630a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c630fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c631510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c6319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c6322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c632790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c632c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c6330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c633570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c633a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c633eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c634350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c6347f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c635130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c6355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c635a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c635f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c6363b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c637630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c637f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c638410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c6388b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c638d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c6391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c639690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c639b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c639fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c63a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c63a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c63adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c63b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c63b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c63bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c63c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c63c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c63c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c63ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c63d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c63d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c63dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c63e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c63e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c63e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c63ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c63f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c63f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c63fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c6400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c640590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c640a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c640ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c641370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c641810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c641cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c642150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c6425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c642a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c642f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c6433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c643870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c643d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c6441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c644650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c644f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c645430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c6458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c645d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c646210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c6466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c646b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c646ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c647490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c647930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c647dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c648270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c648710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c648c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c6491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c649700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c649c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c64a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c64ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c64b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c64b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c64bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c64c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c64c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c64ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c64d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c64d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c64dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c64e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c64ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c64ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c64f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c64fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c64ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c6504c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c650a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c650f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c6514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c651a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c651f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c6524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c6529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c652f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c653490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c6539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c653f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c654480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c6549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c654f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c655470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c6559c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c655f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c656460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c6569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c656f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c657450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c6579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c657ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c658440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c658990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c658ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c659430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c659980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c659ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c65a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c65a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c65aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c65b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c65b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c65beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c65c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c65c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c65cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c65d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c65d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c65de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c65e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c65e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c65ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c65f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c65f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c65fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c6603c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c660910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c660e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c6613b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c661850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c661cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c662190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c662630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c662ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c662f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c663410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c6638b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c663d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c6641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c664690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c664b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c664fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c665470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c665910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c665e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c666580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c666ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c6673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c667ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c668590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c668850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c668e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11fc04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11fc04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11fc05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11fc058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11fc05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11fc06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11fc065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11fc06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11fc06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11fc07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11fc077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11fc07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11fc089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11fc09170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11fc09980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11fc0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11fc0a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11fc0aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11fc0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11fc0bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11fc0c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11fc0cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11fc0d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11fc0d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11fc0e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11fc0e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11fc0e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11fc0eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11fc0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11fc0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11fc0f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11fc0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11fc101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11fc10470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11fc108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11fc10d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11fc111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11fc11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11fc11aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11fc11f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11fc12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11fc127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11fc12c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11fc130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11fc13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11fc139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11fc13e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11fc14290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11fc14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11fc14b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11fc14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11fc15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11fc158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11fc15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11fc161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11fc16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11fc16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11fc17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11fc174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11fc17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11fc17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11fc18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11fc186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11fc18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11fc18f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11fc19400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11fc19870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11fc19ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11fc1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11fc1a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11fc1aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11fc1aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11fc1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11fc1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11fc1bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11fc1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11fc1c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11fc1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11fc1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11fc1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11fc1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11fc1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11fc1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11fc1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11fc1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11fc1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11fc1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11fc1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11fc1fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11fc1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11fc202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11fc20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11fc20bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11fc21040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11fc214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11fc21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11fc21d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11fc22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11fc22670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11fc22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11fc22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11fc233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11fc23830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11fc23ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11fc24110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11fc24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11fc249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11fc24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11fc252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11fc25740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11fc25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11fc26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11fc26490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11fc26900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11fc26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11fc271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11fc27650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11fc27ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11fc27f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11fc283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11fc28810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11fc28c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11fc290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11fc29560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11fc299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11fc29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11fc2a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11fc2a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11fc2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11fc2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11fc2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11fc2b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11fc2bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11fc2c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11fc2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11fc2caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11fc2cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11fc2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11fc2d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11fc2dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11fc2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11fc2e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11fc2e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11fc2ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11fc2f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11fc2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11fc2fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11fc2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11fc30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11fc308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11fc30d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11fc311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11fc31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11fc31a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11fc31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11fc32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11fc327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11fc32c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11fc330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11fc33520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11fc33990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11fc33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11fc34270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11fc346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11fc34b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11fc34fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11fc35430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11fc358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11fc35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11fc36180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11fc365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11fc36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11fc36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11fc37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11fc377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11fc37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11fc38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11fc38500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11fc38970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11fc38de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11fc39250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11fc396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11fc39b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11fc39fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11fc3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11fc3a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11fc3acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11fc3b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11fc3b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11fc3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11fc3beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11fc3c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11fc3c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11fc3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11fc3d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11fc3d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11fc3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11fc3ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11fc3e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11fc3e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11fc3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11fc3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11fc3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11fc3f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11fc3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11fc40140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11fc405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11fc40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11fc40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11fc41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11fc41f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11fc42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11fc424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11fc42960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11fc42dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11fc43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11fc436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11fc43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11fc43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11fc44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11fc44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11fc44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11fc45150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11fc455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11fc45a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11fc45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11fc46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11fc46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11fc46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11fc47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11fc474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11fc47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11fc47db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11fc48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11fc48690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11fc48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11fc48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11fc493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11fc49850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11fc49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11fc4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11fc4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11fc4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11fc4ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11fc4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11fc4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11fc4bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11fc4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11fc4c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11fc4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11fc4cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11fc4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11fc4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11fc4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11fc4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11fc4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11fc4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11fc4eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11fc4f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11fc4f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11fc4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11fc4fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11fc502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11fc50740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11fc50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11fc51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11fc51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11fc51900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11fc51d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11fc521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11fc52650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11fc52ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11fc52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11fc533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11fc53810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11fc53c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11fc540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11fc54560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11fc549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11fc54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11fc552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11fc55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11fc55b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11fc56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11fc56d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11fc57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11fc57b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11fc57e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11fc58290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11fc58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11fc58ea0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d004da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d005210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d005680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d005af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d005f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d0063d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d006840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d006cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d007120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d007590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d007a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d008120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d008c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d0093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d009c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d00a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d00aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d00b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d00b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d00bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d00c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d00cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d00d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d00dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d00e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d00e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d00e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d00ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d00f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d00f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d00fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d00ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d010430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d0106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d010b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d010fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d011440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d0118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d011d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d012190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d012a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d012ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d013350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d0137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d013c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d0140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d014510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d014980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d014df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d015260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d0156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d015b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d015fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d016420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d016890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d016e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d017300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d017770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d017be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d018050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d0184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d018930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d018da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d019210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d019680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d019af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d019f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d01a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d01a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d01acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d01b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d01b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d01ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d01be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d01c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d01c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d01cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d01d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d01d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d01d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d01dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d01e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d01e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d01ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d01ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d01f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d01f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d01fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d020100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d020570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d0209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d020e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d0212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d021730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d021ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d022010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d022480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d0228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d022d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d0231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d023640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d023ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d023f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d024390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d024800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d024c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d0250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d025550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d0259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d025e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d0262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d026710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d026b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d026ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d027460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d0278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d027d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d0281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d028620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d028a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d028f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d029370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d0297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d029c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d02a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d02a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d02a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d02ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d02b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d02b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d02bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d02bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d02c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d02c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d02cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d02d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d02d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d02da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d02dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d02e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d02e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d02ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d02f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d02f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d02f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d02fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d030260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d0306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d030b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d030fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d031420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d031890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d031d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d032170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d0325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d032a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d032ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d033330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d0337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d033c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d034080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d0344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d034960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d034dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d035240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d0356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d035b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d035f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d036400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d036870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d036ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d037150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d0375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d037a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d037ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d038310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d038780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d038bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d039060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d0394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d039940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d039db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d03a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d03a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d03ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d03af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d03b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d03b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d03bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d03c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d03c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d03ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d03ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d03d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d03d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d03dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d03e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d03e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d03e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d03ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d03f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d03f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d03fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d03ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d0403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d040830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d0416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d0421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d0424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d042770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d042be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d043050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d0434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d043930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d043da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d044210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d044680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d044af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d044f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d0453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d045840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d045cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d046120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d046590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d046a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d046e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d0472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d047750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d047bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d048030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d0484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d048910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d048d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d0491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d049660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d049ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d049f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d04a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d04a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d04ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d04b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d04ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d04bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d04c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d04c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d04ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d04cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d04d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d04d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d04dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d04e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d04e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d04e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d04ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d04f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d04f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d04fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d04ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d050430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d0508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d050d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d051180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d0515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d051a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d051ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d052340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d0527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d052c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d053090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d053500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d053970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d053de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d054250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d0546c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d054b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d054fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d055410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d055880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d055cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d056160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d056bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d0572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d057a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d058130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d0583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d058860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d058e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d059470 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.937s
user	0m0.243s
sys	0m0.146s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
