Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.625s
user	0m0.825s
sys	0m1.281s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target build_info
[  4%] Built target sha256
[  4%] Built target xxhash
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Built target llava
[ 33%] Linking C executable ../bin/test-c
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX static library libcommon.a
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-quantize-fns
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-gguf
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 62%] Built target test-gguf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-backend-ops
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-embedding
[ 71%] Built target llama-batched
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-infill
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-cli
[ 81%] Generating index.html.gz.hpp
[ 81%] Built target llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-passkey
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-llava-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.282s
user	0m6.585s
sys	0m9.936s

main: quantize time =  4254.67 ms
main:    total time =  4254.67 ms

main: quantize time =  2574.47 ms
main:    total time =  2574.47 ms

main: quantize time =  2149.81 ms
main:    total time =  2149.81 ms

main: quantize time =  2124.72 ms
main:    total time =  2124.72 ms

main: quantize time =  1445.10 ms
main:    total time =  1445.10 ms

main: quantize time =  5320.45 ms
main:    total time =  5320.45 ms

main: quantize time =  5783.99 ms
main:    total time =  5783.99 ms

main: quantize time =  6888.87 ms
main:    total time =  6888.87 ms

main: quantize time =  6255.07 ms
main:    total time =  6255.07 ms

main: quantize time =  4402.57 ms
main:    total time =  4402.57 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.144 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.332 I main: llama backend init
0.00.000.338 I main: load the model and apply lora adapter, if any
0.00.046.680 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.058.908 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.058.926 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.058.931 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.058.932 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.058.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.058.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.058.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.058.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.058.937 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.058.938 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.058.939 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.058.940 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.058.941 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.058.942 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.058.948 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.058.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.058.949 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.066.120 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.068.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.943 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.943 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.944 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.946 I llama_model_loader: - type  f32:  194 tensors
0.00.076.946 I llama_model_loader: - type  f16:   98 tensors
0.00.076.947 I print_info: file format = GGUF V3 (latest)
0.00.076.948 I print_info: file type   = all F32 (guessed)
0.00.076.950 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.367 I load: special tokens cache size = 25
0.00.100.391 I load: token to piece cache size = 0.2984 MB
0.00.100.396 I print_info: arch             = gptneox
0.00.100.396 I print_info: vocab_only       = 0
0.00.100.396 I print_info: n_ctx_train      = 2048
0.00.100.396 I print_info: n_embd           = 2048
0.00.100.397 I print_info: n_layer          = 24
0.00.100.402 I print_info: n_head           = 16
0.00.100.403 I print_info: n_head_kv        = 16
0.00.100.403 I print_info: n_rot            = 32
0.00.100.406 I print_info: n_swa            = 0
0.00.100.406 I print_info: n_embd_head_k    = 128
0.00.100.407 I print_info: n_embd_head_v    = 128
0.00.100.408 I print_info: n_gqa            = 1
0.00.100.409 I print_info: n_embd_k_gqa     = 2048
0.00.100.409 I print_info: n_embd_v_gqa     = 2048
0.00.100.410 I print_info: f_norm_eps       = 1.0e-05
0.00.100.411 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.100.411 I print_info: f_clamp_kqv      = 0.0e+00
0.00.100.411 I print_info: f_max_alibi_bias = 0.0e+00
0.00.100.411 I print_info: f_logit_scale    = 0.0e+00
0.00.100.422 I print_info: n_ff             = 8192
0.00.100.424 I print_info: n_expert         = 0
0.00.100.425 I print_info: n_expert_used    = 0
0.00.100.425 I print_info: causal attn      = 1
0.00.100.425 I print_info: pooling type     = 0
0.00.100.427 I print_info: rope type        = 2
0.00.100.428 I print_info: rope scaling     = linear
0.00.100.428 I print_info: freq_base_train  = 10000.0
0.00.100.430 I print_info: freq_scale_train = 1
0.00.100.430 I print_info: n_ctx_orig_yarn  = 2048
0.00.100.431 I print_info: rope_finetuned   = unknown
0.00.100.431 I print_info: ssm_d_conv       = 0
0.00.100.431 I print_info: ssm_d_inner      = 0
0.00.100.431 I print_info: ssm_d_state      = 0
0.00.100.432 I print_info: ssm_dt_rank      = 0
0.00.100.432 I print_info: ssm_dt_b_c_rms   = 0
0.00.100.432 I print_info: model type       = 1.4B
0.00.100.433 I print_info: model params     = 1.41 B
0.00.100.433 I print_info: general.name     = 1.4B
0.00.100.433 I print_info: vocab type       = BPE
0.00.100.434 I print_info: n_vocab          = 50304
0.00.100.434 I print_info: n_merges         = 50009
0.00.100.434 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.100.435 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.100.435 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.100.435 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.100.435 I print_info: LF token         = 187 'Ċ'
0.00.100.436 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.100.436 I print_info: max token length = 1024
0.00.100.438 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.137.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.137.342 I load_tensors: offloading output layer to GPU
0.00.137.342 I load_tensors: offloaded 25/25 layers to GPU
0.00.137.366 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.137.368 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.137.765 I llama_init_from_model: n_seq_max     = 1
0.00.137.766 I llama_init_from_model: n_ctx         = 2048
0.00.137.766 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.137.766 I llama_init_from_model: n_batch       = 2048
0.00.137.766 I llama_init_from_model: n_ubatch      = 512
0.00.137.767 I llama_init_from_model: flash_attn    = 0
0.00.137.767 I llama_init_from_model: freq_base     = 10000.0
0.00.137.768 I llama_init_from_model: freq_scale    = 1
0.00.137.768 I ggml_metal_init: allocating
0.00.137.801 I ggml_metal_init: found device: Apple M4
0.00.137.808 I ggml_metal_init: picking default device: Apple M4
0.00.138.408 I ggml_metal_init: using embedded metal library
0.00.814.650 I ggml_metal_init: GPU name:   Apple M4
0.00.814.667 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.814.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.814.668 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.814.669 I ggml_metal_init: simdgroup reduction   = true
0.00.814.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.814.670 I ggml_metal_init: has residency sets    = true
0.00.814.670 I ggml_metal_init: has bfloat            = true
0.00.814.670 I ggml_metal_init: use bfloat            = true
0.00.814.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.814.679 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.529.983 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.583.403 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.583.409 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.583.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.587.485 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.587.487 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.587.488 I llama_init_from_model: graph nodes  = 967
0.01.587.488 I llama_init_from_model: graph splits = 2
0.01.587.491 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.587.684 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.587.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.675.293 I main: llama threadpool init, n_threads = 4
0.01.675.327 I 
0.01.675.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.675.344 I 
0.01.675.405 I sampler seed: 1234
0.01.675.409 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.675.446 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.675.447 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.675.448 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.03.505.265 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58532.56 tokens per second)
0.03.505.266 I llama_perf_context_print:        load time =    1627.59 ms
0.03.505.266 I llama_perf_context_print: prompt eval time =      44.06 ms /     7 tokens (    6.29 ms per token,   158.87 tokens per second)
0.03.505.267 I llama_perf_context_print:        eval time =    1782.88 ms /    63 runs   (   28.30 ms per token,    35.34 tokens per second)
0.03.505.271 I llama_perf_context_print:       total time =    1830.97 ms /    70 tokens
0.03.505.466 I ggml_metal_free: deallocating

real	0m3.828s
user	0m0.154s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.939 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.967 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.968 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.969 I llama_model_loader: - type  f32:  194 tensors
0.00.034.970 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.971 I print_info: file format = GGUF V3 (latest)
0.00.034.971 I print_info: file type   = Q8_0
0.00.034.972 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.467 I load: special tokens cache size = 25
0.00.049.596 I load: token to piece cache size = 0.2984 MB
0.00.049.601 I print_info: arch             = gptneox
0.00.049.601 I print_info: vocab_only       = 0
0.00.049.601 I print_info: n_ctx_train      = 2048
0.00.049.603 I print_info: n_embd           = 2048
0.00.049.603 I print_info: n_layer          = 24
0.00.049.610 I print_info: n_head           = 16
0.00.049.611 I print_info: n_head_kv        = 16
0.00.049.611 I print_info: n_rot            = 32
0.00.049.611 I print_info: n_swa            = 0
0.00.049.611 I print_info: n_embd_head_k    = 128
0.00.049.611 I print_info: n_embd_head_v    = 128
0.00.049.612 I print_info: n_gqa            = 1
0.00.049.613 I print_info: n_embd_k_gqa     = 2048
0.00.049.613 I print_info: n_embd_v_gqa     = 2048
0.00.049.614 I print_info: f_norm_eps       = 1.0e-05
0.00.049.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.615 I print_info: f_logit_scale    = 0.0e+00
0.00.049.616 I print_info: n_ff             = 8192
0.00.049.616 I print_info: n_expert         = 0
0.00.049.617 I print_info: n_expert_used    = 0
0.00.049.617 I print_info: causal attn      = 1
0.00.049.617 I print_info: pooling type     = 0
0.00.049.617 I print_info: rope type        = 2
0.00.049.618 I print_info: rope scaling     = linear
0.00.049.618 I print_info: freq_base_train  = 10000.0
0.00.049.618 I print_info: freq_scale_train = 1
0.00.049.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.619 I print_info: rope_finetuned   = unknown
0.00.049.619 I print_info: ssm_d_conv       = 0
0.00.049.619 I print_info: ssm_d_inner      = 0
0.00.049.619 I print_info: ssm_d_state      = 0
0.00.049.620 I print_info: ssm_dt_rank      = 0
0.00.049.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.620 I print_info: model type       = 1.4B
0.00.049.622 I print_info: model params     = 1.41 B
0.00.049.622 I print_info: general.name     = 1.4B
0.00.049.623 I print_info: vocab type       = BPE
0.00.049.623 I print_info: n_vocab          = 50304
0.00.049.623 I print_info: n_merges         = 50009
0.00.049.624 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.624 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.624 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.624 I print_info: LF token         = 187 'Ċ'
0.00.049.625 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: max token length = 1024
0.00.049.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.02.574.887 I load_tensors: offloading 24 repeating layers to GPU
0.02.574.893 I load_tensors: offloading output layer to GPU
0.02.574.894 I load_tensors: offloaded 25/25 layers to GPU
0.02.574.919 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.02.574.920 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.02.575.701 I llama_init_from_model: n_seq_max     = 1
0.02.575.703 I llama_init_from_model: n_ctx         = 2048
0.02.575.703 I llama_init_from_model: n_ctx_per_seq = 2048
0.02.575.704 I llama_init_from_model: n_batch       = 2048
0.02.575.704 I llama_init_from_model: n_ubatch      = 512
0.02.575.704 I llama_init_from_model: flash_attn    = 0
0.02.575.705 I llama_init_from_model: freq_base     = 10000.0
0.02.575.706 I llama_init_from_model: freq_scale    = 1
0.02.575.707 I ggml_metal_init: allocating
0.02.575.720 I ggml_metal_init: found device: Apple M4
0.02.575.728 I ggml_metal_init: picking default device: Apple M4
0.02.577.006 I ggml_metal_init: using embedded metal library
0.02.582.115 I ggml_metal_init: GPU name:   Apple M4
0.02.582.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.02.582.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.02.582.119 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.02.582.120 I ggml_metal_init: simdgroup reduction   = true
0.02.582.120 I ggml_metal_init: simdgroup matrix mul. = true
0.02.582.120 I ggml_metal_init: has residency sets    = true
0.02.582.121 I ggml_metal_init: has bfloat            = true
0.02.582.121 I ggml_metal_init: use bfloat            = true
0.02.582.121 I ggml_metal_init: hasUnifiedMemory      = true
0.02.582.123 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.02.597.878 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.02.648.486 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.02.648.495 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.02.648.529 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.02.653.095 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.02.653.097 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.02.653.097 I llama_init_from_model: graph nodes  = 967
0.02.653.097 I llama_init_from_model: graph splits = 2
0.02.653.103 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.02.653.232 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.02.653.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.02.707.706 I main: llama threadpool init, n_threads = 4
0.02.707.745 I 
0.02.707.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.02.707.763 I 
0.02.707.919 I sampler seed: 1234
0.02.707.924 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.02.707.959 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.02.707.962 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.02.707.963 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.03.788.762 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49719.89 tokens per second)
0.03.788.763 I llama_perf_context_print:        load time =    2697.06 ms
0.03.788.764 I llama_perf_context_print: prompt eval time =      49.23 ms /     7 tokens (    7.03 ms per token,   142.20 tokens per second)
0.03.788.765 I llama_perf_context_print:        eval time =    1029.34 ms /    63 runs   (   16.34 ms per token,    61.20 tokens per second)
0.03.788.765 I llama_perf_context_print:       total time =    1081.79 ms /    70 tokens
0.03.789.064 I ggml_metal_free: deallocating

real	0m3.807s
user	0m0.105s
sys	0m0.285s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.016.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.762 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.765 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.657 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.628 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.629 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.629 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.041.630 I llama_model_loader: - type  f32:  194 tensors
0.00.041.630 I llama_model_loader: - type q4_0:   97 tensors
0.00.041.630 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.631 I print_info: file format = GGUF V3 (latest)
0.00.041.632 I print_info: file type   = Q4_0
0.00.041.633 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.042 I load: special tokens cache size = 25
0.00.056.316 I load: token to piece cache size = 0.2984 MB
0.00.056.321 I print_info: arch             = gptneox
0.00.056.321 I print_info: vocab_only       = 0
0.00.056.321 I print_info: n_ctx_train      = 2048
0.00.056.321 I print_info: n_embd           = 2048
0.00.056.322 I print_info: n_layer          = 24
0.00.056.327 I print_info: n_head           = 16
0.00.056.327 I print_info: n_head_kv        = 16
0.00.056.327 I print_info: n_rot            = 32
0.00.056.328 I print_info: n_swa            = 0
0.00.056.328 I print_info: n_embd_head_k    = 128
0.00.056.329 I print_info: n_embd_head_v    = 128
0.00.056.330 I print_info: n_gqa            = 1
0.00.056.331 I print_info: n_embd_k_gqa     = 2048
0.00.056.333 I print_info: n_embd_v_gqa     = 2048
0.00.056.333 I print_info: f_norm_eps       = 1.0e-05
0.00.056.334 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.334 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.334 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.334 I print_info: f_logit_scale    = 0.0e+00
0.00.056.335 I print_info: n_ff             = 8192
0.00.056.335 I print_info: n_expert         = 0
0.00.056.335 I print_info: n_expert_used    = 0
0.00.056.335 I print_info: causal attn      = 1
0.00.056.335 I print_info: pooling type     = 0
0.00.056.335 I print_info: rope type        = 2
0.00.056.336 I print_info: rope scaling     = linear
0.00.056.336 I print_info: freq_base_train  = 10000.0
0.00.056.336 I print_info: freq_scale_train = 1
0.00.056.337 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.337 I print_info: rope_finetuned   = unknown
0.00.056.337 I print_info: ssm_d_conv       = 0
0.00.056.337 I print_info: ssm_d_inner      = 0
0.00.056.337 I print_info: ssm_d_state      = 0
0.00.056.337 I print_info: ssm_dt_rank      = 0
0.00.056.337 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.338 I print_info: model type       = 1.4B
0.00.056.338 I print_info: model params     = 1.41 B
0.00.056.338 I print_info: general.name     = 1.4B
0.00.056.338 I print_info: vocab type       = BPE
0.00.056.339 I print_info: n_vocab          = 50304
0.00.056.339 I print_info: n_merges         = 50009
0.00.056.339 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.339 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.339 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.340 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.340 I print_info: LF token         = 187 'Ċ'
0.00.056.340 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.340 I print_info: max token length = 1024
0.00.056.340 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.471.055 I load_tensors: offloading 24 repeating layers to GPU
0.01.471.063 I load_tensors: offloading output layer to GPU
0.01.471.064 I load_tensors: offloaded 25/25 layers to GPU
0.01.471.082 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.01.471.083 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.01.471.934 I llama_init_from_model: n_seq_max     = 1
0.01.471.937 I llama_init_from_model: n_ctx         = 2048
0.01.471.937 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.471.938 I llama_init_from_model: n_batch       = 2048
0.01.471.938 I llama_init_from_model: n_ubatch      = 512
0.01.471.938 I llama_init_from_model: flash_attn    = 0
0.01.471.940 I llama_init_from_model: freq_base     = 10000.0
0.01.471.940 I llama_init_from_model: freq_scale    = 1
0.01.471.942 I ggml_metal_init: allocating
0.01.471.987 I ggml_metal_init: found device: Apple M4
0.01.472.000 I ggml_metal_init: picking default device: Apple M4
0.01.473.022 I ggml_metal_init: using embedded metal library
0.01.477.096 I ggml_metal_init: GPU name:   Apple M4
0.01.477.102 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.477.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.477.103 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.477.104 I ggml_metal_init: simdgroup reduction   = true
0.01.477.104 I ggml_metal_init: simdgroup matrix mul. = true
0.01.477.104 I ggml_metal_init: has residency sets    = true
0.01.477.105 I ggml_metal_init: has bfloat            = true
0.01.477.105 I ggml_metal_init: use bfloat            = true
0.01.477.106 I ggml_metal_init: hasUnifiedMemory      = true
0.01.477.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.493.031 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.524.860 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.524.866 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.524.901 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.529.282 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.529.285 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.529.285 I llama_init_from_model: graph nodes  = 967
0.01.529.285 I llama_init_from_model: graph splits = 2
0.01.529.290 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.529.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.529.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.586.002 I main: llama threadpool init, n_threads = 4
0.01.586.054 I 
0.01.586.070 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.586.070 I 
0.01.586.223 I sampler seed: 1234
0.01.586.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.586.284 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.586.286 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.586.286 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.02.280.561 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.02.280.562 I llama_perf_context_print:        load time =    1569.28 ms
0.02.280.562 I llama_perf_context_print: prompt eval time =      49.42 ms /     7 tokens (    7.06 ms per token,   141.64 tokens per second)
0.02.280.563 I llama_perf_context_print:        eval time =     641.98 ms /    63 runs   (   10.19 ms per token,    98.13 tokens per second)
0.02.280.563 I llama_perf_context_print:       total time =     695.25 ms /    70 tokens
0.02.280.800 I ggml_metal_free: deallocating

real	0m2.326s
user	0m0.107s
sys	0m0.167s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.020.042 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.039.471 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.484 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.486 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.365 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.048.321 I llama_model_loader: - type  f32:  194 tensors
0.00.048.321 I llama_model_loader: - type q4_1:   97 tensors
0.00.048.321 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.322 I print_info: file format = GGUF V3 (latest)
0.00.048.322 I print_info: file type   = Q4_1
0.00.048.324 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.056.717 I load: special tokens cache size = 25
0.00.062.964 I load: token to piece cache size = 0.2984 MB
0.00.062.969 I print_info: arch             = gptneox
0.00.062.969 I print_info: vocab_only       = 0
0.00.062.969 I print_info: n_ctx_train      = 2048
0.00.062.969 I print_info: n_embd           = 2048
0.00.062.970 I print_info: n_layer          = 24
0.00.062.974 I print_info: n_head           = 16
0.00.062.974 I print_info: n_head_kv        = 16
0.00.062.975 I print_info: n_rot            = 32
0.00.062.975 I print_info: n_swa            = 0
0.00.062.975 I print_info: n_embd_head_k    = 128
0.00.062.975 I print_info: n_embd_head_v    = 128
0.00.062.976 I print_info: n_gqa            = 1
0.00.062.977 I print_info: n_embd_k_gqa     = 2048
0.00.062.977 I print_info: n_embd_v_gqa     = 2048
0.00.062.978 I print_info: f_norm_eps       = 1.0e-05
0.00.062.978 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.978 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.978 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.979 I print_info: f_logit_scale    = 0.0e+00
0.00.062.979 I print_info: n_ff             = 8192
0.00.062.979 I print_info: n_expert         = 0
0.00.062.979 I print_info: n_expert_used    = 0
0.00.062.980 I print_info: causal attn      = 1
0.00.062.980 I print_info: pooling type     = 0
0.00.062.980 I print_info: rope type        = 2
0.00.062.982 I print_info: rope scaling     = linear
0.00.062.982 I print_info: freq_base_train  = 10000.0
0.00.062.982 I print_info: freq_scale_train = 1
0.00.062.983 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.983 I print_info: rope_finetuned   = unknown
0.00.062.983 I print_info: ssm_d_conv       = 0
0.00.062.983 I print_info: ssm_d_inner      = 0
0.00.062.983 I print_info: ssm_d_state      = 0
0.00.062.983 I print_info: ssm_dt_rank      = 0
0.00.062.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.984 I print_info: model type       = 1.4B
0.00.062.984 I print_info: model params     = 1.41 B
0.00.062.984 I print_info: general.name     = 1.4B
0.00.062.985 I print_info: vocab type       = BPE
0.00.062.985 I print_info: n_vocab          = 50304
0.00.062.985 I print_info: n_merges         = 50009
0.00.062.985 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.986 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.986 I print_info: LF token         = 187 'Ċ'
0.00.062.987 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.987 I print_info: max token length = 1024
0.00.062.987 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.644.493 I load_tensors: offloading 24 repeating layers to GPU
0.01.644.513 I load_tensors: offloading output layer to GPU
0.01.644.513 I load_tensors: offloaded 25/25 layers to GPU
0.01.644.551 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.01.644.552 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.01.645.713 I llama_init_from_model: n_seq_max     = 1
0.01.645.715 I llama_init_from_model: n_ctx         = 2048
0.01.645.716 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.645.717 I llama_init_from_model: n_batch       = 2048
0.01.645.717 I llama_init_from_model: n_ubatch      = 512
0.01.645.718 I llama_init_from_model: flash_attn    = 0
0.01.645.720 I llama_init_from_model: freq_base     = 10000.0
0.01.645.720 I llama_init_from_model: freq_scale    = 1
0.01.645.725 I ggml_metal_init: allocating
0.01.645.835 I ggml_metal_init: found device: Apple M4
0.01.645.850 I ggml_metal_init: picking default device: Apple M4
0.01.647.634 I ggml_metal_init: using embedded metal library
0.01.652.873 I ggml_metal_init: GPU name:   Apple M4
0.01.652.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.652.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.652.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.652.888 I ggml_metal_init: simdgroup reduction   = true
0.01.652.888 I ggml_metal_init: simdgroup matrix mul. = true
0.01.652.888 I ggml_metal_init: has residency sets    = true
0.01.652.889 I ggml_metal_init: has bfloat            = true
0.01.652.889 I ggml_metal_init: use bfloat            = true
0.01.652.891 I ggml_metal_init: hasUnifiedMemory      = true
0.01.652.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.673.390 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.732.099 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.732.106 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.732.140 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.736.172 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.736.173 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.736.174 I llama_init_from_model: graph nodes  = 967
0.01.736.174 I llama_init_from_model: graph splits = 2
0.01.736.178 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.736.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.736.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.789.809 I main: llama threadpool init, n_threads = 4
0.01.789.852 I 
0.01.789.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.789.867 I 
0.01.790.044 I sampler seed: 1234
0.01.790.048 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.790.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.790.060 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.790.060 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.02.528.895 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53951.37 tokens per second)
0.02.528.895 I llama_perf_context_print:        load time =    1769.07 ms
0.02.528.896 I llama_perf_context_print: prompt eval time =      48.93 ms /     7 tokens (    6.99 ms per token,   143.06 tokens per second)
0.02.528.897 I llama_perf_context_print:        eval time =     687.22 ms /    63 runs   (   10.91 ms per token,    91.67 tokens per second)
0.02.528.897 I llama_perf_context_print:       total time =     739.78 ms /    70 tokens
0.02.529.116 I ggml_metal_free: deallocating

real	0m2.547s
user	0m0.111s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.018.477 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.035.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.965 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.967 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.969 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.970 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.970 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.971 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.972 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.973 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.973 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.391 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.967 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.968 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.969 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.969 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.970 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.971 I llama_model_loader: - type  f32:  194 tensors
0.00.048.971 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.971 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.972 I print_info: file format = GGUF V3 (latest)
0.00.048.973 I print_info: file type   = Q5_0
0.00.048.974 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.062.508 I load: special tokens cache size = 25
0.00.075.061 I load: token to piece cache size = 0.2984 MB
0.00.075.066 I print_info: arch             = gptneox
0.00.075.066 I print_info: vocab_only       = 0
0.00.075.067 I print_info: n_ctx_train      = 2048
0.00.075.067 I print_info: n_embd           = 2048
0.00.075.067 I print_info: n_layer          = 24
0.00.075.071 I print_info: n_head           = 16
0.00.075.072 I print_info: n_head_kv        = 16
0.00.075.073 I print_info: n_rot            = 32
0.00.075.073 I print_info: n_swa            = 0
0.00.075.073 I print_info: n_embd_head_k    = 128
0.00.075.074 I print_info: n_embd_head_v    = 128
0.00.075.075 I print_info: n_gqa            = 1
0.00.075.076 I print_info: n_embd_k_gqa     = 2048
0.00.075.077 I print_info: n_embd_v_gqa     = 2048
0.00.075.078 I print_info: f_norm_eps       = 1.0e-05
0.00.075.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.082 I print_info: f_logit_scale    = 0.0e+00
0.00.075.083 I print_info: n_ff             = 8192
0.00.075.084 I print_info: n_expert         = 0
0.00.075.084 I print_info: n_expert_used    = 0
0.00.075.084 I print_info: causal attn      = 1
0.00.075.084 I print_info: pooling type     = 0
0.00.075.085 I print_info: rope type        = 2
0.00.075.085 I print_info: rope scaling     = linear
0.00.075.086 I print_info: freq_base_train  = 10000.0
0.00.075.086 I print_info: freq_scale_train = 1
0.00.075.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.087 I print_info: rope_finetuned   = unknown
0.00.075.087 I print_info: ssm_d_conv       = 0
0.00.075.087 I print_info: ssm_d_inner      = 0
0.00.075.088 I print_info: ssm_d_state      = 0
0.00.075.088 I print_info: ssm_dt_rank      = 0
0.00.075.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.088 I print_info: model type       = 1.4B
0.00.075.089 I print_info: model params     = 1.41 B
0.00.075.089 I print_info: general.name     = 1.4B
0.00.075.090 I print_info: vocab type       = BPE
0.00.075.090 I print_info: n_vocab          = 50304
0.00.075.091 I print_info: n_merges         = 50009
0.00.075.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.092 I print_info: LF token         = 187 'Ċ'
0.00.075.094 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.094 I print_info: max token length = 1024
0.00.075.095 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.814.656 I load_tensors: offloading 24 repeating layers to GPU
0.01.814.672 I load_tensors: offloading output layer to GPU
0.01.814.673 I load_tensors: offloaded 25/25 layers to GPU
0.01.814.711 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.01.814.712 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.01.816.050 I llama_init_from_model: n_seq_max     = 1
0.01.816.052 I llama_init_from_model: n_ctx         = 2048
0.01.816.053 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.816.054 I llama_init_from_model: n_batch       = 2048
0.01.816.054 I llama_init_from_model: n_ubatch      = 512
0.01.816.055 I llama_init_from_model: flash_attn    = 0
0.01.816.057 I llama_init_from_model: freq_base     = 10000.0
0.01.816.057 I llama_init_from_model: freq_scale    = 1
0.01.816.060 I ggml_metal_init: allocating
0.01.816.145 I ggml_metal_init: found device: Apple M4
0.01.816.159 I ggml_metal_init: picking default device: Apple M4
0.01.817.870 I ggml_metal_init: using embedded metal library
0.01.823.990 I ggml_metal_init: GPU name:   Apple M4
0.01.824.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.824.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.824.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.824.003 I ggml_metal_init: simdgroup reduction   = true
0.01.824.003 I ggml_metal_init: simdgroup matrix mul. = true
0.01.824.003 I ggml_metal_init: has residency sets    = true
0.01.824.003 I ggml_metal_init: has bfloat            = true
0.01.824.004 I ggml_metal_init: use bfloat            = true
0.01.824.005 I ggml_metal_init: hasUnifiedMemory      = true
0.01.824.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.842.354 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.900.964 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.900.971 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.901.004 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.905.558 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.905.559 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.905.560 I llama_init_from_model: graph nodes  = 967
0.01.905.560 I llama_init_from_model: graph splits = 2
0.01.905.569 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.905.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.905.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.960.781 I main: llama threadpool init, n_threads = 4
0.01.960.823 I 
0.01.960.838 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.960.838 I 
0.01.960.987 I sampler seed: 1234
0.01.960.993 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.961.003 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.961.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.961.004 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.743.898 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.02.743.899 I llama_perf_context_print:        load time =    1941.58 ms
0.02.743.899 I llama_perf_context_print: prompt eval time =      42.78 ms /     7 tokens (    6.11 ms per token,   163.62 tokens per second)
0.02.743.900 I llama_perf_context_print:        eval time =     737.29 ms /    63 runs   (   11.70 ms per token,    85.45 tokens per second)
0.02.743.900 I llama_perf_context_print:       total time =     783.83 ms /    70 tokens
0.02.744.185 I ggml_metal_free: deallocating

real	0m2.780s
user	0m0.129s
sys	0m0.236s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.911 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.026.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.360 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.362 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.366 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.366 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.206 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.054 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.055 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.035.056 I llama_model_loader: - type  f32:  194 tensors
0.00.035.056 I llama_model_loader: - type q5_1:   97 tensors
0.00.035.056 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.057 I print_info: file format = GGUF V3 (latest)
0.00.035.057 I print_info: file type   = Q5_1
0.00.035.060 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.824 I load: special tokens cache size = 25
0.00.049.988 I load: token to piece cache size = 0.2984 MB
0.00.049.991 I print_info: arch             = gptneox
0.00.049.991 I print_info: vocab_only       = 0
0.00.049.991 I print_info: n_ctx_train      = 2048
0.00.049.991 I print_info: n_embd           = 2048
0.00.049.991 I print_info: n_layer          = 24
0.00.049.994 I print_info: n_head           = 16
0.00.049.995 I print_info: n_head_kv        = 16
0.00.049.995 I print_info: n_rot            = 32
0.00.049.997 I print_info: n_swa            = 0
0.00.049.997 I print_info: n_embd_head_k    = 128
0.00.049.998 I print_info: n_embd_head_v    = 128
0.00.049.998 I print_info: n_gqa            = 1
0.00.049.999 I print_info: n_embd_k_gqa     = 2048
0.00.050.000 I print_info: n_embd_v_gqa     = 2048
0.00.050.000 I print_info: f_norm_eps       = 1.0e-05
0.00.050.001 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.001 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.001 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.001 I print_info: f_logit_scale    = 0.0e+00
0.00.050.002 I print_info: n_ff             = 8192
0.00.050.002 I print_info: n_expert         = 0
0.00.050.002 I print_info: n_expert_used    = 0
0.00.050.002 I print_info: causal attn      = 1
0.00.050.002 I print_info: pooling type     = 0
0.00.050.004 I print_info: rope type        = 2
0.00.050.006 I print_info: rope scaling     = linear
0.00.050.006 I print_info: freq_base_train  = 10000.0
0.00.050.006 I print_info: freq_scale_train = 1
0.00.050.007 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.007 I print_info: rope_finetuned   = unknown
0.00.050.008 I print_info: ssm_d_conv       = 0
0.00.050.008 I print_info: ssm_d_inner      = 0
0.00.050.009 I print_info: ssm_d_state      = 0
0.00.050.009 I print_info: ssm_dt_rank      = 0
0.00.050.009 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.009 I print_info: model type       = 1.4B
0.00.050.009 I print_info: model params     = 1.41 B
0.00.050.010 I print_info: general.name     = 1.4B
0.00.050.010 I print_info: vocab type       = BPE
0.00.050.010 I print_info: n_vocab          = 50304
0.00.050.010 I print_info: n_merges         = 50009
0.00.050.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.012 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.012 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.013 I print_info: LF token         = 187 'Ċ'
0.00.050.013 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.013 I print_info: max token length = 1024
0.00.050.014 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.769.045 I load_tensors: offloading 24 repeating layers to GPU
0.01.769.055 I load_tensors: offloading output layer to GPU
0.01.769.056 I load_tensors: offloaded 25/25 layers to GPU
0.01.769.091 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.01.769.092 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.01.770.560 I llama_init_from_model: n_seq_max     = 1
0.01.770.562 I llama_init_from_model: n_ctx         = 2048
0.01.770.563 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.770.563 I llama_init_from_model: n_batch       = 2048
0.01.770.564 I llama_init_from_model: n_ubatch      = 512
0.01.770.564 I llama_init_from_model: flash_attn    = 0
0.01.770.566 I llama_init_from_model: freq_base     = 10000.0
0.01.770.567 I llama_init_from_model: freq_scale    = 1
0.01.770.568 I ggml_metal_init: allocating
0.01.770.651 I ggml_metal_init: found device: Apple M4
0.01.770.663 I ggml_metal_init: picking default device: Apple M4
0.01.772.479 I ggml_metal_init: using embedded metal library
0.01.778.144 I ggml_metal_init: GPU name:   Apple M4
0.01.778.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.778.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.778.152 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.778.152 I ggml_metal_init: simdgroup reduction   = true
0.01.778.153 I ggml_metal_init: simdgroup matrix mul. = true
0.01.778.153 I ggml_metal_init: has residency sets    = true
0.01.778.153 I ggml_metal_init: has bfloat            = true
0.01.778.153 I ggml_metal_init: use bfloat            = true
0.01.778.154 I ggml_metal_init: hasUnifiedMemory      = true
0.01.778.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.797.515 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.854.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.854.304 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.854.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.859.176 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.859.178 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.859.178 I llama_init_from_model: graph nodes  = 967
0.01.859.178 I llama_init_from_model: graph splits = 2
0.01.859.185 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.859.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.859.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.915.823 I main: llama threadpool init, n_threads = 4
0.01.915.869 I 
0.01.915.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.915.887 I 
0.01.916.054 I sampler seed: 1234
0.01.916.059 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.916.083 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.916.084 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.916.084 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.748.123 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50390.35 tokens per second)
0.02.748.124 I llama_perf_context_print:        load time =    1906.20 ms
0.02.748.124 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.81 tokens per second)
0.02.748.126 I llama_perf_context_print:        eval time =     786.84 ms /    63 runs   (   12.49 ms per token,    80.07 tokens per second)
0.02.748.126 I llama_perf_context_print:       total time =     833.01 ms /    70 tokens
0.02.748.355 I ggml_metal_free: deallocating

real	0m2.764s
user	0m0.111s
sys	0m0.242s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.016.332 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.695 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.696 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.696 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.701 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.474 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.476 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.034.477 I llama_model_loader: - type  f32:  194 tensors
0.00.034.477 I llama_model_loader: - type q2_K:   49 tensors
0.00.034.478 I llama_model_loader: - type q3_K:   48 tensors
0.00.034.478 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.478 I print_info: file format = GGUF V3 (latest)
0.00.034.479 I print_info: file type   = Q2_K - Medium
0.00.034.480 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.473 I load: special tokens cache size = 25
0.00.052.488 I load: token to piece cache size = 0.2984 MB
0.00.052.492 I print_info: arch             = gptneox
0.00.052.492 I print_info: vocab_only       = 0
0.00.052.492 I print_info: n_ctx_train      = 2048
0.00.052.493 I print_info: n_embd           = 2048
0.00.052.493 I print_info: n_layer          = 24
0.00.052.495 I print_info: n_head           = 16
0.00.052.496 I print_info: n_head_kv        = 16
0.00.052.497 I print_info: n_rot            = 32
0.00.052.497 I print_info: n_swa            = 0
0.00.052.497 I print_info: n_embd_head_k    = 128
0.00.052.497 I print_info: n_embd_head_v    = 128
0.00.052.498 I print_info: n_gqa            = 1
0.00.052.499 I print_info: n_embd_k_gqa     = 2048
0.00.052.500 I print_info: n_embd_v_gqa     = 2048
0.00.052.502 I print_info: f_norm_eps       = 1.0e-05
0.00.052.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.503 I print_info: f_logit_scale    = 0.0e+00
0.00.052.504 I print_info: n_ff             = 8192
0.00.052.504 I print_info: n_expert         = 0
0.00.052.504 I print_info: n_expert_used    = 0
0.00.052.504 I print_info: causal attn      = 1
0.00.052.505 I print_info: pooling type     = 0
0.00.052.505 I print_info: rope type        = 2
0.00.052.505 I print_info: rope scaling     = linear
0.00.052.507 I print_info: freq_base_train  = 10000.0
0.00.052.507 I print_info: freq_scale_train = 1
0.00.052.508 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.508 I print_info: rope_finetuned   = unknown
0.00.052.508 I print_info: ssm_d_conv       = 0
0.00.052.508 I print_info: ssm_d_inner      = 0
0.00.052.508 I print_info: ssm_d_state      = 0
0.00.052.508 I print_info: ssm_dt_rank      = 0
0.00.052.509 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.509 I print_info: model type       = 1.4B
0.00.052.509 I print_info: model params     = 1.41 B
0.00.052.509 I print_info: general.name     = 1.4B
0.00.052.510 I print_info: vocab type       = BPE
0.00.052.510 I print_info: n_vocab          = 50304
0.00.052.510 I print_info: n_merges         = 50009
0.00.052.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.516 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.516 I print_info: LF token         = 187 'Ċ'
0.00.052.518 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.518 I print_info: max token length = 1024
0.00.052.518 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.387.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.387.140 I load_tensors: offloading output layer to GPU
0.00.387.141 I load_tensors: offloaded 25/25 layers to GPU
0.00.387.170 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.387.171 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.388.673 I llama_init_from_model: n_seq_max     = 1
0.00.388.676 I llama_init_from_model: n_ctx         = 2048
0.00.388.677 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.388.678 I llama_init_from_model: n_batch       = 2048
0.00.388.678 I llama_init_from_model: n_ubatch      = 512
0.00.388.678 I llama_init_from_model: flash_attn    = 0
0.00.388.681 I llama_init_from_model: freq_base     = 10000.0
0.00.388.682 I llama_init_from_model: freq_scale    = 1
0.00.388.684 I ggml_metal_init: allocating
0.00.388.750 I ggml_metal_init: found device: Apple M4
0.00.388.764 I ggml_metal_init: picking default device: Apple M4
0.00.390.581 I ggml_metal_init: using embedded metal library
0.00.396.106 I ggml_metal_init: GPU name:   Apple M4
0.00.396.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.396.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.396.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.396.121 I ggml_metal_init: simdgroup reduction   = true
0.00.396.121 I ggml_metal_init: simdgroup matrix mul. = true
0.00.396.121 I ggml_metal_init: has residency sets    = true
0.00.396.121 I ggml_metal_init: has bfloat            = true
0.00.396.122 I ggml_metal_init: use bfloat            = true
0.00.396.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.396.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.418.318 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.481.433 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.481.440 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.481.477 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.485.721 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.485.723 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.485.723 I llama_init_from_model: graph nodes  = 967
0.00.485.724 I llama_init_from_model: graph splits = 2
0.00.485.730 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.485.854 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.485.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.541.874 I main: llama threadpool init, n_threads = 4
0.00.541.919 I 
0.00.541.959 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.541.965 I 
0.00.542.198 I sampler seed: 1234
0.00.542.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.225 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.227 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.214.115 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.214.115 I llama_perf_context_print:        load time =     524.84 ms
0.01.214.116 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.68 tokens per second)
0.01.214.117 I llama_perf_context_print:        eval time =     633.32 ms /    63 runs   (   10.05 ms per token,    99.48 tokens per second)
0.01.214.117 I llama_perf_context_print:       total time =     672.94 ms /    70 tokens
0.01.214.346 I ggml_metal_free: deallocating

real	0m1.243s
user	0m0.119s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.765 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.496 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.497 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.497 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.499 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.500 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.380 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.395 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.147 I llama_model_loader: - type  f32:  194 tensors
0.00.025.148 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.148 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.148 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.148 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.149 I print_info: file format = GGUF V3 (latest)
0.00.025.149 I print_info: file type   = Q3_K - Medium
0.00.025.150 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.923 I load: special tokens cache size = 25
0.00.038.927 I load: token to piece cache size = 0.2984 MB
0.00.038.930 I print_info: arch             = gptneox
0.00.038.930 I print_info: vocab_only       = 0
0.00.038.930 I print_info: n_ctx_train      = 2048
0.00.038.930 I print_info: n_embd           = 2048
0.00.038.931 I print_info: n_layer          = 24
0.00.038.933 I print_info: n_head           = 16
0.00.038.934 I print_info: n_head_kv        = 16
0.00.038.934 I print_info: n_rot            = 32
0.00.038.935 I print_info: n_swa            = 0
0.00.038.936 I print_info: n_embd_head_k    = 128
0.00.038.936 I print_info: n_embd_head_v    = 128
0.00.038.937 I print_info: n_gqa            = 1
0.00.038.938 I print_info: n_embd_k_gqa     = 2048
0.00.038.939 I print_info: n_embd_v_gqa     = 2048
0.00.038.940 I print_info: f_norm_eps       = 1.0e-05
0.00.038.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.941 I print_info: f_logit_scale    = 0.0e+00
0.00.038.942 I print_info: n_ff             = 8192
0.00.038.942 I print_info: n_expert         = 0
0.00.038.942 I print_info: n_expert_used    = 0
0.00.038.943 I print_info: causal attn      = 1
0.00.038.945 I print_info: pooling type     = 0
0.00.038.945 I print_info: rope type        = 2
0.00.038.945 I print_info: rope scaling     = linear
0.00.038.946 I print_info: freq_base_train  = 10000.0
0.00.038.946 I print_info: freq_scale_train = 1
0.00.038.946 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.946 I print_info: rope_finetuned   = unknown
0.00.038.946 I print_info: ssm_d_conv       = 0
0.00.038.947 I print_info: ssm_d_inner      = 0
0.00.038.947 I print_info: ssm_d_state      = 0
0.00.038.947 I print_info: ssm_dt_rank      = 0
0.00.038.947 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.948 I print_info: model type       = 1.4B
0.00.038.949 I print_info: model params     = 1.41 B
0.00.038.949 I print_info: general.name     = 1.4B
0.00.038.950 I print_info: vocab type       = BPE
0.00.038.950 I print_info: n_vocab          = 50304
0.00.038.950 I print_info: n_merges         = 50009
0.00.038.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.954 I print_info: LF token         = 187 'Ċ'
0.00.038.954 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.955 I print_info: max token length = 1024
0.00.038.955 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.433.311 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.328 I load_tensors: offloading output layer to GPU
0.00.433.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.362 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.364 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.434.962 I llama_init_from_model: n_seq_max     = 1
0.00.434.965 I llama_init_from_model: n_ctx         = 2048
0.00.434.966 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.434.967 I llama_init_from_model: n_batch       = 2048
0.00.434.967 I llama_init_from_model: n_ubatch      = 512
0.00.434.967 I llama_init_from_model: flash_attn    = 0
0.00.434.971 I llama_init_from_model: freq_base     = 10000.0
0.00.434.971 I llama_init_from_model: freq_scale    = 1
0.00.434.973 I ggml_metal_init: allocating
0.00.435.049 I ggml_metal_init: found device: Apple M4
0.00.435.063 I ggml_metal_init: picking default device: Apple M4
0.00.436.932 I ggml_metal_init: using embedded metal library
0.00.442.560 I ggml_metal_init: GPU name:   Apple M4
0.00.442.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.582 I ggml_metal_init: simdgroup reduction   = true
0.00.442.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.583 I ggml_metal_init: has residency sets    = true
0.00.442.583 I ggml_metal_init: has bfloat            = true
0.00.442.584 I ggml_metal_init: use bfloat            = true
0.00.442.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.592 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.462.421 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.522.882 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.522.890 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.527.442 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.527.444 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.527.444 I llama_init_from_model: graph nodes  = 967
0.00.527.445 I llama_init_from_model: graph splits = 2
0.00.527.450 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.527.578 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.527.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.595 I main: llama threadpool init, n_threads = 4
0.00.582.630 I 
0.00.582.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.642 I 
0.00.582.750 I sampler seed: 1234
0.00.582.754 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.582.765 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.582.765 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.582.765 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.081 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.334.082 I llama_perf_context_print:        load time =     573.16 ms
0.01.334.083 I llama_perf_context_print: prompt eval time =      49.87 ms /     7 tokens (    7.12 ms per token,   140.37 tokens per second)
0.01.334.084 I llama_perf_context_print:        eval time =     698.58 ms /    63 runs   (   11.09 ms per token,    90.18 tokens per second)
0.01.334.084 I llama_perf_context_print:       total time =     752.15 ms /    70 tokens
0.01.334.312 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.110s
sys	0m0.180s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.357 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.976 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.981 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.983 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.983 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.984 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.984 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.984 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.985 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.986 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.986 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.987 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.987 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.989 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.989 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.610 I llama_model_loader: - type  f32:  194 tensors
0.00.025.610 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.610 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.611 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.611 I print_info: file format = GGUF V3 (latest)
0.00.025.612 I print_info: file type   = Q4_K - Medium
0.00.025.613 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.639 I load: special tokens cache size = 25
0.00.039.556 I load: token to piece cache size = 0.2984 MB
0.00.039.559 I print_info: arch             = gptneox
0.00.039.559 I print_info: vocab_only       = 0
0.00.039.559 I print_info: n_ctx_train      = 2048
0.00.039.559 I print_info: n_embd           = 2048
0.00.039.559 I print_info: n_layer          = 24
0.00.039.562 I print_info: n_head           = 16
0.00.039.563 I print_info: n_head_kv        = 16
0.00.039.564 I print_info: n_rot            = 32
0.00.039.564 I print_info: n_swa            = 0
0.00.039.564 I print_info: n_embd_head_k    = 128
0.00.039.564 I print_info: n_embd_head_v    = 128
0.00.039.565 I print_info: n_gqa            = 1
0.00.039.566 I print_info: n_embd_k_gqa     = 2048
0.00.039.566 I print_info: n_embd_v_gqa     = 2048
0.00.039.567 I print_info: f_norm_eps       = 1.0e-05
0.00.039.568 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.568 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.568 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.568 I print_info: f_logit_scale    = 0.0e+00
0.00.039.569 I print_info: n_ff             = 8192
0.00.039.569 I print_info: n_expert         = 0
0.00.039.569 I print_info: n_expert_used    = 0
0.00.039.569 I print_info: causal attn      = 1
0.00.039.570 I print_info: pooling type     = 0
0.00.039.570 I print_info: rope type        = 2
0.00.039.570 I print_info: rope scaling     = linear
0.00.039.570 I print_info: freq_base_train  = 10000.0
0.00.039.571 I print_info: freq_scale_train = 1
0.00.039.571 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.571 I print_info: rope_finetuned   = unknown
0.00.039.572 I print_info: ssm_d_conv       = 0
0.00.039.572 I print_info: ssm_d_inner      = 0
0.00.039.572 I print_info: ssm_d_state      = 0
0.00.039.572 I print_info: ssm_dt_rank      = 0
0.00.039.572 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.572 I print_info: model type       = 1.4B
0.00.039.573 I print_info: model params     = 1.41 B
0.00.039.573 I print_info: general.name     = 1.4B
0.00.039.574 I print_info: vocab type       = BPE
0.00.039.574 I print_info: n_vocab          = 50304
0.00.039.574 I print_info: n_merges         = 50009
0.00.039.577 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.577 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.577 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.577 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.578 I print_info: LF token         = 187 'Ċ'
0.00.039.578 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.578 I print_info: max token length = 1024
0.00.039.579 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.517.887 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.902 I load_tensors: offloading output layer to GPU
0.00.517.903 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.939 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.940 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.519.755 I llama_init_from_model: n_seq_max     = 1
0.00.519.762 I llama_init_from_model: n_ctx         = 2048
0.00.519.763 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.763 I llama_init_from_model: n_batch       = 2048
0.00.519.763 I llama_init_from_model: n_ubatch      = 512
0.00.519.764 I llama_init_from_model: flash_attn    = 0
0.00.519.766 I llama_init_from_model: freq_base     = 10000.0
0.00.519.766 I llama_init_from_model: freq_scale    = 1
0.00.519.770 I ggml_metal_init: allocating
0.00.519.884 I ggml_metal_init: found device: Apple M4
0.00.519.904 I ggml_metal_init: picking default device: Apple M4
0.00.521.787 I ggml_metal_init: using embedded metal library
0.00.528.252 I ggml_metal_init: GPU name:   Apple M4
0.00.528.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.258 I ggml_metal_init: simdgroup reduction   = true
0.00.528.258 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.259 I ggml_metal_init: has residency sets    = true
0.00.528.259 I ggml_metal_init: has bfloat            = true
0.00.528.259 I ggml_metal_init: use bfloat            = true
0.00.528.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.222 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.600.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.600.165 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.600.208 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.322 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.604.324 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.604.324 I llama_init_from_model: graph nodes  = 967
0.00.604.324 I llama_init_from_model: graph splits = 2
0.00.604.329 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.604.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.604.454 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.879 I main: llama threadpool init, n_threads = 4
0.00.663.922 I 
0.00.663.936 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.936 I 
0.00.664.098 I sampler seed: 1234
0.00.664.102 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.113 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.113 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.114 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.438.636 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47491.64 tokens per second)
0.01.438.638 I llama_perf_context_print:        load time =     652.82 ms
0.01.438.638 I llama_perf_context_print: prompt eval time =      57.59 ms /     7 tokens (    8.23 ms per token,   121.54 tokens per second)
0.01.438.639 I llama_perf_context_print:        eval time =     714.16 ms /    63 runs   (   11.34 ms per token,    88.22 tokens per second)
0.01.438.639 I llama_perf_context_print:       total time =     775.45 ms /    70 tokens
0.01.438.902 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.110s
sys	0m0.189s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.263 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.201 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.204 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.206 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.206 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.206 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.074 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.074 I llama_model_loader: - type  f32:  194 tensors
0.00.025.084 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.087 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.093 I print_info: file format = GGUF V3 (latest)
0.00.025.093 I print_info: file type   = Q5_K - Medium
0.00.025.095 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.411 I load: special tokens cache size = 25
0.00.039.617 I load: token to piece cache size = 0.2984 MB
0.00.039.623 I print_info: arch             = gptneox
0.00.039.623 I print_info: vocab_only       = 0
0.00.039.624 I print_info: n_ctx_train      = 2048
0.00.039.629 I print_info: n_embd           = 2048
0.00.039.629 I print_info: n_layer          = 24
0.00.039.633 I print_info: n_head           = 16
0.00.039.634 I print_info: n_head_kv        = 16
0.00.039.634 I print_info: n_rot            = 32
0.00.039.634 I print_info: n_swa            = 0
0.00.039.634 I print_info: n_embd_head_k    = 128
0.00.039.635 I print_info: n_embd_head_v    = 128
0.00.039.635 I print_info: n_gqa            = 1
0.00.039.636 I print_info: n_embd_k_gqa     = 2048
0.00.039.636 I print_info: n_embd_v_gqa     = 2048
0.00.039.637 I print_info: f_norm_eps       = 1.0e-05
0.00.039.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.637 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.637 I print_info: f_logit_scale    = 0.0e+00
0.00.039.638 I print_info: n_ff             = 8192
0.00.039.638 I print_info: n_expert         = 0
0.00.039.639 I print_info: n_expert_used    = 0
0.00.039.639 I print_info: causal attn      = 1
0.00.039.639 I print_info: pooling type     = 0
0.00.039.639 I print_info: rope type        = 2
0.00.039.640 I print_info: rope scaling     = linear
0.00.039.640 I print_info: freq_base_train  = 10000.0
0.00.039.640 I print_info: freq_scale_train = 1
0.00.039.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.641 I print_info: rope_finetuned   = unknown
0.00.039.641 I print_info: ssm_d_conv       = 0
0.00.039.641 I print_info: ssm_d_inner      = 0
0.00.039.641 I print_info: ssm_d_state      = 0
0.00.039.641 I print_info: ssm_dt_rank      = 0
0.00.039.641 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.642 I print_info: model type       = 1.4B
0.00.039.642 I print_info: model params     = 1.41 B
0.00.039.642 I print_info: general.name     = 1.4B
0.00.039.642 I print_info: vocab type       = BPE
0.00.039.643 I print_info: n_vocab          = 50304
0.00.039.643 I print_info: n_merges         = 50009
0.00.039.643 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.644 I print_info: LF token         = 187 'Ċ'
0.00.039.644 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.644 I print_info: max token length = 1024
0.00.039.644 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.288 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.295 I load_tensors: offloading output layer to GPU
0.00.591.296 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.315 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.316 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.130 I llama_init_from_model: n_seq_max     = 1
0.00.592.134 I llama_init_from_model: n_ctx         = 2048
0.00.592.135 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.592.135 I llama_init_from_model: n_batch       = 2048
0.00.592.136 I llama_init_from_model: n_ubatch      = 512
0.00.592.136 I llama_init_from_model: flash_attn    = 0
0.00.592.137 I llama_init_from_model: freq_base     = 10000.0
0.00.592.138 I llama_init_from_model: freq_scale    = 1
0.00.592.139 I ggml_metal_init: allocating
0.00.592.188 I ggml_metal_init: found device: Apple M4
0.00.592.199 I ggml_metal_init: picking default device: Apple M4
0.00.593.275 I ggml_metal_init: using embedded metal library
0.00.597.444 I ggml_metal_init: GPU name:   Apple M4
0.00.597.451 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.452 I ggml_metal_init: simdgroup reduction   = true
0.00.597.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.453 I ggml_metal_init: has residency sets    = true
0.00.597.453 I ggml_metal_init: has bfloat            = true
0.00.597.453 I ggml_metal_init: use bfloat            = true
0.00.597.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.457 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.183 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.772 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.641.779 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.641.816 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.724 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.646.726 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.646.726 I llama_init_from_model: graph nodes  = 967
0.00.646.727 I llama_init_from_model: graph splits = 2
0.00.646.732 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.646.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.646.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.426 I main: llama threadpool init, n_threads = 4
0.00.711.470 I 
0.00.711.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.485 I 
0.00.711.670 I sampler seed: 1234
0.00.711.675 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.711.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.711.685 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.711.686 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.571.714 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48332.20 tokens per second)
0.01.571.715 I llama_perf_context_print:        load time =     701.47 ms
0.01.571.716 I llama_perf_context_print: prompt eval time =      51.24 ms /     7 tokens (    7.32 ms per token,   136.62 tokens per second)
0.01.571.718 I llama_perf_context_print:        eval time =     806.22 ms /    63 runs   (   12.80 ms per token,    78.14 tokens per second)
0.01.571.718 I llama_perf_context_print:       total time =     860.97 ms /    70 tokens
0.01.571.955 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.105s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.008 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.934 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.936 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.937 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.938 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.939 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.941 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.954 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.822 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.823 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.823 I llama_model_loader: - type  f32:  194 tensors
0.00.024.824 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.824 I print_info: file format = GGUF V3 (latest)
0.00.024.825 I print_info: file type   = Q6_K
0.00.024.826 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.088 I load: special tokens cache size = 25
0.00.039.255 I load: token to piece cache size = 0.2984 MB
0.00.039.261 I print_info: arch             = gptneox
0.00.039.261 I print_info: vocab_only       = 0
0.00.039.262 I print_info: n_ctx_train      = 2048
0.00.039.262 I print_info: n_embd           = 2048
0.00.039.262 I print_info: n_layer          = 24
0.00.039.268 I print_info: n_head           = 16
0.00.039.269 I print_info: n_head_kv        = 16
0.00.039.269 I print_info: n_rot            = 32
0.00.039.269 I print_info: n_swa            = 0
0.00.039.269 I print_info: n_embd_head_k    = 128
0.00.039.269 I print_info: n_embd_head_v    = 128
0.00.039.270 I print_info: n_gqa            = 1
0.00.039.271 I print_info: n_embd_k_gqa     = 2048
0.00.039.271 I print_info: n_embd_v_gqa     = 2048
0.00.039.272 I print_info: f_norm_eps       = 1.0e-05
0.00.039.272 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.273 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.273 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.273 I print_info: f_logit_scale    = 0.0e+00
0.00.039.274 I print_info: n_ff             = 8192
0.00.039.276 I print_info: n_expert         = 0
0.00.039.276 I print_info: n_expert_used    = 0
0.00.039.276 I print_info: causal attn      = 1
0.00.039.276 I print_info: pooling type     = 0
0.00.039.277 I print_info: rope type        = 2
0.00.039.277 I print_info: rope scaling     = linear
0.00.039.277 I print_info: freq_base_train  = 10000.0
0.00.039.278 I print_info: freq_scale_train = 1
0.00.039.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.278 I print_info: rope_finetuned   = unknown
0.00.039.278 I print_info: ssm_d_conv       = 0
0.00.039.278 I print_info: ssm_d_inner      = 0
0.00.039.279 I print_info: ssm_d_state      = 0
0.00.039.279 I print_info: ssm_dt_rank      = 0
0.00.039.279 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.279 I print_info: model type       = 1.4B
0.00.039.279 I print_info: model params     = 1.41 B
0.00.039.283 I print_info: general.name     = 1.4B
0.00.039.284 I print_info: vocab type       = BPE
0.00.039.284 I print_info: n_vocab          = 50304
0.00.039.284 I print_info: n_merges         = 50009
0.00.039.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: LF token         = 187 'Ċ'
0.00.039.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.285 I print_info: max token length = 1024
0.00.039.286 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.204 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.209 I load_tensors: offloading output layer to GPU
0.00.683.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.229 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.683.231 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.684.281 I llama_init_from_model: n_seq_max     = 1
0.00.684.283 I llama_init_from_model: n_ctx         = 2048
0.00.684.283 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.684.284 I llama_init_from_model: n_batch       = 2048
0.00.684.284 I llama_init_from_model: n_ubatch      = 512
0.00.684.284 I llama_init_from_model: flash_attn    = 0
0.00.684.285 I llama_init_from_model: freq_base     = 10000.0
0.00.684.286 I llama_init_from_model: freq_scale    = 1
0.00.684.295 I ggml_metal_init: allocating
0.00.684.332 I ggml_metal_init: found device: Apple M4
0.00.684.343 I ggml_metal_init: picking default device: Apple M4
0.00.685.451 I ggml_metal_init: using embedded metal library
0.00.689.782 I ggml_metal_init: GPU name:   Apple M4
0.00.689.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.788 I ggml_metal_init: simdgroup reduction   = true
0.00.689.789 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.789 I ggml_metal_init: has residency sets    = true
0.00.689.789 I ggml_metal_init: has bfloat            = true
0.00.689.789 I ggml_metal_init: use bfloat            = true
0.00.689.790 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.269 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.468 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.474 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.510 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.741.358 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.741.361 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.741.361 I llama_init_from_model: graph nodes  = 967
0.00.741.362 I llama_init_from_model: graph splits = 2
0.00.741.367 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.807.535 I main: llama threadpool init, n_threads = 4
0.00.807.579 I 
0.00.807.595 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.807.595 I 
0.00.807.780 I sampler seed: 1234
0.00.807.785 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.807.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.807.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.807.797 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.676.891 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.676.892 I llama_perf_context_print:        load time =     797.79 ms
0.01.676.892 I llama_perf_context_print: prompt eval time =      54.28 ms /     7 tokens (    7.75 ms per token,   128.96 tokens per second)
0.01.676.894 I llama_perf_context_print:        eval time =     811.91 ms /    63 runs   (   12.89 ms per token,    77.59 tokens per second)
0.01.676.894 I llama_perf_context_print:       total time =     870.09 ms /    70 tokens
0.01.677.133 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.103s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.789 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.828 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.415 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.423 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.428 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.428 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.431 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.014 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.089 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.547 I llama_model_loader: - type  f32:  194 tensors
0.00.052.547 I llama_model_loader: - type  f16:   98 tensors
0.00.052.548 I print_info: file format = GGUF V3 (latest)
0.00.052.549 I print_info: file type   = all F32 (guessed)
0.00.052.550 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.078 I load: special tokens cache size = 25
0.00.071.712 I load: token to piece cache size = 0.2984 MB
0.00.071.715 I print_info: arch             = gptneox
0.00.071.715 I print_info: vocab_only       = 0
0.00.071.715 I print_info: n_ctx_train      = 2048
0.00.071.716 I print_info: n_embd           = 2048
0.00.071.716 I print_info: n_layer          = 24
0.00.071.719 I print_info: n_head           = 16
0.00.071.720 I print_info: n_head_kv        = 16
0.00.071.720 I print_info: n_rot            = 32
0.00.071.720 I print_info: n_swa            = 0
0.00.071.721 I print_info: n_embd_head_k    = 128
0.00.071.721 I print_info: n_embd_head_v    = 128
0.00.071.721 I print_info: n_gqa            = 1
0.00.071.722 I print_info: n_embd_k_gqa     = 2048
0.00.071.723 I print_info: n_embd_v_gqa     = 2048
0.00.071.723 I print_info: f_norm_eps       = 1.0e-05
0.00.071.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.724 I print_info: f_logit_scale    = 0.0e+00
0.00.071.725 I print_info: n_ff             = 8192
0.00.071.725 I print_info: n_expert         = 0
0.00.071.725 I print_info: n_expert_used    = 0
0.00.071.725 I print_info: causal attn      = 1
0.00.071.726 I print_info: pooling type     = 0
0.00.071.726 I print_info: rope type        = 2
0.00.071.726 I print_info: rope scaling     = linear
0.00.071.726 I print_info: freq_base_train  = 10000.0
0.00.071.729 I print_info: freq_scale_train = 1
0.00.071.729 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.729 I print_info: rope_finetuned   = unknown
0.00.071.730 I print_info: ssm_d_conv       = 0
0.00.071.730 I print_info: ssm_d_inner      = 0
0.00.071.730 I print_info: ssm_d_state      = 0
0.00.071.730 I print_info: ssm_dt_rank      = 0
0.00.071.730 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.730 I print_info: model type       = 1.4B
0.00.071.731 I print_info: model params     = 1.41 B
0.00.071.731 I print_info: general.name     = 1.4B
0.00.071.731 I print_info: vocab type       = BPE
0.00.071.731 I print_info: n_vocab          = 50304
0.00.071.732 I print_info: n_merges         = 50009
0.00.071.732 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.732 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.732 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.738 I print_info: LF token         = 187 'Ċ'
0.00.071.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.738 I print_info: max token length = 1024
0.00.071.738 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.257.069 I load_tensors: offloading 24 repeating layers to GPU
0.01.257.074 I load_tensors: offloading output layer to GPU
0.01.257.074 I load_tensors: offloaded 25/25 layers to GPU
0.01.257.102 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.257.104 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.258.139 I llama_init_from_model: n_seq_max     = 1
0.01.258.140 I llama_init_from_model: n_ctx         = 128
0.01.258.140 I llama_init_from_model: n_ctx_per_seq = 128
0.01.258.140 I llama_init_from_model: n_batch       = 128
0.01.258.140 I llama_init_from_model: n_ubatch      = 128
0.01.258.140 I llama_init_from_model: flash_attn    = 0
0.01.258.141 I llama_init_from_model: freq_base     = 10000.0
0.01.258.141 I llama_init_from_model: freq_scale    = 1
0.01.258.142 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.258.143 I ggml_metal_init: allocating
0.01.258.208 I ggml_metal_init: found device: Apple M4
0.01.258.219 I ggml_metal_init: picking default device: Apple M4
0.01.259.315 I ggml_metal_init: using embedded metal library
0.01.263.200 I ggml_metal_init: GPU name:   Apple M4
0.01.263.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.263.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.263.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.263.203 I ggml_metal_init: simdgroup reduction   = true
0.01.263.203 I ggml_metal_init: simdgroup matrix mul. = true
0.01.263.204 I ggml_metal_init: has residency sets    = true
0.01.263.204 I ggml_metal_init: has bfloat            = true
0.01.263.204 I ggml_metal_init: use bfloat            = true
0.01.263.204 I ggml_metal_init: hasUnifiedMemory      = true
0.01.263.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.273.830 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.275.557 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.275.559 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.275.584 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.277.351 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.277.352 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.277.352 I llama_init_from_model: graph nodes  = 967
0.01.277.353 I llama_init_from_model: graph splits = 2
0.01.277.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.277.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.313.301 I 
0.01.313.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.313.333 I perplexity: tokenizing the input ..
0.01.318.394 I perplexity: tokenization took 5.058 ms
0.01.318.399 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.450.183 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.451.529 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.451.560 I llama_perf_context_print:        load time =    1292.45 ms
0.01.451.561 I llama_perf_context_print: prompt eval time =     131.48 ms /   128 tokens (    1.03 ms per token,   973.55 tokens per second)
0.01.451.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.451.562 I llama_perf_context_print:       total time =     138.26 ms /   129 tokens
0.01.451.925 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.095s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.198 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.656 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.662 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.666 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.670 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.572 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.572 I llama_model_loader: - type  f32:  194 tensors
0.00.025.573 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.574 I print_info: file format = GGUF V3 (latest)
0.00.025.574 I print_info: file type   = Q8_0
0.00.025.575 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.860 I load: special tokens cache size = 25
0.00.040.067 I load: token to piece cache size = 0.2984 MB
0.00.040.071 I print_info: arch             = gptneox
0.00.040.072 I print_info: vocab_only       = 0
0.00.040.072 I print_info: n_ctx_train      = 2048
0.00.040.072 I print_info: n_embd           = 2048
0.00.040.072 I print_info: n_layer          = 24
0.00.040.076 I print_info: n_head           = 16
0.00.040.077 I print_info: n_head_kv        = 16
0.00.040.077 I print_info: n_rot            = 32
0.00.040.080 I print_info: n_swa            = 0
0.00.040.080 I print_info: n_embd_head_k    = 128
0.00.040.080 I print_info: n_embd_head_v    = 128
0.00.040.081 I print_info: n_gqa            = 1
0.00.040.082 I print_info: n_embd_k_gqa     = 2048
0.00.040.082 I print_info: n_embd_v_gqa     = 2048
0.00.040.083 I print_info: f_norm_eps       = 1.0e-05
0.00.040.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.087 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.087 I print_info: f_logit_scale    = 0.0e+00
0.00.040.088 I print_info: n_ff             = 8192
0.00.040.088 I print_info: n_expert         = 0
0.00.040.088 I print_info: n_expert_used    = 0
0.00.040.088 I print_info: causal attn      = 1
0.00.040.088 I print_info: pooling type     = 0
0.00.040.088 I print_info: rope type        = 2
0.00.040.089 I print_info: rope scaling     = linear
0.00.040.089 I print_info: freq_base_train  = 10000.0
0.00.040.089 I print_info: freq_scale_train = 1
0.00.040.089 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.090 I print_info: rope_finetuned   = unknown
0.00.040.090 I print_info: ssm_d_conv       = 0
0.00.040.090 I print_info: ssm_d_inner      = 0
0.00.040.090 I print_info: ssm_d_state      = 0
0.00.040.090 I print_info: ssm_dt_rank      = 0
0.00.040.090 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.091 I print_info: model type       = 1.4B
0.00.040.091 I print_info: model params     = 1.41 B
0.00.040.091 I print_info: general.name     = 1.4B
0.00.040.092 I print_info: vocab type       = BPE
0.00.040.092 I print_info: n_vocab          = 50304
0.00.040.092 I print_info: n_merges         = 50009
0.00.040.092 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.097 I print_info: LF token         = 187 'Ċ'
0.00.040.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.097 I print_info: max token length = 1024
0.00.040.097 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.855.096 I load_tensors: offloading 24 repeating layers to GPU
0.00.855.104 I load_tensors: offloading output layer to GPU
0.00.855.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.855.132 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.855.135 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.856.562 I llama_init_from_model: n_seq_max     = 1
0.00.856.564 I llama_init_from_model: n_ctx         = 128
0.00.856.564 I llama_init_from_model: n_ctx_per_seq = 128
0.00.856.565 I llama_init_from_model: n_batch       = 128
0.00.856.565 I llama_init_from_model: n_ubatch      = 128
0.00.856.565 I llama_init_from_model: flash_attn    = 0
0.00.856.566 I llama_init_from_model: freq_base     = 10000.0
0.00.856.566 I llama_init_from_model: freq_scale    = 1
0.00.856.567 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.856.569 I ggml_metal_init: allocating
0.00.856.620 I ggml_metal_init: found device: Apple M4
0.00.856.631 I ggml_metal_init: picking default device: Apple M4
0.00.857.950 I ggml_metal_init: using embedded metal library
0.00.863.254 I ggml_metal_init: GPU name:   Apple M4
0.00.863.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.863.257 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.863.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.863.259 I ggml_metal_init: simdgroup reduction   = true
0.00.863.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.863.259 I ggml_metal_init: has residency sets    = true
0.00.863.260 I ggml_metal_init: has bfloat            = true
0.00.863.260 I ggml_metal_init: use bfloat            = true
0.00.863.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.863.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.877.942 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.881.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.881.286 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.881.325 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.884.500 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.884.502 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.884.502 I llama_init_from_model: graph nodes  = 967
0.00.884.503 I llama_init_from_model: graph splits = 2
0.00.884.505 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.884.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.911.188 I 
0.00.911.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.911.242 I perplexity: tokenizing the input ..
0.00.918.138 I perplexity: tokenization took 6.895 ms
0.00.918.143 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.055.252 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.056.604 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.056.630 I llama_perf_context_print:        load time =     901.98 ms
0.01.056.631 I llama_perf_context_print: prompt eval time =     136.88 ms /   128 tokens (    1.07 ms per token,   935.14 tokens per second)
0.01.056.631 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.056.632 I llama_perf_context_print:       total time =     145.45 ms /   129 tokens
0.01.057.000 I ggml_metal_free: deallocating

real	0m1.072s
user	0m0.075s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.593 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.631 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.718 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.719 I llama_model_loader: - type  f32:  194 tensors
0.00.025.719 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.721 I print_info: file format = GGUF V3 (latest)
0.00.025.723 I print_info: file type   = Q4_0
0.00.025.724 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.652 I load: special tokens cache size = 25
0.00.039.740 I load: token to piece cache size = 0.2984 MB
0.00.039.744 I print_info: arch             = gptneox
0.00.039.744 I print_info: vocab_only       = 0
0.00.039.745 I print_info: n_ctx_train      = 2048
0.00.039.745 I print_info: n_embd           = 2048
0.00.039.745 I print_info: n_layer          = 24
0.00.039.749 I print_info: n_head           = 16
0.00.039.750 I print_info: n_head_kv        = 16
0.00.039.750 I print_info: n_rot            = 32
0.00.039.750 I print_info: n_swa            = 0
0.00.039.750 I print_info: n_embd_head_k    = 128
0.00.039.750 I print_info: n_embd_head_v    = 128
0.00.039.751 I print_info: n_gqa            = 1
0.00.039.752 I print_info: n_embd_k_gqa     = 2048
0.00.039.753 I print_info: n_embd_v_gqa     = 2048
0.00.039.753 I print_info: f_norm_eps       = 1.0e-05
0.00.039.754 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.754 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.754 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.754 I print_info: f_logit_scale    = 0.0e+00
0.00.039.756 I print_info: n_ff             = 8192
0.00.039.757 I print_info: n_expert         = 0
0.00.039.757 I print_info: n_expert_used    = 0
0.00.039.757 I print_info: causal attn      = 1
0.00.039.757 I print_info: pooling type     = 0
0.00.039.757 I print_info: rope type        = 2
0.00.039.757 I print_info: rope scaling     = linear
0.00.039.758 I print_info: freq_base_train  = 10000.0
0.00.039.758 I print_info: freq_scale_train = 1
0.00.039.758 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.758 I print_info: rope_finetuned   = unknown
0.00.039.758 I print_info: ssm_d_conv       = 0
0.00.039.759 I print_info: ssm_d_inner      = 0
0.00.039.759 I print_info: ssm_d_state      = 0
0.00.039.759 I print_info: ssm_dt_rank      = 0
0.00.039.759 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.759 I print_info: model type       = 1.4B
0.00.039.760 I print_info: model params     = 1.41 B
0.00.039.760 I print_info: general.name     = 1.4B
0.00.039.760 I print_info: vocab type       = BPE
0.00.039.760 I print_info: n_vocab          = 50304
0.00.039.761 I print_info: n_merges         = 50009
0.00.039.761 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.761 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.761 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: LF token         = 187 'Ċ'
0.00.039.765 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: max token length = 1024
0.00.039.765 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.245 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.260 I load_tensors: offloading output layer to GPU
0.00.632.260 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.300 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.632.302 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.634.153 I llama_init_from_model: n_seq_max     = 1
0.00.634.156 I llama_init_from_model: n_ctx         = 128
0.00.634.157 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.157 I llama_init_from_model: n_batch       = 128
0.00.634.157 I llama_init_from_model: n_ubatch      = 128
0.00.634.158 I llama_init_from_model: flash_attn    = 0
0.00.634.160 I llama_init_from_model: freq_base     = 10000.0
0.00.634.161 I llama_init_from_model: freq_scale    = 1
0.00.634.161 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.163 I ggml_metal_init: allocating
0.00.634.291 I ggml_metal_init: found device: Apple M4
0.00.634.304 I ggml_metal_init: picking default device: Apple M4
0.00.636.267 I ggml_metal_init: using embedded metal library
0.00.641.628 I ggml_metal_init: GPU name:   Apple M4
0.00.641.635 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.636 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.637 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.637 I ggml_metal_init: simdgroup reduction   = true
0.00.641.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.638 I ggml_metal_init: has residency sets    = true
0.00.641.638 I ggml_metal_init: has bfloat            = true
0.00.641.639 I ggml_metal_init: use bfloat            = true
0.00.641.640 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.977 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.553 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.561 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.631 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.667.826 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.667.828 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.667.829 I llama_init_from_model: graph nodes  = 967
0.00.667.829 I llama_init_from_model: graph splits = 2
0.00.667.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.667.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.226 I 
0.00.698.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.299 I perplexity: tokenizing the input ..
0.00.705.678 I perplexity: tokenization took 7.375 ms
0.00.705.685 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.768 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.840.111 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.840.133 I llama_perf_context_print:        load time =     688.62 ms
0.00.840.140 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.65 tokens per second)
0.00.840.142 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.142 I llama_perf_context_print:       total time =     141.91 ms /   129 tokens
0.00.840.506 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.080s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.296 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.302 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.305 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.305 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.305 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.306 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.306 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.308 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.309 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.141 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.972 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.974 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.974 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.975 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.976 I llama_model_loader: - type  f32:  194 tensors
0.00.024.976 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.977 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.977 I print_info: file format = GGUF V3 (latest)
0.00.024.978 I print_info: file type   = Q4_1
0.00.024.980 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.153 I load: special tokens cache size = 25
0.00.039.469 I load: token to piece cache size = 0.2984 MB
0.00.039.473 I print_info: arch             = gptneox
0.00.039.473 I print_info: vocab_only       = 0
0.00.039.473 I print_info: n_ctx_train      = 2048
0.00.039.473 I print_info: n_embd           = 2048
0.00.039.473 I print_info: n_layer          = 24
0.00.039.477 I print_info: n_head           = 16
0.00.039.478 I print_info: n_head_kv        = 16
0.00.039.478 I print_info: n_rot            = 32
0.00.039.478 I print_info: n_swa            = 0
0.00.039.479 I print_info: n_embd_head_k    = 128
0.00.039.479 I print_info: n_embd_head_v    = 128
0.00.039.480 I print_info: n_gqa            = 1
0.00.039.480 I print_info: n_embd_k_gqa     = 2048
0.00.039.482 I print_info: n_embd_v_gqa     = 2048
0.00.039.483 I print_info: f_norm_eps       = 1.0e-05
0.00.039.484 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.484 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.484 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.484 I print_info: f_logit_scale    = 0.0e+00
0.00.039.485 I print_info: n_ff             = 8192
0.00.039.487 I print_info: n_expert         = 0
0.00.039.487 I print_info: n_expert_used    = 0
0.00.039.487 I print_info: causal attn      = 1
0.00.039.488 I print_info: pooling type     = 0
0.00.039.488 I print_info: rope type        = 2
0.00.039.488 I print_info: rope scaling     = linear
0.00.039.488 I print_info: freq_base_train  = 10000.0
0.00.039.489 I print_info: freq_scale_train = 1
0.00.039.489 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.489 I print_info: rope_finetuned   = unknown
0.00.039.489 I print_info: ssm_d_conv       = 0
0.00.039.489 I print_info: ssm_d_inner      = 0
0.00.039.491 I print_info: ssm_d_state      = 0
0.00.039.491 I print_info: ssm_dt_rank      = 0
0.00.039.491 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.491 I print_info: model type       = 1.4B
0.00.039.492 I print_info: model params     = 1.41 B
0.00.039.493 I print_info: general.name     = 1.4B
0.00.039.494 I print_info: vocab type       = BPE
0.00.039.494 I print_info: n_vocab          = 50304
0.00.039.494 I print_info: n_merges         = 50009
0.00.039.494 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.495 I print_info: LF token         = 187 'Ċ'
0.00.039.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.496 I print_info: max token length = 1024
0.00.039.496 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.366 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.382 I load_tensors: offloading output layer to GPU
0.00.642.383 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.417 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.642.418 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.643.967 I llama_init_from_model: n_seq_max     = 1
0.00.643.970 I llama_init_from_model: n_ctx         = 128
0.00.643.971 I llama_init_from_model: n_ctx_per_seq = 128
0.00.643.971 I llama_init_from_model: n_batch       = 128
0.00.643.971 I llama_init_from_model: n_ubatch      = 128
0.00.643.972 I llama_init_from_model: flash_attn    = 0
0.00.643.974 I llama_init_from_model: freq_base     = 10000.0
0.00.643.974 I llama_init_from_model: freq_scale    = 1
0.00.643.975 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.643.978 I ggml_metal_init: allocating
0.00.644.060 I ggml_metal_init: found device: Apple M4
0.00.644.075 I ggml_metal_init: picking default device: Apple M4
0.00.645.850 I ggml_metal_init: using embedded metal library
0.00.652.445 I ggml_metal_init: GPU name:   Apple M4
0.00.652.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.455 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.456 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.457 I ggml_metal_init: simdgroup reduction   = true
0.00.652.457 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.458 I ggml_metal_init: has residency sets    = true
0.00.652.458 I ggml_metal_init: has bfloat            = true
0.00.652.458 I ggml_metal_init: use bfloat            = true
0.00.652.460 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.617 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.178 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.674.186 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.674.233 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.416 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.677.418 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.677.418 I llama_init_from_model: graph nodes  = 967
0.00.677.419 I llama_init_from_model: graph splits = 2
0.00.677.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.677.425 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.581 I 
0.00.701.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.651 I perplexity: tokenizing the input ..
0.00.708.655 I perplexity: tokenization took 7.002 ms
0.00.708.663 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.909 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.842.265 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.842.290 I llama_perf_context_print:        load time =     692.66 ms
0.00.842.290 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.70 tokens per second)
0.00.842.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.291 I llama_perf_context_print:       total time =     140.71 ms /   129 tokens
0.00.842.649 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.079s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.966 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.071 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.074 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.074 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.973 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.834 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.835 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.836 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.836 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.836 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.837 I llama_model_loader: - type  f32:  194 tensors
0.00.024.837 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.838 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.838 I print_info: file format = GGUF V3 (latest)
0.00.024.839 I print_info: file type   = Q5_0
0.00.024.840 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.125 I load: special tokens cache size = 25
0.00.039.321 I load: token to piece cache size = 0.2984 MB
0.00.039.326 I print_info: arch             = gptneox
0.00.039.326 I print_info: vocab_only       = 0
0.00.039.326 I print_info: n_ctx_train      = 2048
0.00.039.326 I print_info: n_embd           = 2048
0.00.039.327 I print_info: n_layer          = 24
0.00.039.331 I print_info: n_head           = 16
0.00.039.332 I print_info: n_head_kv        = 16
0.00.039.332 I print_info: n_rot            = 32
0.00.039.332 I print_info: n_swa            = 0
0.00.039.332 I print_info: n_embd_head_k    = 128
0.00.039.332 I print_info: n_embd_head_v    = 128
0.00.039.333 I print_info: n_gqa            = 1
0.00.039.334 I print_info: n_embd_k_gqa     = 2048
0.00.039.335 I print_info: n_embd_v_gqa     = 2048
0.00.039.335 I print_info: f_norm_eps       = 1.0e-05
0.00.039.338 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.339 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.339 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.339 I print_info: f_logit_scale    = 0.0e+00
0.00.039.339 I print_info: n_ff             = 8192
0.00.039.340 I print_info: n_expert         = 0
0.00.039.340 I print_info: n_expert_used    = 0
0.00.039.340 I print_info: causal attn      = 1
0.00.039.340 I print_info: pooling type     = 0
0.00.039.340 I print_info: rope type        = 2
0.00.039.340 I print_info: rope scaling     = linear
0.00.039.341 I print_info: freq_base_train  = 10000.0
0.00.039.341 I print_info: freq_scale_train = 1
0.00.039.341 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.341 I print_info: rope_finetuned   = unknown
0.00.039.342 I print_info: ssm_d_conv       = 0
0.00.039.342 I print_info: ssm_d_inner      = 0
0.00.039.342 I print_info: ssm_d_state      = 0
0.00.039.342 I print_info: ssm_dt_rank      = 0
0.00.039.345 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.345 I print_info: model type       = 1.4B
0.00.039.346 I print_info: model params     = 1.41 B
0.00.039.346 I print_info: general.name     = 1.4B
0.00.039.346 I print_info: vocab type       = BPE
0.00.039.346 I print_info: n_vocab          = 50304
0.00.039.347 I print_info: n_merges         = 50009
0.00.039.348 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.348 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.348 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.348 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.349 I print_info: LF token         = 187 'Ċ'
0.00.039.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.349 I print_info: max token length = 1024
0.00.039.349 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.703.250 I load_tensors: offloading 24 repeating layers to GPU
0.00.703.265 I load_tensors: offloading output layer to GPU
0.00.703.265 I load_tensors: offloaded 25/25 layers to GPU
0.00.703.298 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.703.300 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.704.921 I llama_init_from_model: n_seq_max     = 1
0.00.704.924 I llama_init_from_model: n_ctx         = 128
0.00.704.925 I llama_init_from_model: n_ctx_per_seq = 128
0.00.704.925 I llama_init_from_model: n_batch       = 128
0.00.704.926 I llama_init_from_model: n_ubatch      = 128
0.00.704.926 I llama_init_from_model: flash_attn    = 0
0.00.704.929 I llama_init_from_model: freq_base     = 10000.0
0.00.704.929 I llama_init_from_model: freq_scale    = 1
0.00.704.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.704.932 I ggml_metal_init: allocating
0.00.705.012 I ggml_metal_init: found device: Apple M4
0.00.705.027 I ggml_metal_init: picking default device: Apple M4
0.00.706.555 I ggml_metal_init: using embedded metal library
0.00.713.002 I ggml_metal_init: GPU name:   Apple M4
0.00.713.008 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.713.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.713.010 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.713.010 I ggml_metal_init: simdgroup reduction   = true
0.00.713.010 I ggml_metal_init: simdgroup matrix mul. = true
0.00.713.011 I ggml_metal_init: has residency sets    = true
0.00.713.011 I ggml_metal_init: has bfloat            = true
0.00.713.011 I ggml_metal_init: use bfloat            = true
0.00.713.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.713.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.382 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.733.944 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.734.002 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.290 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.737.291 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.737.292 I llama_init_from_model: graph nodes  = 967
0.00.737.292 I llama_init_from_model: graph splits = 2
0.00.737.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.737.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.412 I 
0.00.764.467 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.473 I perplexity: tokenizing the input ..
0.00.772.019 I perplexity: tokenization took 7.542 ms
0.00.772.031 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.908.096 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.909.534 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.909.566 I llama_perf_context_print:        load time =     755.43 ms
0.00.909.567 I llama_perf_context_print: prompt eval time =     135.20 ms /   128 tokens (    1.06 ms per token,   946.74 tokens per second)
0.00.909.568 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.909.568 I llama_perf_context_print:       total time =     145.16 ms /   129 tokens
0.00.909.919 I ggml_metal_free: deallocating

real	0m0.924s
user	0m0.080s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.962 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.873 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.875 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.711 I llama_model_loader: - type  f32:  194 tensors
0.00.025.711 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.712 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.712 I print_info: file format = GGUF V3 (latest)
0.00.025.713 I print_info: file type   = Q5_1
0.00.025.714 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.748 I load: special tokens cache size = 25
0.00.039.792 I load: token to piece cache size = 0.2984 MB
0.00.039.796 I print_info: arch             = gptneox
0.00.039.796 I print_info: vocab_only       = 0
0.00.039.797 I print_info: n_ctx_train      = 2048
0.00.039.797 I print_info: n_embd           = 2048
0.00.039.797 I print_info: n_layer          = 24
0.00.039.802 I print_info: n_head           = 16
0.00.039.802 I print_info: n_head_kv        = 16
0.00.039.802 I print_info: n_rot            = 32
0.00.039.803 I print_info: n_swa            = 0
0.00.039.803 I print_info: n_embd_head_k    = 128
0.00.039.803 I print_info: n_embd_head_v    = 128
0.00.039.804 I print_info: n_gqa            = 1
0.00.039.806 I print_info: n_embd_k_gqa     = 2048
0.00.039.806 I print_info: n_embd_v_gqa     = 2048
0.00.039.807 I print_info: f_norm_eps       = 1.0e-05
0.00.039.807 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.807 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.808 I print_info: f_logit_scale    = 0.0e+00
0.00.039.810 I print_info: n_ff             = 8192
0.00.039.810 I print_info: n_expert         = 0
0.00.039.810 I print_info: n_expert_used    = 0
0.00.039.810 I print_info: causal attn      = 1
0.00.039.810 I print_info: pooling type     = 0
0.00.039.810 I print_info: rope type        = 2
0.00.039.811 I print_info: rope scaling     = linear
0.00.039.811 I print_info: freq_base_train  = 10000.0
0.00.039.811 I print_info: freq_scale_train = 1
0.00.039.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.812 I print_info: rope_finetuned   = unknown
0.00.039.812 I print_info: ssm_d_conv       = 0
0.00.039.812 I print_info: ssm_d_inner      = 0
0.00.039.812 I print_info: ssm_d_state      = 0
0.00.039.812 I print_info: ssm_dt_rank      = 0
0.00.039.812 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.813 I print_info: model type       = 1.4B
0.00.039.814 I print_info: model params     = 1.41 B
0.00.039.814 I print_info: general.name     = 1.4B
0.00.039.815 I print_info: vocab type       = BPE
0.00.039.815 I print_info: n_vocab          = 50304
0.00.039.815 I print_info: n_merges         = 50009
0.00.039.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: LF token         = 187 'Ċ'
0.00.039.816 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.816 I print_info: max token length = 1024
0.00.039.817 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.832 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.848 I load_tensors: offloading output layer to GPU
0.00.643.848 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.881 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.643.883 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.645.253 I llama_init_from_model: n_seq_max     = 1
0.00.645.256 I llama_init_from_model: n_ctx         = 128
0.00.645.256 I llama_init_from_model: n_ctx_per_seq = 128
0.00.645.257 I llama_init_from_model: n_batch       = 128
0.00.645.258 I llama_init_from_model: n_ubatch      = 128
0.00.645.258 I llama_init_from_model: flash_attn    = 0
0.00.645.260 I llama_init_from_model: freq_base     = 10000.0
0.00.645.260 I llama_init_from_model: freq_scale    = 1
0.00.645.261 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.645.267 I ggml_metal_init: allocating
0.00.645.344 I ggml_metal_init: found device: Apple M4
0.00.645.357 I ggml_metal_init: picking default device: Apple M4
0.00.646.874 I ggml_metal_init: using embedded metal library
0.00.653.304 I ggml_metal_init: GPU name:   Apple M4
0.00.653.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.309 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.310 I ggml_metal_init: simdgroup reduction   = true
0.00.653.310 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.310 I ggml_metal_init: has residency sets    = true
0.00.653.311 I ggml_metal_init: has bfloat            = true
0.00.653.311 I ggml_metal_init: use bfloat            = true
0.00.653.312 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.117 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.709 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.718 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.776 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.007 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.677.008 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.677.009 I llama_init_from_model: graph nodes  = 967
0.00.677.009 I llama_init_from_model: graph splits = 2
0.00.677.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.677.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.933 I 
0.00.704.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.001 I perplexity: tokenizing the input ..
0.00.712.286 I perplexity: tokenization took 7.282 ms
0.00.712.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.237 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.849.582 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.849.607 I llama_perf_context_print:        load time =     694.96 ms
0.00.849.607 I llama_perf_context_print: prompt eval time =     135.04 ms /   128 tokens (    1.06 ms per token,   947.84 tokens per second)
0.00.849.608 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.608 I llama_perf_context_print:       total time =     144.68 ms /   129 tokens
0.00.850.003 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.079s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.648 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.660 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.663 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.664 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.665 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.665 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.669 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.669 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.669 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.534 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.592 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.423 I llama_model_loader: - type  f32:  194 tensors
0.00.024.424 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.424 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.425 I print_info: file format = GGUF V3 (latest)
0.00.024.425 I print_info: file type   = Q2_K - Medium
0.00.024.427 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.496 I load: special tokens cache size = 25
0.00.038.680 I load: token to piece cache size = 0.2984 MB
0.00.038.684 I print_info: arch             = gptneox
0.00.038.685 I print_info: vocab_only       = 0
0.00.038.685 I print_info: n_ctx_train      = 2048
0.00.038.685 I print_info: n_embd           = 2048
0.00.038.685 I print_info: n_layer          = 24
0.00.038.690 I print_info: n_head           = 16
0.00.038.690 I print_info: n_head_kv        = 16
0.00.038.691 I print_info: n_rot            = 32
0.00.038.691 I print_info: n_swa            = 0
0.00.038.691 I print_info: n_embd_head_k    = 128
0.00.038.691 I print_info: n_embd_head_v    = 128
0.00.038.694 I print_info: n_gqa            = 1
0.00.038.695 I print_info: n_embd_k_gqa     = 2048
0.00.038.696 I print_info: n_embd_v_gqa     = 2048
0.00.038.696 I print_info: f_norm_eps       = 1.0e-05
0.00.038.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.697 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.697 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.697 I print_info: f_logit_scale    = 0.0e+00
0.00.038.698 I print_info: n_ff             = 8192
0.00.038.698 I print_info: n_expert         = 0
0.00.038.698 I print_info: n_expert_used    = 0
0.00.038.698 I print_info: causal attn      = 1
0.00.038.698 I print_info: pooling type     = 0
0.00.038.698 I print_info: rope type        = 2
0.00.038.699 I print_info: rope scaling     = linear
0.00.038.699 I print_info: freq_base_train  = 10000.0
0.00.038.699 I print_info: freq_scale_train = 1
0.00.038.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.700 I print_info: rope_finetuned   = unknown
0.00.038.700 I print_info: ssm_d_conv       = 0
0.00.038.700 I print_info: ssm_d_inner      = 0
0.00.038.700 I print_info: ssm_d_state      = 0
0.00.038.700 I print_info: ssm_dt_rank      = 0
0.00.038.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.700 I print_info: model type       = 1.4B
0.00.038.701 I print_info: model params     = 1.41 B
0.00.038.701 I print_info: general.name     = 1.4B
0.00.038.701 I print_info: vocab type       = BPE
0.00.038.702 I print_info: n_vocab          = 50304
0.00.038.703 I print_info: n_merges         = 50009
0.00.038.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: LF token         = 187 'Ċ'
0.00.038.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.704 I print_info: max token length = 1024
0.00.038.705 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.345.688 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.703 I load_tensors: offloading output layer to GPU
0.00.345.703 I load_tensors: offloaded 25/25 layers to GPU
0.00.345.733 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.345.734 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.347.316 I llama_init_from_model: n_seq_max     = 1
0.00.347.322 I llama_init_from_model: n_ctx         = 128
0.00.347.323 I llama_init_from_model: n_ctx_per_seq = 128
0.00.347.323 I llama_init_from_model: n_batch       = 128
0.00.347.324 I llama_init_from_model: n_ubatch      = 128
0.00.347.324 I llama_init_from_model: flash_attn    = 0
0.00.347.326 I llama_init_from_model: freq_base     = 10000.0
0.00.347.327 I llama_init_from_model: freq_scale    = 1
0.00.347.327 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.347.330 I ggml_metal_init: allocating
0.00.347.407 I ggml_metal_init: found device: Apple M4
0.00.347.419 I ggml_metal_init: picking default device: Apple M4
0.00.349.221 I ggml_metal_init: using embedded metal library
0.00.354.576 I ggml_metal_init: GPU name:   Apple M4
0.00.354.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.601 I ggml_metal_init: simdgroup reduction   = true
0.00.354.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.601 I ggml_metal_init: has residency sets    = true
0.00.354.602 I ggml_metal_init: has bfloat            = true
0.00.354.602 I ggml_metal_init: use bfloat            = true
0.00.354.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.817 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.379.504 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.379.511 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.379.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.383.232 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.383.234 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.383.235 I llama_init_from_model: graph nodes  = 967
0.00.383.235 I llama_init_from_model: graph splits = 2
0.00.383.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.383.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.107 I 
0.00.416.168 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.175 I perplexity: tokenizing the input ..
0.00.422.710 I perplexity: tokenization took 6.532 ms
0.00.422.716 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.463 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.812 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.834 I llama_perf_context_print:        load time =     407.27 ms
0.00.563.835 I llama_perf_context_print: prompt eval time =     139.20 ms /   128 tokens (    1.09 ms per token,   919.53 tokens per second)
0.00.563.836 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.836 I llama_perf_context_print:       total time =     147.73 ms /   129 tokens
0.00.564.210 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.079 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.080 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.080 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.081 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.084 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.994 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.026 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.925 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.926 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.927 I llama_model_loader: - type  f32:  194 tensors
0.00.024.927 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.928 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.928 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.928 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.929 I print_info: file format = GGUF V3 (latest)
0.00.024.929 I print_info: file type   = Q3_K - Medium
0.00.024.930 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.424 I load: special tokens cache size = 25
0.00.039.563 I load: token to piece cache size = 0.2984 MB
0.00.039.566 I print_info: arch             = gptneox
0.00.039.567 I print_info: vocab_only       = 0
0.00.039.567 I print_info: n_ctx_train      = 2048
0.00.039.567 I print_info: n_embd           = 2048
0.00.039.567 I print_info: n_layer          = 24
0.00.039.571 I print_info: n_head           = 16
0.00.039.572 I print_info: n_head_kv        = 16
0.00.039.572 I print_info: n_rot            = 32
0.00.039.572 I print_info: n_swa            = 0
0.00.039.573 I print_info: n_embd_head_k    = 128
0.00.039.576 I print_info: n_embd_head_v    = 128
0.00.039.577 I print_info: n_gqa            = 1
0.00.039.578 I print_info: n_embd_k_gqa     = 2048
0.00.039.578 I print_info: n_embd_v_gqa     = 2048
0.00.039.579 I print_info: f_norm_eps       = 1.0e-05
0.00.039.579 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.580 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.580 I print_info: f_logit_scale    = 0.0e+00
0.00.039.582 I print_info: n_ff             = 8192
0.00.039.582 I print_info: n_expert         = 0
0.00.039.582 I print_info: n_expert_used    = 0
0.00.039.582 I print_info: causal attn      = 1
0.00.039.584 I print_info: pooling type     = 0
0.00.039.584 I print_info: rope type        = 2
0.00.039.584 I print_info: rope scaling     = linear
0.00.039.585 I print_info: freq_base_train  = 10000.0
0.00.039.585 I print_info: freq_scale_train = 1
0.00.039.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.586 I print_info: rope_finetuned   = unknown
0.00.039.586 I print_info: ssm_d_conv       = 0
0.00.039.586 I print_info: ssm_d_inner      = 0
0.00.039.586 I print_info: ssm_d_state      = 0
0.00.039.586 I print_info: ssm_dt_rank      = 0
0.00.039.586 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.586 I print_info: model type       = 1.4B
0.00.039.587 I print_info: model params     = 1.41 B
0.00.039.587 I print_info: general.name     = 1.4B
0.00.039.588 I print_info: vocab type       = BPE
0.00.039.588 I print_info: n_vocab          = 50304
0.00.039.588 I print_info: n_merges         = 50009
0.00.039.588 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.588 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: LF token         = 187 'Ċ'
0.00.039.589 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: max token length = 1024
0.00.039.590 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.434.742 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.760 I load_tensors: offloading output layer to GPU
0.00.434.760 I load_tensors: offloaded 25/25 layers to GPU
0.00.434.796 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.434.797 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.505 I llama_init_from_model: n_seq_max     = 1
0.00.436.508 I llama_init_from_model: n_ctx         = 128
0.00.436.508 I llama_init_from_model: n_ctx_per_seq = 128
0.00.436.509 I llama_init_from_model: n_batch       = 128
0.00.436.509 I llama_init_from_model: n_ubatch      = 128
0.00.436.509 I llama_init_from_model: flash_attn    = 0
0.00.436.512 I llama_init_from_model: freq_base     = 10000.0
0.00.436.512 I llama_init_from_model: freq_scale    = 1
0.00.436.513 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.436.518 I ggml_metal_init: allocating
0.00.436.641 I ggml_metal_init: found device: Apple M4
0.00.436.658 I ggml_metal_init: picking default device: Apple M4
0.00.438.535 I ggml_metal_init: using embedded metal library
0.00.443.723 I ggml_metal_init: GPU name:   Apple M4
0.00.443.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.443.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.443.729 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.443.730 I ggml_metal_init: simdgroup reduction   = true
0.00.443.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.443.731 I ggml_metal_init: has residency sets    = true
0.00.443.731 I ggml_metal_init: has bfloat            = true
0.00.443.731 I ggml_metal_init: use bfloat            = true
0.00.443.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.443.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.876 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.467.477 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.467.480 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.467.523 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.470.995 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.470.996 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.470.997 I llama_init_from_model: graph nodes  = 967
0.00.470.997 I llama_init_from_model: graph splits = 2
0.00.471.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.471.001 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.432 I 
0.00.501.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.501.498 I perplexity: tokenizing the input ..
0.00.507.301 I perplexity: tokenization took 5.801 ms
0.00.507.305 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.308 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.644.732 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.644.755 I llama_perf_context_print:        load time =     492.58 ms
0.00.644.756 I llama_perf_context_print: prompt eval time =     135.76 ms /   128 tokens (    1.06 ms per token,   942.82 tokens per second)
0.00.644.761 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.762 I llama_perf_context_print:       total time =     143.33 ms /   129 tokens
0.00.645.105 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.871 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.665 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.666 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.666 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.667 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.668 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.668 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.670 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.359 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.359 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.360 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.360 I llama_model_loader: - type  f32:  194 tensors
0.00.025.361 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.361 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.361 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.362 I print_info: file format = GGUF V3 (latest)
0.00.025.362 I print_info: file type   = Q4_K - Medium
0.00.025.363 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.373 I load: special tokens cache size = 25
0.00.039.438 I load: token to piece cache size = 0.2984 MB
0.00.039.442 I print_info: arch             = gptneox
0.00.039.442 I print_info: vocab_only       = 0
0.00.039.443 I print_info: n_ctx_train      = 2048
0.00.039.443 I print_info: n_embd           = 2048
0.00.039.443 I print_info: n_layer          = 24
0.00.039.447 I print_info: n_head           = 16
0.00.039.448 I print_info: n_head_kv        = 16
0.00.039.448 I print_info: n_rot            = 32
0.00.039.448 I print_info: n_swa            = 0
0.00.039.448 I print_info: n_embd_head_k    = 128
0.00.039.448 I print_info: n_embd_head_v    = 128
0.00.039.452 I print_info: n_gqa            = 1
0.00.039.453 I print_info: n_embd_k_gqa     = 2048
0.00.039.453 I print_info: n_embd_v_gqa     = 2048
0.00.039.454 I print_info: f_norm_eps       = 1.0e-05
0.00.039.455 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.455 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.455 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.456 I print_info: f_logit_scale    = 0.0e+00
0.00.039.456 I print_info: n_ff             = 8192
0.00.039.457 I print_info: n_expert         = 0
0.00.039.457 I print_info: n_expert_used    = 0
0.00.039.457 I print_info: causal attn      = 1
0.00.039.457 I print_info: pooling type     = 0
0.00.039.457 I print_info: rope type        = 2
0.00.039.458 I print_info: rope scaling     = linear
0.00.039.458 I print_info: freq_base_train  = 10000.0
0.00.039.458 I print_info: freq_scale_train = 1
0.00.039.458 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.459 I print_info: rope_finetuned   = unknown
0.00.039.459 I print_info: ssm_d_conv       = 0
0.00.039.459 I print_info: ssm_d_inner      = 0
0.00.039.459 I print_info: ssm_d_state      = 0
0.00.039.459 I print_info: ssm_dt_rank      = 0
0.00.039.459 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.460 I print_info: model type       = 1.4B
0.00.039.460 I print_info: model params     = 1.41 B
0.00.039.460 I print_info: general.name     = 1.4B
0.00.039.461 I print_info: vocab type       = BPE
0.00.039.461 I print_info: n_vocab          = 50304
0.00.039.462 I print_info: n_merges         = 50009
0.00.039.462 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.463 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.463 I print_info: LF token         = 187 'Ċ'
0.00.039.463 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.464 I print_info: max token length = 1024
0.00.039.464 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.541.338 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.354 I load_tensors: offloading output layer to GPU
0.00.541.355 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.388 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.541.389 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.543.122 I llama_init_from_model: n_seq_max     = 1
0.00.543.125 I llama_init_from_model: n_ctx         = 128
0.00.543.125 I llama_init_from_model: n_ctx_per_seq = 128
0.00.543.126 I llama_init_from_model: n_batch       = 128
0.00.543.126 I llama_init_from_model: n_ubatch      = 128
0.00.543.127 I llama_init_from_model: flash_attn    = 0
0.00.543.129 I llama_init_from_model: freq_base     = 10000.0
0.00.543.129 I llama_init_from_model: freq_scale    = 1
0.00.543.130 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.543.132 I ggml_metal_init: allocating
0.00.543.206 I ggml_metal_init: found device: Apple M4
0.00.543.219 I ggml_metal_init: picking default device: Apple M4
0.00.545.079 I ggml_metal_init: using embedded metal library
0.00.551.480 I ggml_metal_init: GPU name:   Apple M4
0.00.551.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.551.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.551.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.551.488 I ggml_metal_init: simdgroup reduction   = true
0.00.551.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.551.488 I ggml_metal_init: has residency sets    = true
0.00.551.489 I ggml_metal_init: has bfloat            = true
0.00.551.489 I ggml_metal_init: use bfloat            = true
0.00.551.490 I ggml_metal_init: hasUnifiedMemory      = true
0.00.551.495 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.294 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.573.780 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.573.787 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.573.848 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.577.103 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.577.105 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.577.105 I llama_init_from_model: graph nodes  = 967
0.00.577.106 I llama_init_from_model: graph splits = 2
0.00.577.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.577.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.608.750 I 
0.00.608.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.608.837 I perplexity: tokenizing the input ..
0.00.614.354 I perplexity: tokenization took 5.516 ms
0.00.614.358 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.923 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.750.263 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.750.284 I llama_perf_context_print:        load time =     598.86 ms
0.00.750.285 I llama_perf_context_print: prompt eval time =     134.33 ms /   128 tokens (    1.05 ms per token,   952.88 tokens per second)
0.00.750.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.286 I llama_perf_context_print:       total time =     141.54 ms /   129 tokens
0.00.750.720 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.078s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.618 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.620 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.432 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.296 I llama_model_loader: - type  f32:  194 tensors
0.00.024.296 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.296 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.297 I print_info: file format = GGUF V3 (latest)
0.00.024.301 I print_info: file type   = Q5_K - Medium
0.00.024.303 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.280 I load: special tokens cache size = 25
0.00.038.298 I load: token to piece cache size = 0.2984 MB
0.00.038.302 I print_info: arch             = gptneox
0.00.038.303 I print_info: vocab_only       = 0
0.00.038.303 I print_info: n_ctx_train      = 2048
0.00.038.303 I print_info: n_embd           = 2048
0.00.038.303 I print_info: n_layer          = 24
0.00.038.307 I print_info: n_head           = 16
0.00.038.308 I print_info: n_head_kv        = 16
0.00.038.308 I print_info: n_rot            = 32
0.00.038.308 I print_info: n_swa            = 0
0.00.038.309 I print_info: n_embd_head_k    = 128
0.00.038.309 I print_info: n_embd_head_v    = 128
0.00.038.312 I print_info: n_gqa            = 1
0.00.038.312 I print_info: n_embd_k_gqa     = 2048
0.00.038.313 I print_info: n_embd_v_gqa     = 2048
0.00.038.313 I print_info: f_norm_eps       = 1.0e-05
0.00.038.314 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.314 I print_info: f_logit_scale    = 0.0e+00
0.00.038.315 I print_info: n_ff             = 8192
0.00.038.318 I print_info: n_expert         = 0
0.00.038.319 I print_info: n_expert_used    = 0
0.00.038.319 I print_info: causal attn      = 1
0.00.038.320 I print_info: pooling type     = 0
0.00.038.320 I print_info: rope type        = 2
0.00.038.320 I print_info: rope scaling     = linear
0.00.038.320 I print_info: freq_base_train  = 10000.0
0.00.038.320 I print_info: freq_scale_train = 1
0.00.038.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.322 I print_info: rope_finetuned   = unknown
0.00.038.322 I print_info: ssm_d_conv       = 0
0.00.038.322 I print_info: ssm_d_inner      = 0
0.00.038.322 I print_info: ssm_d_state      = 0
0.00.038.323 I print_info: ssm_dt_rank      = 0
0.00.038.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.323 I print_info: model type       = 1.4B
0.00.038.323 I print_info: model params     = 1.41 B
0.00.038.323 I print_info: general.name     = 1.4B
0.00.038.324 I print_info: vocab type       = BPE
0.00.038.324 I print_info: n_vocab          = 50304
0.00.038.327 I print_info: n_merges         = 50009
0.00.038.327 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.327 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.327 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.328 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.328 I print_info: LF token         = 187 'Ċ'
0.00.038.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.328 I print_info: max token length = 1024
0.00.038.329 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.382 I load_tensors: offloading output layer to GPU
0.00.600.383 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.420 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.600.421 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.602.046 I llama_init_from_model: n_seq_max     = 1
0.00.602.048 I llama_init_from_model: n_ctx         = 128
0.00.602.049 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.049 I llama_init_from_model: n_batch       = 128
0.00.602.049 I llama_init_from_model: n_ubatch      = 128
0.00.602.050 I llama_init_from_model: flash_attn    = 0
0.00.602.051 I llama_init_from_model: freq_base     = 10000.0
0.00.602.051 I llama_init_from_model: freq_scale    = 1
0.00.602.052 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.053 I ggml_metal_init: allocating
0.00.602.101 I ggml_metal_init: found device: Apple M4
0.00.602.115 I ggml_metal_init: picking default device: Apple M4
0.00.603.588 I ggml_metal_init: using embedded metal library
0.00.609.582 I ggml_metal_init: GPU name:   Apple M4
0.00.609.586 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.587 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.588 I ggml_metal_init: simdgroup reduction   = true
0.00.609.588 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.588 I ggml_metal_init: has residency sets    = true
0.00.609.589 I ggml_metal_init: has bfloat            = true
0.00.609.589 I ggml_metal_init: use bfloat            = true
0.00.609.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.931 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.235 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.238 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.283 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.469 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.471 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.471 I llama_init_from_model: graph nodes  = 967
0.00.632.471 I llama_init_from_model: graph splits = 2
0.00.632.475 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.475 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.570 I 
0.00.666.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.644 I perplexity: tokenizing the input ..
0.00.673.706 I perplexity: tokenization took 7.058 ms
0.00.673.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.871 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.816.206 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.816.235 I llama_perf_context_print:        load time =     657.68 ms
0.00.816.236 I llama_perf_context_print: prompt eval time =     140.22 ms /   128 tokens (    1.10 ms per token,   912.87 tokens per second)
0.00.816.237 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.237 I llama_perf_context_print:       total time =     149.67 ms /   129 tokens
0.00.816.635 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.078s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.530 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.198 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.198 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.199 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.199 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.201 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.201 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.202 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.202 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.202 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.203 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.205 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.205 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.205 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.132 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.134 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.134 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.135 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.136 I llama_model_loader: - type  f32:  194 tensors
0.00.025.136 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.137 I print_info: file format = GGUF V3 (latest)
0.00.025.137 I print_info: file type   = Q6_K
0.00.025.140 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.387 I load: special tokens cache size = 25
0.00.039.448 I load: token to piece cache size = 0.2984 MB
0.00.039.451 I print_info: arch             = gptneox
0.00.039.452 I print_info: vocab_only       = 0
0.00.039.452 I print_info: n_ctx_train      = 2048
0.00.039.452 I print_info: n_embd           = 2048
0.00.039.452 I print_info: n_layer          = 24
0.00.039.455 I print_info: n_head           = 16
0.00.039.456 I print_info: n_head_kv        = 16
0.00.039.456 I print_info: n_rot            = 32
0.00.039.457 I print_info: n_swa            = 0
0.00.039.457 I print_info: n_embd_head_k    = 128
0.00.039.457 I print_info: n_embd_head_v    = 128
0.00.039.458 I print_info: n_gqa            = 1
0.00.039.458 I print_info: n_embd_k_gqa     = 2048
0.00.039.459 I print_info: n_embd_v_gqa     = 2048
0.00.039.460 I print_info: f_norm_eps       = 1.0e-05
0.00.039.460 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.460 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.460 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.460 I print_info: f_logit_scale    = 0.0e+00
0.00.039.461 I print_info: n_ff             = 8192
0.00.039.461 I print_info: n_expert         = 0
0.00.039.462 I print_info: n_expert_used    = 0
0.00.039.462 I print_info: causal attn      = 1
0.00.039.462 I print_info: pooling type     = 0
0.00.039.462 I print_info: rope type        = 2
0.00.039.462 I print_info: rope scaling     = linear
0.00.039.463 I print_info: freq_base_train  = 10000.0
0.00.039.463 I print_info: freq_scale_train = 1
0.00.039.463 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.463 I print_info: rope_finetuned   = unknown
0.00.039.464 I print_info: ssm_d_conv       = 0
0.00.039.471 I print_info: ssm_d_inner      = 0
0.00.039.473 I print_info: ssm_d_state      = 0
0.00.039.474 I print_info: ssm_dt_rank      = 0
0.00.039.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.474 I print_info: model type       = 1.4B
0.00.039.475 I print_info: model params     = 1.41 B
0.00.039.475 I print_info: general.name     = 1.4B
0.00.039.475 I print_info: vocab type       = BPE
0.00.039.476 I print_info: n_vocab          = 50304
0.00.039.476 I print_info: n_merges         = 50009
0.00.039.476 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: LF token         = 187 'Ċ'
0.00.039.477 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: max token length = 1024
0.00.039.478 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.457.383 I load_tensors: offloading 24 repeating layers to GPU
0.00.457.388 I load_tensors: offloading output layer to GPU
0.00.457.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.457.413 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.457.416 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.458.745 I llama_init_from_model: n_seq_max     = 1
0.00.458.747 I llama_init_from_model: n_ctx         = 128
0.00.458.747 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.748 I llama_init_from_model: n_batch       = 128
0.00.458.748 I llama_init_from_model: n_ubatch      = 128
0.00.458.748 I llama_init_from_model: flash_attn    = 0
0.00.458.749 I llama_init_from_model: freq_base     = 10000.0
0.00.458.750 I llama_init_from_model: freq_scale    = 1
0.00.458.750 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.751 I ggml_metal_init: allocating
0.00.458.771 I ggml_metal_init: found device: Apple M4
0.00.458.778 I ggml_metal_init: picking default device: Apple M4
0.00.459.994 I ggml_metal_init: using embedded metal library
0.00.466.012 I ggml_metal_init: GPU name:   Apple M4
0.00.466.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.466.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.466.017 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.466.018 I ggml_metal_init: simdgroup reduction   = true
0.00.466.018 I ggml_metal_init: simdgroup matrix mul. = true
0.00.466.018 I ggml_metal_init: has residency sets    = true
0.00.466.018 I ggml_metal_init: has bfloat            = true
0.00.466.019 I ggml_metal_init: use bfloat            = true
0.00.466.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.466.029 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.482.201 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.485.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.485.850 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.485.893 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.243 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.489.245 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.489.246 I llama_init_from_model: graph nodes  = 967
0.00.489.246 I llama_init_from_model: graph splits = 2
0.00.489.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.489.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.807 I 
0.00.521.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.881 I perplexity: tokenizing the input ..
0.00.527.890 I perplexity: tokenization took 6.007 ms
0.00.527.894 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.667.161 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.668.569 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.668.598 I llama_perf_context_print:        load time =     512.26 ms
0.00.668.600 I llama_perf_context_print: prompt eval time =     139.03 ms /   128 tokens (    1.09 ms per token,   920.67 tokens per second)
0.00.668.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.668.601 I llama_perf_context_print:       total time =     146.79 ms /   129 tokens
0.00.669.001 I ggml_metal_free: deallocating

real	0m0.683s
user	0m0.075s
sys	0m0.126s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.355 I build: 4723 (b46f4c35) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.367 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.349 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.356 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.360 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.361 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.371 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.372 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.375 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.874 I llama_model_loader: - type  f32:  194 tensors
0.00.055.874 I llama_model_loader: - type  f16:   98 tensors
0.00.055.875 I print_info: file format = GGUF V3 (latest)
0.00.055.876 I print_info: file type   = all F32 (guessed)
0.00.055.877 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.992 I load: special tokens cache size = 25
0.00.075.830 I load: token to piece cache size = 0.2984 MB
0.00.075.833 I print_info: arch             = gptneox
0.00.075.833 I print_info: vocab_only       = 0
0.00.075.834 I print_info: n_ctx_train      = 2048
0.00.075.834 I print_info: n_embd           = 2048
0.00.075.834 I print_info: n_layer          = 24
0.00.075.837 I print_info: n_head           = 16
0.00.075.838 I print_info: n_head_kv        = 16
0.00.075.838 I print_info: n_rot            = 32
0.00.075.839 I print_info: n_swa            = 0
0.00.075.839 I print_info: n_embd_head_k    = 128
0.00.075.839 I print_info: n_embd_head_v    = 128
0.00.075.840 I print_info: n_gqa            = 1
0.00.075.840 I print_info: n_embd_k_gqa     = 2048
0.00.075.841 I print_info: n_embd_v_gqa     = 2048
0.00.075.842 I print_info: f_norm_eps       = 1.0e-05
0.00.075.842 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.843 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.844 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.844 I print_info: f_logit_scale    = 0.0e+00
0.00.075.844 I print_info: n_ff             = 8192
0.00.075.845 I print_info: n_expert         = 0
0.00.075.845 I print_info: n_expert_used    = 0
0.00.075.845 I print_info: causal attn      = 1
0.00.075.845 I print_info: pooling type     = 0
0.00.075.845 I print_info: rope type        = 2
0.00.075.845 I print_info: rope scaling     = linear
0.00.075.846 I print_info: freq_base_train  = 10000.0
0.00.075.846 I print_info: freq_scale_train = 1
0.00.075.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.847 I print_info: rope_finetuned   = unknown
0.00.075.847 I print_info: ssm_d_conv       = 0
0.00.075.847 I print_info: ssm_d_inner      = 0
0.00.075.847 I print_info: ssm_d_state      = 0
0.00.075.849 I print_info: ssm_dt_rank      = 0
0.00.075.849 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.849 I print_info: model type       = 1.4B
0.00.075.849 I print_info: model params     = 1.41 B
0.00.075.850 I print_info: general.name     = 1.4B
0.00.075.850 I print_info: vocab type       = BPE
0.00.075.850 I print_info: n_vocab          = 50304
0.00.075.851 I print_info: n_merges         = 50009
0.00.075.851 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.851 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.851 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.855 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.856 I print_info: LF token         = 187 'Ċ'
0.00.075.856 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.858 I print_info: max token length = 1024
0.00.075.858 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.306.430 I load_tensors: offloading 24 repeating layers to GPU
0.01.306.432 I load_tensors: offloading output layer to GPU
0.01.306.432 I load_tensors: offloaded 25/25 layers to GPU
0.01.306.452 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.306.454 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.307.023 I llama_init_from_model: n_seq_max     = 1
0.01.307.024 I llama_init_from_model: n_ctx         = 128
0.01.307.024 I llama_init_from_model: n_ctx_per_seq = 128
0.01.307.024 I llama_init_from_model: n_batch       = 128
0.01.307.024 I llama_init_from_model: n_ubatch      = 128
0.01.307.024 I llama_init_from_model: flash_attn    = 0
0.01.307.025 I llama_init_from_model: freq_base     = 10000.0
0.01.307.025 I llama_init_from_model: freq_scale    = 1
0.01.307.026 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.307.028 I ggml_metal_init: allocating
0.01.307.080 I ggml_metal_init: found device: Apple M4
0.01.307.088 I ggml_metal_init: picking default device: Apple M4
0.01.307.694 I ggml_metal_init: using embedded metal library
0.01.310.157 I ggml_metal_init: GPU name:   Apple M4
0.01.310.159 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.310.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.310.160 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.310.161 I ggml_metal_init: simdgroup reduction   = true
0.01.310.161 I ggml_metal_init: simdgroup matrix mul. = true
0.01.310.161 I ggml_metal_init: has residency sets    = true
0.01.310.161 I ggml_metal_init: has bfloat            = true
0.01.310.161 I ggml_metal_init: use bfloat            = true
0.01.310.162 I ggml_metal_init: hasUnifiedMemory      = true
0.01.310.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.320.414 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.322.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.322.101 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.322.157 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.323.681 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.323.682 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.323.682 I llama_init_from_model: graph nodes  = 967
0.01.323.683 I llama_init_from_model: graph splits = 2
0.01.323.684 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.323.684 I 
0.01.323.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.323.715 I compute_imatrix: tokenizing the input ..
0.01.327.841 I compute_imatrix: tokenization took 4.124 ms
0.01.327.845 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.596.461 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.599.706 I llama_perf_context_print:        load time =    1572.09 ms
0.01.599.707 I llama_perf_context_print: prompt eval time =     266.71 ms /   128 tokens (    2.08 ms per token,   479.93 tokens per second)
0.01.599.708 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.599.708 I llama_perf_context_print:       total time =    1575.33 ms /   129 tokens
0.01.600.385 I ggml_metal_free: deallocating

real	0m1.798s
user	0m0.133s
sys	0m0.226s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4723 (b46f4c35)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136b07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136b07a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136b07f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136b0abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136b0b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136b0b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136b0ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136b0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136b0c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136b0cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136b0cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136b0d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136b0e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136b0e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136b0efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136b0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136b0fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136b10520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136b10c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136b11410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136b11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136b12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136b12970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136b13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136b13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136b13bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136b14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136b14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136b153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136b15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136b15b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136b15dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136b16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136b16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136b16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136b17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136b177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136b17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136b180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136b18580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136b18a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136b18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136b19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136b19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136b19ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136b1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136b1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136b1b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136b1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136b1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136b1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136b1c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136b1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136b1d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136b1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136b1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136b1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136b1e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136b1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136b1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136b1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136b1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136b20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136b206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136b20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136b21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136b214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136b21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136b21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136b222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136b22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136b22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136b23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136b235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136b23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136b24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136b245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136b24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136b25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136b255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136b25b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136b26060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136b265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136b26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136b27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136b275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136b27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136b28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136b28590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136b28ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136b29030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136b29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136b29ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136b2a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136b2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136b2aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136b2b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136b1acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136b2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136b2bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136b2c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136b2c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136b2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136b2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136b2d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136b2dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136b2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136b2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136b2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136b2f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136b2f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136b2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136b30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136b305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136b30a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136b30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136b313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136b31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136b31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136b321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136b32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136b32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136b32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136b33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136b338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136b33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136b34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136b346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136b34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136b34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136b35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136b35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136b35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136b36260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136b36700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136b36ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136b37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136b374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136b37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136b37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136b382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136b38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136b38c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136b390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136b39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136b399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136b39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136b3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136b3a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136b3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136b3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136b3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136b3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136b3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136b3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136b3c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136b3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136b3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136b3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136b3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136b3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136b3e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136b3e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136b3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136b3f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136b3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136b3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136b3ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136b40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136b408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136b40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136b41220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136b416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136b41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136b42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136b424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136b42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136b42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136b43280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136b43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136b43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136b44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136b44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136b449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136b44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136b452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136b45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136b45c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136b460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136b46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136b46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136b46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136b47340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136b47890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136b47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136b48330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136b48880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136b48b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136b49150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136b49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136b49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136b4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136b4aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136b4acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136b4b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136b4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136b4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136b4c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136b4ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136b4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136b4d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136b4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136b4e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136b4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136b4eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136b4f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136b4f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136b4fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136b500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136b50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136b50b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136b510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136b51620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136b51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136b520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136b52610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136b52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136b530b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136b53600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136b53b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136b540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136b545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136b54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136b55090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136b555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136b55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136b56080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136b565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136b56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136b57070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136b575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136b57b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136b58060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136b585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136b58b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136b59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136b595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136b59af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136b5a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136b5a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136b5aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136b5b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136b5b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136b5bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136b5c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136b5c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136b5cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136b5d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136b5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136b5dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136b5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136b5e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136b5eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136b5eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136b5f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136b5fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136b5ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136b60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136b60920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136b60dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136b61260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136b61700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136b61ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136b62040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136b624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136b62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136b62e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136b632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136b63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136b63c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136b640a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136b64540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136b64a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136b651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136b658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136b65ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136b66710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136b669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136b671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136b67480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136b67a90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.740.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136d04c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136d050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136d05540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136d059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136d05e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136d06290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136d06700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136d06b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136d06fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136d07450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136d078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136d07f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136d08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136d09250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136d09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136d0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136d0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136d0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136d0b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136d0beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136d0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136d0ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136d0d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136d0db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136d0e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136d0e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136d0e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136d0ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136d0f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136d0f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136d0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136d0ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136d103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136d10660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136d10ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136d10f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136d114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136d119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136d11ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136d123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136d128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136d12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136d132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136d137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136d13ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136d14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136d14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136d149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136d14e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136d152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136d15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136d15bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136d16020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136d16490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136d16900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136d170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136d17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136d17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136d17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136d18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136d18ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136d18f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136d19410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136d198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136d19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136d1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136d1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136d1ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136d1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136d1b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136d1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136d1bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136d1c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136d1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136d1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136d1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136d1d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136d1dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136d1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136d1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136d1ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136d1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136d1f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136d1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136d20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136d20760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136d20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136d21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136d21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136d21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136d221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136d22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136d22c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136d231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136d23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136d23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136d241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136d24720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136d24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136d251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136d25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136d25c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136d261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136d26700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136d26c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136d271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136d276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136d27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136d28190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136d286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136d28c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136d29180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136d296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136d29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136d2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136d2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136d2a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136d2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136d2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136d2b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136d2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136d2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136d2c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136d2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136d2ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136d2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136d2d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136d2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136d2e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136d2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136d2ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136d2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136d2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136d2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136d2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136d30130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136d305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136d30a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136d30f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136d313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136d31850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136d31cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136d32190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136d32630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136d32ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136d32f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136d33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136d338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136d33d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136d341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136d34690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136d34b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136d34fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136d35470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136d35910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136d35db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136d36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136d366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136d36b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136d37030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136d374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136d37970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136d37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136d382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136d38750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136d38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136d39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136d39530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136d399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136d39e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136d3a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136d3a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136d3ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136d3b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136d3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136d3ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136d3bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136d3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136d3c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136d3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136d3d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136d3d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136d3da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136d3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136d3e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136d3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136d3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136d3f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136d3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136d3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136d3ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136d40430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136d408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136d40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136d41370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136d418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136d41e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136d420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136d426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136d42cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136d43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136d43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136d43f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136d44250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136d44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136d44e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136d45660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136d45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136d45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136d46440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136d46bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136d47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136d47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136d47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136d48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136d48680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136d48bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136d49120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136d49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136d49bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136d4a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136d4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136d4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136d4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136d4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136d4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136d4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136d4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136d4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136d4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136d4d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136d4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136d4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136d4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136d4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136d4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136d4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136d4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136d500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136d50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136d50b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136d510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136d515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136d51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136d52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136d525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136d52b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136d53080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136d535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136d53b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136d54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136d545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136d54b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136d55060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136d555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136d55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136d56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136d565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136d56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136d57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136d57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136d57ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136d58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136d58580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136d58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136d59020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136d59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136d59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136d59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136d5a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136d5a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136d5ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136d5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136d5b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136d5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136d5bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136d5c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136d5c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136d5ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136d5d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136d5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136d5dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136d5e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136d5e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136d5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136d5f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136d5fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136d5ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136d60750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136d60a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136d61020 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136b67740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136b4af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136b48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136b49a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136b1cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136b1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136b1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136b13eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136b1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136b1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136b1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136b1a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136b1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136b12eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136b4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136b06a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136b1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136b1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136b2b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136b66c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136b16090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136b16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136b4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136b144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136b14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136b14a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136b67ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136b681b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136b68470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136b68730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136b689f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136b68cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136b68f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136b69230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136b694f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136b697b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136b69a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136b69d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136b69ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136b6a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136b6a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136b6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136b6aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136b6adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136b6b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136b6b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136b6b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136b6b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136b6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136b6be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136b6c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136b6c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136b6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136b6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136b6cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136b6ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136b6d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136b6d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136b6d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136b6d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136b6dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136b6df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136b6e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136b6e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136b6e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136b6ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136b6ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136b6efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136b6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136b6f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136b6f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136b6fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136b6fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136b70030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136b702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136b705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136b70870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136b70b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136b70df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136b710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136b71370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136b71630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136b718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136b71bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136b71e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136b72130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136b723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136b726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136b72970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136b72c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136b72ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136b731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136b73470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136b73730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136b739f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136b73cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136b73f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136b74230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136b744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136b747b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136b74a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136b74d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136b74ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136b752b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136b75570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136b75830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136b75af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136b75db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136b76070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136b76330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136b765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136b768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136b76b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136b76e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136b770f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136b773b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136b77670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136b77930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136b77bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136b77eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136b78170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136b78430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136b786f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136b789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136b78c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136b78f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136b791f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136b794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136b79770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136b79a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136b79cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136b79fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136b7a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136b7a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136b7a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136b7aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136b7ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136b7b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136b7b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136b7b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136b7b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136b7bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136b7bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136b7c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136b7c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136b7c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136b7c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136b7cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136b7ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136b7d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136b7d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136b7d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136b7d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136b7dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136b7def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136b7e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136b7e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136b7e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136b7e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136b7ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136b7ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136b7f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136b7f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136b7f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136b7fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136b7fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136b7fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136b802b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136b80570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136b80830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136b80af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136b80db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136b81070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136b81330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136b815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136b818b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136b81b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136b81e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136b820f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136b823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136b82670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136b82930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136b82bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136b82eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136b83170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136b83430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136b836f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136b839b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136b83c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136b83f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136b841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136b844b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136b84770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136b84a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136b84cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136b84fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136b85270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136b85530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136b857f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136b85ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136b85d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136b86030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136b862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136b865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136b86870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136b86b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136b86df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136b870b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136b87370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136b87630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136b87c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136b87ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136b88180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136b88440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136b88700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136b889c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136b88c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136b88f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136b89200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136b894c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136b89780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136b89a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136b89d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136b89fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136b8a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136b8a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136b8a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136b8aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136b8ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136b8b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136b8b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136b8b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136b8b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136b8bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136b8be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136b8c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136b8c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136b8c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136b8c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136b8cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136b8ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136b8d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136b8d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136b8dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136b8e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136b8e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136b8ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136b8f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136b8f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136b8fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136b90110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136b90660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136b90bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136b91100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136b91650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136b91ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136b920f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136b92640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136b92b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136b930e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136b93630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136b93b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136b940d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136b94620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136b94b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136b950c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136b95610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136b95ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136b95f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136b963f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136b96890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136b96d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136b971d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136b97670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136b97b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136b97fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136b98450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136b988f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136b98d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136b99230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136b996d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136b99b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136b9a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136b9a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136b9af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136b9b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136b9bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136b9c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136b9c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136b9cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136b9d0c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.792s
user	0m0.275s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4723 (b46f4c35)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12760e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12760f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12760f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12760fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127610190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127610740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127610cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1276112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127611850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127611d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127612250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127612750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127613270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127614230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127614950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x127615070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127615790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127615eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127616680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1276174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127617be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127618ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x127618e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127619470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12761a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12761a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12761a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12761ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12761b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12761b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12761be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12761c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12761c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12761ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12761ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12761d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12761d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12761dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12761e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12761e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12761ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12761ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12761f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12761f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127620270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127620880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127620e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1276214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127621ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1276220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1276226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127622ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127623360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127623800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127623ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1276240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1276248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127624b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1276254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127625960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127625e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1276262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127626be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127627080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127627520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1276279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127627e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127628300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127628850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127628da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1276292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127629840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127629d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12762a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12762a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12762ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12762b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12762b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12762bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12762c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12762c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12762cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12762d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12762d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12762dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12762e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12762e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12762ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12762f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12762f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12762fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127630280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12761ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1276306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127630ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1276313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127631940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1276323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127632e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1276333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127633920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127633e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1276343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127634910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127634e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1276353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127635850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127635cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127636190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127636630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127636ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127636f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127637410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1276378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127637d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1276381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127638690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127638fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127639470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127639910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127639db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12763a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12763a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12763ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12763b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12763b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12763b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12763be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12763c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12763c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12763cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12763d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12763d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12763d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12763de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12763e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12763e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12763ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12763f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12763f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12763fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12763fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127640370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127640810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127640cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127641150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1276415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127641a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127641f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1276423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127642870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127642d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1276431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127643650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127643af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127643f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127644430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1276448d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127645210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1276456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127645b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127645ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127646930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127646dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127647270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127647bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1276484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127648990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127648e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1276492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127649c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12764a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12764a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12764a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12764ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12764b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12764b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12764bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12764c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12764c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12764cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12764d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12764d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12764daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12764ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12764e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12764e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12764efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12764f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12764fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12764ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x127650540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127650b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1276517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127651c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127652120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1276528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127652e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127653370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1276538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127653e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127654360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1276548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127654e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127655350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1276558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127655df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127656340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127656890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127656de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127657330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127657880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127657dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127658320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127658870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127658dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127659310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127659860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127659db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12765a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12765a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12765ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12765b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12765b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12765bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12765c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12765c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12765cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12765d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12765d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12765dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12765e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12765e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12765ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12765f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12765f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12765fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1276602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1276607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127660d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127661290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1276617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127661d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127662280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1276627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127662d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127663270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1276637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127663d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127664260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1276647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127664d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127665250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1276656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127665b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127666030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1276664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127666970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127666e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1276672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127667750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127667bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127668090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1276689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127668e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127669310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1276697b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127669d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12766a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12766ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12766b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12766b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12766bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12766c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12766c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12766cd00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128804c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1288050d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x128805540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1288059b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128805e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128806290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128806700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128806b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128806fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x128807450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1288078c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128807f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128808aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128809250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128809a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12880a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12880a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12880afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12880b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12880beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12880c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12880ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12880d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12880db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12880e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12880e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12880e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12880ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12880f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12880f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12880fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12880ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1288103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128810660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128810ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x128810f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1288114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1288119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128811ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1288123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1288128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128812da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1288132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1288137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x128813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x128814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1288149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1288152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x128815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x128816490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128816900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1288170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128817570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128817830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128817e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128818630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128818ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x128818f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128819410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1288198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128819d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12881a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12881a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12881ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12881afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12881b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12881b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12881bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12881c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12881c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12881ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12881d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12881d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12881dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12881e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12881e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12881ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12881f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12881f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12881fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x128820210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x128820760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x128820cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x128821200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x128821750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128821ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1288221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128822740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128822c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1288231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128823730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128823c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1288241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x128824720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128824c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1288251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x128825710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128825c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1288261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128826700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x128826c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1288271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1288276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128827c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128828190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1288286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128828c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x128829180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1288296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128829b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12882a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12882a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12882a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12882adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12882b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12882b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12882bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12882c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12882c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12882c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12882ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12882d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12882d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12882dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12882e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12882e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12882ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12882eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12882f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12882f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12882fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x128830130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1288305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x128830a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x128830f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1288313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x128831850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x128831cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x128832190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x128832630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x128832ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x128832f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128833410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1288338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x128833d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1288341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x128834690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128834b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128834fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128835470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128835910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128835db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128836250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1288366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128836b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x128837030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1288374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x128837970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128837e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1288382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128838750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128838bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128839090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128839530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1288399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128839e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12883a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12883a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12883ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12883b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12883b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12883ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12883bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12883c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12883c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12883ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12883d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12883d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12883da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12883df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12883e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12883e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12883ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12883f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12883f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12883faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12883ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x128840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1288408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x128840e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x128841370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1288418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x128841e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1288420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1288426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x128842cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x128843300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x128843af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x128843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x128844250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128844860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128844e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128845660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x128845b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x128845fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128846440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128847140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x128847690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128847be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128848130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128848680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x128848bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128849120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128849670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128849bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12884a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12884a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12884abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12884b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12884b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12884bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12884c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12884c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12884cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12884d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12884d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12884db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12884e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12884e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12884eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12884f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12884f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12884fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1288500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x128850600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x128850b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1288510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1288515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x128851b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x128852090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1288525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x128852b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x128853080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1288535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128853b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128854070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1288545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x128854b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128855060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1288555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128855b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128856050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1288565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128856af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128857040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x128857590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128857ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128858030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128858580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128858ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128859020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128859570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128859a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128859eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12885a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12885a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12885ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12885b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12885b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12885ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12885bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12885c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12885c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12885ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12885d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12885d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12885dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12885e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12885e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12885ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12885f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12885fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12885ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128860750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x128860a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128861020 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12766c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12764e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12764e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12764ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127621d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127621760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127623d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127650800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12761fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127620530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x127620b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12761eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127621150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127618120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1276309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12766bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12761b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12761b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127650e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12764f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1276199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127619cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12766d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12766d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12766d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12766d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12766dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12766df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12766e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12766e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12766e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12766ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12766ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12766efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12766f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12766f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12766f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12766faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12766fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127670020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1276702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1276705a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127670860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127670b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127670de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1276710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127671360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127671620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1276718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127671ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127671e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127672120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1276723e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1276726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127672960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127672c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127672ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1276731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127673460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127673720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1276739e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127673ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127673f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127674220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1276744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1276747a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127674a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127674d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127674fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1276752a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x127675560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x127675820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x127675ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x127675da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x127676060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x127676320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1276765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1276768a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x127676b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x127676e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1276770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1276773a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x127677660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x127677920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x127677be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x127677ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127678160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x127678420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1276786e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1276789a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127678c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127678f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1276791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1276794a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127679760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127679a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127679ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127679fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12767a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12767a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12767a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12767aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12767ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12767b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12767b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12767b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12767b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12767bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12767bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12767c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12767c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12767c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12767c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12767cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12767ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12767d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12767d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12767d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12767d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12767dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12767dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12767e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12767e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12767e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12767e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12767eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12767ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12767f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12767f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12767f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12767fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12767fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12767ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1276802a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x127680560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x127680820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x127680ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x127680da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127681060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x127681320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1276815e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1276818a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x127681b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x127681e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1276820e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1276823a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127682660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127682920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127682be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127682ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127683160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127683420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1276836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1276839a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127683c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127683f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1276841e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1276844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127684760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127684a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127684ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127684fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127685260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127685520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1276857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127685aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127685d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127686020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1276862e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1276865a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127686860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127686b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127686de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1276870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127687360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127687620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1276878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127687ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127687e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127688120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1276883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1276886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127688960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127688c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127688ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1276891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127689460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127689720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1276899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127689ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x127689f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12768a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12768a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12768a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12768aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12768ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12768afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12768b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12768b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12768b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12768bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12768bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12768c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12768c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12768c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12768c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12768ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12768d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12768d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12768dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12768e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12768e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12768e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12768ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12768f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12768f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12768fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12768ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1276903a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127690810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127690c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1276910f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127691560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1276919d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127691e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1276922b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127692720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127692b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127693000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127693470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1276938e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127693d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1276941c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127694630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127694aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127694f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127695380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1276957f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127695c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1276960d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127696540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1276969b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127696e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x127697290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127697700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127697b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127697fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127698450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1276988c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127698d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1276991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x127699610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127699a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127699ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12769a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12769a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12769ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12769b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12769b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12769b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12769be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12769c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12769c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12769cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12769cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12769d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12769d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12769dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12769e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12769e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12769ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12769eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12769f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12769f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12769fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1276a0090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1276a0500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1276a0970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1276a0de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1276a1250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1276a1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1276a23e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1276a2b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1276a3220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1276a34e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1276a3cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1276a3f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1276a45a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.231s
sys	0m0.186s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
