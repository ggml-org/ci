### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.43 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.45 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.00 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.52 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.74 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.36 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.94 sec*proc (28 tests)

Total Test time (real) = 222.95 sec

real	3m42.988s
user	7m44.776s
sys	0m6.635s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.35 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.29 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.39 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.10 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.45 sec*proc (28 tests)

Total Test time (real) =  51.46 sec

real	0m51.474s
user	1m11.707s
sys	0m5.594s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.144 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.557 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.828 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.835 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.839 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.841 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.842 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.842 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.844 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.845 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.845 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.846 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.846 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.850 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.850 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.851 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.852 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.852 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.853 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.854 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.994 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.031.223 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.225 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.031.225 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.031.226 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.031.226 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.031.227 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.031.227 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.031.228 I llama_model_loader: - type  f32:  124 tensors
0.00.031.228 I llama_model_loader: - type  f16:   73 tensors
0.00.035.624 I llm_load_vocab: special tokens cache size = 5
0.00.037.672 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.676 I llm_load_print_meta: arch             = bert
0.00.037.677 I llm_load_print_meta: vocab type       = WPM
0.00.037.677 I llm_load_print_meta: n_vocab          = 30522
0.00.037.678 I llm_load_print_meta: n_merges         = 0
0.00.037.678 I llm_load_print_meta: vocab_only       = 0
0.00.037.678 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.679 I llm_load_print_meta: n_embd           = 384
0.00.037.680 I llm_load_print_meta: n_layer          = 12
0.00.037.684 I llm_load_print_meta: n_head           = 12
0.00.037.685 I llm_load_print_meta: n_head_kv        = 12
0.00.037.685 I llm_load_print_meta: n_rot            = 32
0.00.037.685 I llm_load_print_meta: n_swa            = 0
0.00.037.685 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.686 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.686 I llm_load_print_meta: n_gqa            = 1
0.00.037.687 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.688 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.689 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.689 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.691 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.691 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.691 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.692 I llm_load_print_meta: n_ff             = 1536
0.00.037.693 I llm_load_print_meta: n_expert         = 0
0.00.037.693 I llm_load_print_meta: n_expert_used    = 0
0.00.037.693 I llm_load_print_meta: causal attn      = 0
0.00.037.693 I llm_load_print_meta: pooling type     = 2
0.00.037.694 I llm_load_print_meta: rope type        = 2
0.00.037.694 I llm_load_print_meta: rope scaling     = linear
0.00.037.694 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.695 I llm_load_print_meta: freq_scale_train = 1
0.00.037.695 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.695 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.696 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.696 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.697 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.698 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.698 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.698 I llm_load_print_meta: model type       = 33M
0.00.037.699 I llm_load_print_meta: model ftype      = F16
0.00.037.699 I llm_load_print_meta: model params     = 33.21 M
0.00.037.700 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.700 I llm_load_print_meta: general.name     = Bge Small
0.00.037.701 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.701 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.702 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.702 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.702 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.702 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.705 I llm_load_print_meta: max token length = 21
0.00.039.842 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.843 I llm_load_tensors: offloading output layer to GPU
0.00.039.845 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.873 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.875 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.454 I llama_new_context_with_model: n_ctx         = 512
0.00.040.454 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.455 I llama_new_context_with_model: n_batch       = 2048
0.00.040.455 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.455 I llama_new_context_with_model: flash_attn    = 0
0.00.040.456 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.456 I llama_new_context_with_model: freq_scale    = 1
0.00.040.457 I ggml_metal_init: allocating
0.00.040.466 I ggml_metal_init: found device: Apple M4
0.00.040.470 I ggml_metal_init: picking default device: Apple M4
0.00.041.337 I ggml_metal_init: using embedded metal library
0.00.045.620 I ggml_metal_init: GPU name:   Apple M4
0.00.045.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.624 I ggml_metal_init: simdgroup reduction   = true
0.00.045.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.624 I ggml_metal_init: has bfloat            = true
0.00.045.625 I ggml_metal_init: use bfloat            = true
0.00.045.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.681 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.231 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.233 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.234 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.987 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.988 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.989 I llama_new_context_with_model: graph nodes  = 429
0.00.058.989 I llama_new_context_with_model: graph splits = 2
0.00.058.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.058.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.691 I 
0.00.065.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.377 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.503 I llama_perf_context_print:        load time =      45.13 ms
0.00.071.504 I llama_perf_context_print: prompt eval time =       4.98 ms /     9 tokens (    0.55 ms per token,  1808.68 tokens per second)
0.00.071.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.505 I llama_perf_context_print:       total time =       5.81 ms /    10 tokens
0.00.071.647 I ggml_metal_free: deallocating

real	0m0.269s
user	0m0.050s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.079 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.036 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.040 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.041 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.041 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.042 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.043 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.043 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.043 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.044 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.044 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.046 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.046 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.047 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.047 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.048 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.048 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.050 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.110 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.111 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.112 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.112 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.112 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.113 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.113 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.114 I llama_model_loader: - type  f32:  124 tensors
0.00.014.114 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.550 I llm_load_vocab: special tokens cache size = 5
0.00.017.801 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.804 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.804 I llm_load_print_meta: arch             = bert
0.00.017.804 I llm_load_print_meta: vocab type       = WPM
0.00.017.805 I llm_load_print_meta: n_vocab          = 30522
0.00.017.805 I llm_load_print_meta: n_merges         = 0
0.00.017.805 I llm_load_print_meta: vocab_only       = 0
0.00.017.805 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.805 I llm_load_print_meta: n_embd           = 384
0.00.017.805 I llm_load_print_meta: n_layer          = 12
0.00.017.808 I llm_load_print_meta: n_head           = 12
0.00.017.809 I llm_load_print_meta: n_head_kv        = 12
0.00.017.809 I llm_load_print_meta: n_rot            = 32
0.00.017.809 I llm_load_print_meta: n_swa            = 0
0.00.017.809 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.809 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.810 I llm_load_print_meta: n_gqa            = 1
0.00.017.811 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.811 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.812 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.812 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.812 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.812 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.813 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.813 I llm_load_print_meta: n_ff             = 1536
0.00.017.813 I llm_load_print_meta: n_expert         = 0
0.00.017.813 I llm_load_print_meta: n_expert_used    = 0
0.00.017.814 I llm_load_print_meta: causal attn      = 0
0.00.017.814 I llm_load_print_meta: pooling type     = 2
0.00.017.814 I llm_load_print_meta: rope type        = 2
0.00.017.814 I llm_load_print_meta: rope scaling     = linear
0.00.017.815 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.815 I llm_load_print_meta: freq_scale_train = 1
0.00.017.815 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.816 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.818 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.818 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.818 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.818 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.818 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.818 I llm_load_print_meta: model type       = 33M
0.00.017.818 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.819 I llm_load_print_meta: model params     = 33.21 M
0.00.017.819 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.820 I llm_load_print_meta: general.name     = Bge Small
0.00.017.820 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.820 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.820 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.820 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.822 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.822 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.822 I llm_load_print_meta: max token length = 21
0.00.019.029 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.029 I llm_load_tensors: offloading output layer to GPU
0.00.019.030 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.039 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.040 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.382 I llama_new_context_with_model: n_ctx         = 512
0.00.019.382 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.382 I llama_new_context_with_model: n_batch       = 2048
0.00.019.382 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.383 I llama_new_context_with_model: flash_attn    = 0
0.00.019.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.383 I llama_new_context_with_model: freq_scale    = 1
0.00.019.384 I ggml_metal_init: allocating
0.00.019.387 I ggml_metal_init: found device: Apple M4
0.00.019.389 I ggml_metal_init: picking default device: Apple M4
0.00.019.996 I ggml_metal_init: using embedded metal library
0.00.022.358 I ggml_metal_init: GPU name:   Apple M4
0.00.022.360 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.360 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.361 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.361 I ggml_metal_init: simdgroup reduction   = true
0.00.022.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.361 I ggml_metal_init: has bfloat            = true
0.00.022.362 I ggml_metal_init: use bfloat            = true
0.00.022.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.363 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.783 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.307 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.310 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.314 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.033.971 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.033.972 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.033.972 I llama_new_context_with_model: graph nodes  = 429
0.00.033.973 I llama_new_context_with_model: graph splits = 2
0.00.033.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.033.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.399 I 
0.00.039.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.944 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.416 I llama_perf_context_print:        load time =      30.32 ms
0.00.044.417 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2068.49 tokens per second)
0.00.044.418 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.419 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.044.585 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.163 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.053 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.059 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.060 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.066 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.067 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.068 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.069 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.073 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.073 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.074 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.076 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.079 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.079 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.041.031 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.046.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.046.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.046.181 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.046.182 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.046.182 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.046.182 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.046.183 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.046.183 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.046.184 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.046.184 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.046.185 I llama_model_loader: - type  f32:   40 tensors
0.00.046.190 I llama_model_loader: - type  f16:   30 tensors
0.00.065.170 W llm_load_vocab: empty token at index 5
0.00.070.028 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.071.457 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.490 I llm_load_vocab: special tokens cache size = 5
0.00.331.139 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.331.177 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.331.178 I llm_load_print_meta: arch             = jina-bert-v2
0.00.331.179 I llm_load_print_meta: vocab type       = BPE
0.00.331.179 I llm_load_print_meta: n_vocab          = 61056
0.00.331.179 I llm_load_print_meta: n_merges         = 39382
0.00.331.179 I llm_load_print_meta: vocab_only       = 0
0.00.331.179 I llm_load_print_meta: n_ctx_train      = 8192
0.00.331.180 I llm_load_print_meta: n_embd           = 384
0.00.331.180 I llm_load_print_meta: n_layer          = 4
0.00.331.197 I llm_load_print_meta: n_head           = 12
0.00.331.198 I llm_load_print_meta: n_head_kv        = 12
0.00.331.198 I llm_load_print_meta: n_rot            = 32
0.00.331.198 I llm_load_print_meta: n_swa            = 0
0.00.331.202 I llm_load_print_meta: n_embd_head_k    = 32
0.00.331.202 I llm_load_print_meta: n_embd_head_v    = 32
0.00.331.203 I llm_load_print_meta: n_gqa            = 1
0.00.331.203 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.331.204 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.331.206 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.331.207 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.331.207 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.331.207 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.331.207 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.331.208 I llm_load_print_meta: n_ff             = 1536
0.00.331.208 I llm_load_print_meta: n_expert         = 0
0.00.331.208 I llm_load_print_meta: n_expert_used    = 0
0.00.331.209 I llm_load_print_meta: causal attn      = 0
0.00.331.209 I llm_load_print_meta: pooling type     = -1
0.00.331.210 I llm_load_print_meta: rope type        = -1
0.00.331.210 I llm_load_print_meta: rope scaling     = linear
0.00.331.210 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.331.211 I llm_load_print_meta: freq_scale_train = 1
0.00.331.211 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.331.211 I llm_load_print_meta: rope_finetuned   = unknown
0.00.331.211 I llm_load_print_meta: ssm_d_conv       = 0
0.00.331.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.331.212 I llm_load_print_meta: ssm_d_state      = 0
0.00.331.212 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.331.212 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.331.213 I llm_load_print_meta: model type       = 33M
0.00.331.214 I llm_load_print_meta: model ftype      = F16
0.00.331.214 I llm_load_print_meta: model params     = 32.90 M
0.00.331.216 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.331.216 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.331.219 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.331.219 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.331.219 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.331.219 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.331.219 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.331.219 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.331.220 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.331.220 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.331.222 I llm_load_print_meta: max token length = 45
0.00.332.986 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.332.987 I llm_load_tensors: offloading output layer to GPU
0.00.332.987 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.333.013 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.014 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.333.914 I llama_new_context_with_model: n_seq_max     = 1
0.00.333.916 I llama_new_context_with_model: n_ctx         = 8192
0.00.333.916 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.333.916 I llama_new_context_with_model: n_batch       = 2048
0.00.333.916 I llama_new_context_with_model: n_ubatch      = 2048
0.00.333.916 I llama_new_context_with_model: flash_attn    = 0
0.00.333.917 I llama_new_context_with_model: freq_base     = 10000.0
0.00.333.917 I llama_new_context_with_model: freq_scale    = 1
0.00.333.917 I ggml_metal_init: allocating
0.00.333.921 I ggml_metal_init: found device: Apple M4
0.00.333.923 I ggml_metal_init: picking default device: Apple M4
0.00.334.710 I ggml_metal_init: using embedded metal library
0.00.337.607 I ggml_metal_init: GPU name:   Apple M4
0.00.337.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.609 I ggml_metal_init: simdgroup reduction   = true
0.00.337.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.610 I ggml_metal_init: has bfloat            = true
0.00.337.610 I ggml_metal_init: use bfloat            = true
0.00.337.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.611 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.215 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.349.776 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.781 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.782 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.491 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.492 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.492 I llama_new_context_with_model: graph nodes  = 154
0.00.350.492 I llama_new_context_with_model: graph splits = 2
0.00.350.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.350.494 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.408 I 
0.00.363.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.594 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.363.595 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.363.607 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.363.607 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.363.613 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.363.613 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.172 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.367.722 I llama_perf_context_print:        load time =     342.57 ms
0.00.367.723 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17519.07 tokens per second)
0.00.367.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.367.724 I llama_perf_context_print:       total time =       4.32 ms /    63 tokens
0.00.367.960 I ggml_metal_free: deallocating

real	0m1.077s
user	0m0.340s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.177 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.286 I main: llama backend init
0.00.000.293 I main: load the model and apply lora adapter, if any
0.00.031.082 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.402 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.418 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.421 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.422 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.426 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.430 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.023 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.025 I llama_model_loader: - type  f32:  194 tensors
0.00.062.025 I llama_model_loader: - type  f16:   98 tensors
0.00.091.237 I llm_load_vocab: special tokens cache size = 25
0.00.097.930 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.933 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.933 I llm_load_print_meta: arch             = gptneox
0.00.097.933 I llm_load_print_meta: vocab type       = BPE
0.00.097.934 I llm_load_print_meta: n_vocab          = 50304
0.00.097.934 I llm_load_print_meta: n_merges         = 50009
0.00.097.934 I llm_load_print_meta: vocab_only       = 0
0.00.097.934 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.934 I llm_load_print_meta: n_embd           = 2048
0.00.097.934 I llm_load_print_meta: n_layer          = 24
0.00.097.938 I llm_load_print_meta: n_head           = 16
0.00.097.939 I llm_load_print_meta: n_head_kv        = 16
0.00.097.939 I llm_load_print_meta: n_rot            = 32
0.00.097.939 I llm_load_print_meta: n_swa            = 0
0.00.097.939 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.939 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.940 I llm_load_print_meta: n_gqa            = 1
0.00.097.941 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.941 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.942 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.942 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.943 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.943 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.943 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.943 I llm_load_print_meta: n_ff             = 8192
0.00.097.944 I llm_load_print_meta: n_expert         = 0
0.00.097.944 I llm_load_print_meta: n_expert_used    = 0
0.00.097.944 I llm_load_print_meta: causal attn      = 1
0.00.097.944 I llm_load_print_meta: pooling type     = 0
0.00.097.944 I llm_load_print_meta: rope type        = 2
0.00.097.944 I llm_load_print_meta: rope scaling     = linear
0.00.097.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.945 I llm_load_print_meta: freq_scale_train = 1
0.00.097.945 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.945 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.945 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.948 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.949 I llm_load_print_meta: model type       = 1.4B
0.00.097.949 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.950 I llm_load_print_meta: model params     = 1.41 B
0.00.097.950 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.952 I llm_load_print_meta: general.name     = 1.4B
0.00.097.952 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.952 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.952 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.952 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.953 I llm_load_print_meta: LF token         = 128 ''
0.00.097.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.953 I llm_load_print_meta: max token length = 1024
0.00.100.474 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.474 I llm_load_tensors: offloading output layer to GPU
0.00.100.474 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.493 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.494 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.399 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.399 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.400 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.400 I llama_new_context_with_model: n_batch       = 2048
0.00.101.400 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.400 I llama_new_context_with_model: flash_attn    = 0
0.00.101.400 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.401 I llama_new_context_with_model: freq_scale    = 1
0.00.101.401 I ggml_metal_init: allocating
0.00.101.404 I ggml_metal_init: found device: Apple M4
0.00.101.406 I ggml_metal_init: picking default device: Apple M4
0.00.102.061 I ggml_metal_init: using embedded metal library
0.00.113.277 I ggml_metal_init: GPU name:   Apple M4
0.00.113.279 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.280 I ggml_metal_init: simdgroup reduction   = true
0.00.113.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.280 I ggml_metal_init: has bfloat            = true
0.00.113.280 I ggml_metal_init: use bfloat            = true
0.00.113.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.950 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.157.602 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.608 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.631 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.158.569 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.158.571 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.158.572 I llama_new_context_with_model: graph nodes  = 967
0.00.158.572 I llama_new_context_with_model: graph splits = 2
0.00.158.575 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.158.710 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.019 I main: llama threadpool init, n_threads = 4
0.00.239.068 I 
0.00.239.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.239.097 I 
0.00.239.168 I sampler seed: 1234
0.00.239.172 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.239.209 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.239.211 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.239.211 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.078.025 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.02.078.027 I llama_perf_context_print:        load time =     207.93 ms
0.02.078.027 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.46 tokens per second)
0.02.078.028 I llama_perf_context_print:        eval time =    1792.31 ms /    63 runs   (   28.45 ms per token,    35.15 tokens per second)
0.02.078.028 I llama_perf_context_print:       total time =    1839.01 ms /    70 tokens
0.02.078.249 I ggml_metal_free: deallocating

real	0m2.391s
user	0m0.142s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.599 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.418 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.550 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.588 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.593 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.602 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.347 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.687 I llama_model_loader: - type  f32:  194 tensors
0.00.054.687 I llama_model_loader: - type  f16:   98 tensors
0.00.085.536 I llm_load_vocab: special tokens cache size = 25
0.00.092.250 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.253 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.253 I llm_load_print_meta: arch             = gptneox
0.00.092.254 I llm_load_print_meta: vocab type       = BPE
0.00.092.254 I llm_load_print_meta: n_vocab          = 50304
0.00.092.254 I llm_load_print_meta: n_merges         = 50009
0.00.092.254 I llm_load_print_meta: vocab_only       = 0
0.00.092.254 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.255 I llm_load_print_meta: n_embd           = 2048
0.00.092.255 I llm_load_print_meta: n_layer          = 24
0.00.092.258 I llm_load_print_meta: n_head           = 16
0.00.092.259 I llm_load_print_meta: n_head_kv        = 16
0.00.092.259 I llm_load_print_meta: n_rot            = 32
0.00.092.262 I llm_load_print_meta: n_swa            = 0
0.00.092.262 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.263 I llm_load_print_meta: n_gqa            = 1
0.00.092.264 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.264 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.272 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.273 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.273 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.273 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.273 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.280 I llm_load_print_meta: n_ff             = 8192
0.00.092.280 I llm_load_print_meta: n_expert         = 0
0.00.092.280 I llm_load_print_meta: n_expert_used    = 0
0.00.092.281 I llm_load_print_meta: causal attn      = 1
0.00.092.281 I llm_load_print_meta: pooling type     = 0
0.00.092.282 I llm_load_print_meta: rope type        = 2
0.00.092.282 I llm_load_print_meta: rope scaling     = linear
0.00.092.282 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.283 I llm_load_print_meta: freq_scale_train = 1
0.00.092.283 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.283 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.283 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.284 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.284 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.284 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.284 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.284 I llm_load_print_meta: model type       = 1.4B
0.00.092.285 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.285 I llm_load_print_meta: model params     = 1.41 B
0.00.092.285 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.287 I llm_load_print_meta: general.name     = 1.4B
0.00.092.287 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.287 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.288 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.288 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.288 I llm_load_print_meta: LF token         = 128 ''
0.00.092.290 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.290 I llm_load_print_meta: max token length = 1024
0.00.094.854 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.854 I llm_load_tensors: offloading output layer to GPU
0.00.094.855 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.865 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.866 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.801 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.801 I llama_new_context_with_model: n_ctx         = 128
0.00.095.802 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.802 I llama_new_context_with_model: n_batch       = 128
0.00.095.802 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.802 I llama_new_context_with_model: flash_attn    = 0
0.00.095.803 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.803 I llama_new_context_with_model: freq_scale    = 1
0.00.095.803 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.803 I ggml_metal_init: allocating
0.00.095.806 I ggml_metal_init: found device: Apple M4
0.00.095.809 I ggml_metal_init: picking default device: Apple M4
0.00.096.447 I ggml_metal_init: using embedded metal library
0.00.099.038 I ggml_metal_init: GPU name:   Apple M4
0.00.099.039 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.040 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.040 I ggml_metal_init: simdgroup reduction   = true
0.00.099.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.041 I ggml_metal_init: has bfloat            = true
0.00.099.041 I ggml_metal_init: use bfloat            = true
0.00.099.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.438 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.676 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.679 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.695 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.578 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.579 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.579 I llama_new_context_with_model: graph nodes  = 967
0.00.110.579 I llama_new_context_with_model: graph splits = 2
0.00.110.581 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.278.442 I 
0.01.278.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.278.556 I perplexity: tokenizing the input ..
0.01.289.108 I perplexity: tokenization took 10.549 ms
0.01.289.116 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.406.958 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.408.377 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.408.391 I llama_perf_context_print:        load time =    1255.00 ms
0.01.408.392 I llama_perf_context_print: prompt eval time =     117.58 ms /   128 tokens (    0.92 ms per token,  1088.58 tokens per second)
0.01.408.393 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.408.393 I llama_perf_context_print:       total time =     129.95 ms /   129 tokens
0.01.408.752 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.117s
sys	0m0.203s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.829 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.568 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.570 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.570 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.574 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.546 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.676 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.677 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.678 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.679 I llama_model_loader: - type  f32:  194 tensors
0.00.033.679 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.336 I llm_load_vocab: special tokens cache size = 25
0.00.062.418 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.422 I llm_load_print_meta: arch             = gptneox
0.00.062.423 I llm_load_print_meta: vocab type       = BPE
0.00.062.423 I llm_load_print_meta: n_vocab          = 50304
0.00.062.423 I llm_load_print_meta: n_merges         = 50009
0.00.062.424 I llm_load_print_meta: vocab_only       = 0
0.00.062.424 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.424 I llm_load_print_meta: n_embd           = 2048
0.00.062.424 I llm_load_print_meta: n_layer          = 24
0.00.062.432 I llm_load_print_meta: n_head           = 16
0.00.062.433 I llm_load_print_meta: n_head_kv        = 16
0.00.062.433 I llm_load_print_meta: n_rot            = 32
0.00.062.433 I llm_load_print_meta: n_swa            = 0
0.00.062.433 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.433 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.434 I llm_load_print_meta: n_gqa            = 1
0.00.062.435 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.438 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.439 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.439 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.439 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.439 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.439 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.440 I llm_load_print_meta: n_ff             = 8192
0.00.062.440 I llm_load_print_meta: n_expert         = 0
0.00.062.440 I llm_load_print_meta: n_expert_used    = 0
0.00.062.441 I llm_load_print_meta: causal attn      = 1
0.00.062.441 I llm_load_print_meta: pooling type     = 0
0.00.062.441 I llm_load_print_meta: rope type        = 2
0.00.062.441 I llm_load_print_meta: rope scaling     = linear
0.00.062.442 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.442 I llm_load_print_meta: freq_scale_train = 1
0.00.062.442 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.442 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.442 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.443 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.443 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.443 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.443 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.443 I llm_load_print_meta: model type       = 1.4B
0.00.062.444 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.444 I llm_load_print_meta: model params     = 1.41 B
0.00.062.445 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.445 I llm_load_print_meta: general.name     = 1.4B
0.00.062.445 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.446 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.447 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.447 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.447 I llm_load_print_meta: LF token         = 128 ''
0.00.062.447 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.448 I llm_load_print_meta: max token length = 1024
0.00.064.896 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.896 I llm_load_tensors: offloading output layer to GPU
0.00.064.896 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.909 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.910 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.869 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.870 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.870 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.870 I llama_new_context_with_model: n_batch       = 2048
0.00.065.871 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.871 I llama_new_context_with_model: flash_attn    = 0
0.00.065.871 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.872 I llama_new_context_with_model: freq_scale    = 1
0.00.065.872 I ggml_metal_init: allocating
0.00.065.875 I ggml_metal_init: found device: Apple M4
0.00.065.877 I ggml_metal_init: picking default device: Apple M4
0.00.066.587 I ggml_metal_init: using embedded metal library
0.00.069.131 I ggml_metal_init: GPU name:   Apple M4
0.00.069.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.133 I ggml_metal_init: simdgroup reduction   = true
0.00.069.133 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.134 I ggml_metal_init: has bfloat            = true
0.00.069.134 I ggml_metal_init: use bfloat            = true
0.00.069.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.520 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.379 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.389 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.419 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.672 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.106.674 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.106.674 I llama_new_context_with_model: graph nodes  = 967
0.00.106.674 I llama_new_context_with_model: graph splits = 2
0.00.106.681 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.251.978 I main: llama threadpool init, n_threads = 4
0.01.252.013 I 
0.01.252.040 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.252.040 I 
0.01.252.278 I sampler seed: 1234
0.01.252.284 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.252.327 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.252.331 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.252.332 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.344.038 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.02.344.039 I llama_perf_context_print:        load time =    1242.14 ms
0.02.344.040 I llama_perf_context_print: prompt eval time =      39.79 ms /     7 tokens (    5.68 ms per token,   175.94 tokens per second)
0.02.344.040 I llama_perf_context_print:        eval time =    1049.11 ms /    63 runs   (   16.65 ms per token,    60.05 tokens per second)
0.02.344.041 I llama_perf_context_print:       total time =    1092.06 ms /    70 tokens
0.02.344.339 I ggml_metal_free: deallocating

real	0m2.363s
user	0m0.115s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.491 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.349 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.521 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.507 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.511 I llama_model_loader: - type  f32:  194 tensors
0.00.025.512 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.298 I llm_load_vocab: special tokens cache size = 25
0.00.053.406 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.412 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.416 I llm_load_print_meta: arch             = gptneox
0.00.053.417 I llm_load_print_meta: vocab type       = BPE
0.00.053.417 I llm_load_print_meta: n_vocab          = 50304
0.00.053.418 I llm_load_print_meta: n_merges         = 50009
0.00.053.418 I llm_load_print_meta: vocab_only       = 0
0.00.053.419 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.419 I llm_load_print_meta: n_embd           = 2048
0.00.053.419 I llm_load_print_meta: n_layer          = 24
0.00.053.422 I llm_load_print_meta: n_head           = 16
0.00.053.423 I llm_load_print_meta: n_head_kv        = 16
0.00.053.424 I llm_load_print_meta: n_rot            = 32
0.00.053.425 I llm_load_print_meta: n_swa            = 0
0.00.053.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.425 I llm_load_print_meta: n_gqa            = 1
0.00.053.426 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.427 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.427 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.428 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.428 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.428 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.428 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.429 I llm_load_print_meta: n_ff             = 8192
0.00.053.430 I llm_load_print_meta: n_expert         = 0
0.00.053.432 I llm_load_print_meta: n_expert_used    = 0
0.00.053.432 I llm_load_print_meta: causal attn      = 1
0.00.053.432 I llm_load_print_meta: pooling type     = 0
0.00.053.432 I llm_load_print_meta: rope type        = 2
0.00.053.432 I llm_load_print_meta: rope scaling     = linear
0.00.053.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.433 I llm_load_print_meta: freq_scale_train = 1
0.00.053.433 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.433 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.433 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.434 I llm_load_print_meta: model type       = 1.4B
0.00.053.438 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.439 I llm_load_print_meta: model params     = 1.41 B
0.00.053.439 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.439 I llm_load_print_meta: general.name     = 1.4B
0.00.053.439 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.440 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.440 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.440 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: LF token         = 128 ''
0.00.053.442 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: max token length = 1024
0.00.055.543 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.543 I llm_load_tensors: offloading output layer to GPU
0.00.055.543 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.555 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.557 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.483 I llama_new_context_with_model: n_ctx         = 128
0.00.056.483 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.483 I llama_new_context_with_model: n_batch       = 128
0.00.056.483 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.483 I llama_new_context_with_model: flash_attn    = 0
0.00.056.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.484 I llama_new_context_with_model: freq_scale    = 1
0.00.056.484 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.485 I ggml_metal_init: allocating
0.00.056.489 I ggml_metal_init: found device: Apple M4
0.00.056.491 I ggml_metal_init: picking default device: Apple M4
0.00.057.141 I ggml_metal_init: using embedded metal library
0.00.059.657 I ggml_metal_init: GPU name:   Apple M4
0.00.059.661 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.662 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.663 I ggml_metal_init: simdgroup reduction   = true
0.00.059.663 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.664 I ggml_metal_init: has bfloat            = true
0.00.059.664 I ggml_metal_init: use bfloat            = true
0.00.059.665 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.666 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.208 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.545 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.548 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.566 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.513 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.514 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.514 I llama_new_context_with_model: graph nodes  = 967
0.00.071.514 I llama_new_context_with_model: graph splits = 2
0.00.071.516 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.516 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.869 I 
0.00.813.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.929 I perplexity: tokenizing the input ..
0.00.821.788 I perplexity: tokenization took 7.857 ms
0.00.821.791 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.946.245 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.947.393 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.947.407 I llama_perf_context_print:        load time =     804.37 ms
0.00.947.408 I llama_perf_context_print: prompt eval time =     124.23 ms /   128 tokens (    0.97 ms per token,  1030.37 tokens per second)
0.00.947.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.947.410 I llama_perf_context_print:       total time =     133.54 ms /   129 tokens
0.00.947.836 I ggml_metal_free: deallocating

real	0m0.962s
user	0m0.081s
sys	0m0.147s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.015.492 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.518 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.523 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.524 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.527 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.531 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.597 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.760 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.761 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.762 I llama_model_loader: - type  f32:  194 tensors
0.00.037.762 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.762 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.885 I llm_load_vocab: special tokens cache size = 25
0.00.070.239 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.242 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.243 I llm_load_print_meta: arch             = gptneox
0.00.070.243 I llm_load_print_meta: vocab type       = BPE
0.00.070.243 I llm_load_print_meta: n_vocab          = 50304
0.00.070.243 I llm_load_print_meta: n_merges         = 50009
0.00.070.243 I llm_load_print_meta: vocab_only       = 0
0.00.070.244 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.244 I llm_load_print_meta: n_embd           = 2048
0.00.070.244 I llm_load_print_meta: n_layer          = 24
0.00.070.248 I llm_load_print_meta: n_head           = 16
0.00.070.248 I llm_load_print_meta: n_head_kv        = 16
0.00.070.249 I llm_load_print_meta: n_rot            = 32
0.00.070.249 I llm_load_print_meta: n_swa            = 0
0.00.070.249 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.249 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.252 I llm_load_print_meta: n_gqa            = 1
0.00.070.253 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.253 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.254 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.255 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.256 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.256 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.256 I llm_load_print_meta: n_ff             = 8192
0.00.070.257 I llm_load_print_meta: n_expert         = 0
0.00.070.257 I llm_load_print_meta: n_expert_used    = 0
0.00.070.258 I llm_load_print_meta: causal attn      = 1
0.00.070.258 I llm_load_print_meta: pooling type     = 0
0.00.070.259 I llm_load_print_meta: rope type        = 2
0.00.070.259 I llm_load_print_meta: rope scaling     = linear
0.00.070.259 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.263 I llm_load_print_meta: freq_scale_train = 1
0.00.070.263 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.264 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.264 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.264 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.264 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.264 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.264 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.265 I llm_load_print_meta: model type       = 1.4B
0.00.070.265 I llm_load_print_meta: model ftype      = Q4_0
0.00.070.266 I llm_load_print_meta: model params     = 1.41 B
0.00.070.266 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.070.266 I llm_load_print_meta: general.name     = 1.4B
0.00.070.266 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.267 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.267 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.267 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.267 I llm_load_print_meta: LF token         = 128 ''
0.00.070.267 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.268 I llm_load_print_meta: max token length = 1024
0.00.072.526 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.526 I llm_load_tensors: offloading output layer to GPU
0.00.072.526 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.537 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.072.538 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.073.527 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.528 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.528 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.528 I llama_new_context_with_model: n_batch       = 2048
0.00.073.528 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.529 I llama_new_context_with_model: flash_attn    = 0
0.00.073.529 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.530 I llama_new_context_with_model: freq_scale    = 1
0.00.073.530 I ggml_metal_init: allocating
0.00.073.536 I ggml_metal_init: found device: Apple M4
0.00.073.538 I ggml_metal_init: picking default device: Apple M4
0.00.074.178 I ggml_metal_init: using embedded metal library
0.00.076.950 I ggml_metal_init: GPU name:   Apple M4
0.00.076.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.953 I ggml_metal_init: simdgroup reduction   = true
0.00.076.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.953 I ggml_metal_init: has bfloat            = true
0.00.076.955 I ggml_metal_init: use bfloat            = true
0.00.076.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.439 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.112.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.187 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.206 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.229 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.232 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.232 I llama_new_context_with_model: graph nodes  = 967
0.00.113.232 I llama_new_context_with_model: graph splits = 2
0.00.113.235 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.113.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.508 I main: llama threadpool init, n_threads = 4
0.00.742.549 I 
0.00.742.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.577 I 
0.00.742.825 I sampler seed: 1234
0.00.742.830 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.876 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.878 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.878 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.419.436 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.419.436 I llama_perf_context_print:        load time =     727.01 ms
0.01.419.437 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.19 tokens per second)
0.01.419.438 I llama_perf_context_print:        eval time =     629.94 ms /    63 runs   (   10.00 ms per token,   100.01 tokens per second)
0.01.419.439 I llama_perf_context_print:       total time =     676.93 ms /    70 tokens
0.01.419.665 I ggml_metal_free: deallocating

real	0m1.435s
user	0m0.119s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.030 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.576 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.581 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.584 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.584 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.585 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.586 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.586 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.587 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.587 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.737 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.859 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.898 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.900 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.900 I llama_model_loader: - type  f32:  194 tensors
0.00.024.900 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.901 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.799 I llm_load_vocab: special tokens cache size = 25
0.00.051.831 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.833 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.833 I llm_load_print_meta: arch             = gptneox
0.00.051.834 I llm_load_print_meta: vocab type       = BPE
0.00.051.834 I llm_load_print_meta: n_vocab          = 50304
0.00.051.834 I llm_load_print_meta: n_merges         = 50009
0.00.051.834 I llm_load_print_meta: vocab_only       = 0
0.00.051.835 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.835 I llm_load_print_meta: n_embd           = 2048
0.00.051.835 I llm_load_print_meta: n_layer          = 24
0.00.051.838 I llm_load_print_meta: n_head           = 16
0.00.051.839 I llm_load_print_meta: n_head_kv        = 16
0.00.051.839 I llm_load_print_meta: n_rot            = 32
0.00.051.839 I llm_load_print_meta: n_swa            = 0
0.00.051.842 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.842 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.842 I llm_load_print_meta: n_gqa            = 1
0.00.051.843 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.844 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.844 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.845 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.845 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.846 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.846 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.846 I llm_load_print_meta: n_ff             = 8192
0.00.051.847 I llm_load_print_meta: n_expert         = 0
0.00.051.847 I llm_load_print_meta: n_expert_used    = 0
0.00.051.847 I llm_load_print_meta: causal attn      = 1
0.00.051.847 I llm_load_print_meta: pooling type     = 0
0.00.051.847 I llm_load_print_meta: rope type        = 2
0.00.051.847 I llm_load_print_meta: rope scaling     = linear
0.00.051.848 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.848 I llm_load_print_meta: freq_scale_train = 1
0.00.051.848 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.849 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.850 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.851 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.851 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.851 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.851 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.851 I llm_load_print_meta: model type       = 1.4B
0.00.051.852 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.852 I llm_load_print_meta: model params     = 1.41 B
0.00.051.853 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.853 I llm_load_print_meta: general.name     = 1.4B
0.00.051.853 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.854 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.855 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.855 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.855 I llm_load_print_meta: LF token         = 128 ''
0.00.051.856 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.856 I llm_load_print_meta: max token length = 1024
0.00.053.833 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.834 I llm_load_tensors: offloading output layer to GPU
0.00.053.834 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.845 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.846 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.751 I llama_new_context_with_model: n_ctx         = 128
0.00.054.751 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.751 I llama_new_context_with_model: n_batch       = 128
0.00.054.752 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.752 I llama_new_context_with_model: flash_attn    = 0
0.00.054.752 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.752 I llama_new_context_with_model: freq_scale    = 1
0.00.054.753 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.753 I ggml_metal_init: allocating
0.00.054.756 I ggml_metal_init: found device: Apple M4
0.00.054.758 I ggml_metal_init: picking default device: Apple M4
0.00.055.341 I ggml_metal_init: using embedded metal library
0.00.057.688 I ggml_metal_init: GPU name:   Apple M4
0.00.057.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.690 I ggml_metal_init: simdgroup reduction   = true
0.00.057.691 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.691 I ggml_metal_init: has bfloat            = true
0.00.057.691 I ggml_metal_init: use bfloat            = true
0.00.057.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.877 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.163 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.166 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.180 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.080 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.081 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.081 I llama_new_context_with_model: graph nodes  = 967
0.00.070.082 I llama_new_context_with_model: graph splits = 2
0.00.070.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.099 I 
0.00.568.144 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.160 I perplexity: tokenizing the input ..
0.00.575.966 I perplexity: tokenization took 7.803 ms
0.00.575.970 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.698.753 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.700.086 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.700.099 I llama_perf_context_print:        load time =     558.06 ms
0.00.700.100 I llama_perf_context_print: prompt eval time =     122.54 ms /   128 tokens (    0.96 ms per token,  1044.54 tokens per second)
0.00.700.101 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.102 I llama_perf_context_print:       total time =     132.01 ms /   129 tokens
0.00.700.441 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.079s
sys	0m0.088s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.470 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.099 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.106 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.106 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.106 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.108 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.109 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.365 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.505 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.506 I llama_model_loader: - type  f32:  194 tensors
0.00.033.506 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.506 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.106 I llm_load_vocab: special tokens cache size = 25
0.00.062.439 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.442 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.442 I llm_load_print_meta: arch             = gptneox
0.00.062.442 I llm_load_print_meta: vocab type       = BPE
0.00.062.443 I llm_load_print_meta: n_vocab          = 50304
0.00.062.443 I llm_load_print_meta: n_merges         = 50009
0.00.062.443 I llm_load_print_meta: vocab_only       = 0
0.00.062.443 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.443 I llm_load_print_meta: n_embd           = 2048
0.00.062.443 I llm_load_print_meta: n_layer          = 24
0.00.062.446 I llm_load_print_meta: n_head           = 16
0.00.062.447 I llm_load_print_meta: n_head_kv        = 16
0.00.062.447 I llm_load_print_meta: n_rot            = 32
0.00.062.447 I llm_load_print_meta: n_swa            = 0
0.00.062.447 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.447 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.448 I llm_load_print_meta: n_gqa            = 1
0.00.062.449 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.449 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.452 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.452 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.452 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.452 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.453 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.453 I llm_load_print_meta: n_ff             = 8192
0.00.062.453 I llm_load_print_meta: n_expert         = 0
0.00.062.453 I llm_load_print_meta: n_expert_used    = 0
0.00.062.454 I llm_load_print_meta: causal attn      = 1
0.00.062.454 I llm_load_print_meta: pooling type     = 0
0.00.062.454 I llm_load_print_meta: rope type        = 2
0.00.062.454 I llm_load_print_meta: rope scaling     = linear
0.00.062.455 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.455 I llm_load_print_meta: freq_scale_train = 1
0.00.062.455 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.455 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.455 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.456 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.456 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.456 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.456 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.456 I llm_load_print_meta: model type       = 1.4B
0.00.062.457 I llm_load_print_meta: model ftype      = Q4_1
0.00.062.457 I llm_load_print_meta: model params     = 1.41 B
0.00.062.457 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.062.457 I llm_load_print_meta: general.name     = 1.4B
0.00.062.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.459 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.459 I llm_load_print_meta: LF token         = 128 ''
0.00.062.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.460 I llm_load_print_meta: max token length = 1024
0.00.064.539 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.539 I llm_load_tensors: offloading output layer to GPU
0.00.064.540 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.550 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.064.551 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.065.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.483 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.483 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.483 I llama_new_context_with_model: n_batch       = 2048
0.00.065.484 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.484 I llama_new_context_with_model: flash_attn    = 0
0.00.065.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.484 I llama_new_context_with_model: freq_scale    = 1
0.00.065.485 I ggml_metal_init: allocating
0.00.065.490 I ggml_metal_init: found device: Apple M4
0.00.065.495 I ggml_metal_init: picking default device: Apple M4
0.00.066.155 I ggml_metal_init: using embedded metal library
0.00.068.767 I ggml_metal_init: GPU name:   Apple M4
0.00.068.769 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.770 I ggml_metal_init: simdgroup reduction   = true
0.00.068.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.770 I ggml_metal_init: has bfloat            = true
0.00.068.770 I ggml_metal_init: use bfloat            = true
0.00.068.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.603 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.159 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.167 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.188 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.212 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.213 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.214 I llama_new_context_with_model: graph nodes  = 967
0.00.099.214 I llama_new_context_with_model: graph splits = 2
0.00.099.217 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.427 I main: llama threadpool init, n_threads = 4
0.00.822.464 I 
0.00.822.488 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.488 I 
0.00.822.713 I sampler seed: 1234
0.00.822.717 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.762 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.779 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.779 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.549.874 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62062.94 tokens per second)
0.01.549.874 I llama_perf_context_print:        load time =     812.95 ms
0.01.549.875 I llama_perf_context_print: prompt eval time =      42.73 ms /     7 tokens (    6.10 ms per token,   163.81 tokens per second)
0.01.549.875 I llama_perf_context_print:        eval time =     681.46 ms /    63 runs   (   10.82 ms per token,    92.45 tokens per second)
0.01.549.876 I llama_perf_context_print:       total time =     727.45 ms /    70 tokens
0.01.550.069 I ggml_metal_free: deallocating

real	0m1.569s
user	0m0.113s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.054 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.804 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.805 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.805 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.806 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.806 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.807 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.822 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.005 I llama_model_loader: - type  f32:  194 tensors
0.00.024.005 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.115 I llm_load_vocab: special tokens cache size = 25
0.00.050.040 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.043 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.043 I llm_load_print_meta: arch             = gptneox
0.00.050.043 I llm_load_print_meta: vocab type       = BPE
0.00.050.044 I llm_load_print_meta: n_vocab          = 50304
0.00.050.044 I llm_load_print_meta: n_merges         = 50009
0.00.050.044 I llm_load_print_meta: vocab_only       = 0
0.00.050.044 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.044 I llm_load_print_meta: n_embd           = 2048
0.00.050.045 I llm_load_print_meta: n_layer          = 24
0.00.050.047 I llm_load_print_meta: n_head           = 16
0.00.050.048 I llm_load_print_meta: n_head_kv        = 16
0.00.050.048 I llm_load_print_meta: n_rot            = 32
0.00.050.048 I llm_load_print_meta: n_swa            = 0
0.00.050.049 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.049 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.049 I llm_load_print_meta: n_gqa            = 1
0.00.050.050 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.051 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.051 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.052 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.052 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.052 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.052 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.053 I llm_load_print_meta: n_ff             = 8192
0.00.050.053 I llm_load_print_meta: n_expert         = 0
0.00.050.053 I llm_load_print_meta: n_expert_used    = 0
0.00.050.054 I llm_load_print_meta: causal attn      = 1
0.00.050.054 I llm_load_print_meta: pooling type     = 0
0.00.050.054 I llm_load_print_meta: rope type        = 2
0.00.050.054 I llm_load_print_meta: rope scaling     = linear
0.00.050.054 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.055 I llm_load_print_meta: freq_scale_train = 1
0.00.050.055 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.055 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.056 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.056 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.056 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.056 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.056 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.057 I llm_load_print_meta: model type       = 1.4B
0.00.050.057 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.057 I llm_load_print_meta: model params     = 1.41 B
0.00.050.058 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.058 I llm_load_print_meta: general.name     = 1.4B
0.00.050.058 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: LF token         = 128 ''
0.00.050.059 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.060 I llm_load_print_meta: max token length = 1024
0.00.051.834 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.835 I llm_load_tensors: offloading output layer to GPU
0.00.051.835 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.841 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.841 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.724 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.725 I llama_new_context_with_model: n_ctx         = 128
0.00.052.725 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.725 I llama_new_context_with_model: n_batch       = 128
0.00.052.725 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.725 I llama_new_context_with_model: flash_attn    = 0
0.00.052.726 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.726 I llama_new_context_with_model: freq_scale    = 1
0.00.052.726 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.727 I ggml_metal_init: allocating
0.00.052.731 I ggml_metal_init: found device: Apple M4
0.00.052.733 I ggml_metal_init: picking default device: Apple M4
0.00.053.302 I ggml_metal_init: using embedded metal library
0.00.055.590 I ggml_metal_init: GPU name:   Apple M4
0.00.055.592 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.593 I ggml_metal_init: simdgroup reduction   = true
0.00.055.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.593 I ggml_metal_init: has bfloat            = true
0.00.055.593 I ggml_metal_init: use bfloat            = true
0.00.055.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.116 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.368 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.370 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.385 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.285 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.286 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.286 I llama_new_context_with_model: graph nodes  = 967
0.00.067.286 I llama_new_context_with_model: graph splits = 2
0.00.067.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.302 I 
0.00.620.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.346 I perplexity: tokenizing the input ..
0.00.628.369 I perplexity: tokenization took 8.021 ms
0.00.628.373 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.092 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.752.264 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.752.285 I llama_perf_context_print:        load time =     611.24 ms
0.00.752.287 I llama_perf_context_print: prompt eval time =     122.48 ms /   128 tokens (    0.96 ms per token,  1045.11 tokens per second)
0.00.752.288 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.288 I llama_perf_context_print:       total time =     131.99 ms /   129 tokens
0.00.752.785 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.078s
sys	0m0.099s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.012.642 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.413 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.414 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.416 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.338 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.338 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.338 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.339 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.339 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.031.340 I llama_model_loader: - type  f32:  194 tensors
0.00.031.340 I llama_model_loader: - type q5_0:   97 tensors
0.00.031.340 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.939 I llm_load_vocab: special tokens cache size = 25
0.00.063.096 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.099 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.099 I llm_load_print_meta: arch             = gptneox
0.00.063.099 I llm_load_print_meta: vocab type       = BPE
0.00.063.099 I llm_load_print_meta: n_vocab          = 50304
0.00.063.100 I llm_load_print_meta: n_merges         = 50009
0.00.063.100 I llm_load_print_meta: vocab_only       = 0
0.00.063.100 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.100 I llm_load_print_meta: n_embd           = 2048
0.00.063.100 I llm_load_print_meta: n_layer          = 24
0.00.063.102 I llm_load_print_meta: n_head           = 16
0.00.063.103 I llm_load_print_meta: n_head_kv        = 16
0.00.063.103 I llm_load_print_meta: n_rot            = 32
0.00.063.104 I llm_load_print_meta: n_swa            = 0
0.00.063.104 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.104 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.105 I llm_load_print_meta: n_gqa            = 1
0.00.063.105 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.108 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.108 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.109 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.109 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.109 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.109 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.110 I llm_load_print_meta: n_ff             = 8192
0.00.063.110 I llm_load_print_meta: n_expert         = 0
0.00.063.110 I llm_load_print_meta: n_expert_used    = 0
0.00.063.112 I llm_load_print_meta: causal attn      = 1
0.00.063.113 I llm_load_print_meta: pooling type     = 0
0.00.063.113 I llm_load_print_meta: rope type        = 2
0.00.063.113 I llm_load_print_meta: rope scaling     = linear
0.00.063.114 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.114 I llm_load_print_meta: freq_scale_train = 1
0.00.063.114 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.115 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.115 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.115 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.115 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.115 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.115 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.116 I llm_load_print_meta: model type       = 1.4B
0.00.063.116 I llm_load_print_meta: model ftype      = Q5_0
0.00.063.119 I llm_load_print_meta: model params     = 1.41 B
0.00.063.120 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.063.120 I llm_load_print_meta: general.name     = 1.4B
0.00.063.120 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.121 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.121 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.122 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.122 I llm_load_print_meta: LF token         = 128 ''
0.00.063.122 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.122 I llm_load_print_meta: max token length = 1024
0.00.065.227 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.227 I llm_load_tensors: offloading output layer to GPU
0.00.065.227 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.238 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.065.240 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.066.239 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.240 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.240 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.240 I llama_new_context_with_model: n_batch       = 2048
0.00.066.240 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.240 I llama_new_context_with_model: flash_attn    = 0
0.00.066.241 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.241 I llama_new_context_with_model: freq_scale    = 1
0.00.066.241 I ggml_metal_init: allocating
0.00.066.247 I ggml_metal_init: found device: Apple M4
0.00.066.250 I ggml_metal_init: picking default device: Apple M4
0.00.066.878 I ggml_metal_init: using embedded metal library
0.00.069.654 I ggml_metal_init: GPU name:   Apple M4
0.00.069.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.658 I ggml_metal_init: simdgroup reduction   = true
0.00.069.658 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.658 I ggml_metal_init: has bfloat            = true
0.00.069.658 I ggml_metal_init: use bfloat            = true
0.00.069.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.984 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.182 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.191 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.210 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.220 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.222 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.222 I llama_new_context_with_model: graph nodes  = 967
0.00.107.222 I llama_new_context_with_model: graph splits = 2
0.00.107.225 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.891.013 I main: llama threadpool init, n_threads = 4
0.00.891.052 I 
0.00.891.075 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.891.076 I 
0.00.891.311 I sampler seed: 1234
0.00.891.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.891.361 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.891.366 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.891.366 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.683.640 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.683.641 I llama_perf_context_print:        load time =     878.37 ms
0.01.683.641 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.35 tokens per second)
0.01.683.642 I llama_perf_context_print:        eval time =     746.09 ms /    63 runs   (   11.84 ms per token,    84.44 tokens per second)
0.01.683.642 I llama_perf_context_print:       total time =     792.63 ms /    70 tokens
0.01.683.870 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.123s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.009 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.840 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.851 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.852 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.854 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.510 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.511 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.511 I llama_model_loader: - type  f32:  194 tensors
0.00.024.512 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.586 I llm_load_vocab: special tokens cache size = 25
0.00.050.469 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.471 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.472 I llm_load_print_meta: arch             = gptneox
0.00.050.472 I llm_load_print_meta: vocab type       = BPE
0.00.050.472 I llm_load_print_meta: n_vocab          = 50304
0.00.050.472 I llm_load_print_meta: n_merges         = 50009
0.00.050.473 I llm_load_print_meta: vocab_only       = 0
0.00.050.473 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.473 I llm_load_print_meta: n_embd           = 2048
0.00.050.473 I llm_load_print_meta: n_layer          = 24
0.00.050.476 I llm_load_print_meta: n_head           = 16
0.00.050.477 I llm_load_print_meta: n_head_kv        = 16
0.00.050.477 I llm_load_print_meta: n_rot            = 32
0.00.050.477 I llm_load_print_meta: n_swa            = 0
0.00.050.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.478 I llm_load_print_meta: n_gqa            = 1
0.00.050.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.481 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.482 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.484 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.484 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.484 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.484 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.485 I llm_load_print_meta: n_ff             = 8192
0.00.050.485 I llm_load_print_meta: n_expert         = 0
0.00.050.485 I llm_load_print_meta: n_expert_used    = 0
0.00.050.486 I llm_load_print_meta: causal attn      = 1
0.00.050.486 I llm_load_print_meta: pooling type     = 0
0.00.050.486 I llm_load_print_meta: rope type        = 2
0.00.050.486 I llm_load_print_meta: rope scaling     = linear
0.00.050.486 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.487 I llm_load_print_meta: freq_scale_train = 1
0.00.050.487 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.487 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.487 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.488 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.489 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.489 I llm_load_print_meta: model type       = 1.4B
0.00.050.490 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.490 I llm_load_print_meta: model params     = 1.41 B
0.00.050.491 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.491 I llm_load_print_meta: general.name     = 1.4B
0.00.050.491 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.492 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.492 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.492 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.492 I llm_load_print_meta: LF token         = 128 ''
0.00.050.493 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.493 I llm_load_print_meta: max token length = 1024
0.00.052.492 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.492 I llm_load_tensors: offloading output layer to GPU
0.00.052.492 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.503 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.504 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.397 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.397 I llama_new_context_with_model: n_ctx         = 128
0.00.053.397 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.398 I llama_new_context_with_model: n_batch       = 128
0.00.053.398 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.398 I llama_new_context_with_model: flash_attn    = 0
0.00.053.398 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.399 I llama_new_context_with_model: freq_scale    = 1
0.00.053.399 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.399 I ggml_metal_init: allocating
0.00.053.402 I ggml_metal_init: found device: Apple M4
0.00.053.404 I ggml_metal_init: picking default device: Apple M4
0.00.053.990 I ggml_metal_init: using embedded metal library
0.00.056.318 I ggml_metal_init: GPU name:   Apple M4
0.00.056.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.320 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.320 I ggml_metal_init: simdgroup reduction   = true
0.00.056.320 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.321 I ggml_metal_init: has bfloat            = true
0.00.056.321 I ggml_metal_init: use bfloat            = true
0.00.056.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.844 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.094 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.096 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.112 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.038 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.039 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.039 I llama_new_context_with_model: graph nodes  = 967
0.00.068.039 I llama_new_context_with_model: graph splits = 2
0.00.068.040 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.156 I 
0.00.732.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.200 I perplexity: tokenizing the input ..
0.00.739.868 I perplexity: tokenization took 7.667 ms
0.00.739.872 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.971 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.876.185 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.876.209 I llama_perf_context_print:        load time =     722.14 ms
0.00.876.210 I llama_perf_context_print: prompt eval time =     134.87 ms /   128 tokens (    1.05 ms per token,   949.06 tokens per second)
0.00.876.211 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.211 I llama_perf_context_print:       total time =     144.05 ms /   129 tokens
0.00.876.730 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.078s
sys	0m0.126s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.747 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.234 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.240 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.241 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.241 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.241 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.242 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.243 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.380 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.407 I llama_model_loader: - type  f32:  194 tensors
0.00.027.408 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.408 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.367 I llm_load_vocab: special tokens cache size = 25
0.00.054.382 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.385 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.385 I llm_load_print_meta: arch             = gptneox
0.00.054.386 I llm_load_print_meta: vocab type       = BPE
0.00.054.386 I llm_load_print_meta: n_vocab          = 50304
0.00.054.386 I llm_load_print_meta: n_merges         = 50009
0.00.054.386 I llm_load_print_meta: vocab_only       = 0
0.00.054.386 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.386 I llm_load_print_meta: n_embd           = 2048
0.00.054.387 I llm_load_print_meta: n_layer          = 24
0.00.054.390 I llm_load_print_meta: n_head           = 16
0.00.054.391 I llm_load_print_meta: n_head_kv        = 16
0.00.054.391 I llm_load_print_meta: n_rot            = 32
0.00.054.391 I llm_load_print_meta: n_swa            = 0
0.00.054.391 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.391 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.392 I llm_load_print_meta: n_gqa            = 1
0.00.054.393 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.394 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.394 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.394 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.395 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.395 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.395 I llm_load_print_meta: n_ff             = 8192
0.00.054.405 I llm_load_print_meta: n_expert         = 0
0.00.054.407 I llm_load_print_meta: n_expert_used    = 0
0.00.054.407 I llm_load_print_meta: causal attn      = 1
0.00.054.407 I llm_load_print_meta: pooling type     = 0
0.00.054.407 I llm_load_print_meta: rope type        = 2
0.00.054.408 I llm_load_print_meta: rope scaling     = linear
0.00.054.408 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.408 I llm_load_print_meta: freq_scale_train = 1
0.00.054.409 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.409 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.410 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.411 I llm_load_print_meta: model type       = 1.4B
0.00.054.411 I llm_load_print_meta: model ftype      = Q5_1
0.00.054.412 I llm_load_print_meta: model params     = 1.41 B
0.00.054.414 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.054.414 I llm_load_print_meta: general.name     = 1.4B
0.00.054.414 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.414 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.414 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.416 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.416 I llm_load_print_meta: LF token         = 128 ''
0.00.054.419 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.419 I llm_load_print_meta: max token length = 1024
0.00.056.421 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.421 I llm_load_tensors: offloading output layer to GPU
0.00.056.422 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.432 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.056.433 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.057.317 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.318 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.318 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.318 I llama_new_context_with_model: n_batch       = 2048
0.00.057.318 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.318 I llama_new_context_with_model: flash_attn    = 0
0.00.057.319 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.319 I llama_new_context_with_model: freq_scale    = 1
0.00.057.319 I ggml_metal_init: allocating
0.00.057.322 I ggml_metal_init: found device: Apple M4
0.00.057.324 I ggml_metal_init: picking default device: Apple M4
0.00.057.905 I ggml_metal_init: using embedded metal library
0.00.060.219 I ggml_metal_init: GPU name:   Apple M4
0.00.060.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.222 I ggml_metal_init: simdgroup reduction   = true
0.00.060.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.222 I ggml_metal_init: has bfloat            = true
0.00.060.222 I ggml_metal_init: use bfloat            = true
0.00.060.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.028 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.865 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.881 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.902 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.915 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.916 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.916 I llama_new_context_with_model: graph nodes  = 967
0.00.090.916 I llama_new_context_with_model: graph splits = 2
0.00.090.919 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.750 I main: llama threadpool init, n_threads = 4
0.00.800.790 I 
0.00.800.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.800.816 I 
0.00.801.042 I sampler seed: 1234
0.00.801.046 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.062 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.062 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.062 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.639.975 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59364.55 tokens per second)
0.01.639.976 I llama_perf_context_print:        load time =     792.00 ms
0.01.639.977 I llama_perf_context_print: prompt eval time =      45.50 ms /     7 tokens (    6.50 ms per token,   153.85 tokens per second)
0.01.639.978 I llama_perf_context_print:        eval time =     790.43 ms /    63 runs   (   12.55 ms per token,    79.70 tokens per second)
0.01.639.978 I llama_perf_context_print:       total time =     839.23 ms /    70 tokens
0.01.640.205 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.111s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.892 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.738 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.743 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.744 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.744 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.748 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.750 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.072 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.072 I llama_model_loader: - type  f32:  194 tensors
0.00.024.073 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.073 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.046 I llm_load_vocab: special tokens cache size = 25
0.00.050.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.130 I llm_load_print_meta: arch             = gptneox
0.00.050.130 I llm_load_print_meta: vocab type       = BPE
0.00.050.130 I llm_load_print_meta: n_vocab          = 50304
0.00.050.131 I llm_load_print_meta: n_merges         = 50009
0.00.050.131 I llm_load_print_meta: vocab_only       = 0
0.00.050.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.131 I llm_load_print_meta: n_embd           = 2048
0.00.050.131 I llm_load_print_meta: n_layer          = 24
0.00.050.134 I llm_load_print_meta: n_head           = 16
0.00.050.135 I llm_load_print_meta: n_head_kv        = 16
0.00.050.135 I llm_load_print_meta: n_rot            = 32
0.00.050.135 I llm_load_print_meta: n_swa            = 0
0.00.050.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.136 I llm_load_print_meta: n_gqa            = 1
0.00.050.137 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.138 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.139 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.139 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.139 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.139 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.142 I llm_load_print_meta: n_ff             = 8192
0.00.050.142 I llm_load_print_meta: n_expert         = 0
0.00.050.142 I llm_load_print_meta: n_expert_used    = 0
0.00.050.142 I llm_load_print_meta: causal attn      = 1
0.00.050.142 I llm_load_print_meta: pooling type     = 0
0.00.050.143 I llm_load_print_meta: rope type        = 2
0.00.050.144 I llm_load_print_meta: rope scaling     = linear
0.00.050.145 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.145 I llm_load_print_meta: freq_scale_train = 1
0.00.050.145 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.147 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.147 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.147 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.147 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.147 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.147 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.147 I llm_load_print_meta: model type       = 1.4B
0.00.050.148 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.148 I llm_load_print_meta: model params     = 1.41 B
0.00.050.149 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.149 I llm_load_print_meta: general.name     = 1.4B
0.00.050.149 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.150 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.150 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.150 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.150 I llm_load_print_meta: LF token         = 128 ''
0.00.050.151 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.151 I llm_load_print_meta: max token length = 1024
0.00.052.142 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.142 I llm_load_tensors: offloading output layer to GPU
0.00.052.143 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.153 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.155 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.037 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.038 I llama_new_context_with_model: n_ctx         = 128
0.00.053.038 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.039 I llama_new_context_with_model: n_batch       = 128
0.00.053.039 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.039 I llama_new_context_with_model: flash_attn    = 0
0.00.053.039 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.040 I llama_new_context_with_model: freq_scale    = 1
0.00.053.040 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.040 I ggml_metal_init: allocating
0.00.053.043 I ggml_metal_init: found device: Apple M4
0.00.053.045 I ggml_metal_init: picking default device: Apple M4
0.00.053.609 I ggml_metal_init: using embedded metal library
0.00.055.898 I ggml_metal_init: GPU name:   Apple M4
0.00.055.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.901 I ggml_metal_init: simdgroup reduction   = true
0.00.055.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.901 I ggml_metal_init: has bfloat            = true
0.00.055.901 I ggml_metal_init: use bfloat            = true
0.00.055.902 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.902 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.392 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.733 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.734 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.748 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.638 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.639 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.639 I llama_new_context_with_model: graph nodes  = 967
0.00.067.640 I llama_new_context_with_model: graph splits = 2
0.00.067.641 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.641 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.344 I 
0.00.729.390 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.406 I perplexity: tokenizing the input ..
0.00.737.527 I perplexity: tokenization took 8.117 ms
0.00.737.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.393 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.873.654 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.873.666 I llama_perf_context_print:        load time =     720.44 ms
0.00.873.667 I llama_perf_context_print: prompt eval time =     134.64 ms /   128 tokens (    1.05 ms per token,   950.70 tokens per second)
0.00.873.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.668 I llama_perf_context_print:       total time =     144.33 ms /   129 tokens
0.00.874.000 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.078s
sys	0m0.117s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.836 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.298 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.309 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.310 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.310 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.313 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.437 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.426 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.426 I llama_model_loader: - type  f32:  194 tensors
0.00.024.426 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.427 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.270 I llm_load_vocab: special tokens cache size = 25
0.00.051.287 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.290 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.290 I llm_load_print_meta: arch             = gptneox
0.00.051.291 I llm_load_print_meta: vocab type       = BPE
0.00.051.291 I llm_load_print_meta: n_vocab          = 50304
0.00.051.291 I llm_load_print_meta: n_merges         = 50009
0.00.051.291 I llm_load_print_meta: vocab_only       = 0
0.00.051.292 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.292 I llm_load_print_meta: n_embd           = 2048
0.00.051.292 I llm_load_print_meta: n_layer          = 24
0.00.051.295 I llm_load_print_meta: n_head           = 16
0.00.051.295 I llm_load_print_meta: n_head_kv        = 16
0.00.051.296 I llm_load_print_meta: n_rot            = 32
0.00.051.296 I llm_load_print_meta: n_swa            = 0
0.00.051.296 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.296 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.297 I llm_load_print_meta: n_gqa            = 1
0.00.051.298 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.298 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.299 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.299 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.301 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.301 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.301 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.302 I llm_load_print_meta: n_ff             = 8192
0.00.051.302 I llm_load_print_meta: n_expert         = 0
0.00.051.302 I llm_load_print_meta: n_expert_used    = 0
0.00.051.302 I llm_load_print_meta: causal attn      = 1
0.00.051.303 I llm_load_print_meta: pooling type     = 0
0.00.051.303 I llm_load_print_meta: rope type        = 2
0.00.051.303 I llm_load_print_meta: rope scaling     = linear
0.00.051.303 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.304 I llm_load_print_meta: freq_scale_train = 1
0.00.051.304 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.304 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.304 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.304 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.306 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.306 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.307 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.307 I llm_load_print_meta: model type       = 1.4B
0.00.051.307 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.308 I llm_load_print_meta: model params     = 1.41 B
0.00.051.308 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.308 I llm_load_print_meta: general.name     = 1.4B
0.00.051.309 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.309 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.309 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.309 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.313 I llm_load_print_meta: LF token         = 128 ''
0.00.051.314 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.314 I llm_load_print_meta: max token length = 1024
0.00.053.247 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.247 I llm_load_tensors: offloading output layer to GPU
0.00.053.247 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.258 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.259 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.139 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.139 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.139 I llama_new_context_with_model: n_batch       = 2048
0.00.054.139 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.140 I llama_new_context_with_model: flash_attn    = 0
0.00.054.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.140 I llama_new_context_with_model: freq_scale    = 1
0.00.054.141 I ggml_metal_init: allocating
0.00.054.144 I ggml_metal_init: found device: Apple M4
0.00.054.146 I ggml_metal_init: picking default device: Apple M4
0.00.054.740 I ggml_metal_init: using embedded metal library
0.00.057.059 I ggml_metal_init: GPU name:   Apple M4
0.00.057.061 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.061 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.062 I ggml_metal_init: simdgroup reduction   = true
0.00.057.062 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.062 I ggml_metal_init: has bfloat            = true
0.00.057.062 I ggml_metal_init: use bfloat            = true
0.00.057.063 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.847 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.863 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.868 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.838 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.840 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.840 I llama_new_context_with_model: graph nodes  = 967
0.00.088.840 I llama_new_context_with_model: graph splits = 2
0.00.088.843 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.989 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.445 I main: llama threadpool init, n_threads = 4
0.00.487.490 I 
0.00.487.518 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.521 I 
0.00.487.761 I sampler seed: 1234
0.00.487.767 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.487.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.487.816 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.487.816 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.151.535 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.151.536 I llama_perf_context_print:        load time =     477.60 ms
0.01.151.537 I llama_perf_context_print: prompt eval time =      35.67 ms /     7 tokens (    5.10 ms per token,   196.23 tokens per second)
0.01.151.538 I llama_perf_context_print:        eval time =     625.68 ms /    63 runs   (    9.93 ms per token,   100.69 tokens per second)
0.01.151.538 I llama_perf_context_print:       total time =     664.10 ms /    70 tokens
0.01.151.819 I ggml_metal_free: deallocating

real	0m1.169s
user	0m0.110s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.964 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.441 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.442 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.442 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.443 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.443 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.444 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.444 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.445 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.445 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.447 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.461 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.599 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.478 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.479 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.479 I llama_model_loader: - type  f32:  194 tensors
0.00.024.480 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.480 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.480 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.479 I llm_load_vocab: special tokens cache size = 25
0.00.050.506 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.509 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.510 I llm_load_print_meta: arch             = gptneox
0.00.050.510 I llm_load_print_meta: vocab type       = BPE
0.00.050.510 I llm_load_print_meta: n_vocab          = 50304
0.00.050.511 I llm_load_print_meta: n_merges         = 50009
0.00.050.511 I llm_load_print_meta: vocab_only       = 0
0.00.050.511 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.511 I llm_load_print_meta: n_embd           = 2048
0.00.050.511 I llm_load_print_meta: n_layer          = 24
0.00.050.515 I llm_load_print_meta: n_head           = 16
0.00.050.516 I llm_load_print_meta: n_head_kv        = 16
0.00.050.516 I llm_load_print_meta: n_rot            = 32
0.00.050.517 I llm_load_print_meta: n_swa            = 0
0.00.050.517 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.517 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.518 I llm_load_print_meta: n_gqa            = 1
0.00.050.518 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.519 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.520 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.520 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.520 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.520 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.523 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.524 I llm_load_print_meta: n_ff             = 8192
0.00.050.524 I llm_load_print_meta: n_expert         = 0
0.00.050.524 I llm_load_print_meta: n_expert_used    = 0
0.00.050.524 I llm_load_print_meta: causal attn      = 1
0.00.050.524 I llm_load_print_meta: pooling type     = 0
0.00.050.526 I llm_load_print_meta: rope type        = 2
0.00.050.526 I llm_load_print_meta: rope scaling     = linear
0.00.050.527 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.527 I llm_load_print_meta: freq_scale_train = 1
0.00.050.527 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.527 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.527 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.528 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.528 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.528 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.528 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.528 I llm_load_print_meta: model type       = 1.4B
0.00.050.532 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.533 I llm_load_print_meta: model params     = 1.41 B
0.00.050.533 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.533 I llm_load_print_meta: general.name     = 1.4B
0.00.050.534 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.534 I llm_load_print_meta: LF token         = 128 ''
0.00.050.535 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.535 I llm_load_print_meta: max token length = 1024
0.00.052.326 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.326 I llm_load_tensors: offloading output layer to GPU
0.00.052.326 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.332 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.332 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.302 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.302 I llama_new_context_with_model: n_ctx         = 128
0.00.053.303 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.303 I llama_new_context_with_model: n_batch       = 128
0.00.053.303 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.303 I llama_new_context_with_model: flash_attn    = 0
0.00.053.304 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.304 I llama_new_context_with_model: freq_scale    = 1
0.00.053.304 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.305 I ggml_metal_init: allocating
0.00.053.308 I ggml_metal_init: found device: Apple M4
0.00.053.310 I ggml_metal_init: picking default device: Apple M4
0.00.053.839 I ggml_metal_init: using embedded metal library
0.00.056.162 I ggml_metal_init: GPU name:   Apple M4
0.00.056.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.164 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.164 I ggml_metal_init: simdgroup reduction   = true
0.00.056.165 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.165 I ggml_metal_init: has bfloat            = true
0.00.056.165 I ggml_metal_init: use bfloat            = true
0.00.056.165 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.166 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.506 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.781 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.785 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.808 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.704 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.705 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.705 I llama_new_context_with_model: graph nodes  = 967
0.00.067.706 I llama_new_context_with_model: graph splits = 2
0.00.067.707 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.404 I 
0.00.422.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.498 I perplexity: tokenizing the input ..
0.00.430.828 I perplexity: tokenization took 8.329 ms
0.00.430.831 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.686 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.870 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.889 I llama_perf_context_print:        load time =     412.43 ms
0.00.563.890 I llama_perf_context_print: prompt eval time =     131.63 ms /   128 tokens (    1.03 ms per token,   972.42 tokens per second)
0.00.563.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.891 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.564.472 I ggml_metal_free: deallocating

real	0m0.579s
user	0m0.078s
sys	0m0.073s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.267 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.715 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.717 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.721 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.721 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.722 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.722 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.723 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.891 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.954 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.955 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.957 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.957 I llama_model_loader: - type  f32:  194 tensors
0.00.024.958 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.958 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.958 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.958 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.171 I llm_load_vocab: special tokens cache size = 25
0.00.052.299 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.304 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.305 I llm_load_print_meta: arch             = gptneox
0.00.052.305 I llm_load_print_meta: vocab type       = BPE
0.00.052.306 I llm_load_print_meta: n_vocab          = 50304
0.00.052.306 I llm_load_print_meta: n_merges         = 50009
0.00.052.306 I llm_load_print_meta: vocab_only       = 0
0.00.052.306 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.306 I llm_load_print_meta: n_embd           = 2048
0.00.052.306 I llm_load_print_meta: n_layer          = 24
0.00.052.310 I llm_load_print_meta: n_head           = 16
0.00.052.310 I llm_load_print_meta: n_head_kv        = 16
0.00.052.311 I llm_load_print_meta: n_rot            = 32
0.00.052.311 I llm_load_print_meta: n_swa            = 0
0.00.052.311 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.311 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.312 I llm_load_print_meta: n_gqa            = 1
0.00.052.313 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.313 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.314 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.314 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.315 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.315 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.315 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.315 I llm_load_print_meta: n_ff             = 8192
0.00.052.316 I llm_load_print_meta: n_expert         = 0
0.00.052.316 I llm_load_print_meta: n_expert_used    = 0
0.00.052.316 I llm_load_print_meta: causal attn      = 1
0.00.052.316 I llm_load_print_meta: pooling type     = 0
0.00.052.316 I llm_load_print_meta: rope type        = 2
0.00.052.316 I llm_load_print_meta: rope scaling     = linear
0.00.052.317 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.317 I llm_load_print_meta: freq_scale_train = 1
0.00.052.317 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.317 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.317 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.318 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.318 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.318 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.318 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.318 I llm_load_print_meta: model type       = 1.4B
0.00.052.319 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.319 I llm_load_print_meta: model params     = 1.41 B
0.00.052.319 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.320 I llm_load_print_meta: general.name     = 1.4B
0.00.052.320 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.320 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.320 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.323 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.323 I llm_load_print_meta: LF token         = 128 ''
0.00.052.324 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.324 I llm_load_print_meta: max token length = 1024
0.00.054.336 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.336 I llm_load_tensors: offloading output layer to GPU
0.00.054.337 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.347 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.349 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.243 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.244 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.244 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.244 I llama_new_context_with_model: n_batch       = 2048
0.00.055.245 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.245 I llama_new_context_with_model: flash_attn    = 0
0.00.055.246 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.246 I llama_new_context_with_model: freq_scale    = 1
0.00.055.246 I ggml_metal_init: allocating
0.00.055.251 I ggml_metal_init: found device: Apple M4
0.00.055.253 I ggml_metal_init: picking default device: Apple M4
0.00.055.858 I ggml_metal_init: using embedded metal library
0.00.058.182 I ggml_metal_init: GPU name:   Apple M4
0.00.058.184 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.184 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.184 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.185 I ggml_metal_init: simdgroup reduction   = true
0.00.058.185 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.185 I ggml_metal_init: has bfloat            = true
0.00.058.185 I ggml_metal_init: use bfloat            = true
0.00.058.186 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.731 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.668 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.676 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.706 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.616 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.618 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.618 I llama_new_context_with_model: graph nodes  = 967
0.00.088.618 I llama_new_context_with_model: graph splits = 2
0.00.088.621 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.024 I main: llama threadpool init, n_threads = 4
0.00.686.065 I 
0.00.686.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.085 I 
0.00.686.322 I sampler seed: 1234
0.00.686.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.372 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.377 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.429.533 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.429.534 I llama_perf_context_print:        load time =     677.75 ms
0.01.429.534 I llama_perf_context_print: prompt eval time =      40.43 ms /     7 tokens (    5.78 ms per token,   173.14 tokens per second)
0.01.429.535 I llama_perf_context_print:        eval time =     699.54 ms /    63 runs   (   11.10 ms per token,    90.06 tokens per second)
0.01.429.535 I llama_perf_context_print:       total time =     743.51 ms /    70 tokens
0.01.429.723 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.113s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.777 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.243 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.248 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.250 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.324 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.458 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.460 I llama_model_loader: - type  f32:  194 tensors
0.00.023.461 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.461 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.461 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.461 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.311 I llm_load_vocab: special tokens cache size = 25
0.00.050.278 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.281 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.281 I llm_load_print_meta: arch             = gptneox
0.00.050.281 I llm_load_print_meta: vocab type       = BPE
0.00.050.282 I llm_load_print_meta: n_vocab          = 50304
0.00.050.282 I llm_load_print_meta: n_merges         = 50009
0.00.050.282 I llm_load_print_meta: vocab_only       = 0
0.00.050.282 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.282 I llm_load_print_meta: n_embd           = 2048
0.00.050.282 I llm_load_print_meta: n_layer          = 24
0.00.050.285 I llm_load_print_meta: n_head           = 16
0.00.050.286 I llm_load_print_meta: n_head_kv        = 16
0.00.050.286 I llm_load_print_meta: n_rot            = 32
0.00.050.289 I llm_load_print_meta: n_swa            = 0
0.00.050.289 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.289 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.290 I llm_load_print_meta: n_gqa            = 1
0.00.050.291 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.292 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.293 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.293 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.294 I llm_load_print_meta: n_ff             = 8192
0.00.050.294 I llm_load_print_meta: n_expert         = 0
0.00.050.294 I llm_load_print_meta: n_expert_used    = 0
0.00.050.294 I llm_load_print_meta: causal attn      = 1
0.00.050.294 I llm_load_print_meta: pooling type     = 0
0.00.050.296 I llm_load_print_meta: rope type        = 2
0.00.050.296 I llm_load_print_meta: rope scaling     = linear
0.00.050.296 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.297 I llm_load_print_meta: freq_scale_train = 1
0.00.050.297 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.297 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.297 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.297 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.298 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.298 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.298 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.298 I llm_load_print_meta: model type       = 1.4B
0.00.050.299 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.299 I llm_load_print_meta: model params     = 1.41 B
0.00.050.300 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.300 I llm_load_print_meta: general.name     = 1.4B
0.00.050.300 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.300 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.301 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.301 I llm_load_print_meta: LF token         = 128 ''
0.00.050.301 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.301 I llm_load_print_meta: max token length = 1024
0.00.052.248 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.248 I llm_load_tensors: offloading output layer to GPU
0.00.052.249 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.259 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.260 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.139 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.140 I llama_new_context_with_model: n_ctx         = 128
0.00.053.140 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.140 I llama_new_context_with_model: n_batch       = 128
0.00.053.140 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.140 I llama_new_context_with_model: flash_attn    = 0
0.00.053.141 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.141 I llama_new_context_with_model: freq_scale    = 1
0.00.053.142 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.142 I ggml_metal_init: allocating
0.00.053.149 I ggml_metal_init: found device: Apple M4
0.00.053.152 I ggml_metal_init: picking default device: Apple M4
0.00.053.715 I ggml_metal_init: using embedded metal library
0.00.056.030 I ggml_metal_init: GPU name:   Apple M4
0.00.056.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.032 I ggml_metal_init: simdgroup reduction   = true
0.00.056.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.033 I ggml_metal_init: has bfloat            = true
0.00.056.033 I ggml_metal_init: use bfloat            = true
0.00.056.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.427 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.669 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.672 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.689 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.556 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.557 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.558 I llama_new_context_with_model: graph nodes  = 967
0.00.067.558 I llama_new_context_with_model: graph splits = 2
0.00.067.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.560 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.527 I 
0.00.472.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.572 I perplexity: tokenizing the input ..
0.00.480.992 I perplexity: tokenization took 8.418 ms
0.00.480.996 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.612.972 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.243 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.262 I llama_perf_context_print:        load time =     463.74 ms
0.00.614.262 I llama_perf_context_print: prompt eval time =     131.75 ms /   128 tokens (    1.03 ms per token,   971.56 tokens per second)
0.00.614.263 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.264 I llama_perf_context_print:       total time =     141.74 ms /   129 tokens
0.00.614.749 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.079s
sys	0m0.080s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.746 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.472 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.478 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.479 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.479 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.481 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.482 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.485 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.686 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.688 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.689 I llama_model_loader: - type  f32:  194 tensors
0.00.023.689 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.689 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.689 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.982 I llm_load_vocab: special tokens cache size = 25
0.00.050.044 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.046 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.047 I llm_load_print_meta: arch             = gptneox
0.00.050.047 I llm_load_print_meta: vocab type       = BPE
0.00.050.047 I llm_load_print_meta: n_vocab          = 50304
0.00.050.047 I llm_load_print_meta: n_merges         = 50009
0.00.050.048 I llm_load_print_meta: vocab_only       = 0
0.00.050.048 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.048 I llm_load_print_meta: n_embd           = 2048
0.00.050.048 I llm_load_print_meta: n_layer          = 24
0.00.050.051 I llm_load_print_meta: n_head           = 16
0.00.050.052 I llm_load_print_meta: n_head_kv        = 16
0.00.050.053 I llm_load_print_meta: n_rot            = 32
0.00.050.053 I llm_load_print_meta: n_swa            = 0
0.00.050.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.054 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.057 I llm_load_print_meta: n_gqa            = 1
0.00.050.057 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.058 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.059 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.059 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.059 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.059 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.060 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.060 I llm_load_print_meta: n_ff             = 8192
0.00.050.060 I llm_load_print_meta: n_expert         = 0
0.00.050.062 I llm_load_print_meta: n_expert_used    = 0
0.00.050.064 I llm_load_print_meta: causal attn      = 1
0.00.050.064 I llm_load_print_meta: pooling type     = 0
0.00.050.064 I llm_load_print_meta: rope type        = 2
0.00.050.064 I llm_load_print_meta: rope scaling     = linear
0.00.050.065 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.065 I llm_load_print_meta: freq_scale_train = 1
0.00.050.065 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.065 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.065 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.069 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.069 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.071 I llm_load_print_meta: model type       = 1.4B
0.00.050.071 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.072 I llm_load_print_meta: model params     = 1.41 B
0.00.050.072 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.072 I llm_load_print_meta: general.name     = 1.4B
0.00.050.072 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.073 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.073 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.073 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.076 I llm_load_print_meta: LF token         = 128 ''
0.00.050.076 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.076 I llm_load_print_meta: max token length = 1024
0.00.052.083 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.083 I llm_load_tensors: offloading output layer to GPU
0.00.052.084 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.094 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.095 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.050 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.050 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.051 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.051 I llama_new_context_with_model: n_batch       = 2048
0.00.053.051 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.051 I llama_new_context_with_model: flash_attn    = 0
0.00.053.052 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.052 I llama_new_context_with_model: freq_scale    = 1
0.00.053.052 I ggml_metal_init: allocating
0.00.053.056 I ggml_metal_init: found device: Apple M4
0.00.053.057 I ggml_metal_init: picking default device: Apple M4
0.00.053.663 I ggml_metal_init: using embedded metal library
0.00.055.992 I ggml_metal_init: GPU name:   Apple M4
0.00.055.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.995 I ggml_metal_init: simdgroup reduction   = true
0.00.055.996 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.996 I ggml_metal_init: has bfloat            = true
0.00.055.996 I ggml_metal_init: use bfloat            = true
0.00.055.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.661 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.894 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.900 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.920 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.856 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.857 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.858 I llama_new_context_with_model: graph nodes  = 967
0.00.085.858 I llama_new_context_with_model: graph splits = 2
0.00.085.861 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.991 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.482 I main: llama threadpool init, n_threads = 4
0.00.613.526 I 
0.00.613.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.573 I 
0.00.613.807 I sampler seed: 1234
0.00.613.811 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.858 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.869 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.870 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.374.662 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.01.374.663 I llama_perf_context_print:        load time =     604.73 ms
0.01.374.663 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.74 tokens per second)
0.01.374.664 I llama_perf_context_print:        eval time =     710.62 ms /    63 runs   (   11.28 ms per token,    88.66 tokens per second)
0.01.374.664 I llama_perf_context_print:       total time =     761.18 ms /    70 tokens
0.01.374.860 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.871 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.447 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.455 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.455 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.456 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.456 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.457 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.457 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.457 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.458 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.458 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.460 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.461 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.343 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.179 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.179 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.180 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.180 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.181 I llama_model_loader: - type  f32:  194 tensors
0.00.023.181 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.181 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.182 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.928 I llm_load_vocab: special tokens cache size = 25
0.00.050.089 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.094 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.094 I llm_load_print_meta: arch             = gptneox
0.00.050.094 I llm_load_print_meta: vocab type       = BPE
0.00.050.095 I llm_load_print_meta: n_vocab          = 50304
0.00.050.096 I llm_load_print_meta: n_merges         = 50009
0.00.050.097 I llm_load_print_meta: vocab_only       = 0
0.00.050.097 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.097 I llm_load_print_meta: n_embd           = 2048
0.00.050.097 I llm_load_print_meta: n_layer          = 24
0.00.050.100 I llm_load_print_meta: n_head           = 16
0.00.050.101 I llm_load_print_meta: n_head_kv        = 16
0.00.050.101 I llm_load_print_meta: n_rot            = 32
0.00.050.101 I llm_load_print_meta: n_swa            = 0
0.00.050.101 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.101 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.102 I llm_load_print_meta: n_gqa            = 1
0.00.050.103 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.104 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.104 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.105 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.105 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.105 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.107 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.108 I llm_load_print_meta: n_ff             = 8192
0.00.050.108 I llm_load_print_meta: n_expert         = 0
0.00.050.108 I llm_load_print_meta: n_expert_used    = 0
0.00.050.108 I llm_load_print_meta: causal attn      = 1
0.00.050.108 I llm_load_print_meta: pooling type     = 0
0.00.050.108 I llm_load_print_meta: rope type        = 2
0.00.050.110 I llm_load_print_meta: rope scaling     = linear
0.00.050.112 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.112 I llm_load_print_meta: freq_scale_train = 1
0.00.050.112 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.113 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.113 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.113 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.113 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.113 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.113 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.114 I llm_load_print_meta: model type       = 1.4B
0.00.050.114 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.116 I llm_load_print_meta: model params     = 1.41 B
0.00.050.117 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.117 I llm_load_print_meta: general.name     = 1.4B
0.00.050.117 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: LF token         = 128 ''
0.00.050.118 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.118 I llm_load_print_meta: max token length = 1024
0.00.052.051 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.051 I llm_load_tensors: offloading output layer to GPU
0.00.052.052 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.062 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.063 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.001 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.001 I llama_new_context_with_model: n_ctx         = 128
0.00.053.002 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.002 I llama_new_context_with_model: n_batch       = 128
0.00.053.002 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.002 I llama_new_context_with_model: flash_attn    = 0
0.00.053.003 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.003 I llama_new_context_with_model: freq_scale    = 1
0.00.053.003 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.004 I ggml_metal_init: allocating
0.00.053.006 I ggml_metal_init: found device: Apple M4
0.00.053.008 I ggml_metal_init: picking default device: Apple M4
0.00.053.583 I ggml_metal_init: using embedded metal library
0.00.055.908 I ggml_metal_init: GPU name:   Apple M4
0.00.055.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.911 I ggml_metal_init: simdgroup reduction   = true
0.00.055.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.911 I ggml_metal_init: has bfloat            = true
0.00.055.912 I ggml_metal_init: use bfloat            = true
0.00.055.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.246 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.574 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.587 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.457 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.458 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.459 I llama_new_context_with_model: graph nodes  = 967
0.00.067.459 I llama_new_context_with_model: graph splits = 2
0.00.067.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.460 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.561.996 I 
0.00.562.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.562.052 I perplexity: tokenizing the input ..
0.00.570.201 I perplexity: tokenization took 8.148 ms
0.00.570.205 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.704.807 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.705.978 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.705.992 I llama_perf_context_print:        load time =     553.12 ms
0.00.705.994 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.70 tokens per second)
0.00.705.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.705.995 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.706.429 I ggml_metal_free: deallocating

real	0m0.720s
user	0m0.078s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.245 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.677 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.689 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.914 I llama_model_loader: - type  f32:  194 tensors
0.00.023.915 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.915 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.852 I llm_load_vocab: special tokens cache size = 25
0.00.050.822 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.824 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.825 I llm_load_print_meta: arch             = gptneox
0.00.050.825 I llm_load_print_meta: vocab type       = BPE
0.00.050.825 I llm_load_print_meta: n_vocab          = 50304
0.00.050.826 I llm_load_print_meta: n_merges         = 50009
0.00.050.826 I llm_load_print_meta: vocab_only       = 0
0.00.050.826 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.826 I llm_load_print_meta: n_embd           = 2048
0.00.050.826 I llm_load_print_meta: n_layer          = 24
0.00.050.829 I llm_load_print_meta: n_head           = 16
0.00.050.830 I llm_load_print_meta: n_head_kv        = 16
0.00.050.830 I llm_load_print_meta: n_rot            = 32
0.00.050.830 I llm_load_print_meta: n_swa            = 0
0.00.050.833 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.833 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.834 I llm_load_print_meta: n_gqa            = 1
0.00.050.834 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.835 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.836 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.837 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.838 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.838 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.838 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.839 I llm_load_print_meta: n_ff             = 8192
0.00.050.839 I llm_load_print_meta: n_expert         = 0
0.00.050.839 I llm_load_print_meta: n_expert_used    = 0
0.00.050.839 I llm_load_print_meta: causal attn      = 1
0.00.050.839 I llm_load_print_meta: pooling type     = 0
0.00.050.839 I llm_load_print_meta: rope type        = 2
0.00.050.840 I llm_load_print_meta: rope scaling     = linear
0.00.050.840 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.840 I llm_load_print_meta: freq_scale_train = 1
0.00.050.840 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.841 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.841 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.841 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.841 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.841 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.841 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.843 I llm_load_print_meta: model type       = 1.4B
0.00.050.843 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.844 I llm_load_print_meta: model params     = 1.41 B
0.00.050.844 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.845 I llm_load_print_meta: general.name     = 1.4B
0.00.050.845 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.845 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.845 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.845 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.846 I llm_load_print_meta: LF token         = 128 ''
0.00.050.846 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.846 I llm_load_print_meta: max token length = 1024
0.00.052.874 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.875 I llm_load_tensors: offloading output layer to GPU
0.00.052.875 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.885 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.887 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.815 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.816 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.816 I llama_new_context_with_model: n_batch       = 2048
0.00.053.816 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.816 I llama_new_context_with_model: flash_attn    = 0
0.00.053.816 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.817 I llama_new_context_with_model: freq_scale    = 1
0.00.053.817 I ggml_metal_init: allocating
0.00.053.820 I ggml_metal_init: found device: Apple M4
0.00.053.822 I ggml_metal_init: picking default device: Apple M4
0.00.054.397 I ggml_metal_init: using embedded metal library
0.00.056.754 I ggml_metal_init: GPU name:   Apple M4
0.00.056.756 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.756 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.756 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.757 I ggml_metal_init: simdgroup reduction   = true
0.00.056.757 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.757 I ggml_metal_init: has bfloat            = true
0.00.056.757 I ggml_metal_init: use bfloat            = true
0.00.056.758 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.707 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.686 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.695 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.716 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.816 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.817 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.818 I llama_new_context_with_model: graph nodes  = 967
0.00.087.818 I llama_new_context_with_model: graph splits = 2
0.00.087.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.667 I main: llama threadpool init, n_threads = 4
0.00.684.700 I 
0.00.684.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.725 I 
0.00.684.955 I sampler seed: 1234
0.00.684.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.998 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.002 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.002 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.538.790 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.538.790 I llama_perf_context_print:        load time =     675.42 ms
0.01.538.791 I llama_perf_context_print: prompt eval time =      55.52 ms /     7 tokens (    7.93 ms per token,   126.09 tokens per second)
0.01.538.792 I llama_perf_context_print:        eval time =     795.11 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.538.792 I llama_perf_context_print:       total time =     854.12 ms /    70 tokens
0.01.539.047 I ggml_metal_free: deallocating

real	0m1.557s
user	0m0.111s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.493 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.873 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.873 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.874 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.875 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.875 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.875 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.876 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.876 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.876 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.877 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.879 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.880 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.880 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.959 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.883 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.883 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.883 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.884 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.884 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.884 I llama_model_loader: - type  f32:  194 tensors
0.00.023.885 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.885 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.597 I llm_load_vocab: special tokens cache size = 25
0.00.050.546 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.549 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.549 I llm_load_print_meta: arch             = gptneox
0.00.050.549 I llm_load_print_meta: vocab type       = BPE
0.00.050.550 I llm_load_print_meta: n_vocab          = 50304
0.00.050.550 I llm_load_print_meta: n_merges         = 50009
0.00.050.550 I llm_load_print_meta: vocab_only       = 0
0.00.050.550 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.550 I llm_load_print_meta: n_embd           = 2048
0.00.050.550 I llm_load_print_meta: n_layer          = 24
0.00.050.554 I llm_load_print_meta: n_head           = 16
0.00.050.555 I llm_load_print_meta: n_head_kv        = 16
0.00.050.555 I llm_load_print_meta: n_rot            = 32
0.00.050.555 I llm_load_print_meta: n_swa            = 0
0.00.050.555 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.555 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.556 I llm_load_print_meta: n_gqa            = 1
0.00.050.557 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.558 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.558 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.559 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.559 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.559 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.559 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.560 I llm_load_print_meta: n_ff             = 8192
0.00.050.560 I llm_load_print_meta: n_expert         = 0
0.00.050.560 I llm_load_print_meta: n_expert_used    = 0
0.00.050.561 I llm_load_print_meta: causal attn      = 1
0.00.050.561 I llm_load_print_meta: pooling type     = 0
0.00.050.561 I llm_load_print_meta: rope type        = 2
0.00.050.561 I llm_load_print_meta: rope scaling     = linear
0.00.050.562 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.562 I llm_load_print_meta: freq_scale_train = 1
0.00.050.562 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.563 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.565 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.565 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.565 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.565 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.566 I llm_load_print_meta: model type       = 1.4B
0.00.050.566 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.566 I llm_load_print_meta: model params     = 1.41 B
0.00.050.567 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.567 I llm_load_print_meta: general.name     = 1.4B
0.00.050.568 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: LF token         = 128 ''
0.00.050.572 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: max token length = 1024
0.00.052.564 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.564 I llm_load_tensors: offloading output layer to GPU
0.00.052.565 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.575 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.576 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.468 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.469 I llama_new_context_with_model: n_ctx         = 128
0.00.053.469 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.470 I llama_new_context_with_model: n_batch       = 128
0.00.053.470 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.470 I llama_new_context_with_model: flash_attn    = 0
0.00.053.471 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.471 I llama_new_context_with_model: freq_scale    = 1
0.00.053.471 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.472 I ggml_metal_init: allocating
0.00.053.477 I ggml_metal_init: found device: Apple M4
0.00.053.479 I ggml_metal_init: picking default device: Apple M4
0.00.054.054 I ggml_metal_init: using embedded metal library
0.00.056.388 I ggml_metal_init: GPU name:   Apple M4
0.00.056.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.390 I ggml_metal_init: simdgroup reduction   = true
0.00.056.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.391 I ggml_metal_init: has bfloat            = true
0.00.056.391 I ggml_metal_init: use bfloat            = true
0.00.056.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.392 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.987 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.272 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.276 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.290 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.149 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.150 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.151 I llama_new_context_with_model: graph nodes  = 967
0.00.068.151 I llama_new_context_with_model: graph splits = 2
0.00.068.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.738 I 
0.00.620.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.780 I perplexity: tokenizing the input ..
0.00.628.106 I perplexity: tokenization took 7.324 ms
0.00.628.110 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.253 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.770.593 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.770.613 I llama_perf_context_print:        load time =     611.24 ms
0.00.770.615 I llama_perf_context_print: prompt eval time =     140.91 ms /   128 tokens (    1.10 ms per token,   908.35 tokens per second)
0.00.770.616 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.617 I llama_perf_context_print:       total time =     149.88 ms /   129 tokens
0.00.771.121 I ggml_metal_free: deallocating

real	0m0.785s
user	0m0.078s
sys	0m0.104s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.624 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.876 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.881 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.885 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.890 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.890 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.892 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.892 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.893 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.040 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.940 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.941 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.941 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.942 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.942 I llama_model_loader: - type  f32:  194 tensors
0.00.024.942 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.161 I llm_load_vocab: special tokens cache size = 25
0.00.051.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.153 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.153 I llm_load_print_meta: arch             = gptneox
0.00.051.154 I llm_load_print_meta: vocab type       = BPE
0.00.051.154 I llm_load_print_meta: n_vocab          = 50304
0.00.051.154 I llm_load_print_meta: n_merges         = 50009
0.00.051.155 I llm_load_print_meta: vocab_only       = 0
0.00.051.155 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.155 I llm_load_print_meta: n_embd           = 2048
0.00.051.155 I llm_load_print_meta: n_layer          = 24
0.00.051.158 I llm_load_print_meta: n_head           = 16
0.00.051.158 I llm_load_print_meta: n_head_kv        = 16
0.00.051.159 I llm_load_print_meta: n_rot            = 32
0.00.051.159 I llm_load_print_meta: n_swa            = 0
0.00.051.159 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.159 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.160 I llm_load_print_meta: n_gqa            = 1
0.00.051.160 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.163 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.164 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.164 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.165 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.165 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.165 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.166 I llm_load_print_meta: n_ff             = 8192
0.00.051.166 I llm_load_print_meta: n_expert         = 0
0.00.051.166 I llm_load_print_meta: n_expert_used    = 0
0.00.051.166 I llm_load_print_meta: causal attn      = 1
0.00.051.167 I llm_load_print_meta: pooling type     = 0
0.00.051.167 I llm_load_print_meta: rope type        = 2
0.00.051.167 I llm_load_print_meta: rope scaling     = linear
0.00.051.168 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.168 I llm_load_print_meta: freq_scale_train = 1
0.00.051.168 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.168 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.168 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.169 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.169 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.169 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.169 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.169 I llm_load_print_meta: model type       = 1.4B
0.00.051.170 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.170 I llm_load_print_meta: model params     = 1.41 B
0.00.051.172 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.172 I llm_load_print_meta: general.name     = 1.4B
0.00.051.173 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.173 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.174 I llm_load_print_meta: LF token         = 128 ''
0.00.051.175 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.175 I llm_load_print_meta: max token length = 1024
0.00.053.206 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.207 I llm_load_tensors: offloading output layer to GPU
0.00.053.207 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.217 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.219 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.104 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.104 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.104 I llama_new_context_with_model: n_batch       = 2048
0.00.054.104 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.104 I llama_new_context_with_model: flash_attn    = 0
0.00.054.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.105 I llama_new_context_with_model: freq_scale    = 1
0.00.054.106 I ggml_metal_init: allocating
0.00.054.112 I ggml_metal_init: found device: Apple M4
0.00.054.114 I ggml_metal_init: picking default device: Apple M4
0.00.054.702 I ggml_metal_init: using embedded metal library
0.00.057.055 I ggml_metal_init: GPU name:   Apple M4
0.00.057.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.058 I ggml_metal_init: simdgroup reduction   = true
0.00.057.058 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.058 I ggml_metal_init: has bfloat            = true
0.00.057.058 I ggml_metal_init: use bfloat            = true
0.00.057.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.686 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.581 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.589 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.568 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.569 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.569 I llama_new_context_with_model: graph nodes  = 967
0.00.086.569 I llama_new_context_with_model: graph splits = 2
0.00.086.572 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.715 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.952 I main: llama threadpool init, n_threads = 4
0.00.771.988 I 
0.00.772.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.040 I 
0.00.772.277 I sampler seed: 1234
0.00.772.282 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.297 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.298 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.298 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.644.957 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.644.957 I llama_perf_context_print:        load time =     763.32 ms
0.01.644.958 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.78 tokens per second)
0.01.644.958 I llama_perf_context_print:        eval time =     815.29 ms /    63 runs   (   12.94 ms per token,    77.27 tokens per second)
0.01.644.959 I llama_perf_context_print:       total time =     873.01 ms /    70 tokens
0.01.645.161 I ggml_metal_free: deallocating

real	0m1.663s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4425 (6369f867) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.976 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.761 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.768 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.772 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.772 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.776 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.776 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.776 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.810 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.796 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.796 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.797 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.797 I llama_model_loader: - type  f32:  194 tensors
0.00.023.798 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.545 I llm_load_vocab: special tokens cache size = 25
0.00.050.548 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.551 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.551 I llm_load_print_meta: arch             = gptneox
0.00.050.551 I llm_load_print_meta: vocab type       = BPE
0.00.050.552 I llm_load_print_meta: n_vocab          = 50304
0.00.050.552 I llm_load_print_meta: n_merges         = 50009
0.00.050.552 I llm_load_print_meta: vocab_only       = 0
0.00.050.552 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.552 I llm_load_print_meta: n_embd           = 2048
0.00.050.552 I llm_load_print_meta: n_layer          = 24
0.00.050.556 I llm_load_print_meta: n_head           = 16
0.00.050.556 I llm_load_print_meta: n_head_kv        = 16
0.00.050.559 I llm_load_print_meta: n_rot            = 32
0.00.050.559 I llm_load_print_meta: n_swa            = 0
0.00.050.559 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.559 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.560 I llm_load_print_meta: n_gqa            = 1
0.00.050.561 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.562 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.562 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.562 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.563 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.563 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.564 I llm_load_print_meta: n_ff             = 8192
0.00.050.564 I llm_load_print_meta: n_expert         = 0
0.00.050.564 I llm_load_print_meta: n_expert_used    = 0
0.00.050.564 I llm_load_print_meta: causal attn      = 1
0.00.050.564 I llm_load_print_meta: pooling type     = 0
0.00.050.564 I llm_load_print_meta: rope type        = 2
0.00.050.565 I llm_load_print_meta: rope scaling     = linear
0.00.050.566 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.567 I llm_load_print_meta: freq_scale_train = 1
0.00.050.569 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.569 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.569 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.569 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.570 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.570 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.570 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.570 I llm_load_print_meta: model type       = 1.4B
0.00.050.571 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.571 I llm_load_print_meta: model params     = 1.41 B
0.00.050.571 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.572 I llm_load_print_meta: general.name     = 1.4B
0.00.050.572 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.572 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.573 I llm_load_print_meta: LF token         = 128 ''
0.00.050.574 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.574 I llm_load_print_meta: max token length = 1024
0.00.052.693 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.693 I llm_load_tensors: offloading output layer to GPU
0.00.052.693 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.704 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.705 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.613 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.613 I llama_new_context_with_model: n_ctx         = 128
0.00.053.614 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.614 I llama_new_context_with_model: n_batch       = 128
0.00.053.614 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.614 I llama_new_context_with_model: flash_attn    = 0
0.00.053.614 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.615 I llama_new_context_with_model: freq_scale    = 1
0.00.053.615 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.616 I ggml_metal_init: allocating
0.00.053.621 I ggml_metal_init: found device: Apple M4
0.00.053.624 I ggml_metal_init: picking default device: Apple M4
0.00.054.180 I ggml_metal_init: using embedded metal library
0.00.056.516 I ggml_metal_init: GPU name:   Apple M4
0.00.056.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.519 I ggml_metal_init: simdgroup reduction   = true
0.00.056.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.519 I ggml_metal_init: has bfloat            = true
0.00.056.519 I ggml_metal_init: use bfloat            = true
0.00.056.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.846 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.069 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.071 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.084 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.933 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.934 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.934 I llama_new_context_with_model: graph nodes  = 967
0.00.067.934 I llama_new_context_with_model: graph splits = 2
0.00.067.935 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.935 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.512.979 I 
0.00.513.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.028 I perplexity: tokenizing the input ..
0.00.521.694 I perplexity: tokenization took 8.664 ms
0.00.521.698 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.661.009 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.662.438 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.662.450 I llama_perf_context_print:        load time =     504.00 ms
0.00.662.451 I llama_perf_context_print: prompt eval time =     139.08 ms /   128 tokens (    1.09 ms per token,   920.33 tokens per second)
0.00.662.452 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.662.454 I llama_perf_context_print:       total time =     149.47 ms /   129 tokens
0.00.662.778 I ggml_metal_free: deallocating

real	0m0.676s
user	0m0.079s
sys	0m0.101s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4425 (6369f867)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129b0b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129b0b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129b0bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129b0c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129b0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129b0d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129b0d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129b0db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129b0e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129b0e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129b0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129b0f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129b0fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129b10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129b10b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129b11230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129b11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129b12070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129b12790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129b12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129b13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129b13da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129b144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129b14d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129b15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129b15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129b15d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129b169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129b16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129b171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129b17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129b17920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129b181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129b186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129b189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129b18e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129b192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129b19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129b19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129b1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129b1a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129b1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129b1aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129b1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129b1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129b1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129b1c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129b1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129b1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129b1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129b1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129b1e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129b1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129b1efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129b1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129b1fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129b200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129b203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129b209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129b211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129b21460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129b21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129b21da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129b22240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129b226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129b22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129b23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129b234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129b23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129b23e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129b242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129b24740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129b24be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129b25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129b25680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129b25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129b26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129b26670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129b26bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129b27110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129b27660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129b27bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129b28100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129b28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129b28ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129b290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129b29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129b29b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129b2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129b2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129b2ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129b2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129b2b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129b2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129b2c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129b2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129b2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129b1c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129b2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129b2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129b2dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129b2e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129b2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129b2ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129b2f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129b2f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129b2fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129b30200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129b30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129b30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129b311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129b31740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129b31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129b32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129b325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129b32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129b32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129b333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129b33850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129b33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129b34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129b34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129b34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129b34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129b35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129b358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129b35d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129b361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129b36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129b36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129b36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129b37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129b37910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129b37db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129b38250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129b386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129b38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129b39030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129b394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129b39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129b39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129b3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129b3a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129b3abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129b3b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129b3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129b3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129b3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129b3c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129b3c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129b3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129b3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129b3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129b3da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129b3ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129b3e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129b3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129b3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129b3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129b3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129b3fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129b3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129b403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129b40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129b40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129b411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129b41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129b41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129b41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129b42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129b428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129b42d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129b43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129b436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129b43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129b43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129b44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129b44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129b44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129b45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129b45710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129b45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129b46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129b464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129b46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129b46e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129b472d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129b47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129b47c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129b480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129b48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129b489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129b48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129b493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129b49930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129b49e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129b4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129b4a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129b4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129b4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129b4b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129b4c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129b4c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129b4c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129b4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129b4d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129b4dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129b4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129b4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129b4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129b4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129b4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129b4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129b501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129b506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129b50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129b51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129b516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129b51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129b52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129b526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129b52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129b53170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129b536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129b53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129b54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129b546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129b54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129b55150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129b556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129b55bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129b56140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129b56690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129b56be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129b57130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129b57680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129b57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129b58120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129b58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129b58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129b59110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129b59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129b59bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129b5a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129b5a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129b5aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129b5b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129b5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129b5bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129b5c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129b5c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129b5cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129b5d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129b5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129b5db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129b5e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129b5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129b5eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129b5f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129b5f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129b5fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129b600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129b605f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129b60b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129b61090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129b615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129b61b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129b61fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129b62470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129b62910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129b62db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129b63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129b636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129b63b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129b64030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129b644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129b64970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129b64e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129b652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129b65750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129b65bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129b66090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129b665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129b66d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129b67420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129b67b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129b68260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129b68520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129b68d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129b68fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129b695e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.134.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.134.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129c04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129c05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129c056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129c05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129c05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129c06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129c06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129c06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129c07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129c075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129c07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129c08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129c08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129c093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129c09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129c0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129c0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129c0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129c0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129c0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129c0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129c0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129c0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129c0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129c0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129c0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129c0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129c0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129c0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129c0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129c0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129c0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129c10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129c106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129c10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129c10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129c11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129c118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129c11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129c12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129c12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129c12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129c12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129c13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129c137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129c13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129c140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129c14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129c14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129c14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129c15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129c156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129c15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129c15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129c16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129c16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129c16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129c17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129c17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129c17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129c18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129c184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129c18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129c18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129c19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129c19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129c19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129c19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129c1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129c1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129c1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129c1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129c1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129c1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129c1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129c1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129c1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129c1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129c1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129c1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129c1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129c1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129c1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129c1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129c1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129c1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129c1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129c1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129c1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129c20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129c20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129c209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129c20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129c212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129c21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129c21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129c22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129c22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129c228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129c22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129c231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129c23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129c23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129c23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129c24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129c24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129c24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129c250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129c25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129c259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129c25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129c262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129c26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129c26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129c26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129c27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129c278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129c27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129c281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129c28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129c28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129c28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129c29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129c297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129c29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129c2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129c2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129c2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129c2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129c2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129c2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129c2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129c2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129c2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129c2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129c2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129c2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129c2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129c2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129c2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129c2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129c2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129c2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129c2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129c2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129c2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129c2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129c30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129c306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129c30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129c30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129c31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129c31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129c31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129c32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129c325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129c32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129c32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129c33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129c337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129c33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129c34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129c344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129c34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129c34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129c35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129c35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129c36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129c363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129c36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129c36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129c37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129c375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129c37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129c37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129c38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129c38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129c38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129c39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129c394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129c39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129c39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129c3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129c3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129c3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129c3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129c3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129c3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129c3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129c3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129c3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129c3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129c3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129c3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129c3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129c3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129c3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129c3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129c3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129c3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129c3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129c3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129c3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129c400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129c40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129c409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129c40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129c41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129c417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129c41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129c42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129c42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129c430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129c43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129c43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129c441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129c447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129c44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129c45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129c458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129c45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129c46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129c46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129c46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129c475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129c47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129c48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129c486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129c48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129c49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129c49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129c49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129c4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129c4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129c4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129c4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129c4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129c4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129c4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129c4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129c4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129c4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129c4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129c4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129c4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129c4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129c4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129c4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129c4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129c50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129c50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129c510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129c516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129c51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129c52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129c527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129c52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129c53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129c53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129c53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129c544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129c54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129c55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129c555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129c55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129c56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129c56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129c56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129c571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129c576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129c57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129c580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129c585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129c58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129c58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129c594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129c599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129c59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129c5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129c5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129c5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129c5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129c5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129c5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129c5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129c5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129c5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129c5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129c5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129c5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129c5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129c5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129c4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129c4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129c483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129c45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129c552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129c52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129c50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129c4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129c46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129c43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129c48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129c4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129c4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129c4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129c541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129c47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129c513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129c4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129c4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129c47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129c558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129c44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129c43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129c455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129c55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129c4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129c53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129c49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129c4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129c4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129c472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129c50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129c51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129c46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129c54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129c51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129c4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129c569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129c45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129c56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129c444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129c54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129c4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129c50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129c53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129c524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129c4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129c41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129c04900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129c5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129c0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129c5eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129c5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129c5f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129c5f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129c5f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129c5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129c601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129c60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129c60740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129c60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129c60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129c60f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129c61240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129c61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129c617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129c61a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129c61d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129c62000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129c622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129c62580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129c62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129c62d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129c63050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129c63310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129c635d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129c63890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129c63b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129c63e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129c640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129c64390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129c64650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129c64910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129c64bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129c64e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129c65150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129c65410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129c656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129c65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129c65c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129c65f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129c661d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129c66490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129c66750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129c66a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129c66cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129c66f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129c67250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129c67510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129c677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129c67a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129c67d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129c68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129c682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129c68590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129c68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129c68b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129c68dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129c69090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129c69350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129c69610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129c698d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129c69b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129c69e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129c6a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129c6a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129c6a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129c6a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129c6ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129c6aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129c6b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129c6b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129c6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129c6b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129c6bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129c6bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129c6c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129c6c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129c6c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129c6ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129c6cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129c6cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129c6d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129c6d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129c6d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129c6dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129c6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129c6e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129c6e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129c6e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129c6e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129c6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129c6ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129c6f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129c6f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129c6f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129c6f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129c6fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129c6fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129c70150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129c70410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129c706d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129c70990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129c70c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129c70f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129c711d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129c71490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129c71750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129c71a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129c71cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129c71f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129c72250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129c72510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129c727d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129c72a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129c72d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129c73010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129c732d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129c73590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129c73850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129c73b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129c73dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129c74090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129c74350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129c74610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129c748d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129c74b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129c74e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129c75110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129c753d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129c75690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129c75950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129c75c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129c75ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129c76190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129c76450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129c76710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129c769d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129c76c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129c76f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129c77210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129c774d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129c77790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129c77a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129c77d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129c77fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129c78290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129c78550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129c78810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129c78ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129c78d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129c79050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129c79310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129c795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129c79890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129c79b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129c79e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129c7a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129c7a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129c7a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129c7ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129c7aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129c7b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129c7b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129c7b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129c7b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129c7bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129c7bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129c7c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129c7c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129c7c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129c7ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129c7d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129c7d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129c7dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129c7e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129c7e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129c7ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129c7f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129c7f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129c7fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129c80210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129c80760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129c80cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129c81200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129c81750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129c81ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129c821f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129c82740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129c82c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129c831e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129c83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129c83c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129c841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129c84720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129c84c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129c851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129c85710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129c85c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129c861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129c86700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129c86c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129c871a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129c876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129c87c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129c88190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129c886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129c88c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129c89180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129c896d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129c89c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129c8a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129c8a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129c8ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129c8b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129c8b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129c8b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129c8bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129c8bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129c8c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129c8c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129c8cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129c8d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129c8d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129c8d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129c8de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129c8e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129c8e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129c8eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129c8efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129c8f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129c8f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129c8fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129c90a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129c91120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129c91840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129c91b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129c91f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129c92570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129c92b80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.768s
user	0m0.289s
sys	0m0.309s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4425 (6369f867)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e80a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e80a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e80ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e80b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e80b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e80bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e80c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e80cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e80d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e80d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e80da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e80df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e80eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e80f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e80fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e810190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e8108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e810fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e8116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e811ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e8125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e812d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e813420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e813cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e8143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e8146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e814cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e815920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e815e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e816120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e8165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e816880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e817110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e817650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e817910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e817db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e818250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e8186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e818b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e819030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e8194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e819970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e819e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e81a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e81a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e81ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e81b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e81bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e81c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e81c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e81cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e81d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e81d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e81df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e81e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e81eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e81f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e81f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e81f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e820100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e8203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e820860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e820d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e8211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e821640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e821ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e821f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e822420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e8228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e822d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e823200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e8236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e823b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e824090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e8245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e824b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e825080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e8255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e825b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e826070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e8265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e826b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e827060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e8275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e828050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e8285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e828af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e829040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e829590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e829ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e82a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e82a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e82aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e82b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e82bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e82bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e82c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e82cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e82d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e82d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e82dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e82e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e82e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e82ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e82f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e82f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e82fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e830150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e8306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e830bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e831090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e831530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e8319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e831e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e832310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e8327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e832c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e8330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e833590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e833a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e833ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e834370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e834810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e834cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e835150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e8355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e835a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e835f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e8363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e836870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e836d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e8371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e837650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e837af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e837f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e838430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e8388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e838d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e839210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e8396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e839b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e839ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e83a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e83a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e83add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e83b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e83b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e83bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e83c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e83c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e83c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e83ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e83d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e83d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e83dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e83e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e83e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e83e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e83ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e83f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e83f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e83fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e840110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e8405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e840a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e840ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e841390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e841830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e841cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e842170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e842610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e842ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e842f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e8433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e843890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e843d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e8441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e844670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e844fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e845450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e8458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e845d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e846230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e8466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e846b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e847010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e8474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e847950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e847df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e848340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e848890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e848de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e849330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e8495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e849c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e84a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e84a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e84b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e84b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e84b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e84bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e84c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e84cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e84d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e84d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e84d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e84e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e84e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e84ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e84f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e84f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e84fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e8500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e850640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e850b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e8510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e851630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e851b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e8520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e852620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e852b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e8530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e853610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e853b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e8540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e854600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e854b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e8550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e8555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e855b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e856090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e8565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e856b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e857080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e8575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e857b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e858070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e8585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e858b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e859060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e8595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e859b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e85a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e85a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e85aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e85b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e85b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e85bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e85c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e85c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e85cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e85d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e85d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e85dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e85e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e85e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e85eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e85f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e85f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e85faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e85fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e860540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e860a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e860f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e8613d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e861870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e861d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e8621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e862650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e862af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e862f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e863430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e8638d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e863d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e864210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e8646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e864b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e864ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e865540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e865c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e866380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e866aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e8671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e867480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e867c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e867f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e868540 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.084.825 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.830 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e8681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e84ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e8498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e84a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e81d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e81cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e84c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e814960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e81b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e81bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e81c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e81a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e81c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e813960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e81fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e82c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e867740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e816b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e816e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e84c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e84aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e814f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e815230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e8154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e8689a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e868c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e868f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e8691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e8694a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e869760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e869a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e869ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e869fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e86a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e86a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e86a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e86aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e86ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e86b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e86b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e86b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e86b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e86bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e86bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e86c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e86c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e86c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e86c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e86cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e86ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e86d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e86d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e86d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e86d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e86dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e86dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e86e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e86e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e86e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e86e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e86eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e86ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e86f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e86f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e86f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e86fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e86fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e86ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e8702a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e870560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e870820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e870ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e870da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e871060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e871320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e8715e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e8718a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e871b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e871e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e8720e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e8723a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e872660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e872920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e872be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e872ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e873160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e873420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e8736e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e8739a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e873c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e873f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e8741e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e8744a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e874760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e874a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e874ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e874fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e875260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e875520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e8757e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e875aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e875d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e876020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e8762e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e8765a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e876860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e876b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e876de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e8770a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e877360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e877620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e8778e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e877ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e877e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e878120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e8783e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e8786a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e878960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e878c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e878ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e8791a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e879460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e879720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e8799e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e879ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e879f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e87a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e87a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e87a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e87aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e87ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e87afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e87b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e87b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e87b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e87bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e87bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e87c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e87c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e87c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e87c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e87cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e87ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e87d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e87d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e87d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e87d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e87dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e87dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e87e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e87e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e87e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e87e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e87ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e87ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e87f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e87f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e87f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e87fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e87fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e87ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e880260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e880520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e8807e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e880aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e880d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e881020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e8812e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e8815a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e881860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e881b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e881de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e8820a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e882360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e882620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e8828e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e882ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e882e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e883120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e8833e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e8836a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e883960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e883c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e883ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e8841a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e884460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e884720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e8849e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e884ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e884f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e885220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e8854e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e8857a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e885a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e885d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e885fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e8862a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e886560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e886820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e886ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e886da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e887060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e887320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e8875e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e8878a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e887b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e887e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e8880e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e8883a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e888b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e888e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e8890d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e889540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e8899b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e889e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e88a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e88a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e88ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e88afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e88b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e88b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e88bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e88c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e88c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e88ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e88cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e88d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e88d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e88dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e88e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e88e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e88e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e88ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e88f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e88f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e88fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e88ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e890430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e8908a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e890d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e891180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e8915f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e891a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e891ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e892340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e8927b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e892c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e893090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e893500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e893970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e893de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e894250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e8946c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e894b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e894fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e895410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e895880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e895cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e896160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e8965d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e896a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e896eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e897320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e897790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e897c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e898070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e8984e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e898950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e898dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e899230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e8996a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e899b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e899f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e89a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e89a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e89acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e89b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e89b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e89ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e89be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e89c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e89c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e89d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e89d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e89e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e89e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e89ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e89f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e89f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e89fac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e89ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e89f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e89ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e89ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e8a01e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e8a04a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e8a0760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e8a0a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e8a0ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e8a0fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e8a1260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e8a1520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e8a1af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e8a20c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e8a26f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e8a29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e8a2c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e8a2f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e8a31f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e8a34b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e8a3770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e8a3a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e8a3cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e8a3fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e8a4270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e8a4530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e8a47f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e8a4ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e8a4d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e8a5030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e8a52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e8a55b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e8a5870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e8a5b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e8a5df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e8a60b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e8a6370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e8a6630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e8a68f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e8a6bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e8a6e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e8a7130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e8a73f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e8a76b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e8a7970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e8a7c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e8a7ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e8a81b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e8a8470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e8a8730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e8a89f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e8a8cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e8a8f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e8a9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e8a94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e8a97b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e8a9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e8a9d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e8a9ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e8aa2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e8aa570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e8aa830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e8aaaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e8aadb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e8ab070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e8ab330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e8ab5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e8ab8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e8abb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e8abe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e8ac0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e8ac3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e8ac670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e8ac930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e8acbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e8aceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e8ad170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e8ad430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e8ad6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e8ad9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e8adc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e8adf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e8ae1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e8ae4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e8ae770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e8aea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e8aecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e8aefb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e8af270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e8af530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e8af7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e8afab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e8afd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e8b0030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e8b02f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e8b05b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e8b0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e8b0b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e8b0df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e8b10b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e8b1370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e8b1630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e8b18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e8b1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e8b1e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e8b2130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e8b23f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e8b26b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e8b2970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e8b2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e8b2ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e8b31b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e8b3470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e8b3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e8b39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e8b3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e8b3f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e8b4230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e8b44f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e8b47b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e8b4a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e8b4d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e8b4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e8b52b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e8b5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e8b5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e8b5af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e8b5db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e8b6070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e8b6330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e8b65f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e8b68b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e8b6b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e8b6e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e8b70f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e8b73b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e8b7670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e8b7930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e8b7bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e8b7eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e8b8170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e8b8430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e8b86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e8b89b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e8b8c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e8b8f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e8b91f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e8b94b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e8b9770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e8b9a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e8b9cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e8b9fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e8ba270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e8ba530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e8ba7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e8baab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e8bad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e8bb030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e8bb2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e8bb5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e8bb870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e8bbb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e8bbdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e8bc0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e8bc370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e8bc630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e8bc8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e8bcbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e8bce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e8bd130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e8bd3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e8bd6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e8bd970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e8bdc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e8bdef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e8be1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e8be470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e8be730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e8be9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e8becb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e8bef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e8bf230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e8bf4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e8bf7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e8bfa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e8bfd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e8bfff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e8c02b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e8c0570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e8c0830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e8c0af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e8c0db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e8c1070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e8c1330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e8c15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e8c18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e8c1b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e8c1e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e8c20f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e8c23b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e8c2670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e8c2930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e8c2bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e8c2eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e8c3170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e8c3430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e8c36f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e8c39b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e8c3c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e8c3f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e8c4500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e8c47c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e8c4a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e8c4d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e8c5000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e8c52c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e8c5580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e8c5840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e8c5b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e8c5dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e8c6080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e8c6340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e8c6600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e8c68c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e8c6b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e8c6e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e8c7100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e8c73c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e8c7680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e8c7940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e8c7c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e8c7ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e8c8180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e8c8440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e8c8700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e8c89c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e8c8c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e8c8f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e8c9200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e8c94c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e8c9780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e8c9a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e8c9d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e8c9fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e8ca280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e8ca540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e8ca800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e8caac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e8cad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e8cb040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e8cb300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e8cb5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e8cb880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e8cbb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e8cbe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e8cc0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e8cc380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e8cc640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e8cc900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e8ccbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e8cce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e8cd140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e8cd400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e8cd6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e8cd980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e8cdc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e8cdf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e8ce1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e8ce480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e8ce740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e8cea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e8cecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e8cf0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e8cf380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e8cf640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e8cfab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e8cff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e8d0390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e8d0800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e8d0c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e8d10e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e8d1550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e8d19c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e8d2530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e8d2c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e8d3370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e8d3a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e8d3d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e8d4010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e8d4540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e8d49b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.920s
user	0m0.242s
sys	0m0.135s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.14 user         0.04 sys
```
