Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.3)
Requirement already satisfied: gguf>=0.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.15.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)
Requirement already satisfied: torch~=2.2.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: filelock in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)
Requirement already satisfied: packaging>=20.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: requests in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)
Requirement already satisfied: safetensors>=0.4.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.0)
Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: sympy in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: networkx in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: jinja2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /Users/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///Users/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (6.0.2)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /Users/ggml/mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.15.0) (4.67.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.15.0-py3-none-any.whl size=3464 sha256=2a70d5769527de1a88776f0d57cef2d72e4fe38c378b24e9c59d0872482e9918
  Stored in directory: /private/var/folders/3z/y7wjb5257l1dj_dxrng2zq0h0000gn/T/pip-ephem-wheel-cache-4dbtujzh/wheels/57/e8/ad/c90c2fcd445a1780e6969131e27698e4f239a16d34ccd95852
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.15.0
    Uninstalling gguf-0.15.0:
      Successfully uninstalled gguf-0.15.0
Successfully installed gguf-0.15.0
+ gg_run_ctest_debug
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.9s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-debug

real	0m3.187s
user	0m0.903s
sys	0m1.444s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_debug-make.log
++ nproc
+ make -j10
[  0%] Generating build details from Git
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
[  4%] Built target sha1
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Built target build_info
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Built target llama-gguf
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llava_shared
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-tokenizer-0
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-backend-ops
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-arg-parser
[ 62%] Built target test-barrier
[ 62%] Built target test-chat-template
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-embedding
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-eval-callback
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-batched
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-bench
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-cli
[ 83%] Built target llama-lookup-stats
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-passkey
[ 84%] Built target llama-perplexity
[ 84%] Built target llama-parallel
[ 84%] Built target llama-quantize
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-tts
[ 93%] Built target llama-speculative
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-export-lora
[ 95%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m4.290s
user	0m9.706s
sys	0m10.227s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_debug-ctest.log
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.28 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.32 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.91 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.91 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.97 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.90 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.19 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 253.01 sec*proc (29 tests)

Total Test time (real) = 253.03 sec

real	4m13.082s
user	8m32.989s
sys	0m7.204s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_release
+ cd /Users/ggml/work/llama.cpp
+ rm -rf build-ci-release
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_release.log
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_release-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.674s
user	0m0.915s
sys	0m1.312s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_release-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target build_info
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-tokenizer-1-spm
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-gguf
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Built target test-quantize-perf
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched-bench
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Built target llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-cli
[ 80%] Built target llama-lookup
[ 80%] Built target llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Built target llama-quantize
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-speculative
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 92%] Built target llama-tokenize
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 94%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.155s
user	0m6.587s
sys	0m10.184s
+ '[' -z ']'
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_release-ctest.log
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.81 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.46 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.70 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.39 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.69 sec*proc (29 tests)

Total Test time (real) =  54.71 sec

real	0m54.718s
user	1m16.564s
sys	0m6.437s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_embd_bge_small
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/embd_bge_small.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:14 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:14 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer.json [711396/711396] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:15 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/tokenizer_config.json [366/366] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:15 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/special_tokens_map.json [125/125] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:19 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/sentence_bert_config.json [52/52] -> "sentence_bert_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:19 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/vocab.txt [231508/231508] -> "vocab.txt" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:20 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/modules.json [349/349] -> "modules.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/ https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
+ local out=models-mnt/bge-small/
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/
+ cd models-mnt/bge-small/
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:21 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/config.json [743/743] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/bge-small/1_Pooling https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
+ local out=models-mnt/bge-small/1_Pooling
+ local url=https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/bge-small/1_Pooling
+ cd models-mnt/bge-small/1_Pooling
+ wget -nv -N https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:22 URL:https://huggingface.co/BAAI/bge-small-en-v1.5/raw/main/1_Pooling/config.json [190/190] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/bge-small
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/embd_bge_small-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.639s
user	0m0.912s
sys	0m1.275s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/embd_bge_small-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target sha256
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 48%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-gguf
[ 63%] Built target test-arg-parser
[ 63%] Built target test-chat-template
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-infill
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-embedding
[ 73%] Built target llama-infill
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-eval-callback
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Built target llama-imatrix
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-cli
[ 81%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-parallel
[ 83%] Built target llama-perplexity
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Built target llama-passkey
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.163s
user	0m6.509s
sys	0m9.953s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/bge-small --outfile ../models-mnt/bge-small/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: bge-small
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,               torch.float32 --> F16, shape = {384, 30522}
INFO:hf-to-gguf:position_embd.weight,            torch.float32 --> F32, shape = {384, 512}
INFO:hf-to-gguf:token_types.weight,              torch.float32 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.4.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.4.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.4.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.4.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.4.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.5.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.5.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.5.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.5.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.5.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.6.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.6.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.6.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.6.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.6.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.7.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.7.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.7.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.7.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.7.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.8.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.8.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.8.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.8.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.8.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_q.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_q.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_k.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_k.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_v.weight,             torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_v.bias,               torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output.weight,        torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.9.attn_output.bias,          torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.weight,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.attn_output_norm.bias,     torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.ffn_up.weight,             torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.9.ffn_up.bias,               torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.9.ffn_down.weight,           torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.9.ffn_down.bias,             torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.9.layer_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.10.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.10.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.10.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.10.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.10.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_q.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_q.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_k.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_k.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_v.weight,            torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_v.bias,              torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output.weight,       torch.float32 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.11.attn_output.bias,         torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.weight,  torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.attn_output_norm.bias,    torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.ffn_up.weight,            torch.float32 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.11.ffn_up.bias,              torch.float32 --> F32, shape = {1536}
INFO:hf-to-gguf:blk.11.ffn_down.weight,          torch.float32 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.11.ffn_down.bias,            torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.weight, torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:blk.11.layer_output_norm.bias,   torch.float32 --> F32, shape = {384}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 512
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Setting special token type unk to 100
INFO:gguf.vocab:Setting special token type sep to 102
INFO:gguf.vocab:Setting special token type pad to 0
WARNING:gguf.vocab:No handler for special token type cls with id 101 - skipping
INFO:gguf.vocab:Setting special token type mask to 103
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/bge-small/ggml-model-f16.gguf: n_tensors = 197, total_size = 66.9M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/66.9M [00:00<?, ?byte/s]Writing:  37%|███▋      | 24.8M/66.9M [00:00<00:00, 240Mbyte/s]Writing:  73%|███████▎  | 49.1M/66.9M [00:00<00:00, 230Mbyte/s]Writing: 100%|██████████| 66.9M/66.9M [00:00<00:00, 221Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/bge-small/ggml-model-f16.gguf
+ model_f16=../models-mnt/bge-small/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/bge-small/ggml-model-q8_0.gguf
+ ./bin/llama-quantize ../models-mnt/bge-small/ggml-model-f16.gguf ../models-mnt/bge-small/ggml-model-q8_0.gguf q8_0
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/bge-small/ggml-model-f16.gguf' to '../models-mnt/bge-small/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = bert
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Bge Small
llama_model_loader: - kv   3:                           general.basename str              = bge
llama_model_loader: - kv   4:                         general.size_label str              = small
llama_model_loader: - kv   5:                           bert.block_count u32              = 12
llama_model_loader: - kv   6:                        bert.context_length u32              = 512
llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  124 tensors
llama_model_loader: - type  f16:   73 tensors
[   1/ 197]                 position_embd.weight - [  384,   512,     1,     1], type =    f32, size =    0.750 MB
[   2/ 197]                    token_embd.weight - [  384, 30522,     1,     1], type =    f16, converting to q8_0 .. size =    22.35 MiB ->    11.88 MiB
[   3/ 197]                 token_embd_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   4/ 197]               token_embd_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   5/ 197]                   token_types.weight - [  384,     2,     1,     1], type =    f32, size =    0.003 MB
[   6/ 197]                    blk.0.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   7/ 197]                  blk.0.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[   8/ 197]               blk.0.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[   9/ 197]             blk.0.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  10/ 197]          blk.0.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  11/ 197]        blk.0.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  12/ 197]                    blk.0.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  13/ 197]                  blk.0.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  14/ 197]                    blk.0.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  15/ 197]                  blk.0.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  16/ 197]                  blk.0.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  17/ 197]                blk.0.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  18/ 197]                    blk.0.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  19/ 197]                  blk.0.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  20/ 197]         blk.0.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  21/ 197]       blk.0.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  22/ 197]                    blk.1.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  23/ 197]                  blk.1.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  24/ 197]               blk.1.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  25/ 197]             blk.1.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  26/ 197]          blk.1.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  27/ 197]        blk.1.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  28/ 197]                    blk.1.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  29/ 197]                  blk.1.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  30/ 197]                    blk.1.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  31/ 197]                  blk.1.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  32/ 197]                  blk.1.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  33/ 197]                blk.1.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  34/ 197]                    blk.1.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  35/ 197]                  blk.1.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  36/ 197]         blk.1.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  37/ 197]       blk.1.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  38/ 197]                    blk.2.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  39/ 197]                  blk.2.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  40/ 197]               blk.2.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  41/ 197]             blk.2.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  42/ 197]          blk.2.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  43/ 197]        blk.2.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  44/ 197]                    blk.2.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  45/ 197]                  blk.2.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  46/ 197]                    blk.2.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  47/ 197]                  blk.2.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  48/ 197]                  blk.2.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  49/ 197]                blk.2.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  50/ 197]                    blk.2.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  51/ 197]                  blk.2.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  52/ 197]         blk.2.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  53/ 197]       blk.2.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  54/ 197]                    blk.3.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  55/ 197]                  blk.3.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  56/ 197]               blk.3.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  57/ 197]             blk.3.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  58/ 197]          blk.3.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  59/ 197]        blk.3.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  60/ 197]                    blk.3.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  61/ 197]                  blk.3.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  62/ 197]                    blk.3.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  63/ 197]                  blk.3.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  64/ 197]                  blk.3.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  65/ 197]                blk.3.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  66/ 197]                    blk.3.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  67/ 197]                  blk.3.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  68/ 197]         blk.3.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  69/ 197]       blk.3.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  70/ 197]                    blk.4.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  71/ 197]                  blk.4.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  72/ 197]               blk.4.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  73/ 197]             blk.4.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  74/ 197]          blk.4.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  75/ 197]        blk.4.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  76/ 197]                    blk.4.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  77/ 197]                  blk.4.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  78/ 197]                    blk.4.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  79/ 197]                  blk.4.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  80/ 197]                  blk.4.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  81/ 197]                blk.4.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  82/ 197]                    blk.4.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  83/ 197]                  blk.4.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  84/ 197]         blk.4.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  85/ 197]       blk.4.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  86/ 197]                    blk.5.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  87/ 197]                  blk.5.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  88/ 197]               blk.5.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  89/ 197]             blk.5.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  90/ 197]          blk.5.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  91/ 197]        blk.5.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  92/ 197]                    blk.5.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  93/ 197]                  blk.5.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  94/ 197]                    blk.5.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  95/ 197]                  blk.5.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[  96/ 197]                  blk.5.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[  97/ 197]                blk.5.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[  98/ 197]                    blk.5.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[  99/ 197]                  blk.5.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 100/ 197]         blk.5.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 101/ 197]       blk.5.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 102/ 197]                    blk.6.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 103/ 197]                  blk.6.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 104/ 197]               blk.6.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 105/ 197]             blk.6.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 106/ 197]          blk.6.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 107/ 197]        blk.6.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 108/ 197]                    blk.6.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 109/ 197]                  blk.6.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 110/ 197]                    blk.6.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 111/ 197]                  blk.6.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 112/ 197]                  blk.6.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 113/ 197]                blk.6.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 114/ 197]                    blk.6.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 115/ 197]                  blk.6.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 116/ 197]         blk.6.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 117/ 197]       blk.6.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 118/ 197]                    blk.7.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 119/ 197]                  blk.7.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 120/ 197]               blk.7.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 121/ 197]             blk.7.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 122/ 197]          blk.7.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 123/ 197]        blk.7.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 124/ 197]                    blk.7.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 125/ 197]                  blk.7.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 126/ 197]                    blk.7.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 127/ 197]                  blk.7.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 128/ 197]                  blk.7.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 129/ 197]                blk.7.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 130/ 197]                    blk.7.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 131/ 197]                  blk.7.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 132/ 197]         blk.7.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 133/ 197]       blk.7.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 134/ 197]                    blk.8.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 135/ 197]                  blk.8.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 136/ 197]               blk.8.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 137/ 197]             blk.8.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 138/ 197]          blk.8.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 139/ 197]        blk.8.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 140/ 197]                    blk.8.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 141/ 197]                  blk.8.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 142/ 197]                    blk.8.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 143/ 197]                  blk.8.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 144/ 197]                  blk.8.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 145/ 197]                blk.8.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 146/ 197]                    blk.8.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 147/ 197]                  blk.8.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 148/ 197]         blk.8.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 149/ 197]       blk.8.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 150/ 197]                    blk.9.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 151/ 197]                  blk.9.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 152/ 197]               blk.9.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 153/ 197]             blk.9.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 154/ 197]          blk.9.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 155/ 197]        blk.9.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 156/ 197]                    blk.9.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 157/ 197]                  blk.9.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 158/ 197]                    blk.9.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 159/ 197]                  blk.9.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 160/ 197]                  blk.9.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 161/ 197]                blk.9.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 162/ 197]                    blk.9.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 163/ 197]                  blk.9.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 164/ 197]         blk.9.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 165/ 197]       blk.9.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 166/ 197]                   blk.10.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 167/ 197]                 blk.10.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 168/ 197]              blk.10.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 169/ 197]            blk.10.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 170/ 197]         blk.10.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 171/ 197]       blk.10.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 172/ 197]                   blk.10.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 173/ 197]                 blk.10.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 174/ 197]                   blk.10.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 175/ 197]                 blk.10.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 176/ 197]                 blk.10.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 177/ 197]               blk.10.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 178/ 197]                   blk.10.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 179/ 197]                 blk.10.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 180/ 197]        blk.10.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 181/ 197]      blk.10.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 182/ 197]                   blk.11.attn_k.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 183/ 197]                 blk.11.attn_k.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 184/ 197]              blk.11.attn_output.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 185/ 197]            blk.11.attn_output.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 186/ 197]         blk.11.attn_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 187/ 197]       blk.11.attn_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 188/ 197]                   blk.11.attn_q.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 189/ 197]                 blk.11.attn_q.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 190/ 197]                   blk.11.attn_v.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 191/ 197]                 blk.11.attn_v.weight - [  384,   384,     1,     1], type =    f16, converting to q8_0 .. size =     0.28 MiB ->     0.15 MiB
[ 192/ 197]                 blk.11.ffn_down.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 193/ 197]               blk.11.ffn_down.weight - [ 1536,   384,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 194/ 197]                   blk.11.ffn_up.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB
[ 195/ 197]                 blk.11.ffn_up.weight - [  384,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     1.12 MiB ->     0.60 MiB
[ 196/ 197]        blk.11.layer_output_norm.bias - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
[ 197/ 197]      blk.11.layer_output_norm.weight - [  384,     1,     1,     1], type =    f32, size =    0.001 MB
llama_model_quantize_impl: model size  =    63.84 MB
llama_model_quantize_impl: quant size  =    34.38 MB

main: quantize time =    81.71 ms
main:    total time =    81.71 ms
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/embd_bge_small-tg-f16.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.129 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.711 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.262 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.272 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.274 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.275 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.275 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.277 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.278 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.278 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.279 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.280 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.283 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.283 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.284 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.284 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.285 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.286 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.286 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.151 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.153 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.154 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.154 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.155 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.155 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.156 I llama_model_loader: - type  f32:  124 tensors
0.00.027.156 I llama_model_loader: - type  f16:   73 tensors
0.00.027.157 I print_info: file format = GGUF V3 (latest)
0.00.027.158 I print_info: file type   = F16
0.00.027.159 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.808 I load: special tokens cache size = 5
0.00.033.986 I load: token to piece cache size = 0.2032 MB
0.00.033.991 I print_info: arch             = bert
0.00.033.991 I print_info: vocab_only       = 0
0.00.033.991 I print_info: n_ctx_train      = 512
0.00.033.992 I print_info: n_embd           = 384
0.00.033.992 I print_info: n_layer          = 12
0.00.033.995 I print_info: n_head           = 12
0.00.033.996 I print_info: n_head_kv        = 12
0.00.033.999 I print_info: n_rot            = 32
0.00.034.000 I print_info: n_swa            = 0
0.00.034.000 I print_info: n_embd_head_k    = 32
0.00.034.000 I print_info: n_embd_head_v    = 32
0.00.034.001 I print_info: n_gqa            = 1
0.00.034.002 I print_info: n_embd_k_gqa     = 384
0.00.034.002 I print_info: n_embd_v_gqa     = 384
0.00.034.009 I print_info: f_norm_eps       = 1.0e-12
0.00.034.010 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.010 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.010 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.011 I print_info: f_logit_scale    = 0.0e+00
0.00.034.012 I print_info: n_ff             = 1536
0.00.034.012 I print_info: n_expert         = 0
0.00.034.014 I print_info: n_expert_used    = 0
0.00.034.014 I print_info: causal attn      = 0
0.00.034.014 I print_info: pooling type     = 2
0.00.034.015 I print_info: rope type        = 2
0.00.034.015 I print_info: rope scaling     = linear
0.00.034.015 I print_info: freq_base_train  = 10000.0
0.00.034.016 I print_info: freq_scale_train = 1
0.00.034.016 I print_info: n_ctx_orig_yarn  = 512
0.00.034.016 I print_info: rope_finetuned   = unknown
0.00.034.017 I print_info: ssm_d_conv       = 0
0.00.034.017 I print_info: ssm_d_inner      = 0
0.00.034.019 I print_info: ssm_d_state      = 0
0.00.034.019 I print_info: ssm_dt_rank      = 0
0.00.034.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.020 I print_info: model type       = 33M
0.00.034.020 I print_info: model params     = 33.21 M
0.00.034.020 I print_info: general.name     = Bge Small
0.00.034.021 I print_info: vocab type       = WPM
0.00.034.021 I print_info: n_vocab          = 30522
0.00.034.022 I print_info: n_merges         = 0
0.00.034.022 I print_info: BOS token        = 101 '[CLS]'
0.00.034.022 I print_info: UNK token        = 100 '[UNK]'
0.00.034.027 I print_info: SEP token        = 102 '[SEP]'
0.00.034.027 I print_info: PAD token        = 0 '[PAD]'
0.00.034.027 I print_info: MASK token       = 103 '[MASK]'
0.00.034.027 I print_info: LF token         = 0 '[PAD]'
0.00.034.028 I print_info: max token length = 21
0.00.037.133 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.134 I load_tensors: offloading output layer to GPU
0.00.037.135 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.158 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.160 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.037.399 I llama_init_from_model: n_seq_max     = 1
0.00.037.400 I llama_init_from_model: n_ctx         = 512
0.00.037.400 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.400 I llama_init_from_model: n_batch       = 2048
0.00.037.401 I llama_init_from_model: n_ubatch      = 2048
0.00.037.401 I llama_init_from_model: flash_attn    = 0
0.00.037.401 I llama_init_from_model: freq_base     = 10000.0
0.00.037.402 I llama_init_from_model: freq_scale    = 1
0.00.037.402 I ggml_metal_init: allocating
0.00.037.407 I ggml_metal_init: found device: Apple M4
0.00.037.411 I ggml_metal_init: picking default device: Apple M4
0.00.038.099 I ggml_metal_init: using embedded metal library
0.00.042.107 I ggml_metal_init: GPU name:   Apple M4
0.00.042.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.110 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.111 I ggml_metal_init: simdgroup reduction   = true
0.00.042.111 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.111 I ggml_metal_init: has residency sets    = true
0.00.042.111 I ggml_metal_init: has bfloat            = true
0.00.042.112 I ggml_metal_init: use bfloat            = true
0.00.042.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.499 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.247 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.250 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.273 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.620 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.622 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.622 I llama_init_from_model: graph nodes  = 429
0.00.056.622 I llama_init_from_model: graph splits = 2
0.00.056.624 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.624 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.474 I 
0.00.062.500 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.173 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.345 I llama_perf_context_print:        load time =      46.76 ms
0.00.068.346 I llama_perf_context_print: prompt eval time =       5.02 ms /     9 tokens (    0.56 ms per token,  1793.90 tokens per second)
0.00.068.347 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.348 I llama_perf_context_print:       total time =       5.87 ms /    10 tokens
0.00.068.476 I ggml_metal_free: deallocating

real	0m0.246s
user	0m0.049s
sys	0m0.029s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/embd_bge_small-tg-q8_0.log
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.498 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.173 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.176 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.178 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.179 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.179 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.179 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.180 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.180 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.181 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.181 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.181 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.184 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.184 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.185 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.185 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.185 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.185 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.638 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.353 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.354 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.355 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.355 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.355 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.356 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.356 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.356 I llama_model_loader: - type  f32:  124 tensors
0.00.015.357 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.357 I print_info: file format = GGUF V3 (latest)
0.00.015.358 I print_info: file type   = Q8_0
0.00.015.359 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.899 I load: special tokens cache size = 5
0.00.019.199 I load: token to piece cache size = 0.2032 MB
0.00.019.202 I print_info: arch             = bert
0.00.019.202 I print_info: vocab_only       = 0
0.00.019.202 I print_info: n_ctx_train      = 512
0.00.019.203 I print_info: n_embd           = 384
0.00.019.203 I print_info: n_layer          = 12
0.00.019.206 I print_info: n_head           = 12
0.00.019.206 I print_info: n_head_kv        = 12
0.00.019.207 I print_info: n_rot            = 32
0.00.019.207 I print_info: n_swa            = 0
0.00.019.207 I print_info: n_embd_head_k    = 32
0.00.019.207 I print_info: n_embd_head_v    = 32
0.00.019.208 I print_info: n_gqa            = 1
0.00.019.209 I print_info: n_embd_k_gqa     = 384
0.00.019.209 I print_info: n_embd_v_gqa     = 384
0.00.019.210 I print_info: f_norm_eps       = 1.0e-12
0.00.019.210 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.210 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.210 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.211 I print_info: f_logit_scale    = 0.0e+00
0.00.019.211 I print_info: n_ff             = 1536
0.00.019.211 I print_info: n_expert         = 0
0.00.019.212 I print_info: n_expert_used    = 0
0.00.019.212 I print_info: causal attn      = 0
0.00.019.212 I print_info: pooling type     = 2
0.00.019.212 I print_info: rope type        = 2
0.00.019.212 I print_info: rope scaling     = linear
0.00.019.215 I print_info: freq_base_train  = 10000.0
0.00.019.215 I print_info: freq_scale_train = 1
0.00.019.215 I print_info: n_ctx_orig_yarn  = 512
0.00.019.215 I print_info: rope_finetuned   = unknown
0.00.019.216 I print_info: ssm_d_conv       = 0
0.00.019.216 I print_info: ssm_d_inner      = 0
0.00.019.216 I print_info: ssm_d_state      = 0
0.00.019.216 I print_info: ssm_dt_rank      = 0
0.00.019.217 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.217 I print_info: model type       = 33M
0.00.019.218 I print_info: model params     = 33.21 M
0.00.019.218 I print_info: general.name     = Bge Small
0.00.019.218 I print_info: vocab type       = WPM
0.00.019.218 I print_info: n_vocab          = 30522
0.00.019.219 I print_info: n_merges         = 0
0.00.019.219 I print_info: BOS token        = 101 '[CLS]'
0.00.019.219 I print_info: UNK token        = 100 '[UNK]'
0.00.019.219 I print_info: SEP token        = 102 '[SEP]'
0.00.019.219 I print_info: PAD token        = 0 '[PAD]'
0.00.019.219 I print_info: MASK token       = 103 '[MASK]'
0.00.019.220 I print_info: LF token         = 0 '[PAD]'
0.00.019.220 I print_info: max token length = 21
0.00.021.035 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.036 I load_tensors: offloading output layer to GPU
0.00.021.036 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.043 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.043 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.191 I llama_init_from_model: n_seq_max     = 1
0.00.021.192 I llama_init_from_model: n_ctx         = 512
0.00.021.192 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.192 I llama_init_from_model: n_batch       = 2048
0.00.021.193 I llama_init_from_model: n_ubatch      = 2048
0.00.021.193 I llama_init_from_model: flash_attn    = 0
0.00.021.193 I llama_init_from_model: freq_base     = 10000.0
0.00.021.194 I llama_init_from_model: freq_scale    = 1
0.00.021.194 I ggml_metal_init: allocating
0.00.021.197 I ggml_metal_init: found device: Apple M4
0.00.021.202 I ggml_metal_init: picking default device: Apple M4
0.00.021.727 I ggml_metal_init: using embedded metal library
0.00.024.323 I ggml_metal_init: GPU name:   Apple M4
0.00.024.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.325 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.326 I ggml_metal_init: simdgroup reduction   = true
0.00.024.326 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.326 I ggml_metal_init: has residency sets    = true
0.00.024.326 I ggml_metal_init: has bfloat            = true
0.00.024.327 I ggml_metal_init: use bfloat            = true
0.00.024.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.555 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.153 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.155 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.168 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.107 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.108 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.109 I llama_init_from_model: graph nodes  = 429
0.00.036.109 I llama_init_from_model: graph splits = 2
0.00.036.110 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.170 I 
0.00.040.196 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.729 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.152 I llama_perf_context_print:        load time =      30.67 ms
0.00.045.153 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2097.41 tokens per second)
0.00.045.154 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.155 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.045.367 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_rerank_tiny
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:31 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:31 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer.json [2030772/2030772] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:32 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/tokenizer_config.json [1215/1215] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:32 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/special_tokens_map.json [280/280] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/sentence_bert_config.json:
2025-02-05 00:04:33 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/vocab.txt:
2025-02-05 00:04:34 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/modules.json:
2025-02-05 00:04:34 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/ https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
+ local out=models-mnt/rerank-tiny/
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/
+ cd models-mnt/rerank-tiny/
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:35 URL:https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/config.json [1206/1206] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/rerank-tiny/1_Pooling https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
+ local out=models-mnt/rerank-tiny/1_Pooling
+ local url=https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/rerank-tiny/1_Pooling
+ cd models-mnt/rerank-tiny/1_Pooling
+ wget -nv -N https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json
https://huggingface.co/jinaai/jina-reranker-v1-tiny-en/raw/main/1_Pooling/config.json:
2025-02-05 00:04:35 ERROR 404: Not Found.
+ cd /Users/ggml/work/llama.cpp
+ path_models=../models-mnt/rerank-tiny
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.5s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.751s
user	0m0.896s
sys	0m1.324s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-make.log
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Built target sha256
[  4%] Built target build_info
[  4%] Built target sha1
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Built target llava
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target test-c
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-log
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 51%] Built target test-chat
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-backend-ops
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-quantize-perf
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-gritlm
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-cli
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-passkey
[ 80%] Built target llama-lookup-create
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-parallel
[ 80%] Built target llama-lookup-stats
[ 81%] Generating index.html.gz.hpp
[ 81%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 81%] Built target llama-perplexity
[ 81%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-run
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-quantize
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-speculative
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-run
[ 92%] Built target llama-tts
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-cvector-generator
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-qwen2vl-cli
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.261s
user	0m6.567s
sys	0m9.842s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/rerank-tiny --outfile ../models-mnt/rerank-tiny/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: rerank-tiny
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,              torch.bfloat16 --> F16, shape = {384, 61056}
INFO:hf-to-gguf:token_types.weight,             torch.bfloat16 --> F32, shape = {384, 2}
INFO:hf-to-gguf:token_embd_norm.weight,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:token_embd_norm.bias,           torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.0.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.0.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.0.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.0.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.1.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.1.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.1.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.1.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.2.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.2.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.2.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.2.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_q.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_q.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_k.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_k.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_v.weight,            torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_v.bias,              torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output.weight,       torch.bfloat16 --> F16, shape = {384, 384}
INFO:hf-to-gguf:blk.3.attn_output.bias,         torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.weight,  torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.attn_output_norm.bias,    torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.ffn_gate.weight,          torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_up.weight,            torch.bfloat16 --> F16, shape = {384, 1536}
INFO:hf-to-gguf:blk.3.ffn_down.weight,          torch.bfloat16 --> F16, shape = {1536, 384}
INFO:hf-to-gguf:blk.3.ffn_down.bias,            torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.weight, torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:blk.3.layer_output_norm.bias,   torch.bfloat16 --> F32, shape = {384}
INFO:hf-to-gguf:cls.weight,                     torch.bfloat16 --> F16, shape = {384, 1}
INFO:hf-to-gguf:cls.bias,                       torch.bfloat16 --> F32, shape = {1}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 384
INFO:hf-to-gguf:gguf: feed forward length = 1536
INFO:hf-to-gguf:gguf: head count = 12
INFO:hf-to-gguf:gguf: layer norm epsilon = 1e-12
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 39382 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 2
INFO:gguf.vocab:Setting special token type unk to 3
INFO:gguf.vocab:Setting special token type sep to 2
INFO:gguf.vocab:Setting special token type pad to 1
WARNING:gguf.vocab:No handler for special token type cls with id 0 - skipping
INFO:gguf.vocab:Setting special token type mask to 4
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/rerank-tiny/ggml-model-f16.gguf: n_tensors = 70, total_size = 65.8M
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/65.8M [00:00<?, ?byte/s]Writing:  71%|███████   | 46.9M/65.8M [00:00<00:00, 410Mbyte/s]Writing: 100%|██████████| 65.8M/65.8M [00:00<00:00, 291Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ model_f16=../models-mnt/rerank-tiny/ggml-model-f16.gguf
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.217 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.255 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.457 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.465 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.476 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.477 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.477 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.479 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.480 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.480 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.482 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.483 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.486 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.487 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.487 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.925 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.926 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.926 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.926 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.927 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.927 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.927 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.928 I llama_model_loader: - type  f32:   40 tensors
0.00.051.928 I llama_model_loader: - type  f16:   30 tensors
0.00.051.933 I print_info: file format = GGUF V3 (latest)
0.00.051.934 I print_info: file type   = F16
0.00.051.937 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.056.330 W load: empty token at index 5
0.00.061.639 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.290 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.326 I load: special tokens cache size = 5
0.00.329.444 I load: token to piece cache size = 1.5060 MB
0.00.329.451 I print_info: arch             = jina-bert-v2
0.00.329.451 I print_info: vocab_only       = 0
0.00.329.451 I print_info: n_ctx_train      = 8192
0.00.329.452 I print_info: n_embd           = 384
0.00.329.452 I print_info: n_layer          = 4
0.00.329.457 I print_info: n_head           = 12
0.00.329.461 I print_info: n_head_kv        = 12
0.00.329.461 I print_info: n_rot            = 32
0.00.329.462 I print_info: n_swa            = 0
0.00.329.462 I print_info: n_embd_head_k    = 32
0.00.329.462 I print_info: n_embd_head_v    = 32
0.00.329.462 I print_info: n_gqa            = 1
0.00.329.463 I print_info: n_embd_k_gqa     = 384
0.00.329.463 I print_info: n_embd_v_gqa     = 384
0.00.329.465 I print_info: f_norm_eps       = 1.0e-12
0.00.329.466 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.329.466 I print_info: f_clamp_kqv      = 0.0e+00
0.00.329.467 I print_info: f_max_alibi_bias = 8.0e+00
0.00.329.467 I print_info: f_logit_scale    = 0.0e+00
0.00.329.467 I print_info: n_ff             = 1536
0.00.329.468 I print_info: n_expert         = 0
0.00.329.468 I print_info: n_expert_used    = 0
0.00.329.468 I print_info: causal attn      = 0
0.00.329.468 I print_info: pooling type     = -1
0.00.329.468 I print_info: rope type        = -1
0.00.329.470 I print_info: rope scaling     = linear
0.00.329.470 I print_info: freq_base_train  = 10000.0
0.00.329.470 I print_info: freq_scale_train = 1
0.00.329.471 I print_info: n_ctx_orig_yarn  = 8192
0.00.329.471 I print_info: rope_finetuned   = unknown
0.00.329.471 I print_info: ssm_d_conv       = 0
0.00.329.471 I print_info: ssm_d_inner      = 0
0.00.329.471 I print_info: ssm_d_state      = 0
0.00.329.471 I print_info: ssm_dt_rank      = 0
0.00.329.471 I print_info: ssm_dt_b_c_rms   = 0
0.00.329.471 I print_info: model type       = 33M
0.00.329.472 I print_info: model params     = 32.90 M
0.00.329.472 I print_info: general.name     = Jina Bert Implementation
0.00.329.473 I print_info: vocab type       = BPE
0.00.329.473 I print_info: n_vocab          = 61056
0.00.329.474 I print_info: n_merges         = 39382
0.00.329.475 I print_info: BOS token        = 0 '<s>'
0.00.329.475 I print_info: EOS token        = 2 '</s>'
0.00.329.475 I print_info: UNK token        = 3 '<unk>'
0.00.329.475 I print_info: SEP token        = 2 '</s>'
0.00.329.475 I print_info: PAD token        = 1 '<pad>'
0.00.329.476 I print_info: MASK token       = 4 '<mask>'
0.00.329.476 I print_info: EOG token        = 2 '</s>'
0.00.329.476 I print_info: max token length = 45
0.00.331.502 I load_tensors: offloading 4 repeating layers to GPU
0.00.331.504 I load_tensors: offloading output layer to GPU
0.00.331.504 I load_tensors: offloaded 5/5 layers to GPU
0.00.331.530 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.331.532 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.331.856 I llama_init_from_model: n_seq_max     = 1
0.00.331.857 I llama_init_from_model: n_ctx         = 8192
0.00.331.857 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.331.857 I llama_init_from_model: n_batch       = 2048
0.00.331.858 I llama_init_from_model: n_ubatch      = 2048
0.00.331.858 I llama_init_from_model: flash_attn    = 0
0.00.331.858 I llama_init_from_model: freq_base     = 10000.0
0.00.331.858 I llama_init_from_model: freq_scale    = 1
0.00.331.859 I ggml_metal_init: allocating
0.00.331.863 I ggml_metal_init: found device: Apple M4
0.00.331.866 I ggml_metal_init: picking default device: Apple M4
0.00.332.729 I ggml_metal_init: using embedded metal library
0.00.335.480 I ggml_metal_init: GPU name:   Apple M4
0.00.335.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.481 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.482 I ggml_metal_init: simdgroup reduction   = true
0.00.335.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.482 I ggml_metal_init: has residency sets    = true
0.00.335.482 I ggml_metal_init: has bfloat            = true
0.00.335.483 I ggml_metal_init: use bfloat            = true
0.00.335.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.345.058 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.348.085 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.348.087 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.348.108 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.354.074 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.354.075 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.354.075 I llama_init_from_model: graph nodes  = 154
0.00.354.075 I llama_init_from_model: graph splits = 2
0.00.354.077 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.354.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.455 I 
0.00.361.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.581 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.582 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.585 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.585 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.589 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.589 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.362.124 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.664 I llama_perf_context_print:        load time =     337.19 ms
0.00.365.665 I llama_perf_context_print: prompt eval time =       3.53 ms /    62 tokens (    0.06 ms per token, 17553.79 tokens per second)
0.00.365.666 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.666 I llama_perf_context_print:       total time =       4.21 ms /    63 tokens
0.00.365.864 I ggml_metal_free: deallocating

real	0m1.078s
user	0m0.335s
sys	0m0.046s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 0'
+ check_score 'rerank score 0' 'rerank score 0:    0.023' 0.00 0.05
+ qnt='rerank score 0'
++ echo 'rerank score 0:    0.023'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.023
++ echo '0.023 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.023 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 0' 0.023
+ return 0
  - rerank score 0 @ 0.023 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 1'
+ check_score 'rerank score 1' 'rerank score 1:    0.024' 0.00 0.05
+ qnt='rerank score 1'
++ echo 'rerank score 1:    0.024'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.024
++ echo '0.024 < 0.00'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.024 > 0.05'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 1' 0.024
+ return 0
  - rerank score 1 @ 0.024 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/rerank_tiny-rk-f16.log
++ grep 'rerank score 2'
+ check_score 'rerank score 2' 'rerank score 2:    0.199' 0.10 0.30
+ qnt='rerank score 2'
++ echo 'rerank score 2:    0.199'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ score=0.199
++ echo '0.199 < 0.10'
++ bc
+ '[' 0 -eq 1 ']'
++ echo '0.199 > 0.30'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' 'rerank score 2' 0.199
+ return 0
  - rerank score 2 @ 0.199 OK
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_pythia_1_4b
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b.log
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:44 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/config.json [570/570] -> "config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:45 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer.json [2113710/2113710] -> "tokenizer.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:45 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/tokenizer_config.json [396/396] -> "tokenizer_config.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json
Last-modified header missing -- time-stamps turned off.
2025-02-05 00:04:45 URL:https://huggingface.co/EleutherAI/pythia-1.4b/raw/main/special_tokens_map.json [99/99] -> "special_tokens_map.json" [1]
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/pythia/1.4B/ https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ local out=models-mnt/pythia/1.4B/
+ local url=https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/pythia/1.4B/
+ cd models-mnt/pythia/1.4B/
+ wget -nv -N https://huggingface.co/EleutherAI/pythia-1.4b/resolve/main/pytorch_model.bin
+ cd /Users/ggml/work/llama.cpp
+ gg_wget models-mnt/wikitext/ https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ local out=models-mnt/wikitext/
+ local url=https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
++ pwd
+ local cwd=/Users/ggml/work/llama.cpp
+ mkdir -p models-mnt/wikitext/
+ cd models-mnt/wikitext/
+ wget -nv -N https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip
+ cd /Users/ggml/work/llama.cpp
+ unzip -o models-mnt/wikitext/wikitext-2-raw-v1.zip -d models-mnt/wikitext/
Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ head -n 60 models-mnt/wikitext/wikitext-2-raw/wiki.test.raw
+ path_models=../models-mnt/pythia/1.4B
+ path_wiki=../models-mnt/wikitext/wikitext-2-raw
+ rm -rf build-ci-release
+ mkdir build-ci-release
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.669s
user	0m0.917s
sys	0m1.295s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-make.log
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Built target sha256
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-simple
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-arg-parser
[ 62%] Built target test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-autorelease
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Built target test-quantize-perf
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-embedding
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-infill
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-merge
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-parallel
[ 80%] Built target llama-passkey
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-cli
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Built target llama-perplexity
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Built target llama-gen-docs
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-minicpmv-cli
[ 98%] Built target llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.243s
user	0m6.563s
sys	0m10.148s
+ python3 ../convert_hf_to_gguf.py ../models-mnt/pythia/1.4B --outfile ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
INFO:hf-to-gguf:Loading model: 1.4B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Exporting model...
INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'
INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.0.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.0.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.0.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.0.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.1.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.1.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.1.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.1.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.2.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.2.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.2.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.2.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.3.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.3.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.3.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.3.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.4.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.4.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.4.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.4.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.5.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.5.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.5.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.5.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.6.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.6.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.6.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.6.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.7.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.7.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.7.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.7.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.8.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.8.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.8.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.8.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.attn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_norm.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.9.attn_qkv.bias,       torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.9.attn_output.bias,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.9.ffn_up.bias,         torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.9.ffn_down.bias,       torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.10.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.10.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.10.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.10.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.11.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.11.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.11.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.11.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.12.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.12.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.12.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.12.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.13.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.13.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.13.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.13.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.14.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.14.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.14.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.14.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.15.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.15.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.15.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.15.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.16.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.16.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.16.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.16.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.17.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.17.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.17.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.17.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.18.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.18.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.18.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.18.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.19.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.19.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.19.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.19.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.20.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.20.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.20.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.20.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.21.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.21.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.21.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.21.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.22.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.22.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.22.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.22.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.attn_norm.bias,     torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_norm.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:re-format attention.linear_qkv.weight
INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float16 --> F16, shape = {2048, 6144}
INFO:hf-to-gguf:re-format attention.linear_qkv.bias
INFO:hf-to-gguf:blk.23.attn_qkv.bias,      torch.float16 --> F32, shape = {6144}
INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}
INFO:hf-to-gguf:blk.23.attn_output.bias,   torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 8192}
INFO:hf-to-gguf:blk.23.ffn_up.bias,        torch.float16 --> F32, shape = {8192}
INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {8192, 2048}
INFO:hf-to-gguf:blk.23.ffn_down.bias,      torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output_norm.bias,          torch.float16 --> F32, shape = {2048}
INFO:hf-to-gguf:output.weight,             torch.float16 --> F16, shape = {2048, 50304}
INFO:hf-to-gguf:Set meta model
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:Set model tokenizer
INFO:gguf.vocab:Adding 50009 merge(s).
INFO:gguf.vocab:Setting special token type bos to 0
INFO:gguf.vocab:Setting special token type eos to 0
INFO:gguf.vocab:Setting special token type unk to 0
INFO:hf-to-gguf:Set model quantization version
INFO:gguf.gguf_writer:Writing the following files:
INFO:gguf.gguf_writer:../models-mnt/pythia/1.4B/ggml-model-f16.gguf: n_tensors = 292, total_size = 2.8G
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Writing:   0%|          | 0.00/2.83G [00:00<?, ?byte/s]Writing:   7%|▋         | 206M/2.83G [00:00<00:06, 397Mbyte/s]Writing:  10%|▉         | 273M/2.83G [00:00<00:06, 420Mbyte/s]Writing:  12%|█▏        | 340M/2.83G [00:00<00:05, 448Mbyte/s]Writing:  14%|█▍        | 408M/2.83G [00:00<00:05, 458Mbyte/s]Writing:  17%|█▋        | 475M/2.83G [00:01<00:04, 484Mbyte/s]Writing:  19%|█▉        | 542M/2.83G [00:01<00:04, 508Mbyte/s]Writing:  22%|██▏       | 609M/2.83G [00:01<00:04, 493Mbyte/s]Writing:  24%|██▍       | 676M/2.83G [00:01<00:04, 505Mbyte/s]Writing:  26%|██▋       | 744M/2.83G [00:01<00:04, 512Mbyte/s]Writing:  29%|██▊       | 811M/2.83G [00:01<00:04, 488Mbyte/s]Writing:  31%|███       | 878M/2.83G [00:01<00:03, 502Mbyte/s]Writing:  33%|███▎      | 937M/2.83G [00:01<00:03, 511Mbyte/s]Writing:  36%|███▌      | 1.01G/2.83G [00:02<00:03, 485Mbyte/s]Writing:  38%|███▊      | 1.08G/2.83G [00:02<00:03, 500Mbyte/s]Writing:  41%|████      | 1.15G/2.83G [00:02<00:03, 509Mbyte/s]Writing:  43%|████▎     | 1.21G/2.83G [00:02<00:03, 493Mbyte/s]Writing:  45%|████▌     | 1.28G/2.83G [00:02<00:03, 501Mbyte/s]Writing:  48%|████▊     | 1.35G/2.83G [00:02<00:02, 512Mbyte/s]Writing:  50%|████▉     | 1.42G/2.83G [00:02<00:02, 496Mbyte/s]Writing:  52%|█████▏    | 1.48G/2.83G [00:03<00:02, 518Mbyte/s]Writing:  55%|█████▍    | 1.55G/2.83G [00:03<00:02, 527Mbyte/s]Writing:  57%|█████▋    | 1.62G/2.83G [00:03<00:02, 506Mbyte/s]Writing:  59%|█████▉    | 1.68G/2.83G [00:03<00:02, 511Mbyte/s]Writing:  62%|██████▏   | 1.75G/2.83G [00:03<00:02, 519Mbyte/s]Writing:  64%|██████▍   | 1.82G/2.83G [00:03<00:02, 495Mbyte/s]Writing:  67%|██████▋   | 1.89G/2.83G [00:03<00:01, 508Mbyte/s]Writing:  69%|██████▊   | 1.94G/2.83G [00:03<00:01, 465Mbyte/s]Writing:  71%|███████▏  | 2.02G/2.83G [00:04<00:01, 446Mbyte/s]Writing:  74%|███████▎  | 2.09G/2.83G [00:04<00:01, 462Mbyte/s]Writing:  76%|███████▌  | 2.15G/2.83G [00:04<00:01, 483Mbyte/s]Writing:  78%|███████▊  | 2.22G/2.83G [00:04<00:01, 434Mbyte/s]Writing:  81%|████████  | 2.29G/2.83G [00:04<00:01, 425Mbyte/s]Writing:  83%|████████▎ | 2.35G/2.83G [00:04<00:01, 429Mbyte/s]Writing:  86%|████████▌ | 2.42G/2.83G [00:05<00:00, 421Mbyte/s]Writing:  88%|████████▊ | 2.49G/2.83G [00:05<00:00, 435Mbyte/s]Writing:  90%|█████████ | 2.55G/2.83G [00:05<00:00, 445Mbyte/s]Writing:  93%|█████████▎| 2.62G/2.83G [00:05<00:00, 432Mbyte/s]Writing: 100%|██████████| 2.83G/2.83G [00:06<00:00, 385Mbyte/s]Writing: 100%|██████████| 2.83G/2.83G [00:06<00:00, 459Mbyte/s]
INFO:hf-to-gguf:Model successfully exported to ../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_f16=../models-mnt/pythia/1.4B/ggml-model-f16.gguf
+ model_q8_0=../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf
+ model_q4_0=../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf
+ model_q4_1=../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf
+ model_q5_0=../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf
+ model_q5_1=../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf
+ model_q2_k=../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf
+ model_q3_k=../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf
+ model_q4_k=../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf
+ model_q5_k=../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf
+ model_q6_k=../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf
+ wiki_test_60=../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf q8_0
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf' as Q8_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q8_0 .. size =   196.50 MiB ->   104.39 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q8_0 .. size =    24.00 MiB ->    12.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1435.23 MB

main: quantize time =  5766.02 ms
main:    total time =  5766.02 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf q4_0
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf' as Q4_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_0 .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_0 .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   786.31 MB

main: quantize time =  3118.66 ms
main:    total time =  3118.66 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf q4_1
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf' as Q4_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_1 .. size =   196.50 MiB ->    61.41 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_1 .. size =     8.00 MiB ->     2.50 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_1 .. size =    24.00 MiB ->     7.50 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_1 .. size =    32.00 MiB ->    10.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   864.46 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf q5_0

main: quantize time =  3186.29 ms
main:    total time =  3186.29 ms
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf' as Q5_0
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_0 .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_0 .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_0 .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_0 .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   942.60 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf q5_1

main: quantize time =  3310.91 ms
main:    total time =  3310.91 ms
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf' as Q5_1
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_1 .. size =   196.50 MiB ->    73.69 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_1 .. size =     8.00 MiB ->     3.00 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_1 .. size =    24.00 MiB ->     9.00 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_1 .. size =    32.00 MiB ->    12.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1020.74 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf q2_k

main: quantize time =  4777.56 ms
main:    total time =  4777.56 ms
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf' as Q2_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q2_K .. size =   196.50 MiB ->    32.24 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q3_K .. size =     8.00 MiB ->     1.72 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q2_K .. size =    24.00 MiB ->     3.94 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q2_K .. size =    32.00 MiB ->     5.25 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   542.04 MB

main: quantize time =  5272.67 ms
main:    total time =  5272.67 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf q3_k
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf' as Q3_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q3_K .. size =   196.50 MiB ->    42.22 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q3_K .. size =    32.00 MiB ->     6.88 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   724.27 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf q4_k

main: quantize time =  6194.39 ms
main:    total time =  6194.39 ms
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf' as Q4_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q4_K .. size =   196.50 MiB ->    55.27 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q5_K .. size =    24.00 MiB ->     8.25 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =   871.81 MB

main: quantize time =  6925.78 ms
main:    total time =  6925.78 ms
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf q5_k
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf' as Q5_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q5_K .. size =   196.50 MiB ->    67.55 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1006.35 MB
+ ./bin/llama-quantize ../models-mnt/pythia/1.4B/ggml-model-f16.gguf ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf q6_k

main: quantize time =  6388.20 ms
main:    total time =  6388.20 ms
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
main: quantizing '../models-mnt/pythia/1.4B/ggml-model-f16.gguf' to '../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf' as Q6_K
llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type  f16:   98 tensors
[   1/ 292]                        output.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   2/ 292]                     output_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   3/ 292]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   4/ 292]                    token_embd.weight - [ 2048, 50304,     1,     1], type =    f16, converting to q6_K .. size =   196.50 MiB ->    80.60 MiB
[   5/ 292]                 blk.0.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   6/ 292]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   7/ 292]               blk.0.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[   8/ 292]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[   9/ 292]                  blk.0.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  10/ 292]                blk.0.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  11/ 292]                  blk.0.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  12/ 292]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  13/ 292]                  blk.0.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  14/ 292]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  15/ 292]                    blk.0.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  16/ 292]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  17/ 292]                 blk.1.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  18/ 292]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  19/ 292]               blk.1.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  20/ 292]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  21/ 292]                  blk.1.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  22/ 292]                blk.1.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  23/ 292]                  blk.1.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  24/ 292]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  25/ 292]                  blk.1.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  26/ 292]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  27/ 292]                    blk.1.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  28/ 292]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  29/ 292]                 blk.2.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  30/ 292]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  31/ 292]               blk.2.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  32/ 292]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  33/ 292]                  blk.2.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  34/ 292]                blk.2.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  35/ 292]                  blk.2.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  36/ 292]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  37/ 292]                  blk.2.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  38/ 292]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  39/ 292]                    blk.2.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  40/ 292]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  41/ 292]                 blk.3.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  42/ 292]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  43/ 292]               blk.3.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  44/ 292]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  45/ 292]                  blk.3.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  46/ 292]                blk.3.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  47/ 292]                  blk.3.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  48/ 292]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  49/ 292]                  blk.3.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  50/ 292]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  51/ 292]                    blk.3.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  52/ 292]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  53/ 292]                 blk.4.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  54/ 292]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  55/ 292]               blk.4.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  56/ 292]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  57/ 292]                  blk.4.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  58/ 292]                blk.4.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  59/ 292]                  blk.4.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  60/ 292]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  61/ 292]                  blk.4.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  62/ 292]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  63/ 292]                    blk.4.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  64/ 292]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  65/ 292]                 blk.5.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  66/ 292]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  67/ 292]               blk.5.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  68/ 292]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  69/ 292]                  blk.5.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  70/ 292]                blk.5.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  71/ 292]                  blk.5.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  72/ 292]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  73/ 292]                  blk.5.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  74/ 292]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  75/ 292]                    blk.5.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  76/ 292]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  77/ 292]                 blk.6.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  78/ 292]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  79/ 292]               blk.6.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  80/ 292]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  81/ 292]                  blk.6.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  82/ 292]                blk.6.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  83/ 292]                  blk.6.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  84/ 292]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  85/ 292]                  blk.6.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  86/ 292]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  87/ 292]                    blk.6.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[  88/ 292]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  89/ 292]                 blk.7.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  90/ 292]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  91/ 292]               blk.7.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  92/ 292]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  93/ 292]                  blk.7.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[  94/ 292]                blk.7.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[  95/ 292]                  blk.7.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  96/ 292]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[  97/ 292]                  blk.7.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  98/ 292]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[  99/ 292]                    blk.7.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 100/ 292]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 101/ 292]                 blk.8.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 102/ 292]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 103/ 292]               blk.8.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 104/ 292]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 105/ 292]                  blk.8.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 106/ 292]                blk.8.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 107/ 292]                  blk.8.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 108/ 292]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 109/ 292]                  blk.8.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 110/ 292]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 111/ 292]                    blk.8.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 112/ 292]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 113/ 292]                 blk.9.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 114/ 292]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 115/ 292]               blk.9.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 116/ 292]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 117/ 292]                  blk.9.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 118/ 292]                blk.9.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 119/ 292]                  blk.9.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 120/ 292]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 121/ 292]                  blk.9.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 122/ 292]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 123/ 292]                    blk.9.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 124/ 292]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 125/ 292]                blk.10.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 126/ 292]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 127/ 292]              blk.10.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 128/ 292]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 129/ 292]                 blk.10.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 130/ 292]               blk.10.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 131/ 292]                 blk.10.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 132/ 292]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 133/ 292]                 blk.10.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 134/ 292]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 135/ 292]                   blk.10.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 136/ 292]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 137/ 292]                blk.11.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 138/ 292]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 139/ 292]              blk.11.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 140/ 292]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 141/ 292]                 blk.11.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 142/ 292]               blk.11.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 143/ 292]                 blk.11.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 144/ 292]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 145/ 292]                 blk.11.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 146/ 292]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 147/ 292]                   blk.11.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 148/ 292]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 149/ 292]                blk.12.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 150/ 292]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 151/ 292]              blk.12.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 152/ 292]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 153/ 292]                 blk.12.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 154/ 292]               blk.12.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 155/ 292]                 blk.12.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 156/ 292]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 157/ 292]                 blk.12.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 158/ 292]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 159/ 292]                   blk.12.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 160/ 292]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 161/ 292]                blk.13.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 162/ 292]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 163/ 292]              blk.13.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 164/ 292]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 165/ 292]                 blk.13.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 166/ 292]               blk.13.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 167/ 292]                 blk.13.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 168/ 292]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 169/ 292]                 blk.13.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 170/ 292]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 171/ 292]                   blk.13.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 172/ 292]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 173/ 292]                blk.14.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 174/ 292]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 175/ 292]              blk.14.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 176/ 292]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 177/ 292]                 blk.14.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 178/ 292]               blk.14.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 179/ 292]                 blk.14.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 180/ 292]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 181/ 292]                 blk.14.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 182/ 292]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 183/ 292]                   blk.14.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 184/ 292]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 185/ 292]                blk.15.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 186/ 292]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 187/ 292]              blk.15.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 188/ 292]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 189/ 292]                 blk.15.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 190/ 292]               blk.15.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 191/ 292]                 blk.15.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 192/ 292]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 193/ 292]                 blk.15.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 194/ 292]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 195/ 292]                   blk.15.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 196/ 292]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 197/ 292]                blk.16.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 198/ 292]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 199/ 292]              blk.16.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 200/ 292]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 201/ 292]                 blk.16.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 202/ 292]               blk.16.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 203/ 292]                 blk.16.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 204/ 292]               blk.16.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 205/ 292]                 blk.16.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 206/ 292]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 207/ 292]                   blk.16.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 208/ 292]                 blk.16.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 209/ 292]                blk.17.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 210/ 292]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 211/ 292]              blk.17.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 212/ 292]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 213/ 292]                 blk.17.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 214/ 292]               blk.17.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 215/ 292]                 blk.17.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 216/ 292]               blk.17.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 217/ 292]                 blk.17.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 218/ 292]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 219/ 292]                   blk.17.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 220/ 292]                 blk.17.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 221/ 292]                blk.18.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 222/ 292]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 223/ 292]              blk.18.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 224/ 292]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 225/ 292]                 blk.18.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 226/ 292]               blk.18.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 227/ 292]                 blk.18.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 228/ 292]               blk.18.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 229/ 292]                 blk.18.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 230/ 292]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 231/ 292]                   blk.18.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 232/ 292]                 blk.18.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 233/ 292]                blk.19.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 234/ 292]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 235/ 292]              blk.19.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 236/ 292]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 237/ 292]                 blk.19.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 238/ 292]               blk.19.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 239/ 292]                 blk.19.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 240/ 292]               blk.19.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 241/ 292]                 blk.19.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 242/ 292]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 243/ 292]                   blk.19.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 244/ 292]                 blk.19.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 245/ 292]                blk.20.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 246/ 292]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 247/ 292]              blk.20.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 248/ 292]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 249/ 292]                 blk.20.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 250/ 292]               blk.20.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 251/ 292]                 blk.20.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 252/ 292]               blk.20.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 253/ 292]                 blk.20.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 254/ 292]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 255/ 292]                   blk.20.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 256/ 292]                 blk.20.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 257/ 292]                blk.21.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 258/ 292]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 259/ 292]              blk.21.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 260/ 292]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 261/ 292]                 blk.21.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 262/ 292]               blk.21.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 263/ 292]                 blk.21.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 264/ 292]               blk.21.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 265/ 292]                 blk.21.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 266/ 292]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 267/ 292]                   blk.21.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 268/ 292]                 blk.21.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 269/ 292]                blk.22.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 270/ 292]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 271/ 292]              blk.22.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 272/ 292]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 273/ 292]                 blk.22.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 274/ 292]               blk.22.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 275/ 292]                 blk.22.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 276/ 292]               blk.22.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 277/ 292]                 blk.22.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 278/ 292]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 279/ 292]                   blk.22.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 280/ 292]                 blk.22.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 281/ 292]                blk.23.attn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 282/ 292]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 283/ 292]              blk.23.attn_output.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 284/ 292]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 285/ 292]                 blk.23.attn_qkv.bias - [ 6144,     1,     1,     1], type =    f32, size =    0.023 MB
[ 286/ 292]               blk.23.attn_qkv.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB
[ 287/ 292]                 blk.23.ffn_down.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 288/ 292]               blk.23.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
[ 289/ 292]                 blk.23.ffn_norm.bias - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 290/ 292]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB
[ 291/ 292]                   blk.23.ffn_up.bias - [ 8192,     1,     1,     1], type =    f32, size =    0.031 MB
[ 292/ 292]                 blk.23.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB
llama_model_quantize_impl: model size  =  2699.45 MB
llama_model_quantize_impl: quant size  =  1108.64 MB

main: quantize time =  4476.17 ms
main:    total time =  4476.17 ms
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.243 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.423 I main: llama backend init
0.00.000.435 I main: load the model and apply lora adapter, if any
0.00.044.587 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.057.230 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.265 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.266 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.272 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.283 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.284 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.065.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.145 I llama_model_loader: - type  f32:  194 tensors
0.00.076.145 I llama_model_loader: - type  f16:   98 tensors
0.00.076.148 I print_info: file format = GGUF V3 (latest)
0.00.076.149 I print_info: file type   = all F32 (guessed)
0.00.076.151 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.090.425 I load: special tokens cache size = 25
0.00.099.258 I load: token to piece cache size = 0.2984 MB
0.00.099.262 I print_info: arch             = gptneox
0.00.099.262 I print_info: vocab_only       = 0
0.00.099.262 I print_info: n_ctx_train      = 2048
0.00.099.262 I print_info: n_embd           = 2048
0.00.099.262 I print_info: n_layer          = 24
0.00.099.265 I print_info: n_head           = 16
0.00.099.266 I print_info: n_head_kv        = 16
0.00.099.266 I print_info: n_rot            = 32
0.00.099.266 I print_info: n_swa            = 0
0.00.099.267 I print_info: n_embd_head_k    = 128
0.00.099.267 I print_info: n_embd_head_v    = 128
0.00.099.268 I print_info: n_gqa            = 1
0.00.099.269 I print_info: n_embd_k_gqa     = 2048
0.00.099.270 I print_info: n_embd_v_gqa     = 2048
0.00.099.271 I print_info: f_norm_eps       = 1.0e-05
0.00.099.271 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.099.271 I print_info: f_clamp_kqv      = 0.0e+00
0.00.099.271 I print_info: f_max_alibi_bias = 0.0e+00
0.00.099.271 I print_info: f_logit_scale    = 0.0e+00
0.00.099.272 I print_info: n_ff             = 8192
0.00.099.272 I print_info: n_expert         = 0
0.00.099.272 I print_info: n_expert_used    = 0
0.00.099.273 I print_info: causal attn      = 1
0.00.099.273 I print_info: pooling type     = 0
0.00.099.273 I print_info: rope type        = 2
0.00.099.273 I print_info: rope scaling     = linear
0.00.099.273 I print_info: freq_base_train  = 10000.0
0.00.099.274 I print_info: freq_scale_train = 1
0.00.099.274 I print_info: n_ctx_orig_yarn  = 2048
0.00.099.274 I print_info: rope_finetuned   = unknown
0.00.099.274 I print_info: ssm_d_conv       = 0
0.00.099.274 I print_info: ssm_d_inner      = 0
0.00.099.276 I print_info: ssm_d_state      = 0
0.00.099.276 I print_info: ssm_dt_rank      = 0
0.00.099.276 I print_info: ssm_dt_b_c_rms   = 0
0.00.099.277 I print_info: model type       = 1.4B
0.00.099.277 I print_info: model params     = 1.41 B
0.00.099.277 I print_info: general.name     = 1.4B
0.00.099.278 I print_info: vocab type       = BPE
0.00.099.278 I print_info: n_vocab          = 50304
0.00.099.278 I print_info: n_merges         = 50009
0.00.099.278 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.099.278 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.099.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.099.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.099.279 I print_info: LF token         = 187 'Ċ'
0.00.099.279 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.099.280 I print_info: max token length = 1024
0.00.135.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.135.048 I load_tensors: offloading output layer to GPU
0.00.135.048 I load_tensors: offloaded 25/25 layers to GPU
0.00.135.065 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.135.066 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.135.334 I llama_init_from_model: n_seq_max     = 1
0.00.135.335 I llama_init_from_model: n_ctx         = 2048
0.00.135.335 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.135.335 I llama_init_from_model: n_batch       = 2048
0.00.135.336 I llama_init_from_model: n_ubatch      = 512
0.00.135.336 I llama_init_from_model: flash_attn    = 0
0.00.135.336 I llama_init_from_model: freq_base     = 10000.0
0.00.135.337 I llama_init_from_model: freq_scale    = 1
0.00.135.337 I ggml_metal_init: allocating
0.00.135.354 I ggml_metal_init: found device: Apple M4
0.00.135.359 I ggml_metal_init: picking default device: Apple M4
0.00.135.933 I ggml_metal_init: using embedded metal library
0.00.336.429 I ggml_metal_init: GPU name:   Apple M4
0.00.336.443 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.336.444 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.336.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.336.445 I ggml_metal_init: simdgroup reduction   = true
0.00.336.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.336.446 I ggml_metal_init: has residency sets    = true
0.00.336.446 I ggml_metal_init: has bfloat            = true
0.00.336.446 I ggml_metal_init: use bfloat            = true
0.00.336.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.336.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.373.371 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.410.348 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.410.357 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.410.400 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.414.063 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.414.065 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.414.066 I llama_init_from_model: graph nodes  = 967
0.00.414.066 I llama_init_from_model: graph splits = 2
0.00.414.069 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.414.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.414.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.482.263 I main: llama threadpool init, n_threads = 4
0.00.482.305 I 
0.00.482.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.482.339 I 
0.00.482.383 I sampler seed: 1234
0.00.482.387 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.482.436 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.482.436 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.482.436 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.309.680 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.02.309.681 I llama_perf_context_print:        load time =     436.83 ms
0.02.309.682 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.95 tokens per second)
0.02.309.683 I llama_perf_context_print:        eval time =    1780.52 ms /    63 runs   (   28.26 ms per token,    35.38 tokens per second)
0.02.309.683 I llama_perf_context_print:       total time =    1828.26 ms /    70 tokens
0.02.309.878 I ggml_metal_free: deallocating

real	0m2.614s
user	0m0.142s
sys	0m0.143s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.312 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.299 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.300 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.300 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.301 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.302 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.303 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.303 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.106 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.897 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.899 I llama_model_loader: - type  f32:  194 tensors
0.00.031.899 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.900 I print_info: file format = GGUF V3 (latest)
0.00.031.901 I print_info: file type   = Q8_0
0.00.031.902 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.040.462 I load: special tokens cache size = 25
0.00.046.617 I load: token to piece cache size = 0.2984 MB
0.00.046.622 I print_info: arch             = gptneox
0.00.046.622 I print_info: vocab_only       = 0
0.00.046.622 I print_info: n_ctx_train      = 2048
0.00.046.622 I print_info: n_embd           = 2048
0.00.046.623 I print_info: n_layer          = 24
0.00.046.629 I print_info: n_head           = 16
0.00.046.631 I print_info: n_head_kv        = 16
0.00.046.631 I print_info: n_rot            = 32
0.00.046.632 I print_info: n_swa            = 0
0.00.046.632 I print_info: n_embd_head_k    = 128
0.00.046.632 I print_info: n_embd_head_v    = 128
0.00.046.633 I print_info: n_gqa            = 1
0.00.046.633 I print_info: n_embd_k_gqa     = 2048
0.00.046.634 I print_info: n_embd_v_gqa     = 2048
0.00.046.635 I print_info: f_norm_eps       = 1.0e-05
0.00.046.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.637 I print_info: f_logit_scale    = 0.0e+00
0.00.046.637 I print_info: n_ff             = 8192
0.00.046.638 I print_info: n_expert         = 0
0.00.046.638 I print_info: n_expert_used    = 0
0.00.046.638 I print_info: causal attn      = 1
0.00.046.638 I print_info: pooling type     = 0
0.00.046.638 I print_info: rope type        = 2
0.00.046.639 I print_info: rope scaling     = linear
0.00.046.639 I print_info: freq_base_train  = 10000.0
0.00.046.639 I print_info: freq_scale_train = 1
0.00.046.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.640 I print_info: rope_finetuned   = unknown
0.00.046.640 I print_info: ssm_d_conv       = 0
0.00.046.640 I print_info: ssm_d_inner      = 0
0.00.046.640 I print_info: ssm_d_state      = 0
0.00.046.641 I print_info: ssm_dt_rank      = 0
0.00.046.641 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.641 I print_info: model type       = 1.4B
0.00.046.642 I print_info: model params     = 1.41 B
0.00.046.642 I print_info: general.name     = 1.4B
0.00.046.642 I print_info: vocab type       = BPE
0.00.046.642 I print_info: n_vocab          = 50304
0.00.046.642 I print_info: n_merges         = 50009
0.00.046.643 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.643 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.643 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.643 I print_info: LF token         = 187 'Ċ'
0.00.046.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.644 I print_info: max token length = 1024
0.01.017.438 I load_tensors: offloading 24 repeating layers to GPU
0.01.017.443 I load_tensors: offloading output layer to GPU
0.01.017.444 I load_tensors: offloaded 25/25 layers to GPU
0.01.017.464 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.017.466 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.018.197 I llama_init_from_model: n_seq_max     = 1
0.01.018.199 I llama_init_from_model: n_ctx         = 2048
0.01.018.200 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.018.200 I llama_init_from_model: n_batch       = 2048
0.01.018.200 I llama_init_from_model: n_ubatch      = 512
0.01.018.201 I llama_init_from_model: flash_attn    = 0
0.01.018.202 I llama_init_from_model: freq_base     = 10000.0
0.01.018.202 I llama_init_from_model: freq_scale    = 1
0.01.018.203 I ggml_metal_init: allocating
0.01.018.220 I ggml_metal_init: found device: Apple M4
0.01.018.229 I ggml_metal_init: picking default device: Apple M4
0.01.019.453 I ggml_metal_init: using embedded metal library
0.01.025.291 I ggml_metal_init: GPU name:   Apple M4
0.01.025.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.025.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.025.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.025.296 I ggml_metal_init: simdgroup reduction   = true
0.01.025.297 I ggml_metal_init: simdgroup matrix mul. = true
0.01.025.297 I ggml_metal_init: has residency sets    = true
0.01.025.297 I ggml_metal_init: has bfloat            = true
0.01.025.297 I ggml_metal_init: use bfloat            = true
0.01.025.298 I ggml_metal_init: hasUnifiedMemory      = true
0.01.025.308 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.041.730 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.095.735 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.095.742 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.095.777 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.100.413 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.100.415 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.100.415 I llama_init_from_model: graph nodes  = 967
0.01.100.415 I llama_init_from_model: graph splits = 2
0.01.100.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.100.547 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.100.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.156.030 I main: llama threadpool init, n_threads = 4
0.01.156.071 I 
0.01.156.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.156.097 I 
0.01.156.249 I sampler seed: 1234
0.01.156.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.156.300 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.156.303 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.156.303 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.235.913 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.02.235.913 I llama_perf_context_print:        load time =    1145.01 ms
0.02.235.914 I llama_perf_context_print: prompt eval time =      49.22 ms /     7 tokens (    7.03 ms per token,   142.23 tokens per second)
0.02.235.915 I llama_perf_context_print:        eval time =    1027.55 ms /    63 runs   (   16.31 ms per token,    61.31 tokens per second)
0.02.235.915 I llama_perf_context_print:       total time =    1080.59 ms /    70 tokens
0.02.236.163 I ggml_metal_free: deallocating

real	0m2.255s
user	0m0.108s
sys	0m0.285s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.010.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.764 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.767 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.768 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.768 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.770 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.770 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.401 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.402 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.402 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.403 I llama_model_loader: - type  f32:  194 tensors
0.00.027.403 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.403 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.404 I print_info: file format = GGUF V3 (latest)
0.00.027.407 I print_info: file type   = Q4_0
0.00.027.408 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.306 I load: special tokens cache size = 25
0.00.041.311 I load: token to piece cache size = 0.2984 MB
0.00.041.315 I print_info: arch             = gptneox
0.00.041.315 I print_info: vocab_only       = 0
0.00.041.315 I print_info: n_ctx_train      = 2048
0.00.041.315 I print_info: n_embd           = 2048
0.00.041.316 I print_info: n_layer          = 24
0.00.041.320 I print_info: n_head           = 16
0.00.041.321 I print_info: n_head_kv        = 16
0.00.041.321 I print_info: n_rot            = 32
0.00.041.321 I print_info: n_swa            = 0
0.00.041.321 I print_info: n_embd_head_k    = 128
0.00.041.323 I print_info: n_embd_head_v    = 128
0.00.041.323 I print_info: n_gqa            = 1
0.00.041.324 I print_info: n_embd_k_gqa     = 2048
0.00.041.325 I print_info: n_embd_v_gqa     = 2048
0.00.041.325 I print_info: f_norm_eps       = 1.0e-05
0.00.041.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.327 I print_info: f_logit_scale    = 0.0e+00
0.00.041.327 I print_info: n_ff             = 8192
0.00.041.328 I print_info: n_expert         = 0
0.00.041.328 I print_info: n_expert_used    = 0
0.00.041.328 I print_info: causal attn      = 1
0.00.041.328 I print_info: pooling type     = 0
0.00.041.328 I print_info: rope type        = 2
0.00.041.328 I print_info: rope scaling     = linear
0.00.041.329 I print_info: freq_base_train  = 10000.0
0.00.041.329 I print_info: freq_scale_train = 1
0.00.041.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.330 I print_info: rope_finetuned   = unknown
0.00.041.330 I print_info: ssm_d_conv       = 0
0.00.041.330 I print_info: ssm_d_inner      = 0
0.00.041.330 I print_info: ssm_d_state      = 0
0.00.041.330 I print_info: ssm_dt_rank      = 0
0.00.041.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.331 I print_info: model type       = 1.4B
0.00.041.331 I print_info: model params     = 1.41 B
0.00.041.331 I print_info: general.name     = 1.4B
0.00.041.332 I print_info: vocab type       = BPE
0.00.041.334 I print_info: n_vocab          = 50304
0.00.041.334 I print_info: n_merges         = 50009
0.00.041.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.335 I print_info: LF token         = 187 'Ċ'
0.00.041.337 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.337 I print_info: max token length = 1024
0.00.566.819 I load_tensors: offloading 24 repeating layers to GPU
0.00.566.835 I load_tensors: offloading output layer to GPU
0.00.566.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.566.869 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.566.870 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.568.115 I llama_init_from_model: n_seq_max     = 1
0.00.568.119 I llama_init_from_model: n_ctx         = 2048
0.00.568.120 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.568.120 I llama_init_from_model: n_batch       = 2048
0.00.568.121 I llama_init_from_model: n_ubatch      = 512
0.00.568.121 I llama_init_from_model: flash_attn    = 0
0.00.568.123 I llama_init_from_model: freq_base     = 10000.0
0.00.568.123 I llama_init_from_model: freq_scale    = 1
0.00.568.126 I ggml_metal_init: allocating
0.00.568.199 I ggml_metal_init: found device: Apple M4
0.00.568.213 I ggml_metal_init: picking default device: Apple M4
0.00.570.069 I ggml_metal_init: using embedded metal library
0.00.576.589 I ggml_metal_init: GPU name:   Apple M4
0.00.576.594 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.576.595 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.576.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.576.597 I ggml_metal_init: simdgroup reduction   = true
0.00.576.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.576.597 I ggml_metal_init: has residency sets    = true
0.00.576.597 I ggml_metal_init: has bfloat            = true
0.00.576.598 I ggml_metal_init: use bfloat            = true
0.00.576.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.576.601 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.595.378 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.650.592 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.650.599 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.650.632 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.654.998 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.655.000 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.655.001 I llama_init_from_model: graph nodes  = 967
0.00.655.001 I llama_init_from_model: graph splits = 2
0.00.655.006 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.655.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.655.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.096 I main: llama threadpool init, n_threads = 4
0.00.712.143 I 
0.00.712.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.167 I 
0.00.712.345 I sampler seed: 1234
0.00.712.349 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.360 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.361 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.391.309 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51412.02 tokens per second)
0.01.391.310 I llama_perf_context_print:        load time =     700.40 ms
0.01.391.310 I llama_perf_context_print: prompt eval time =      49.13 ms /     7 tokens (    7.02 ms per token,   142.49 tokens per second)
0.01.391.311 I llama_perf_context_print:        eval time =     626.95 ms /    63 runs   (    9.95 ms per token,   100.49 tokens per second)
0.01.391.311 I llama_perf_context_print:       total time =     679.92 ms /    70 tokens
0.01.391.601 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.110s
sys	0m0.212s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.360 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.360 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.364 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.364 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.369 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.369 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.231 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.232 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.009 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.010 I llama_model_loader: - type  f32:  194 tensors
0.00.025.010 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.010 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.011 I print_info: file format = GGUF V3 (latest)
0.00.025.012 I print_info: file type   = Q4_1
0.00.025.013 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.775 I load: special tokens cache size = 25
0.00.038.733 I load: token to piece cache size = 0.2984 MB
0.00.038.736 I print_info: arch             = gptneox
0.00.038.736 I print_info: vocab_only       = 0
0.00.038.736 I print_info: n_ctx_train      = 2048
0.00.038.736 I print_info: n_embd           = 2048
0.00.038.736 I print_info: n_layer          = 24
0.00.038.739 I print_info: n_head           = 16
0.00.038.740 I print_info: n_head_kv        = 16
0.00.038.740 I print_info: n_rot            = 32
0.00.038.740 I print_info: n_swa            = 0
0.00.038.741 I print_info: n_embd_head_k    = 128
0.00.038.741 I print_info: n_embd_head_v    = 128
0.00.038.741 I print_info: n_gqa            = 1
0.00.038.742 I print_info: n_embd_k_gqa     = 2048
0.00.038.743 I print_info: n_embd_v_gqa     = 2048
0.00.038.745 I print_info: f_norm_eps       = 1.0e-05
0.00.038.745 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.745 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.746 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.746 I print_info: f_logit_scale    = 0.0e+00
0.00.038.747 I print_info: n_ff             = 8192
0.00.038.747 I print_info: n_expert         = 0
0.00.038.747 I print_info: n_expert_used    = 0
0.00.038.747 I print_info: causal attn      = 1
0.00.038.747 I print_info: pooling type     = 0
0.00.038.747 I print_info: rope type        = 2
0.00.038.748 I print_info: rope scaling     = linear
0.00.038.750 I print_info: freq_base_train  = 10000.0
0.00.038.750 I print_info: freq_scale_train = 1
0.00.038.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.750 I print_info: rope_finetuned   = unknown
0.00.038.750 I print_info: ssm_d_conv       = 0
0.00.038.751 I print_info: ssm_d_inner      = 0
0.00.038.751 I print_info: ssm_d_state      = 0
0.00.038.751 I print_info: ssm_dt_rank      = 0
0.00.038.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.751 I print_info: model type       = 1.4B
0.00.038.752 I print_info: model params     = 1.41 B
0.00.038.752 I print_info: general.name     = 1.4B
0.00.038.752 I print_info: vocab type       = BPE
0.00.038.752 I print_info: n_vocab          = 50304
0.00.038.752 I print_info: n_merges         = 50009
0.00.038.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.753 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.753 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.753 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.753 I print_info: LF token         = 187 'Ċ'
0.00.038.754 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.757 I print_info: max token length = 1024
0.00.623.712 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.725 I load_tensors: offloading output layer to GPU
0.00.623.726 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.754 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.623.756 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.625.038 I llama_init_from_model: n_seq_max     = 1
0.00.625.048 I llama_init_from_model: n_ctx         = 2048
0.00.625.049 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.625.049 I llama_init_from_model: n_batch       = 2048
0.00.625.050 I llama_init_from_model: n_ubatch      = 512
0.00.625.050 I llama_init_from_model: flash_attn    = 0
0.00.625.051 I llama_init_from_model: freq_base     = 10000.0
0.00.625.052 I llama_init_from_model: freq_scale    = 1
0.00.625.054 I ggml_metal_init: allocating
0.00.625.101 I ggml_metal_init: found device: Apple M4
0.00.625.111 I ggml_metal_init: picking default device: Apple M4
0.00.626.867 I ggml_metal_init: using embedded metal library
0.00.632.534 I ggml_metal_init: GPU name:   Apple M4
0.00.632.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.542 I ggml_metal_init: simdgroup reduction   = true
0.00.632.542 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.543 I ggml_metal_init: has residency sets    = true
0.00.632.543 I ggml_metal_init: has bfloat            = true
0.00.632.543 I ggml_metal_init: use bfloat            = true
0.00.632.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.554 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.525 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.965 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.973 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.005 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.448 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.450 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.450 I llama_init_from_model: graph nodes  = 967
0.00.717.451 I llama_init_from_model: graph splits = 2
0.00.717.456 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.773.117 I main: llama threadpool init, n_threads = 4
0.00.773.157 I 
0.00.773.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.773.180 I 
0.00.773.326 I sampler seed: 1234
0.00.773.332 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.773.374 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.773.377 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.773.377 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.492.698 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.492.699 I llama_perf_context_print:        load time =     763.57 ms
0.01.492.699 I llama_perf_context_print: prompt eval time =      44.15 ms /     7 tokens (    6.31 ms per token,   158.54 tokens per second)
0.01.492.700 I llama_perf_context_print:        eval time =     672.27 ms /    63 runs   (   10.67 ms per token,    93.71 tokens per second)
0.01.492.701 I llama_perf_context_print:       total time =     720.27 ms /    70 tokens
0.01.492.938 I ggml_metal_free: deallocating

real	0m1.510s
user	0m0.110s
sys	0m0.202s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.181 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.633 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.636 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.638 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.639 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.640 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.645 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.645 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.429 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.257 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.258 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.258 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.258 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.259 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.259 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.259 I llama_model_loader: - type  f32:  194 tensors
0.00.027.260 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.260 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.261 I print_info: file format = GGUF V3 (latest)
0.00.027.261 I print_info: file type   = Q5_0
0.00.027.262 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.046 I load: special tokens cache size = 25
0.00.041.028 I load: token to piece cache size = 0.2984 MB
0.00.041.031 I print_info: arch             = gptneox
0.00.041.031 I print_info: vocab_only       = 0
0.00.041.031 I print_info: n_ctx_train      = 2048
0.00.041.031 I print_info: n_embd           = 2048
0.00.041.032 I print_info: n_layer          = 24
0.00.041.034 I print_info: n_head           = 16
0.00.041.035 I print_info: n_head_kv        = 16
0.00.041.035 I print_info: n_rot            = 32
0.00.041.035 I print_info: n_swa            = 0
0.00.041.035 I print_info: n_embd_head_k    = 128
0.00.041.036 I print_info: n_embd_head_v    = 128
0.00.041.037 I print_info: n_gqa            = 1
0.00.041.038 I print_info: n_embd_k_gqa     = 2048
0.00.041.039 I print_info: n_embd_v_gqa     = 2048
0.00.041.039 I print_info: f_norm_eps       = 1.0e-05
0.00.041.040 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.040 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.042 I print_info: f_logit_scale    = 0.0e+00
0.00.041.042 I print_info: n_ff             = 8192
0.00.041.043 I print_info: n_expert         = 0
0.00.041.043 I print_info: n_expert_used    = 0
0.00.041.043 I print_info: causal attn      = 1
0.00.041.043 I print_info: pooling type     = 0
0.00.041.045 I print_info: rope type        = 2
0.00.041.046 I print_info: rope scaling     = linear
0.00.041.046 I print_info: freq_base_train  = 10000.0
0.00.041.047 I print_info: freq_scale_train = 1
0.00.041.048 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.048 I print_info: rope_finetuned   = unknown
0.00.041.048 I print_info: ssm_d_conv       = 0
0.00.041.049 I print_info: ssm_d_inner      = 0
0.00.041.049 I print_info: ssm_d_state      = 0
0.00.041.049 I print_info: ssm_dt_rank      = 0
0.00.041.049 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.049 I print_info: model type       = 1.4B
0.00.041.050 I print_info: model params     = 1.41 B
0.00.041.050 I print_info: general.name     = 1.4B
0.00.041.050 I print_info: vocab type       = BPE
0.00.041.051 I print_info: n_vocab          = 50304
0.00.041.051 I print_info: n_merges         = 50009
0.00.041.051 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.051 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.051 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.052 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.052 I print_info: LF token         = 187 'Ċ'
0.00.041.052 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.052 I print_info: max token length = 1024
0.00.651.063 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.077 I load_tensors: offloading output layer to GPU
0.00.651.078 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.114 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.651.115 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.652.682 I llama_init_from_model: n_seq_max     = 1
0.00.652.687 I llama_init_from_model: n_ctx         = 2048
0.00.652.688 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.652.688 I llama_init_from_model: n_batch       = 2048
0.00.652.688 I llama_init_from_model: n_ubatch      = 512
0.00.652.689 I llama_init_from_model: flash_attn    = 0
0.00.652.691 I llama_init_from_model: freq_base     = 10000.0
0.00.652.691 I llama_init_from_model: freq_scale    = 1
0.00.652.699 I ggml_metal_init: allocating
0.00.652.782 I ggml_metal_init: found device: Apple M4
0.00.652.796 I ggml_metal_init: picking default device: Apple M4
0.00.654.612 I ggml_metal_init: using embedded metal library
0.00.661.022 I ggml_metal_init: GPU name:   Apple M4
0.00.661.025 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.028 I ggml_metal_init: simdgroup reduction   = true
0.00.661.028 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.029 I ggml_metal_init: has residency sets    = true
0.00.661.029 I ggml_metal_init: has bfloat            = true
0.00.661.029 I ggml_metal_init: use bfloat            = true
0.00.661.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.913 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.031 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.037 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.070 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.147 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.737.149 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.737.150 I llama_init_from_model: graph nodes  = 967
0.00.737.150 I llama_init_from_model: graph splits = 2
0.00.737.155 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.737.284 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.324 I main: llama threadpool init, n_threads = 4
0.00.793.368 I 
0.00.793.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.391 I 
0.00.793.546 I sampler seed: 1234
0.00.793.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.595 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.598 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.599 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.592.954 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.592.955 I llama_perf_context_print:        load time =     781.45 ms
0.01.592.955 I llama_perf_context_print: prompt eval time =      53.30 ms /     7 tokens (    7.61 ms per token,   131.34 tokens per second)
0.01.592.956 I llama_perf_context_print:        eval time =     743.14 ms /    63 runs   (   11.80 ms per token,    84.78 tokens per second)
0.01.592.956 I llama_perf_context_print:       total time =     800.32 ms /    70 tokens
0.01.593.167 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.108s
sys	0m0.209s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.977 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.001 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.006 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.010 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.010 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.010 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.012 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.013 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.014 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.017 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.017 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.017 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.897 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.687 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.688 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.688 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.689 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.689 I llama_model_loader: - type  f32:  194 tensors
0.00.025.690 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.690 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.690 I print_info: file format = GGUF V3 (latest)
0.00.025.691 I print_info: file type   = Q5_1
0.00.025.692 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.522 I load: special tokens cache size = 25
0.00.039.268 I load: token to piece cache size = 0.2984 MB
0.00.039.270 I print_info: arch             = gptneox
0.00.039.270 I print_info: vocab_only       = 0
0.00.039.271 I print_info: n_ctx_train      = 2048
0.00.039.271 I print_info: n_embd           = 2048
0.00.039.271 I print_info: n_layer          = 24
0.00.039.273 I print_info: n_head           = 16
0.00.039.274 I print_info: n_head_kv        = 16
0.00.039.274 I print_info: n_rot            = 32
0.00.039.275 I print_info: n_swa            = 0
0.00.039.275 I print_info: n_embd_head_k    = 128
0.00.039.275 I print_info: n_embd_head_v    = 128
0.00.039.276 I print_info: n_gqa            = 1
0.00.039.276 I print_info: n_embd_k_gqa     = 2048
0.00.039.277 I print_info: n_embd_v_gqa     = 2048
0.00.039.278 I print_info: f_norm_eps       = 1.0e-05
0.00.039.278 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.278 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.278 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.279 I print_info: f_logit_scale    = 0.0e+00
0.00.039.280 I print_info: n_ff             = 8192
0.00.039.280 I print_info: n_expert         = 0
0.00.039.281 I print_info: n_expert_used    = 0
0.00.039.281 I print_info: causal attn      = 1
0.00.039.281 I print_info: pooling type     = 0
0.00.039.282 I print_info: rope type        = 2
0.00.039.284 I print_info: rope scaling     = linear
0.00.039.285 I print_info: freq_base_train  = 10000.0
0.00.039.285 I print_info: freq_scale_train = 1
0.00.039.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.285 I print_info: rope_finetuned   = unknown
0.00.039.285 I print_info: ssm_d_conv       = 0
0.00.039.286 I print_info: ssm_d_inner      = 0
0.00.039.286 I print_info: ssm_d_state      = 0
0.00.039.286 I print_info: ssm_dt_rank      = 0
0.00.039.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.286 I print_info: model type       = 1.4B
0.00.039.286 I print_info: model params     = 1.41 B
0.00.039.287 I print_info: general.name     = 1.4B
0.00.039.287 I print_info: vocab type       = BPE
0.00.039.287 I print_info: n_vocab          = 50304
0.00.039.287 I print_info: n_merges         = 50009
0.00.039.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: LF token         = 187 'Ċ'
0.00.039.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: max token length = 1024
0.00.736.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.736.932 I load_tensors: offloading output layer to GPU
0.00.736.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.736.967 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.736.968 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.738.392 I llama_init_from_model: n_seq_max     = 1
0.00.738.397 I llama_init_from_model: n_ctx         = 2048
0.00.738.398 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.738.398 I llama_init_from_model: n_batch       = 2048
0.00.738.399 I llama_init_from_model: n_ubatch      = 512
0.00.738.399 I llama_init_from_model: flash_attn    = 0
0.00.738.401 I llama_init_from_model: freq_base     = 10000.0
0.00.738.402 I llama_init_from_model: freq_scale    = 1
0.00.738.409 I ggml_metal_init: allocating
0.00.738.491 I ggml_metal_init: found device: Apple M4
0.00.738.505 I ggml_metal_init: picking default device: Apple M4
0.00.740.107 I ggml_metal_init: using embedded metal library
0.00.746.520 I ggml_metal_init: GPU name:   Apple M4
0.00.746.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.526 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.526 I ggml_metal_init: simdgroup reduction   = true
0.00.746.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.527 I ggml_metal_init: has residency sets    = true
0.00.746.527 I ggml_metal_init: has bfloat            = true
0.00.746.527 I ggml_metal_init: use bfloat            = true
0.00.746.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.764.464 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.820.522 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.820.528 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.820.563 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.824.742 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.824.744 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.824.744 I llama_init_from_model: graph nodes  = 967
0.00.824.745 I llama_init_from_model: graph splits = 2
0.00.824.752 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.824.865 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.824.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.217 I main: llama threadpool init, n_threads = 4
0.00.881.260 I 
0.00.881.278 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.881.279 I 
0.00.881.389 I sampler seed: 1234
0.00.881.394 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.881.428 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.881.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.881.431 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.731.074 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.01.731.075 I llama_perf_context_print:        load time =     871.56 ms
0.01.731.075 I llama_perf_context_print: prompt eval time =      52.18 ms /     7 tokens (    7.45 ms per token,   134.15 tokens per second)
0.01.731.076 I llama_perf_context_print:        eval time =     794.51 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.731.077 I llama_perf_context_print:       total time =     850.54 ms /    70 tokens
0.01.731.297 I ggml_metal_free: deallocating

real	0m1.749s
user	0m0.109s
sys	0m0.208s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.620 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.136 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.137 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.138 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.138 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.139 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.139 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.140 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.140 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.141 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.141 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.142 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.144 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.463 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.464 I llama_model_loader: - type  f32:  194 tensors
0.00.024.464 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.464 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.465 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.465 I print_info: file format = GGUF V3 (latest)
0.00.024.466 I print_info: file type   = Q2_K - Medium
0.00.024.467 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.290 I load: special tokens cache size = 25
0.00.038.041 I load: token to piece cache size = 0.2984 MB
0.00.038.044 I print_info: arch             = gptneox
0.00.038.044 I print_info: vocab_only       = 0
0.00.038.045 I print_info: n_ctx_train      = 2048
0.00.038.045 I print_info: n_embd           = 2048
0.00.038.045 I print_info: n_layer          = 24
0.00.038.048 I print_info: n_head           = 16
0.00.038.048 I print_info: n_head_kv        = 16
0.00.038.049 I print_info: n_rot            = 32
0.00.038.050 I print_info: n_swa            = 0
0.00.038.050 I print_info: n_embd_head_k    = 128
0.00.038.050 I print_info: n_embd_head_v    = 128
0.00.038.051 I print_info: n_gqa            = 1
0.00.038.052 I print_info: n_embd_k_gqa     = 2048
0.00.038.063 I print_info: n_embd_v_gqa     = 2048
0.00.038.064 I print_info: f_norm_eps       = 1.0e-05
0.00.038.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.067 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.067 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.067 I print_info: f_logit_scale    = 0.0e+00
0.00.038.073 I print_info: n_ff             = 8192
0.00.038.073 I print_info: n_expert         = 0
0.00.038.073 I print_info: n_expert_used    = 0
0.00.038.074 I print_info: causal attn      = 1
0.00.038.074 I print_info: pooling type     = 0
0.00.038.075 I print_info: rope type        = 2
0.00.038.075 I print_info: rope scaling     = linear
0.00.038.075 I print_info: freq_base_train  = 10000.0
0.00.038.076 I print_info: freq_scale_train = 1
0.00.038.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.076 I print_info: rope_finetuned   = unknown
0.00.038.076 I print_info: ssm_d_conv       = 0
0.00.038.076 I print_info: ssm_d_inner      = 0
0.00.038.076 I print_info: ssm_d_state      = 0
0.00.038.076 I print_info: ssm_dt_rank      = 0
0.00.038.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.077 I print_info: model type       = 1.4B
0.00.038.078 I print_info: model params     = 1.41 B
0.00.038.078 I print_info: general.name     = 1.4B
0.00.038.078 I print_info: vocab type       = BPE
0.00.038.078 I print_info: n_vocab          = 50304
0.00.038.079 I print_info: n_merges         = 50009
0.00.038.080 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.080 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.080 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.080 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.081 I print_info: LF token         = 187 'Ċ'
0.00.038.082 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.082 I print_info: max token length = 1024
0.00.381.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.381.089 I load_tensors: offloading output layer to GPU
0.00.381.090 I load_tensors: offloaded 25/25 layers to GPU
0.00.381.122 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.381.123 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.382.547 I llama_init_from_model: n_seq_max     = 1
0.00.382.553 I llama_init_from_model: n_ctx         = 2048
0.00.382.553 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.382.554 I llama_init_from_model: n_batch       = 2048
0.00.382.554 I llama_init_from_model: n_ubatch      = 512
0.00.382.555 I llama_init_from_model: flash_attn    = 0
0.00.382.557 I llama_init_from_model: freq_base     = 10000.0
0.00.382.557 I llama_init_from_model: freq_scale    = 1
0.00.382.560 I ggml_metal_init: allocating
0.00.382.664 I ggml_metal_init: found device: Apple M4
0.00.382.678 I ggml_metal_init: picking default device: Apple M4
0.00.384.534 I ggml_metal_init: using embedded metal library
0.00.390.077 I ggml_metal_init: GPU name:   Apple M4
0.00.390.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.390.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.390.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.390.095 I ggml_metal_init: simdgroup reduction   = true
0.00.390.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.390.096 I ggml_metal_init: has residency sets    = true
0.00.390.096 I ggml_metal_init: has bfloat            = true
0.00.390.097 I ggml_metal_init: use bfloat            = true
0.00.390.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.390.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.410.818 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.469.984 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.469.991 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.470.024 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.498 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.474.500 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.474.500 I llama_init_from_model: graph nodes  = 967
0.00.474.500 I llama_init_from_model: graph splits = 2
0.00.474.506 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.474.630 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.474.630 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.534.181 I main: llama threadpool init, n_threads = 4
0.00.534.225 I 
0.00.534.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.534.248 I 
0.00.534.424 I sampler seed: 1234
0.00.534.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.534.440 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.534.440 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.534.440 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.222.132 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.222.133 I llama_perf_context_print:        load time =     523.87 ms
0.01.222.134 I llama_perf_context_print: prompt eval time =      44.47 ms /     7 tokens (    6.35 ms per token,   157.42 tokens per second)
0.01.222.134 I llama_perf_context_print:        eval time =     640.28 ms /    63 runs   (   10.16 ms per token,    98.39 tokens per second)
0.01.222.134 I llama_perf_context_print:       total time =     688.64 ms /    70 tokens
0.01.222.334 I ggml_metal_free: deallocating

real	0m1.241s
user	0m0.110s
sys	0m0.174s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.758 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.293 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.298 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.303 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.303 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.304 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.304 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.304 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.305 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.305 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.309 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.309 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.901 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.901 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.901 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.902 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.902 I llama_model_loader: - type  f32:  194 tensors
0.00.024.902 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.902 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.903 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.903 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.903 I print_info: file format = GGUF V3 (latest)
0.00.024.904 I print_info: file type   = Q3_K - Medium
0.00.024.904 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.707 I load: special tokens cache size = 25
0.00.038.688 I load: token to piece cache size = 0.2984 MB
0.00.038.691 I print_info: arch             = gptneox
0.00.038.692 I print_info: vocab_only       = 0
0.00.038.692 I print_info: n_ctx_train      = 2048
0.00.038.692 I print_info: n_embd           = 2048
0.00.038.692 I print_info: n_layer          = 24
0.00.038.695 I print_info: n_head           = 16
0.00.038.696 I print_info: n_head_kv        = 16
0.00.038.696 I print_info: n_rot            = 32
0.00.038.696 I print_info: n_swa            = 0
0.00.038.698 I print_info: n_embd_head_k    = 128
0.00.038.698 I print_info: n_embd_head_v    = 128
0.00.038.699 I print_info: n_gqa            = 1
0.00.038.700 I print_info: n_embd_k_gqa     = 2048
0.00.038.700 I print_info: n_embd_v_gqa     = 2048
0.00.038.701 I print_info: f_norm_eps       = 1.0e-05
0.00.038.701 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.701 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.702 I print_info: f_logit_scale    = 0.0e+00
0.00.038.702 I print_info: n_ff             = 8192
0.00.038.702 I print_info: n_expert         = 0
0.00.038.703 I print_info: n_expert_used    = 0
0.00.038.704 I print_info: causal attn      = 1
0.00.038.705 I print_info: pooling type     = 0
0.00.038.706 I print_info: rope type        = 2
0.00.038.706 I print_info: rope scaling     = linear
0.00.038.706 I print_info: freq_base_train  = 10000.0
0.00.038.707 I print_info: freq_scale_train = 1
0.00.038.707 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.707 I print_info: rope_finetuned   = unknown
0.00.038.707 I print_info: ssm_d_conv       = 0
0.00.038.707 I print_info: ssm_d_inner      = 0
0.00.038.708 I print_info: ssm_d_state      = 0
0.00.038.708 I print_info: ssm_dt_rank      = 0
0.00.038.708 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.708 I print_info: model type       = 1.4B
0.00.038.708 I print_info: model params     = 1.41 B
0.00.038.709 I print_info: general.name     = 1.4B
0.00.038.709 I print_info: vocab type       = BPE
0.00.038.709 I print_info: n_vocab          = 50304
0.00.038.709 I print_info: n_merges         = 50009
0.00.038.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.714 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.714 I print_info: LF token         = 187 'Ċ'
0.00.038.716 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.716 I print_info: max token length = 1024
0.00.441.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.580 I load_tensors: offloading output layer to GPU
0.00.441.581 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.615 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.617 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.443.165 I llama_init_from_model: n_seq_max     = 1
0.00.443.169 I llama_init_from_model: n_ctx         = 2048
0.00.443.170 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.443.170 I llama_init_from_model: n_batch       = 2048
0.00.443.170 I llama_init_from_model: n_ubatch      = 512
0.00.443.171 I llama_init_from_model: flash_attn    = 0
0.00.443.176 I llama_init_from_model: freq_base     = 10000.0
0.00.443.177 I llama_init_from_model: freq_scale    = 1
0.00.443.194 I ggml_metal_init: allocating
0.00.443.257 I ggml_metal_init: found device: Apple M4
0.00.443.270 I ggml_metal_init: picking default device: Apple M4
0.00.445.154 I ggml_metal_init: using embedded metal library
0.00.450.703 I ggml_metal_init: GPU name:   Apple M4
0.00.450.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.720 I ggml_metal_init: simdgroup reduction   = true
0.00.450.721 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.721 I ggml_metal_init: has residency sets    = true
0.00.450.721 I ggml_metal_init: has bfloat            = true
0.00.450.722 I ggml_metal_init: use bfloat            = true
0.00.450.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.730 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.830 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.529.719 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.529.728 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.529.811 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.534.349 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.534.351 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.534.352 I llama_init_from_model: graph nodes  = 967
0.00.534.352 I llama_init_from_model: graph splits = 2
0.00.534.358 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.534.482 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.534.482 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.591.941 I main: llama threadpool init, n_threads = 4
0.00.591.986 I 
0.00.592.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.009 I 
0.00.592.182 I sampler seed: 1234
0.00.592.186 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.592.207 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.592.207 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.592.207 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.346.863 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.346.864 I llama_perf_context_print:        load time =     582.49 ms
0.01.346.864 I llama_perf_context_print: prompt eval time =      49.75 ms /     7 tokens (    7.11 ms per token,   140.71 tokens per second)
0.01.346.865 I llama_perf_context_print:        eval time =     702.05 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.346.865 I llama_perf_context_print:       total time =     755.61 ms /    70 tokens
0.01.347.102 I ggml_metal_free: deallocating

real	0m1.364s
user	0m0.111s
sys	0m0.185s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.634 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.644 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.645 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.645 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.646 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.647 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.647 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.647 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.648 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.648 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.649 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.650 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.651 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.383 I llama_model_loader: - type  f32:  194 tensors
0.00.026.383 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.385 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.385 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.385 I print_info: file format = GGUF V3 (latest)
0.00.026.386 I print_info: file type   = Q4_K - Medium
0.00.026.387 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.460 I load: special tokens cache size = 25
0.00.040.696 I load: token to piece cache size = 0.2984 MB
0.00.040.701 I print_info: arch             = gptneox
0.00.040.701 I print_info: vocab_only       = 0
0.00.040.701 I print_info: n_ctx_train      = 2048
0.00.040.702 I print_info: n_embd           = 2048
0.00.040.702 I print_info: n_layer          = 24
0.00.040.705 I print_info: n_head           = 16
0.00.040.706 I print_info: n_head_kv        = 16
0.00.040.706 I print_info: n_rot            = 32
0.00.040.706 I print_info: n_swa            = 0
0.00.040.706 I print_info: n_embd_head_k    = 128
0.00.040.706 I print_info: n_embd_head_v    = 128
0.00.040.707 I print_info: n_gqa            = 1
0.00.040.707 I print_info: n_embd_k_gqa     = 2048
0.00.040.708 I print_info: n_embd_v_gqa     = 2048
0.00.040.712 I print_info: f_norm_eps       = 1.0e-05
0.00.040.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.713 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.713 I print_info: f_logit_scale    = 0.0e+00
0.00.040.714 I print_info: n_ff             = 8192
0.00.040.715 I print_info: n_expert         = 0
0.00.040.715 I print_info: n_expert_used    = 0
0.00.040.715 I print_info: causal attn      = 1
0.00.040.716 I print_info: pooling type     = 0
0.00.040.716 I print_info: rope type        = 2
0.00.040.716 I print_info: rope scaling     = linear
0.00.040.717 I print_info: freq_base_train  = 10000.0
0.00.040.717 I print_info: freq_scale_train = 1
0.00.040.717 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.717 I print_info: rope_finetuned   = unknown
0.00.040.717 I print_info: ssm_d_conv       = 0
0.00.040.718 I print_info: ssm_d_inner      = 0
0.00.040.718 I print_info: ssm_d_state      = 0
0.00.040.718 I print_info: ssm_dt_rank      = 0
0.00.040.718 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.718 I print_info: model type       = 1.4B
0.00.040.719 I print_info: model params     = 1.41 B
0.00.040.719 I print_info: general.name     = 1.4B
0.00.040.719 I print_info: vocab type       = BPE
0.00.040.719 I print_info: n_vocab          = 50304
0.00.040.721 I print_info: n_merges         = 50009
0.00.040.721 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.721 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: LF token         = 187 'Ċ'
0.00.040.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: max token length = 1024
0.00.514.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.514.321 I load_tensors: offloading output layer to GPU
0.00.514.321 I load_tensors: offloaded 25/25 layers to GPU
0.00.514.345 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.514.345 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.515.109 I llama_init_from_model: n_seq_max     = 1
0.00.515.113 I llama_init_from_model: n_ctx         = 2048
0.00.515.113 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.515.114 I llama_init_from_model: n_batch       = 2048
0.00.515.114 I llama_init_from_model: n_ubatch      = 512
0.00.515.114 I llama_init_from_model: flash_attn    = 0
0.00.515.115 I llama_init_from_model: freq_base     = 10000.0
0.00.515.116 I llama_init_from_model: freq_scale    = 1
0.00.515.120 I ggml_metal_init: allocating
0.00.515.180 I ggml_metal_init: found device: Apple M4
0.00.515.192 I ggml_metal_init: picking default device: Apple M4
0.00.516.312 I ggml_metal_init: using embedded metal library
0.00.520.464 I ggml_metal_init: GPU name:   Apple M4
0.00.520.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.520.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.520.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.520.472 I ggml_metal_init: simdgroup reduction   = true
0.00.520.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.520.472 I ggml_metal_init: has residency sets    = true
0.00.520.473 I ggml_metal_init: has bfloat            = true
0.00.520.473 I ggml_metal_init: use bfloat            = true
0.00.520.474 I ggml_metal_init: hasUnifiedMemory      = true
0.00.520.477 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.786 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.567.267 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.567.274 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.567.308 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.572.035 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.572.037 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.572.037 I llama_init_from_model: graph nodes  = 967
0.00.572.037 I llama_init_from_model: graph splits = 2
0.00.572.044 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.572.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.572.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.350 I main: llama threadpool init, n_threads = 4
0.00.629.394 I 
0.00.629.418 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.418 I 
0.00.629.567 I sampler seed: 1234
0.00.629.572 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.583 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.583 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.583 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.400.963 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49339.82 tokens per second)
0.01.400.963 I llama_perf_context_print:        load time =     619.02 ms
0.01.400.964 I llama_perf_context_print: prompt eval time =      57.62 ms /     7 tokens (    8.23 ms per token,   121.49 tokens per second)
0.01.400.965 I llama_perf_context_print:        eval time =     710.76 ms /    63 runs   (   11.28 ms per token,    88.64 tokens per second)
0.01.400.965 I llama_perf_context_print:       total time =     772.31 ms /    70 tokens
0.01.401.187 I ggml_metal_free: deallocating

real	0m1.417s
user	0m0.104s
sys	0m0.141s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.302 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.695 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.707 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.709 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.710 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.710 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.711 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.711 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.720 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.419 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.392 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.054 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.055 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.056 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.056 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.057 I llama_model_loader: - type  f32:  194 tensors
0.00.027.057 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.057 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.058 I print_info: file format = GGUF V3 (latest)
0.00.027.058 I print_info: file type   = Q5_K - Medium
0.00.027.059 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.191 I load: special tokens cache size = 25
0.00.041.187 I load: token to piece cache size = 0.2984 MB
0.00.041.190 I print_info: arch             = gptneox
0.00.041.190 I print_info: vocab_only       = 0
0.00.041.190 I print_info: n_ctx_train      = 2048
0.00.041.191 I print_info: n_embd           = 2048
0.00.041.191 I print_info: n_layer          = 24
0.00.041.193 I print_info: n_head           = 16
0.00.041.194 I print_info: n_head_kv        = 16
0.00.041.194 I print_info: n_rot            = 32
0.00.041.194 I print_info: n_swa            = 0
0.00.041.194 I print_info: n_embd_head_k    = 128
0.00.041.195 I print_info: n_embd_head_v    = 128
0.00.041.195 I print_info: n_gqa            = 1
0.00.041.196 I print_info: n_embd_k_gqa     = 2048
0.00.041.197 I print_info: n_embd_v_gqa     = 2048
0.00.041.198 I print_info: f_norm_eps       = 1.0e-05
0.00.041.198 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.198 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.198 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.202 I print_info: f_logit_scale    = 0.0e+00
0.00.041.203 I print_info: n_ff             = 8192
0.00.041.203 I print_info: n_expert         = 0
0.00.041.204 I print_info: n_expert_used    = 0
0.00.041.204 I print_info: causal attn      = 1
0.00.041.204 I print_info: pooling type     = 0
0.00.041.205 I print_info: rope type        = 2
0.00.041.206 I print_info: rope scaling     = linear
0.00.041.206 I print_info: freq_base_train  = 10000.0
0.00.041.206 I print_info: freq_scale_train = 1
0.00.041.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.207 I print_info: rope_finetuned   = unknown
0.00.041.207 I print_info: ssm_d_conv       = 0
0.00.041.207 I print_info: ssm_d_inner      = 0
0.00.041.207 I print_info: ssm_d_state      = 0
0.00.041.207 I print_info: ssm_dt_rank      = 0
0.00.041.207 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.208 I print_info: model type       = 1.4B
0.00.041.208 I print_info: model params     = 1.41 B
0.00.041.208 I print_info: general.name     = 1.4B
0.00.041.209 I print_info: vocab type       = BPE
0.00.041.209 I print_info: n_vocab          = 50304
0.00.041.209 I print_info: n_merges         = 50009
0.00.041.209 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.213 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.213 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.213 I print_info: LF token         = 187 'Ċ'
0.00.041.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.213 I print_info: max token length = 1024
0.00.602.563 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.577 I load_tensors: offloading output layer to GPU
0.00.602.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.612 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.602.614 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.604.197 I llama_init_from_model: n_seq_max     = 1
0.00.604.200 I llama_init_from_model: n_ctx         = 2048
0.00.604.201 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.604.201 I llama_init_from_model: n_batch       = 2048
0.00.604.202 I llama_init_from_model: n_ubatch      = 512
0.00.604.202 I llama_init_from_model: flash_attn    = 0
0.00.604.203 I llama_init_from_model: freq_base     = 10000.0
0.00.604.203 I llama_init_from_model: freq_scale    = 1
0.00.604.208 I ggml_metal_init: allocating
0.00.604.219 I ggml_metal_init: found device: Apple M4
0.00.604.230 I ggml_metal_init: picking default device: Apple M4
0.00.605.785 I ggml_metal_init: using embedded metal library
0.00.612.110 I ggml_metal_init: GPU name:   Apple M4
0.00.612.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.116 I ggml_metal_init: simdgroup reduction   = true
0.00.612.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.116 I ggml_metal_init: has residency sets    = true
0.00.612.117 I ggml_metal_init: has bfloat            = true
0.00.612.117 I ggml_metal_init: use bfloat            = true
0.00.612.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.119 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.018 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.684.340 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.684.345 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.684.380 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.620 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.621 I llama_init_from_model: graph nodes  = 967
0.00.688.621 I llama_init_from_model: graph splits = 2
0.00.688.626 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.995 I main: llama threadpool init, n_threads = 4
0.00.750.037 I 
0.00.750.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.060 I 
0.00.750.214 I sampler seed: 1234
0.00.750.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.259 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.263 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.264 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.599.374 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.599.375 I llama_perf_context_print:        load time =     738.00 ms
0.01.599.375 I llama_perf_context_print: prompt eval time =      51.48 ms /     7 tokens (    7.35 ms per token,   135.98 tokens per second)
0.01.599.376 I llama_perf_context_print:        eval time =     794.71 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.599.376 I llama_perf_context_print:       total time =     850.07 ms /    70 tokens
0.01.599.637 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.108s
sys	0m0.216s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.403 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.411 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.413 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.413 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.413 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.939 I llama_model_loader: - type  f32:  194 tensors
0.00.023.939 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.940 I print_info: file format = GGUF V3 (latest)
0.00.023.940 I print_info: file type   = Q6_K
0.00.023.941 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.747 I load: special tokens cache size = 25
0.00.037.521 I load: token to piece cache size = 0.2984 MB
0.00.037.524 I print_info: arch             = gptneox
0.00.037.524 I print_info: vocab_only       = 0
0.00.037.524 I print_info: n_ctx_train      = 2048
0.00.037.525 I print_info: n_embd           = 2048
0.00.037.525 I print_info: n_layer          = 24
0.00.037.527 I print_info: n_head           = 16
0.00.037.528 I print_info: n_head_kv        = 16
0.00.037.528 I print_info: n_rot            = 32
0.00.037.530 I print_info: n_swa            = 0
0.00.037.530 I print_info: n_embd_head_k    = 128
0.00.037.530 I print_info: n_embd_head_v    = 128
0.00.037.532 I print_info: n_gqa            = 1
0.00.037.533 I print_info: n_embd_k_gqa     = 2048
0.00.037.534 I print_info: n_embd_v_gqa     = 2048
0.00.037.534 I print_info: f_norm_eps       = 1.0e-05
0.00.037.535 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.536 I print_info: f_logit_scale    = 0.0e+00
0.00.037.537 I print_info: n_ff             = 8192
0.00.037.537 I print_info: n_expert         = 0
0.00.037.543 I print_info: n_expert_used    = 0
0.00.037.544 I print_info: causal attn      = 1
0.00.037.544 I print_info: pooling type     = 0
0.00.037.544 I print_info: rope type        = 2
0.00.037.544 I print_info: rope scaling     = linear
0.00.037.545 I print_info: freq_base_train  = 10000.0
0.00.037.545 I print_info: freq_scale_train = 1
0.00.037.546 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.546 I print_info: rope_finetuned   = unknown
0.00.037.546 I print_info: ssm_d_conv       = 0
0.00.037.547 I print_info: ssm_d_inner      = 0
0.00.037.547 I print_info: ssm_d_state      = 0
0.00.037.547 I print_info: ssm_dt_rank      = 0
0.00.037.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.548 I print_info: model type       = 1.4B
0.00.037.548 I print_info: model params     = 1.41 B
0.00.037.548 I print_info: general.name     = 1.4B
0.00.037.549 I print_info: vocab type       = BPE
0.00.037.549 I print_info: n_vocab          = 50304
0.00.037.549 I print_info: n_merges         = 50009
0.00.037.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.550 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.550 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.550 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.550 I print_info: LF token         = 187 'Ċ'
0.00.037.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.551 I print_info: max token length = 1024
0.00.644.414 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.418 I load_tensors: offloading output layer to GPU
0.00.644.418 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.445 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.644.447 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.645.937 I llama_init_from_model: n_seq_max     = 1
0.00.645.939 I llama_init_from_model: n_ctx         = 2048
0.00.645.946 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.955 I llama_init_from_model: n_batch       = 2048
0.00.645.955 I llama_init_from_model: n_ubatch      = 512
0.00.645.956 I llama_init_from_model: flash_attn    = 0
0.00.645.957 I llama_init_from_model: freq_base     = 10000.0
0.00.645.958 I llama_init_from_model: freq_scale    = 1
0.00.645.960 I ggml_metal_init: allocating
0.00.646.019 I ggml_metal_init: found device: Apple M4
0.00.646.038 I ggml_metal_init: picking default device: Apple M4
0.00.647.545 I ggml_metal_init: using embedded metal library
0.00.653.260 I ggml_metal_init: GPU name:   Apple M4
0.00.653.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.266 I ggml_metal_init: simdgroup reduction   = true
0.00.653.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.266 I ggml_metal_init: has residency sets    = true
0.00.653.267 I ggml_metal_init: has bfloat            = true
0.00.653.267 I ggml_metal_init: use bfloat            = true
0.00.653.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.316 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.184 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.725.192 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.227 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.476 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.478 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.478 I llama_init_from_model: graph nodes  = 967
0.00.729.478 I llama_init_from_model: graph splits = 2
0.00.729.484 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.604 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.921 I main: llama threadpool init, n_threads = 4
0.00.796.963 I 
0.00.796.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.983 I 
0.00.797.153 I sampler seed: 1234
0.00.797.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.204 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.207 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.207 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.685.155 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47619.05 tokens per second)
0.01.685.156 I llama_perf_context_print:        load time =     787.31 ms
0.01.685.156 I llama_perf_context_print: prompt eval time =      54.35 ms /     7 tokens (    7.76 ms per token,   128.80 tokens per second)
0.01.685.157 I llama_perf_context_print:        eval time =     830.82 ms /    63 runs   (   13.19 ms per token,    75.83 tokens per second)
0.01.685.157 I llama_perf_context_print:       total time =     888.94 ms /    70 tokens
0.01.685.403 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.107s
sys	0m0.222s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.564 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.332 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.930 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.941 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.947 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.129 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.131 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.132 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.132 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.132 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.133 I llama_model_loader: - type  f32:  194 tensors
0.00.053.133 I llama_model_loader: - type  f16:   98 tensors
0.00.053.134 I print_info: file format = GGUF V3 (latest)
0.00.053.135 I print_info: file type   = all F32 (guessed)
0.00.053.136 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.613 I load: special tokens cache size = 25
0.00.072.403 I load: token to piece cache size = 0.2984 MB
0.00.072.406 I print_info: arch             = gptneox
0.00.072.407 I print_info: vocab_only       = 0
0.00.072.407 I print_info: n_ctx_train      = 2048
0.00.072.407 I print_info: n_embd           = 2048
0.00.072.407 I print_info: n_layer          = 24
0.00.072.410 I print_info: n_head           = 16
0.00.072.411 I print_info: n_head_kv        = 16
0.00.072.411 I print_info: n_rot            = 32
0.00.072.412 I print_info: n_swa            = 0
0.00.072.412 I print_info: n_embd_head_k    = 128
0.00.072.412 I print_info: n_embd_head_v    = 128
0.00.072.413 I print_info: n_gqa            = 1
0.00.072.413 I print_info: n_embd_k_gqa     = 2048
0.00.072.414 I print_info: n_embd_v_gqa     = 2048
0.00.072.415 I print_info: f_norm_eps       = 1.0e-05
0.00.072.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.416 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.416 I print_info: f_logit_scale    = 0.0e+00
0.00.072.416 I print_info: n_ff             = 8192
0.00.072.417 I print_info: n_expert         = 0
0.00.072.417 I print_info: n_expert_used    = 0
0.00.072.417 I print_info: causal attn      = 1
0.00.072.417 I print_info: pooling type     = 0
0.00.072.417 I print_info: rope type        = 2
0.00.072.418 I print_info: rope scaling     = linear
0.00.072.418 I print_info: freq_base_train  = 10000.0
0.00.072.418 I print_info: freq_scale_train = 1
0.00.072.419 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.419 I print_info: rope_finetuned   = unknown
0.00.072.419 I print_info: ssm_d_conv       = 0
0.00.072.419 I print_info: ssm_d_inner      = 0
0.00.072.419 I print_info: ssm_d_state      = 0
0.00.072.419 I print_info: ssm_dt_rank      = 0
0.00.072.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.420 I print_info: model type       = 1.4B
0.00.072.423 I print_info: model params     = 1.41 B
0.00.072.423 I print_info: general.name     = 1.4B
0.00.072.423 I print_info: vocab type       = BPE
0.00.072.424 I print_info: n_vocab          = 50304
0.00.072.424 I print_info: n_merges         = 50009
0.00.072.424 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.424 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.424 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.426 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.426 I print_info: LF token         = 187 'Ċ'
0.00.072.426 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.427 I print_info: max token length = 1024
0.01.272.712 I load_tensors: offloading 24 repeating layers to GPU
0.01.272.721 I load_tensors: offloading output layer to GPU
0.01.272.722 I load_tensors: offloaded 25/25 layers to GPU
0.01.272.750 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.272.752 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.273.790 I llama_init_from_model: n_seq_max     = 1
0.01.273.791 I llama_init_from_model: n_ctx         = 128
0.01.273.791 I llama_init_from_model: n_ctx_per_seq = 128
0.01.273.791 I llama_init_from_model: n_batch       = 128
0.01.273.791 I llama_init_from_model: n_ubatch      = 128
0.01.273.792 I llama_init_from_model: flash_attn    = 0
0.01.273.792 I llama_init_from_model: freq_base     = 10000.0
0.01.273.792 I llama_init_from_model: freq_scale    = 1
0.01.273.793 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.273.793 I ggml_metal_init: allocating
0.01.273.826 I ggml_metal_init: found device: Apple M4
0.01.273.833 I ggml_metal_init: picking default device: Apple M4
0.01.274.834 I ggml_metal_init: using embedded metal library
0.01.278.697 I ggml_metal_init: GPU name:   Apple M4
0.01.278.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.278.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.278.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.278.700 I ggml_metal_init: simdgroup reduction   = true
0.01.278.700 I ggml_metal_init: simdgroup matrix mul. = true
0.01.278.700 I ggml_metal_init: has residency sets    = true
0.01.278.701 I ggml_metal_init: has bfloat            = true
0.01.278.701 I ggml_metal_init: use bfloat            = true
0.01.278.701 I ggml_metal_init: hasUnifiedMemory      = true
0.01.278.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.289.045 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.290.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.290.717 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.290.742 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.292.443 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.292.445 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.292.445 I llama_init_from_model: graph nodes  = 967
0.01.292.445 I llama_init_from_model: graph splits = 2
0.01.292.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.292.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.326.961 I 
0.01.326.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.327.000 I perplexity: tokenizing the input ..
0.01.332.032 I perplexity: tokenization took 5.03 ms
0.01.332.037 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.463.845 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.465.451 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.465.490 I llama_perf_context_print:        load time =    1304.62 ms
0.01.465.491 I llama_perf_context_print: prompt eval time =     131.54 ms /   128 tokens (    1.03 ms per token,   973.07 tokens per second)
0.01.465.491 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.465.492 I llama_perf_context_print:       total time =     138.53 ms /   129 tokens
0.01.465.885 I ggml_metal_free: deallocating

real	0m1.660s
user	0m0.097s
sys	0m0.230s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.456 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.726 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.727 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.731 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.732 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.735 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.735 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.523 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.291 I llama_model_loader: - type  f32:  194 tensors
0.00.025.292 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.292 I print_info: file format = GGUF V3 (latest)
0.00.025.293 I print_info: file type   = Q8_0
0.00.025.294 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.663 I load: special tokens cache size = 25
0.00.039.731 I load: token to piece cache size = 0.2984 MB
0.00.039.736 I print_info: arch             = gptneox
0.00.039.736 I print_info: vocab_only       = 0
0.00.039.736 I print_info: n_ctx_train      = 2048
0.00.039.736 I print_info: n_embd           = 2048
0.00.039.737 I print_info: n_layer          = 24
0.00.039.741 I print_info: n_head           = 16
0.00.039.744 I print_info: n_head_kv        = 16
0.00.039.745 I print_info: n_rot            = 32
0.00.039.745 I print_info: n_swa            = 0
0.00.039.745 I print_info: n_embd_head_k    = 128
0.00.039.745 I print_info: n_embd_head_v    = 128
0.00.039.746 I print_info: n_gqa            = 1
0.00.039.746 I print_info: n_embd_k_gqa     = 2048
0.00.039.747 I print_info: n_embd_v_gqa     = 2048
0.00.039.748 I print_info: f_norm_eps       = 1.0e-05
0.00.039.748 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.748 I print_info: f_logit_scale    = 0.0e+00
0.00.039.749 I print_info: n_ff             = 8192
0.00.039.749 I print_info: n_expert         = 0
0.00.039.749 I print_info: n_expert_used    = 0
0.00.039.749 I print_info: causal attn      = 1
0.00.039.749 I print_info: pooling type     = 0
0.00.039.750 I print_info: rope type        = 2
0.00.039.750 I print_info: rope scaling     = linear
0.00.039.750 I print_info: freq_base_train  = 10000.0
0.00.039.750 I print_info: freq_scale_train = 1
0.00.039.751 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.751 I print_info: rope_finetuned   = unknown
0.00.039.751 I print_info: ssm_d_conv       = 0
0.00.039.754 I print_info: ssm_d_inner      = 0
0.00.039.754 I print_info: ssm_d_state      = 0
0.00.039.754 I print_info: ssm_dt_rank      = 0
0.00.039.754 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.754 I print_info: model type       = 1.4B
0.00.039.754 I print_info: model params     = 1.41 B
0.00.039.754 I print_info: general.name     = 1.4B
0.00.039.755 I print_info: vocab type       = BPE
0.00.039.755 I print_info: n_vocab          = 50304
0.00.039.755 I print_info: n_merges         = 50009
0.00.039.755 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: LF token         = 187 'Ċ'
0.00.039.756 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.756 I print_info: max token length = 1024
0.00.833.382 I load_tensors: offloading 24 repeating layers to GPU
0.00.833.386 I load_tensors: offloading output layer to GPU
0.00.833.387 I load_tensors: offloaded 25/25 layers to GPU
0.00.833.411 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.833.413 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.834.767 I llama_init_from_model: n_seq_max     = 1
0.00.834.769 I llama_init_from_model: n_ctx         = 128
0.00.834.770 I llama_init_from_model: n_ctx_per_seq = 128
0.00.834.770 I llama_init_from_model: n_batch       = 128
0.00.834.774 I llama_init_from_model: n_ubatch      = 128
0.00.834.775 I llama_init_from_model: flash_attn    = 0
0.00.834.775 I llama_init_from_model: freq_base     = 10000.0
0.00.834.780 I llama_init_from_model: freq_scale    = 1
0.00.834.781 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.834.782 I ggml_metal_init: allocating
0.00.834.848 I ggml_metal_init: found device: Apple M4
0.00.834.858 I ggml_metal_init: picking default device: Apple M4
0.00.836.281 I ggml_metal_init: using embedded metal library
0.00.841.700 I ggml_metal_init: GPU name:   Apple M4
0.00.841.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.841.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.841.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.841.705 I ggml_metal_init: simdgroup reduction   = true
0.00.841.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.841.706 I ggml_metal_init: has residency sets    = true
0.00.841.706 I ggml_metal_init: has bfloat            = true
0.00.841.706 I ggml_metal_init: use bfloat            = true
0.00.841.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.841.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.857.102 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.860.396 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.860.405 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.860.459 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.863.494 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.863.495 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.863.496 I llama_init_from_model: graph nodes  = 967
0.00.863.496 I llama_init_from_model: graph splits = 2
0.00.863.499 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.863.499 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.890.851 I 
0.00.890.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.890.911 I perplexity: tokenizing the input ..
0.00.897.119 I perplexity: tokenization took 6.205 ms
0.00.897.125 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.035.171 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.036.712 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.036.732 I llama_perf_context_print:        load time =     881.39 ms
0.01.036.733 I llama_perf_context_print: prompt eval time =     137.64 ms /   128 tokens (    1.08 ms per token,   929.96 tokens per second)
0.01.036.733 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.036.734 I llama_perf_context_print:       total time =     145.88 ms /   129 tokens
0.01.037.127 I ggml_metal_free: deallocating

real	0m1.053s
user	0m0.077s
sys	0m0.183s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.106 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.111 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.112 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.112 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.113 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.115 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.115 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.115 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.039 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.890 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.890 I llama_model_loader: - type  f32:  194 tensors
0.00.025.891 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.891 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.891 I print_info: file format = GGUF V3 (latest)
0.00.025.892 I print_info: file type   = Q4_0
0.00.025.893 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.158 I load: special tokens cache size = 25
0.00.039.919 I load: token to piece cache size = 0.2984 MB
0.00.039.922 I print_info: arch             = gptneox
0.00.039.922 I print_info: vocab_only       = 0
0.00.039.923 I print_info: n_ctx_train      = 2048
0.00.039.923 I print_info: n_embd           = 2048
0.00.039.923 I print_info: n_layer          = 24
0.00.039.927 I print_info: n_head           = 16
0.00.039.928 I print_info: n_head_kv        = 16
0.00.039.928 I print_info: n_rot            = 32
0.00.039.928 I print_info: n_swa            = 0
0.00.039.928 I print_info: n_embd_head_k    = 128
0.00.039.929 I print_info: n_embd_head_v    = 128
0.00.039.931 I print_info: n_gqa            = 1
0.00.039.932 I print_info: n_embd_k_gqa     = 2048
0.00.039.932 I print_info: n_embd_v_gqa     = 2048
0.00.039.933 I print_info: f_norm_eps       = 1.0e-05
0.00.039.933 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.934 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.934 I print_info: f_logit_scale    = 0.0e+00
0.00.039.935 I print_info: n_ff             = 8192
0.00.039.937 I print_info: n_expert         = 0
0.00.039.937 I print_info: n_expert_used    = 0
0.00.039.937 I print_info: causal attn      = 1
0.00.039.937 I print_info: pooling type     = 0
0.00.039.937 I print_info: rope type        = 2
0.00.039.937 I print_info: rope scaling     = linear
0.00.039.938 I print_info: freq_base_train  = 10000.0
0.00.039.938 I print_info: freq_scale_train = 1
0.00.039.938 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.938 I print_info: rope_finetuned   = unknown
0.00.039.939 I print_info: ssm_d_conv       = 0
0.00.039.939 I print_info: ssm_d_inner      = 0
0.00.039.939 I print_info: ssm_d_state      = 0
0.00.039.939 I print_info: ssm_dt_rank      = 0
0.00.039.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.939 I print_info: model type       = 1.4B
0.00.039.944 I print_info: model params     = 1.41 B
0.00.039.944 I print_info: general.name     = 1.4B
0.00.039.944 I print_info: vocab type       = BPE
0.00.039.944 I print_info: n_vocab          = 50304
0.00.039.945 I print_info: n_merges         = 50009
0.00.039.945 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: LF token         = 187 'Ċ'
0.00.039.946 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.946 I print_info: max token length = 1024
0.00.540.229 I load_tensors: offloading 24 repeating layers to GPU
0.00.540.243 I load_tensors: offloading output layer to GPU
0.00.540.244 I load_tensors: offloaded 25/25 layers to GPU
0.00.540.279 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.540.280 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.541.841 I llama_init_from_model: n_seq_max     = 1
0.00.541.845 I llama_init_from_model: n_ctx         = 128
0.00.541.845 I llama_init_from_model: n_ctx_per_seq = 128
0.00.541.848 I llama_init_from_model: n_batch       = 128
0.00.541.849 I llama_init_from_model: n_ubatch      = 128
0.00.541.849 I llama_init_from_model: flash_attn    = 0
0.00.541.851 I llama_init_from_model: freq_base     = 10000.0
0.00.541.852 I llama_init_from_model: freq_scale    = 1
0.00.541.852 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.541.855 I ggml_metal_init: allocating
0.00.541.947 I ggml_metal_init: found device: Apple M4
0.00.541.961 I ggml_metal_init: picking default device: Apple M4
0.00.543.873 I ggml_metal_init: using embedded metal library
0.00.549.760 I ggml_metal_init: GPU name:   Apple M4
0.00.549.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.766 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.767 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.768 I ggml_metal_init: simdgroup reduction   = true
0.00.549.768 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.768 I ggml_metal_init: has residency sets    = true
0.00.549.769 I ggml_metal_init: has bfloat            = true
0.00.549.769 I ggml_metal_init: use bfloat            = true
0.00.549.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.568.294 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.571.709 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.571.716 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.571.777 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.574.817 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.574.819 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.574.819 I llama_init_from_model: graph nodes  = 967
0.00.574.820 I llama_init_from_model: graph splits = 2
0.00.574.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.574.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.469 I 
0.00.602.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.555 I perplexity: tokenizing the input ..
0.00.609.606 I perplexity: tokenization took 7.049 ms
0.00.609.617 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.363 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.743.887 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.743.910 I llama_perf_context_print:        load time =     592.60 ms
0.00.743.911 I llama_perf_context_print: prompt eval time =     132.35 ms /   128 tokens (    1.03 ms per token,   967.16 tokens per second)
0.00.743.912 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.912 I llama_perf_context_print:       total time =     141.45 ms /   129 tokens
0.00.744.279 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.080s
sys	0m0.119s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.020 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.300 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.302 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.303 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.303 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.303 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.303 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.304 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.306 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.243 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.122 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.122 I llama_model_loader: - type  f32:  194 tensors
0.00.025.122 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.123 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.124 I print_info: file format = GGUF V3 (latest)
0.00.025.124 I print_info: file type   = Q4_1
0.00.025.126 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.417 I load: special tokens cache size = 25
0.00.039.184 I load: token to piece cache size = 0.2984 MB
0.00.039.188 I print_info: arch             = gptneox
0.00.039.188 I print_info: vocab_only       = 0
0.00.039.188 I print_info: n_ctx_train      = 2048
0.00.039.188 I print_info: n_embd           = 2048
0.00.039.188 I print_info: n_layer          = 24
0.00.039.192 I print_info: n_head           = 16
0.00.039.193 I print_info: n_head_kv        = 16
0.00.039.193 I print_info: n_rot            = 32
0.00.039.193 I print_info: n_swa            = 0
0.00.039.194 I print_info: n_embd_head_k    = 128
0.00.039.194 I print_info: n_embd_head_v    = 128
0.00.039.194 I print_info: n_gqa            = 1
0.00.039.195 I print_info: n_embd_k_gqa     = 2048
0.00.039.197 I print_info: n_embd_v_gqa     = 2048
0.00.039.198 I print_info: f_norm_eps       = 1.0e-05
0.00.039.198 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.199 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.199 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.201 I print_info: f_logit_scale    = 0.0e+00
0.00.039.201 I print_info: n_ff             = 8192
0.00.039.202 I print_info: n_expert         = 0
0.00.039.202 I print_info: n_expert_used    = 0
0.00.039.202 I print_info: causal attn      = 1
0.00.039.202 I print_info: pooling type     = 0
0.00.039.202 I print_info: rope type        = 2
0.00.039.204 I print_info: rope scaling     = linear
0.00.039.204 I print_info: freq_base_train  = 10000.0
0.00.039.204 I print_info: freq_scale_train = 1
0.00.039.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.205 I print_info: rope_finetuned   = unknown
0.00.039.205 I print_info: ssm_d_conv       = 0
0.00.039.205 I print_info: ssm_d_inner      = 0
0.00.039.205 I print_info: ssm_d_state      = 0
0.00.039.205 I print_info: ssm_dt_rank      = 0
0.00.039.205 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.206 I print_info: model type       = 1.4B
0.00.039.206 I print_info: model params     = 1.41 B
0.00.039.206 I print_info: general.name     = 1.4B
0.00.039.210 I print_info: vocab type       = BPE
0.00.039.210 I print_info: n_vocab          = 50304
0.00.039.211 I print_info: n_merges         = 50009
0.00.039.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.211 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.211 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.212 I print_info: LF token         = 187 'Ċ'
0.00.039.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.212 I print_info: max token length = 1024
0.00.628.838 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.850 I load_tensors: offloading output layer to GPU
0.00.628.851 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.879 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.880 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.630.341 I llama_init_from_model: n_seq_max     = 1
0.00.630.351 I llama_init_from_model: n_ctx         = 128
0.00.630.352 I llama_init_from_model: n_ctx_per_seq = 128
0.00.630.352 I llama_init_from_model: n_batch       = 128
0.00.630.353 I llama_init_from_model: n_ubatch      = 128
0.00.630.354 I llama_init_from_model: flash_attn    = 0
0.00.630.355 I llama_init_from_model: freq_base     = 10000.0
0.00.630.355 I llama_init_from_model: freq_scale    = 1
0.00.630.356 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.360 I ggml_metal_init: allocating
0.00.630.410 I ggml_metal_init: found device: Apple M4
0.00.630.424 I ggml_metal_init: picking default device: Apple M4
0.00.632.067 I ggml_metal_init: using embedded metal library
0.00.638.097 I ggml_metal_init: GPU name:   Apple M4
0.00.638.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.106 I ggml_metal_init: simdgroup reduction   = true
0.00.638.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.106 I ggml_metal_init: has residency sets    = true
0.00.638.107 I ggml_metal_init: has bfloat            = true
0.00.638.107 I ggml_metal_init: use bfloat            = true
0.00.638.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.110 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.741 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.660.390 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.660.397 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.660.449 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.716 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.718 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.718 I llama_init_from_model: graph nodes  = 967
0.00.663.718 I llama_init_from_model: graph splits = 2
0.00.663.722 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.487 I 
0.00.693.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.572 I perplexity: tokenizing the input ..
0.00.700.134 I perplexity: tokenization took 6.559 ms
0.00.700.141 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.174 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.835.708 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.835.733 I llama_perf_context_print:        load time =     684.46 ms
0.00.835.734 I llama_perf_context_print: prompt eval time =     133.10 ms /   128 tokens (    1.04 ms per token,   961.67 tokens per second)
0.00.835.735 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.735 I llama_perf_context_print:       total time =     142.25 ms /   129 tokens
0.00.836.183 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.080s
sys	0m0.129s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.819 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.954 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.700 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.701 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.702 I llama_model_loader: - type  f32:  194 tensors
0.00.024.702 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.702 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.703 I print_info: file format = GGUF V3 (latest)
0.00.024.704 I print_info: file type   = Q5_0
0.00.024.705 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.626 I load: special tokens cache size = 25
0.00.038.425 I load: token to piece cache size = 0.2984 MB
0.00.038.428 I print_info: arch             = gptneox
0.00.038.428 I print_info: vocab_only       = 0
0.00.038.429 I print_info: n_ctx_train      = 2048
0.00.038.429 I print_info: n_embd           = 2048
0.00.038.429 I print_info: n_layer          = 24
0.00.038.433 I print_info: n_head           = 16
0.00.038.435 I print_info: n_head_kv        = 16
0.00.038.435 I print_info: n_rot            = 32
0.00.038.435 I print_info: n_swa            = 0
0.00.038.435 I print_info: n_embd_head_k    = 128
0.00.038.436 I print_info: n_embd_head_v    = 128
0.00.038.436 I print_info: n_gqa            = 1
0.00.038.437 I print_info: n_embd_k_gqa     = 2048
0.00.038.438 I print_info: n_embd_v_gqa     = 2048
0.00.038.438 I print_info: f_norm_eps       = 1.0e-05
0.00.038.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.443 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.444 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.444 I print_info: f_logit_scale    = 0.0e+00
0.00.038.445 I print_info: n_ff             = 8192
0.00.038.445 I print_info: n_expert         = 0
0.00.038.445 I print_info: n_expert_used    = 0
0.00.038.445 I print_info: causal attn      = 1
0.00.038.445 I print_info: pooling type     = 0
0.00.038.445 I print_info: rope type        = 2
0.00.038.446 I print_info: rope scaling     = linear
0.00.038.446 I print_info: freq_base_train  = 10000.0
0.00.038.446 I print_info: freq_scale_train = 1
0.00.038.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.447 I print_info: rope_finetuned   = unknown
0.00.038.447 I print_info: ssm_d_conv       = 0
0.00.038.447 I print_info: ssm_d_inner      = 0
0.00.038.447 I print_info: ssm_d_state      = 0
0.00.038.447 I print_info: ssm_dt_rank      = 0
0.00.038.447 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.448 I print_info: model type       = 1.4B
0.00.038.448 I print_info: model params     = 1.41 B
0.00.038.448 I print_info: general.name     = 1.4B
0.00.038.449 I print_info: vocab type       = BPE
0.00.038.449 I print_info: n_vocab          = 50304
0.00.038.449 I print_info: n_merges         = 50009
0.00.038.449 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.449 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.454 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.456 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.456 I print_info: LF token         = 187 'Ċ'
0.00.038.457 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.457 I print_info: max token length = 1024
0.00.647.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.886 I load_tensors: offloading output layer to GPU
0.00.647.887 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.919 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.647.920 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.649.334 I llama_init_from_model: n_seq_max     = 1
0.00.649.338 I llama_init_from_model: n_ctx         = 128
0.00.649.339 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.340 I llama_init_from_model: n_batch       = 128
0.00.649.340 I llama_init_from_model: n_ubatch      = 128
0.00.649.341 I llama_init_from_model: flash_attn    = 0
0.00.649.343 I llama_init_from_model: freq_base     = 10000.0
0.00.649.343 I llama_init_from_model: freq_scale    = 1
0.00.649.344 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.351 I ggml_metal_init: allocating
0.00.649.413 I ggml_metal_init: found device: Apple M4
0.00.649.427 I ggml_metal_init: picking default device: Apple M4
0.00.651.259 I ggml_metal_init: using embedded metal library
0.00.657.992 I ggml_metal_init: GPU name:   Apple M4
0.00.657.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.999 I ggml_metal_init: simdgroup reduction   = true
0.00.657.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.999 I ggml_metal_init: has residency sets    = true
0.00.658.000 I ggml_metal_init: has bfloat            = true
0.00.658.000 I ggml_metal_init: use bfloat            = true
0.00.658.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.002 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.395 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.843 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.846 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.915 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.078 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.682.080 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.682.081 I llama_init_from_model: graph nodes  = 967
0.00.682.081 I llama_init_from_model: graph splits = 2
0.00.682.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.682.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.391 I 
0.00.712.476 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.485 I perplexity: tokenizing the input ..
0.00.719.861 I perplexity: tokenization took 7.373 ms
0.00.719.876 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.867.540 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.869.069 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.869.093 I llama_perf_context_print:        load time =     703.56 ms
0.00.869.094 I llama_perf_context_print: prompt eval time =     146.71 ms /   128 tokens (    1.15 ms per token,   872.46 tokens per second)
0.00.869.094 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.869.095 I llama_perf_context_print:       total time =     156.71 ms /   129 tokens
0.00.869.450 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.080s
sys	0m0.138s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.142 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.143 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.143 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.145 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.146 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.146 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.147 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.148 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.150 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.150 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.931 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.777 I llama_model_loader: - type  f32:  194 tensors
0.00.025.777 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.778 I print_info: file format = GGUF V3 (latest)
0.00.025.779 I print_info: file type   = Q5_1
0.00.025.780 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.829 I load: special tokens cache size = 25
0.00.039.800 I load: token to piece cache size = 0.2984 MB
0.00.039.803 I print_info: arch             = gptneox
0.00.039.803 I print_info: vocab_only       = 0
0.00.039.804 I print_info: n_ctx_train      = 2048
0.00.039.804 I print_info: n_embd           = 2048
0.00.039.804 I print_info: n_layer          = 24
0.00.039.807 I print_info: n_head           = 16
0.00.039.807 I print_info: n_head_kv        = 16
0.00.039.808 I print_info: n_rot            = 32
0.00.039.808 I print_info: n_swa            = 0
0.00.039.808 I print_info: n_embd_head_k    = 128
0.00.039.808 I print_info: n_embd_head_v    = 128
0.00.039.809 I print_info: n_gqa            = 1
0.00.039.810 I print_info: n_embd_k_gqa     = 2048
0.00.039.811 I print_info: n_embd_v_gqa     = 2048
0.00.039.812 I print_info: f_norm_eps       = 1.0e-05
0.00.039.812 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.812 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.812 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.812 I print_info: f_logit_scale    = 0.0e+00
0.00.039.813 I print_info: n_ff             = 8192
0.00.039.813 I print_info: n_expert         = 0
0.00.039.813 I print_info: n_expert_used    = 0
0.00.039.814 I print_info: causal attn      = 1
0.00.039.814 I print_info: pooling type     = 0
0.00.039.814 I print_info: rope type        = 2
0.00.039.816 I print_info: rope scaling     = linear
0.00.039.816 I print_info: freq_base_train  = 10000.0
0.00.039.817 I print_info: freq_scale_train = 1
0.00.039.817 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.817 I print_info: rope_finetuned   = unknown
0.00.039.817 I print_info: ssm_d_conv       = 0
0.00.039.817 I print_info: ssm_d_inner      = 0
0.00.039.818 I print_info: ssm_d_state      = 0
0.00.039.818 I print_info: ssm_dt_rank      = 0
0.00.039.818 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.818 I print_info: model type       = 1.4B
0.00.039.818 I print_info: model params     = 1.41 B
0.00.039.819 I print_info: general.name     = 1.4B
0.00.039.819 I print_info: vocab type       = BPE
0.00.039.823 I print_info: n_vocab          = 50304
0.00.039.823 I print_info: n_merges         = 50009
0.00.039.824 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.824 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.824 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.824 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.824 I print_info: LF token         = 187 'Ċ'
0.00.039.825 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.825 I print_info: max token length = 1024
0.00.734.927 I load_tensors: offloading 24 repeating layers to GPU
0.00.734.932 I load_tensors: offloading output layer to GPU
0.00.734.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.734.959 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.734.960 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.736.409 I llama_init_from_model: n_seq_max     = 1
0.00.736.411 I llama_init_from_model: n_ctx         = 128
0.00.736.412 I llama_init_from_model: n_ctx_per_seq = 128
0.00.736.412 I llama_init_from_model: n_batch       = 128
0.00.736.412 I llama_init_from_model: n_ubatch      = 128
0.00.736.413 I llama_init_from_model: flash_attn    = 0
0.00.736.414 I llama_init_from_model: freq_base     = 10000.0
0.00.736.414 I llama_init_from_model: freq_scale    = 1
0.00.736.415 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.736.416 I ggml_metal_init: allocating
0.00.736.432 I ggml_metal_init: found device: Apple M4
0.00.736.442 I ggml_metal_init: picking default device: Apple M4
0.00.737.823 I ggml_metal_init: using embedded metal library
0.00.743.965 I ggml_metal_init: GPU name:   Apple M4
0.00.743.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.743.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.743.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.743.971 I ggml_metal_init: simdgroup reduction   = true
0.00.743.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.743.972 I ggml_metal_init: has residency sets    = true
0.00.743.972 I ggml_metal_init: has bfloat            = true
0.00.743.972 I ggml_metal_init: use bfloat            = true
0.00.743.973 I ggml_metal_init: hasUnifiedMemory      = true
0.00.743.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.761.102 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.631 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.764.637 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.764.687 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.767.815 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.767.817 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.767.818 I llama_init_from_model: graph nodes  = 967
0.00.767.819 I llama_init_from_model: graph splits = 2
0.00.767.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.767.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.296 I 
0.00.797.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.384 I perplexity: tokenizing the input ..
0.00.804.641 I perplexity: tokenization took 7.253 ms
0.00.804.651 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.939.833 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.941.624 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.941.648 I llama_perf_context_print:        load time =     787.43 ms
0.00.941.649 I llama_perf_context_print: prompt eval time =     134.63 ms /   128 tokens (    1.05 ms per token,   950.73 tokens per second)
0.00.941.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.941.650 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.942.022 I ggml_metal_free: deallocating

real	0m0.958s
user	0m0.079s
sys	0m0.140s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q2_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.171 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.963 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.971 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.972 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.974 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.974 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.759 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.807 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.642 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.644 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.646 I llama_model_loader: - type  f32:  194 tensors
0.00.024.646 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.647 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.648 I print_info: file format = GGUF V3 (latest)
0.00.024.648 I print_info: file type   = Q2_K - Medium
0.00.024.649 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.598 I load: special tokens cache size = 25
0.00.038.595 I load: token to piece cache size = 0.2984 MB
0.00.038.599 I print_info: arch             = gptneox
0.00.038.600 I print_info: vocab_only       = 0
0.00.038.600 I print_info: n_ctx_train      = 2048
0.00.038.600 I print_info: n_embd           = 2048
0.00.038.600 I print_info: n_layer          = 24
0.00.038.604 I print_info: n_head           = 16
0.00.038.605 I print_info: n_head_kv        = 16
0.00.038.605 I print_info: n_rot            = 32
0.00.038.606 I print_info: n_swa            = 0
0.00.038.606 I print_info: n_embd_head_k    = 128
0.00.038.606 I print_info: n_embd_head_v    = 128
0.00.038.607 I print_info: n_gqa            = 1
0.00.038.607 I print_info: n_embd_k_gqa     = 2048
0.00.038.608 I print_info: n_embd_v_gqa     = 2048
0.00.038.609 I print_info: f_norm_eps       = 1.0e-05
0.00.038.609 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.609 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.609 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.609 I print_info: f_logit_scale    = 0.0e+00
0.00.038.610 I print_info: n_ff             = 8192
0.00.038.610 I print_info: n_expert         = 0
0.00.038.610 I print_info: n_expert_used    = 0
0.00.038.610 I print_info: causal attn      = 1
0.00.038.611 I print_info: pooling type     = 0
0.00.038.611 I print_info: rope type        = 2
0.00.038.611 I print_info: rope scaling     = linear
0.00.038.611 I print_info: freq_base_train  = 10000.0
0.00.038.612 I print_info: freq_scale_train = 1
0.00.038.612 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.612 I print_info: rope_finetuned   = unknown
0.00.038.613 I print_info: ssm_d_conv       = 0
0.00.038.613 I print_info: ssm_d_inner      = 0
0.00.038.613 I print_info: ssm_d_state      = 0
0.00.038.613 I print_info: ssm_dt_rank      = 0
0.00.038.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.613 I print_info: model type       = 1.4B
0.00.038.614 I print_info: model params     = 1.41 B
0.00.038.614 I print_info: general.name     = 1.4B
0.00.038.614 I print_info: vocab type       = BPE
0.00.038.615 I print_info: n_vocab          = 50304
0.00.038.615 I print_info: n_merges         = 50009
0.00.038.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.616 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.619 I print_info: LF token         = 187 'Ċ'
0.00.038.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.619 I print_info: max token length = 1024
0.00.390.184 I load_tensors: offloading 24 repeating layers to GPU
0.00.390.192 I load_tensors: offloading output layer to GPU
0.00.390.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.390.225 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.390.230 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.391.711 I llama_init_from_model: n_seq_max     = 1
0.00.391.715 I llama_init_from_model: n_ctx         = 128
0.00.391.716 I llama_init_from_model: n_ctx_per_seq = 128
0.00.391.716 I llama_init_from_model: n_batch       = 128
0.00.391.717 I llama_init_from_model: n_ubatch      = 128
0.00.391.717 I llama_init_from_model: flash_attn    = 0
0.00.391.720 I llama_init_from_model: freq_base     = 10000.0
0.00.391.720 I llama_init_from_model: freq_scale    = 1
0.00.391.721 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.391.723 I ggml_metal_init: allocating
0.00.391.800 I ggml_metal_init: found device: Apple M4
0.00.391.813 I ggml_metal_init: picking default device: Apple M4
0.00.393.619 I ggml_metal_init: using embedded metal library
0.00.399.101 I ggml_metal_init: GPU name:   Apple M4
0.00.399.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.399.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.399.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.399.115 I ggml_metal_init: simdgroup reduction   = true
0.00.399.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.399.116 I ggml_metal_init: has residency sets    = true
0.00.399.116 I ggml_metal_init: has bfloat            = true
0.00.399.116 I ggml_metal_init: use bfloat            = true
0.00.399.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.399.126 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.420.331 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.424.180 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.424.184 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.424.228 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.427.780 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.427.782 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.427.783 I llama_init_from_model: graph nodes  = 967
0.00.427.783 I llama_init_from_model: graph splits = 2
0.00.427.787 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.427.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.456.979 I 
0.00.457.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.457.057 I perplexity: tokenizing the input ..
0.00.464.107 I perplexity: tokenization took 7.046 ms
0.00.464.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.597.754 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.599.529 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.599.546 I llama_perf_context_print:        load time =     447.80 ms
0.00.599.547 I llama_perf_context_print: prompt eval time =     132.74 ms /   128 tokens (    1.04 ms per token,   964.30 tokens per second)
0.00.599.547 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.599.548 I llama_perf_context_print:       total time =     142.57 ms /   129 tokens
0.00.599.883 I ggml_metal_free: deallocating

real	0m0.614s
user	0m0.083s
sys	0m0.106s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.352 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.810 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.811 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.818 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.823 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.850 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.953 I llama_model_loader: - type  f32:  194 tensors
0.00.025.953 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.954 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.954 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.954 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.955 I print_info: file format = GGUF V3 (latest)
0.00.025.955 I print_info: file type   = Q3_K - Medium
0.00.025.959 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.253 I load: special tokens cache size = 25
0.00.040.308 I load: token to piece cache size = 0.2984 MB
0.00.040.312 I print_info: arch             = gptneox
0.00.040.313 I print_info: vocab_only       = 0
0.00.040.313 I print_info: n_ctx_train      = 2048
0.00.040.313 I print_info: n_embd           = 2048
0.00.040.313 I print_info: n_layer          = 24
0.00.040.317 I print_info: n_head           = 16
0.00.040.318 I print_info: n_head_kv        = 16
0.00.040.318 I print_info: n_rot            = 32
0.00.040.318 I print_info: n_swa            = 0
0.00.040.318 I print_info: n_embd_head_k    = 128
0.00.040.319 I print_info: n_embd_head_v    = 128
0.00.040.319 I print_info: n_gqa            = 1
0.00.040.320 I print_info: n_embd_k_gqa     = 2048
0.00.040.321 I print_info: n_embd_v_gqa     = 2048
0.00.040.321 I print_info: f_norm_eps       = 1.0e-05
0.00.040.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.322 I print_info: f_logit_scale    = 0.0e+00
0.00.040.323 I print_info: n_ff             = 8192
0.00.040.323 I print_info: n_expert         = 0
0.00.040.323 I print_info: n_expert_used    = 0
0.00.040.323 I print_info: causal attn      = 1
0.00.040.324 I print_info: pooling type     = 0
0.00.040.325 I print_info: rope type        = 2
0.00.040.326 I print_info: rope scaling     = linear
0.00.040.326 I print_info: freq_base_train  = 10000.0
0.00.040.326 I print_info: freq_scale_train = 1
0.00.040.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.327 I print_info: rope_finetuned   = unknown
0.00.040.327 I print_info: ssm_d_conv       = 0
0.00.040.330 I print_info: ssm_d_inner      = 0
0.00.040.330 I print_info: ssm_d_state      = 0
0.00.040.330 I print_info: ssm_dt_rank      = 0
0.00.040.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.331 I print_info: model type       = 1.4B
0.00.040.331 I print_info: model params     = 1.41 B
0.00.040.331 I print_info: general.name     = 1.4B
0.00.040.332 I print_info: vocab type       = BPE
0.00.040.332 I print_info: n_vocab          = 50304
0.00.040.332 I print_info: n_merges         = 50009
0.00.040.332 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.332 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.333 I print_info: LF token         = 187 'Ċ'
0.00.040.333 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.335 I print_info: max token length = 1024
0.00.528.963 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.968 I load_tensors: offloading output layer to GPU
0.00.528.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.992 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.528.994 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.529.749 I llama_init_from_model: n_seq_max     = 1
0.00.529.754 I llama_init_from_model: n_ctx         = 128
0.00.529.755 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.755 I llama_init_from_model: n_batch       = 128
0.00.529.755 I llama_init_from_model: n_ubatch      = 128
0.00.529.756 I llama_init_from_model: flash_attn    = 0
0.00.529.757 I llama_init_from_model: freq_base     = 10000.0
0.00.529.757 I llama_init_from_model: freq_scale    = 1
0.00.529.761 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.762 I ggml_metal_init: allocating
0.00.529.800 I ggml_metal_init: found device: Apple M4
0.00.529.812 I ggml_metal_init: picking default device: Apple M4
0.00.530.844 I ggml_metal_init: using embedded metal library
0.00.537.002 I ggml_metal_init: GPU name:   Apple M4
0.00.537.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.013 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.014 I ggml_metal_init: simdgroup reduction   = true
0.00.537.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.014 I ggml_metal_init: has residency sets    = true
0.00.537.015 I ggml_metal_init: has bfloat            = true
0.00.537.015 I ggml_metal_init: use bfloat            = true
0.00.537.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.553.214 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.554.820 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.554.824 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.554.855 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.556.379 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.556.380 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.556.381 I llama_init_from_model: graph nodes  = 967
0.00.556.381 I llama_init_from_model: graph splits = 2
0.00.556.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.556.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.335 I 
0.00.583.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.421 I perplexity: tokenizing the input ..
0.00.588.263 I perplexity: tokenization took 4.84 ms
0.00.588.268 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.474 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.737.717 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.737.764 I llama_perf_context_print:        load time =     573.98 ms
0.00.737.766 I llama_perf_context_print: prompt eval time =     145.96 ms /   128 tokens (    1.14 ms per token,   876.94 tokens per second)
0.00.737.767 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.768 I llama_perf_context_print:       total time =     154.43 ms /   129 tokens
0.00.738.406 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.080s
sys	0m0.091s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.166 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.066 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.415 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.020.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.425 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.427 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.428 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.428 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.431 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.431 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.219 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.200 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.029.200 I llama_model_loader: - type  f32:  194 tensors
0.00.029.200 I llama_model_loader: - type q4_K:   61 tensors
0.00.029.201 I llama_model_loader: - type q5_K:   24 tensors
0.00.029.201 I llama_model_loader: - type q6_K:   13 tensors
0.00.029.201 I print_info: file format = GGUF V3 (latest)
0.00.029.202 I print_info: file type   = Q4_K - Medium
0.00.029.203 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.037.350 I load: special tokens cache size = 25
0.00.043.417 I load: token to piece cache size = 0.2984 MB
0.00.043.421 I print_info: arch             = gptneox
0.00.043.421 I print_info: vocab_only       = 0
0.00.043.422 I print_info: n_ctx_train      = 2048
0.00.043.422 I print_info: n_embd           = 2048
0.00.043.422 I print_info: n_layer          = 24
0.00.043.426 I print_info: n_head           = 16
0.00.043.427 I print_info: n_head_kv        = 16
0.00.043.427 I print_info: n_rot            = 32
0.00.043.427 I print_info: n_swa            = 0
0.00.043.427 I print_info: n_embd_head_k    = 128
0.00.043.427 I print_info: n_embd_head_v    = 128
0.00.043.428 I print_info: n_gqa            = 1
0.00.043.429 I print_info: n_embd_k_gqa     = 2048
0.00.043.430 I print_info: n_embd_v_gqa     = 2048
0.00.043.430 I print_info: f_norm_eps       = 1.0e-05
0.00.043.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.431 I print_info: f_logit_scale    = 0.0e+00
0.00.043.432 I print_info: n_ff             = 8192
0.00.043.432 I print_info: n_expert         = 0
0.00.043.432 I print_info: n_expert_used    = 0
0.00.043.432 I print_info: causal attn      = 1
0.00.043.432 I print_info: pooling type     = 0
0.00.043.432 I print_info: rope type        = 2
0.00.043.433 I print_info: rope scaling     = linear
0.00.043.433 I print_info: freq_base_train  = 10000.0
0.00.043.433 I print_info: freq_scale_train = 1
0.00.043.433 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.434 I print_info: rope_finetuned   = unknown
0.00.043.434 I print_info: ssm_d_conv       = 0
0.00.043.434 I print_info: ssm_d_inner      = 0
0.00.043.434 I print_info: ssm_d_state      = 0
0.00.043.434 I print_info: ssm_dt_rank      = 0
0.00.043.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.434 I print_info: model type       = 1.4B
0.00.043.435 I print_info: model params     = 1.41 B
0.00.043.435 I print_info: general.name     = 1.4B
0.00.043.435 I print_info: vocab type       = BPE
0.00.043.436 I print_info: n_vocab          = 50304
0.00.043.436 I print_info: n_merges         = 50009
0.00.043.436 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.436 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.436 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.436 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.437 I print_info: LF token         = 187 'Ċ'
0.00.043.437 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.437 I print_info: max token length = 1024
0.00.571.483 I load_tensors: offloading 24 repeating layers to GPU
0.00.571.494 I load_tensors: offloading output layer to GPU
0.00.571.494 I load_tensors: offloaded 25/25 layers to GPU
0.00.571.536 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.571.537 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.572.580 I llama_init_from_model: n_seq_max     = 1
0.00.572.583 I llama_init_from_model: n_ctx         = 128
0.00.572.584 I llama_init_from_model: n_ctx_per_seq = 128
0.00.572.584 I llama_init_from_model: n_batch       = 128
0.00.572.584 I llama_init_from_model: n_ubatch      = 128
0.00.572.585 I llama_init_from_model: flash_attn    = 0
0.00.572.586 I llama_init_from_model: freq_base     = 10000.0
0.00.572.586 I llama_init_from_model: freq_scale    = 1
0.00.572.587 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.572.592 I ggml_metal_init: allocating
0.00.572.642 I ggml_metal_init: found device: Apple M4
0.00.572.653 I ggml_metal_init: picking default device: Apple M4
0.00.573.816 I ggml_metal_init: using embedded metal library
0.00.582.725 I ggml_metal_init: GPU name:   Apple M4
0.00.582.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.582.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.582.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.582.737 I ggml_metal_init: simdgroup reduction   = true
0.00.582.737 I ggml_metal_init: simdgroup matrix mul. = true
0.00.582.737 I ggml_metal_init: has residency sets    = true
0.00.582.738 I ggml_metal_init: has bfloat            = true
0.00.582.738 I ggml_metal_init: use bfloat            = true
0.00.582.739 I ggml_metal_init: hasUnifiedMemory      = true
0.00.582.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.595.936 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.597.679 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.597.682 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.597.708 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.599.334 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.599.335 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.599.336 I llama_init_from_model: graph nodes  = 967
0.00.599.336 I llama_init_from_model: graph splits = 2
0.00.599.337 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.599.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.663 I 
0.00.623.705 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.717 I perplexity: tokenizing the input ..
0.00.627.612 I perplexity: tokenization took 3.893 ms
0.00.627.616 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.773.470 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.777.439 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.777.487 I llama_perf_context_print:        load time =     610.59 ms
0.00.777.488 I llama_perf_context_print: prompt eval time =     145.62 ms /   128 tokens (    1.14 ms per token,   878.99 tokens per second)
0.00.777.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.777.490 I llama_perf_context_print:       total time =     153.82 ms /   129 tokens
0.00.778.204 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.087s
sys	0m0.107s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.183 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.660 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.664 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.673 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.674 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.674 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.675 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.676 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.677 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.683 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.456 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.491 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.434 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.436 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.436 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.437 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.437 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.438 I llama_model_loader: - type  f32:  194 tensors
0.00.025.439 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.439 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.440 I print_info: file format = GGUF V3 (latest)
0.00.025.440 I print_info: file type   = Q5_K - Medium
0.00.025.441 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.698 I load: special tokens cache size = 25
0.00.039.918 I load: token to piece cache size = 0.2984 MB
0.00.039.922 I print_info: arch             = gptneox
0.00.039.923 I print_info: vocab_only       = 0
0.00.039.923 I print_info: n_ctx_train      = 2048
0.00.039.923 I print_info: n_embd           = 2048
0.00.039.923 I print_info: n_layer          = 24
0.00.039.927 I print_info: n_head           = 16
0.00.039.928 I print_info: n_head_kv        = 16
0.00.039.928 I print_info: n_rot            = 32
0.00.039.929 I print_info: n_swa            = 0
0.00.039.929 I print_info: n_embd_head_k    = 128
0.00.039.929 I print_info: n_embd_head_v    = 128
0.00.039.930 I print_info: n_gqa            = 1
0.00.039.930 I print_info: n_embd_k_gqa     = 2048
0.00.039.931 I print_info: n_embd_v_gqa     = 2048
0.00.039.932 I print_info: f_norm_eps       = 1.0e-05
0.00.039.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.933 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.933 I print_info: f_logit_scale    = 0.0e+00
0.00.039.933 I print_info: n_ff             = 8192
0.00.039.934 I print_info: n_expert         = 0
0.00.039.934 I print_info: n_expert_used    = 0
0.00.039.934 I print_info: causal attn      = 1
0.00.039.934 I print_info: pooling type     = 0
0.00.039.934 I print_info: rope type        = 2
0.00.039.934 I print_info: rope scaling     = linear
0.00.039.935 I print_info: freq_base_train  = 10000.0
0.00.039.935 I print_info: freq_scale_train = 1
0.00.039.935 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.935 I print_info: rope_finetuned   = unknown
0.00.039.938 I print_info: ssm_d_conv       = 0
0.00.039.938 I print_info: ssm_d_inner      = 0
0.00.039.938 I print_info: ssm_d_state      = 0
0.00.039.938 I print_info: ssm_dt_rank      = 0
0.00.039.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.939 I print_info: model type       = 1.4B
0.00.039.939 I print_info: model params     = 1.41 B
0.00.039.939 I print_info: general.name     = 1.4B
0.00.039.941 I print_info: vocab type       = BPE
0.00.039.941 I print_info: n_vocab          = 50304
0.00.039.941 I print_info: n_merges         = 50009
0.00.039.941 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.942 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.942 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.948 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.949 I print_info: LF token         = 187 'Ċ'
0.00.039.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.952 I print_info: max token length = 1024
0.00.593.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.963 I load_tensors: offloading output layer to GPU
0.00.593.963 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.013 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.016 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.595.526 I llama_init_from_model: n_seq_max     = 1
0.00.595.530 I llama_init_from_model: n_ctx         = 128
0.00.595.530 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.531 I llama_init_from_model: n_batch       = 128
0.00.595.531 I llama_init_from_model: n_ubatch      = 128
0.00.595.532 I llama_init_from_model: flash_attn    = 0
0.00.595.534 I llama_init_from_model: freq_base     = 10000.0
0.00.595.534 I llama_init_from_model: freq_scale    = 1
0.00.595.535 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.542 I ggml_metal_init: allocating
0.00.595.622 I ggml_metal_init: found device: Apple M4
0.00.595.635 I ggml_metal_init: picking default device: Apple M4
0.00.597.380 I ggml_metal_init: using embedded metal library
0.00.603.804 I ggml_metal_init: GPU name:   Apple M4
0.00.603.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.810 I ggml_metal_init: simdgroup reduction   = true
0.00.603.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.811 I ggml_metal_init: has residency sets    = true
0.00.603.811 I ggml_metal_init: has bfloat            = true
0.00.603.811 I ggml_metal_init: use bfloat            = true
0.00.603.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.814 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.180 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.184 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.224 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.453 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.455 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.456 I llama_init_from_model: graph nodes  = 967
0.00.627.456 I llama_init_from_model: graph splits = 2
0.00.627.460 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.392 I 
0.00.658.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.492 I perplexity: tokenizing the input ..
0.00.665.609 I perplexity: tokenization took 7.113 ms
0.00.665.616 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.740 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.808.345 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.808.369 I llama_perf_context_print:        load time =     648.72 ms
0.00.808.370 I llama_perf_context_print: prompt eval time =     140.24 ms /   128 tokens (    1.10 ms per token,   912.73 tokens per second)
0.00.808.371 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.371 I llama_perf_context_print:       total time =     149.98 ms /   129 tokens
0.00.808.746 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.084s
sys	0m0.139s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.993 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.645 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.651 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.658 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.658 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.659 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.594 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.619 I llama_model_loader: - type  f32:  194 tensors
0.00.024.619 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.620 I print_info: file format = GGUF V3 (latest)
0.00.024.620 I print_info: file type   = Q6_K
0.00.024.621 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.403 I load: special tokens cache size = 25
0.00.038.419 I load: token to piece cache size = 0.2984 MB
0.00.038.422 I print_info: arch             = gptneox
0.00.038.422 I print_info: vocab_only       = 0
0.00.038.423 I print_info: n_ctx_train      = 2048
0.00.038.423 I print_info: n_embd           = 2048
0.00.038.423 I print_info: n_layer          = 24
0.00.038.426 I print_info: n_head           = 16
0.00.038.427 I print_info: n_head_kv        = 16
0.00.038.427 I print_info: n_rot            = 32
0.00.038.427 I print_info: n_swa            = 0
0.00.038.428 I print_info: n_embd_head_k    = 128
0.00.038.428 I print_info: n_embd_head_v    = 128
0.00.038.431 I print_info: n_gqa            = 1
0.00.038.431 I print_info: n_embd_k_gqa     = 2048
0.00.038.432 I print_info: n_embd_v_gqa     = 2048
0.00.038.433 I print_info: f_norm_eps       = 1.0e-05
0.00.038.433 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.434 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.434 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.434 I print_info: f_logit_scale    = 0.0e+00
0.00.038.435 I print_info: n_ff             = 8192
0.00.038.435 I print_info: n_expert         = 0
0.00.038.435 I print_info: n_expert_used    = 0
0.00.038.435 I print_info: causal attn      = 1
0.00.038.436 I print_info: pooling type     = 0
0.00.038.436 I print_info: rope type        = 2
0.00.038.437 I print_info: rope scaling     = linear
0.00.038.437 I print_info: freq_base_train  = 10000.0
0.00.038.437 I print_info: freq_scale_train = 1
0.00.038.439 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.439 I print_info: rope_finetuned   = unknown
0.00.038.439 I print_info: ssm_d_conv       = 0
0.00.038.439 I print_info: ssm_d_inner      = 0
0.00.038.440 I print_info: ssm_d_state      = 0
0.00.038.440 I print_info: ssm_dt_rank      = 0
0.00.038.440 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.440 I print_info: model type       = 1.4B
0.00.038.442 I print_info: model params     = 1.41 B
0.00.038.442 I print_info: general.name     = 1.4B
0.00.038.443 I print_info: vocab type       = BPE
0.00.038.443 I print_info: n_vocab          = 50304
0.00.038.443 I print_info: n_merges         = 50009
0.00.038.444 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.445 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.445 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.445 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.445 I print_info: LF token         = 187 'Ċ'
0.00.038.445 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.446 I print_info: max token length = 1024
0.00.484.242 I load_tensors: offloading 24 repeating layers to GPU
0.00.484.249 I load_tensors: offloading output layer to GPU
0.00.484.250 I load_tensors: offloaded 25/25 layers to GPU
0.00.484.276 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.484.279 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.485.696 I llama_init_from_model: n_seq_max     = 1
0.00.485.698 I llama_init_from_model: n_ctx         = 128
0.00.485.698 I llama_init_from_model: n_ctx_per_seq = 128
0.00.485.699 I llama_init_from_model: n_batch       = 128
0.00.485.699 I llama_init_from_model: n_ubatch      = 128
0.00.485.700 I llama_init_from_model: flash_attn    = 0
0.00.485.701 I llama_init_from_model: freq_base     = 10000.0
0.00.485.702 I llama_init_from_model: freq_scale    = 1
0.00.485.703 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.485.704 I ggml_metal_init: allocating
0.00.485.744 I ggml_metal_init: found device: Apple M4
0.00.485.755 I ggml_metal_init: picking default device: Apple M4
0.00.487.171 I ggml_metal_init: using embedded metal library
0.00.493.082 I ggml_metal_init: GPU name:   Apple M4
0.00.493.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.493.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.493.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.493.087 I ggml_metal_init: simdgroup reduction   = true
0.00.493.087 I ggml_metal_init: simdgroup matrix mul. = true
0.00.493.088 I ggml_metal_init: has residency sets    = true
0.00.493.088 I ggml_metal_init: has bfloat            = true
0.00.493.088 I ggml_metal_init: use bfloat            = true
0.00.493.089 I ggml_metal_init: hasUnifiedMemory      = true
0.00.493.090 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.509.766 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.513.199 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.513.203 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.513.244 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.516.326 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.516.327 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.516.328 I llama_init_from_model: graph nodes  = 967
0.00.516.328 I llama_init_from_model: graph splits = 2
0.00.516.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.516.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.551.216 I 
0.00.551.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.551.308 I perplexity: tokenizing the input ..
0.00.558.158 I perplexity: tokenization took 6.849 ms
0.00.558.162 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.697.527 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.699.060 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.699.085 I llama_perf_context_print:        load time =     542.21 ms
0.00.699.086 I llama_perf_context_print: prompt eval time =     139.14 ms /   128 tokens (    1.09 ms per token,   919.95 tokens per second)
0.00.699.087 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.699.087 I llama_perf_context_print:       total time =     147.88 ms /   129 tokens
0.00.699.465 I ggml_metal_free: deallocating

real	0m0.713s
user	0m0.078s
sys	0m0.129s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.309 I build: 4643 (fa62da9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.543 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.906 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.916 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.928 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.928 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.630 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.903 I llama_model_loader: - type  f32:  194 tensors
0.00.050.904 I llama_model_loader: - type  f16:   98 tensors
0.00.050.904 I print_info: file format = GGUF V3 (latest)
0.00.050.905 I print_info: file type   = all F32 (guessed)
0.00.050.906 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.523 I load: special tokens cache size = 25
0.00.070.095 I load: token to piece cache size = 0.2984 MB
0.00.070.098 I print_info: arch             = gptneox
0.00.070.098 I print_info: vocab_only       = 0
0.00.070.098 I print_info: n_ctx_train      = 2048
0.00.070.098 I print_info: n_embd           = 2048
0.00.070.098 I print_info: n_layer          = 24
0.00.070.101 I print_info: n_head           = 16
0.00.070.102 I print_info: n_head_kv        = 16
0.00.070.102 I print_info: n_rot            = 32
0.00.070.102 I print_info: n_swa            = 0
0.00.070.104 I print_info: n_embd_head_k    = 128
0.00.070.104 I print_info: n_embd_head_v    = 128
0.00.070.105 I print_info: n_gqa            = 1
0.00.070.106 I print_info: n_embd_k_gqa     = 2048
0.00.070.106 I print_info: n_embd_v_gqa     = 2048
0.00.070.107 I print_info: f_norm_eps       = 1.0e-05
0.00.070.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.108 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.108 I print_info: f_logit_scale    = 0.0e+00
0.00.070.108 I print_info: n_ff             = 8192
0.00.070.109 I print_info: n_expert         = 0
0.00.070.109 I print_info: n_expert_used    = 0
0.00.070.109 I print_info: causal attn      = 1
0.00.070.109 I print_info: pooling type     = 0
0.00.070.109 I print_info: rope type        = 2
0.00.070.109 I print_info: rope scaling     = linear
0.00.070.115 I print_info: freq_base_train  = 10000.0
0.00.070.118 I print_info: freq_scale_train = 1
0.00.070.118 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.118 I print_info: rope_finetuned   = unknown
0.00.070.120 I print_info: ssm_d_conv       = 0
0.00.070.120 I print_info: ssm_d_inner      = 0
0.00.070.120 I print_info: ssm_d_state      = 0
0.00.070.120 I print_info: ssm_dt_rank      = 0
0.00.070.121 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.121 I print_info: model type       = 1.4B
0.00.070.121 I print_info: model params     = 1.41 B
0.00.070.121 I print_info: general.name     = 1.4B
0.00.070.122 I print_info: vocab type       = BPE
0.00.070.122 I print_info: n_vocab          = 50304
0.00.070.122 I print_info: n_merges         = 50009
0.00.070.123 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.123 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.123 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.123 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.125 I print_info: LF token         = 187 'Ċ'
0.00.070.125 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.125 I print_info: max token length = 1024
0.01.297.856 I load_tensors: offloading 24 repeating layers to GPU
0.01.297.863 I load_tensors: offloading output layer to GPU
0.01.297.864 I load_tensors: offloaded 25/25 layers to GPU
0.01.297.891 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.297.893 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.298.877 I llama_init_from_model: n_seq_max     = 1
0.01.298.878 I llama_init_from_model: n_ctx         = 128
0.01.298.878 I llama_init_from_model: n_ctx_per_seq = 128
0.01.298.879 I llama_init_from_model: n_batch       = 128
0.01.298.879 I llama_init_from_model: n_ubatch      = 128
0.01.298.879 I llama_init_from_model: flash_attn    = 0
0.01.298.879 I llama_init_from_model: freq_base     = 10000.0
0.01.298.880 I llama_init_from_model: freq_scale    = 1
0.01.298.880 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.298.881 I ggml_metal_init: allocating
0.01.298.909 I ggml_metal_init: found device: Apple M4
0.01.298.915 I ggml_metal_init: picking default device: Apple M4
0.01.299.920 I ggml_metal_init: using embedded metal library
0.01.303.824 I ggml_metal_init: GPU name:   Apple M4
0.01.303.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.303.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.303.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.303.827 I ggml_metal_init: simdgroup reduction   = true
0.01.303.827 I ggml_metal_init: simdgroup matrix mul. = true
0.01.303.828 I ggml_metal_init: has residency sets    = true
0.01.303.828 I ggml_metal_init: has bfloat            = true
0.01.303.828 I ggml_metal_init: use bfloat            = true
0.01.303.828 I ggml_metal_init: hasUnifiedMemory      = true
0.01.303.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.314.348 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.315.979 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.315.981 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.316.006 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.317.617 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.317.618 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.317.619 I llama_init_from_model: graph nodes  = 967
0.01.317.619 I llama_init_from_model: graph splits = 2
0.01.317.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.317.620 I 
0.01.317.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.317.660 I compute_imatrix: tokenizing the input ..
0.01.321.671 I compute_imatrix: tokenization took 4.011 ms
0.01.321.673 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.585.823 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.588.315 I llama_perf_context_print:        load time =    1565.28 ms
0.01.588.316 I llama_perf_context_print: prompt eval time =     262.37 ms /   128 tokens (    2.05 ms per token,   487.86 tokens per second)
0.01.588.316 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.588.317 I llama_perf_context_print:       total time =    1567.76 ms /   129 tokens
0.01.588.884 I ggml_metal_free: deallocating

real	0m1.774s
user	0m0.122s
sys	0m0.241s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151405b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151406270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151406820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151406dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151407380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151407930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151407ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151408490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151408a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151408f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151409440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151409940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15140a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15140ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15140b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15140bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15140c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15140c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15140d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15140d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15140df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15140e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15140edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15140f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15140fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151410050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151410660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1514112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151411810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151411ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151411f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151412230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151412ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151413000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1514132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151413760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151413c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1514140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151414540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1514149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151414e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151415320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1514157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151415c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151415f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151416530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151416b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151417460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151417a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151418080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151418690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151418ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1514192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1514198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15141a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15141a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15141a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15141acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15141b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15141bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15141bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15141c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15141c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15141cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15141cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15141d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15141d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15141ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15141e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15141e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15141ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15141f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15141f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15141fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15141ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1514204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151420a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151420f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1514214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151421a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151421f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1514224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151422a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151422f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1514234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151423a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151423f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1514244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1514249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151424f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151425490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1514259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151425f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151426480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1514269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151426f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151427470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151417150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1514278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151428090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1514285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151428b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151429080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1514295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151429b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15142a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15142a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15142ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15142b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15142b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151308290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151308700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151308b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151308fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151309450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1513098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151309d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15130a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15130a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15130aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15130aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15130b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15130b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15130bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15130c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15130c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15130cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15130cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15130d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15130d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15130dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15130e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15130e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15130ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15130ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15130f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15130f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15130fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1513100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151310530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1513109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151310e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151311280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1513116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151311b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151311fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151312440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1513128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151312d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151313190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151313600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151313a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151313ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151314350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1513147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151314c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1513150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151315510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151315980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151315df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151316260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1513166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151316b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151316fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151317420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151317890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151317d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151318170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1513185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151318a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151318ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151319330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1513197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151319c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15131a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15131a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15131a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15131add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15131b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15131b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15131bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15131bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15131c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15131c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15131cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15131d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15131d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15131da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15131dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15131e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15131e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15131ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15131f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15131f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15131f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15131fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151320220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151320690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151320b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151320f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1513213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151321850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151321cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151322130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1513225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151322a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151322e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1513232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151323760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151323bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1513247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151324a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151325120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1513256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151325c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151326240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1513267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151326da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151327350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151327900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151327eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151328460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151328a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151328fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151329570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151329b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15132a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15132a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15132ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15132b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15132b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15132bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15132c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15132c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15132ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15132d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15132d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15132df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15132e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15132eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15132f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15132f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15132fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151330180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151330730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151330ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151331290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151331840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151331df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1513323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x151332950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151332f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1513334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x151333a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151334010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1513345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151334b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151335120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1513356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151335c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151336230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1513367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151336d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151337340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1513378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151337ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151338450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151338a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151338f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151339400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151339900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151339e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15133a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15133a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15133ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15133b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15133b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15133bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15133c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15133c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15133cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15133d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15133d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15133df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15133e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15133ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15133f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15133f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15133ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1513401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1513407f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.673.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.673.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151327bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151337050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151334880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151332660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151330440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151328720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151325f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15132aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15132c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151331550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15132e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151335f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151328cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151329de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151330fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15132ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1513320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15132f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15132a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151326ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1513259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151337bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15133d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15132b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151331b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151329280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151333770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151328170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1513364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151333d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15132f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151338710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151337600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151327060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151338160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151326500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151336aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1513309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151332c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151335990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1513342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15132c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1513253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151340fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151341500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1513417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151341a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151341d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151342000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1513422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151342580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151342840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151342b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151342dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151343080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151343340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151343600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1513438c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151343b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151343e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151344100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1513443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151344680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151344940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151344c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151344ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151345180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151345440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151345700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1513459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151345c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151345f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151346200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151346750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151346a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151346cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151346f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151347250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151347510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1513477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151347a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151347fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151348530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151348a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151348fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151349520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151349a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151349fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15134a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15134aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15134afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15134b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15134ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15134bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15134c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15134ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15134cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15134d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15134da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15134df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15134e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15134ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15134ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15134f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15134fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15134ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1513504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151350a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151350f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1513514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1513519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151351f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151352490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151352890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151352b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151352e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151353280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1513536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151353b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151353fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151354440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1513548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151354d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151355190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151355600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151355a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151355ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151356350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1513567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151356c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1513570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151357510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151357980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151357df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151358260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1513586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151358b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151358fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151359420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151359890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151359d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15135a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15135a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15135aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15135aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15135b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15135b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15135bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15135c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15135c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15135c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15135cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15135d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15135d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15135db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15135df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15135e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15135e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15135ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15135f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15135f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15135fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15135fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151360310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151360780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151360bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151361310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151361840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151361d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151362240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151362740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151362c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151363140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151363640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151363b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151364040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151364540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151364a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151364f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151365440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151365940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151365e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151366340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151366840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151366d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151367240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151367740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151367c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151368140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151368640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151368b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151369040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151369540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151369a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151369ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15136a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15136ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15136b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15136b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15136bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15136c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15136cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15136cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15136d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15136d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15136dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15136e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15136eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15136efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15136f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15136fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151370170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1513706c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151370c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151371160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1513716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151371c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151372150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1513726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151372bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151373140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151373690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151373be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151374130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151374680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151374bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151375120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151375670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151375bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151376110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151376660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151376bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151377100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151377650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151377ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1513780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151378640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151378b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1513790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151379630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151379b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15137a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15137a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15137ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15137b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15137b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15137bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15137c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15137c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15137cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15137d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15137d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15137db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15137e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15137e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15137eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15137f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15137f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15137fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151380070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1513805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151380b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151381060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1513815b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151381b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151382050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1513825a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151382a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151382ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151383380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151383820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151383cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151384160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151384600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151384aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151384f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1513853e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151385880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151385d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1513861c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151386660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151386b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151387050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151387770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151387e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1513885b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151388cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151388f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151389780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151389a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15138a050 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x151504760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151504bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151505040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1515054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151505920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151505d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151506200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x151506670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151506ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151506f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1515073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151507ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151508600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151508db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1515095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151509ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15150a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15150ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15150b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15150b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15150c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15150c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15150ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15150d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15150dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15150dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15150e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15150e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15150eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15150efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15150f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15150f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15150fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1515100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151510520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151510990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151510e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151511270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1515116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151511b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151511fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x151512430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1515128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151512d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x151513180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1515135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151513a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x151513ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151514340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1515147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151514c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151515090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151515500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151515970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151515de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151516250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1515167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151516cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151517130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1515175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151517a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151517e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1515182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151518760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151518bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151519040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1515194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151519920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151519d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15151a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15151a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15151aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15151af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15151b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15151b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15151bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15151c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15151c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15151c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15151ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15151d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15151d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15151dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15151e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15151e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15151e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15151ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15151f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15151f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15151fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15151ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1515203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151520810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151520c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1515210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151521560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1515219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151521e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1515222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151522720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151522b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151523000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151523470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151523d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151523fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151524430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1515248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151524d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151525180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1515255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151525a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151525ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151526340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1515267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151526c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151527090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151527500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151527970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151527de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151528250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1515286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151528b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151528fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151529410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151529880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151529cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15152a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15152a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15152aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15152aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15152b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15152b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15152bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15152c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15152c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15152c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15152cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15152d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15152d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15152db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15152df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15152e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15152e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15152ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15152f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15152f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15152fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15152fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151530300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151530770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151530be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151531050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1515314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151531930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151531da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151532210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151532680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151532af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151532f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1515333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151533840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151533cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151534120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151534590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151534a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151534e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1515352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151535750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151535bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151536030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1515364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151536910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151536d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1515371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151537660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151537ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151537f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1515383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151538820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151538c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151539100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151539570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1515399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151539e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15153a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15153a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15153aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15153b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15153b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15153b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15153bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15153c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15153c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15153cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15153cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15153d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15153d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15153dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15153e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15153e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15153e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15153ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15153f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15153f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15153fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15153fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151540460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1515408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151540d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1515411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151541d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151541ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1515422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151542720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151542b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151543000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151543470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1515438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151543d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1515441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151544630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151544aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151544f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151545380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1515457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151545c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1515460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151546540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1515469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151546e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151547290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151547700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151547b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151547fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151548450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1515488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151548d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1515491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151549610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151549a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151549ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15154a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15154a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15154ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15154b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15154b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15154b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15154be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15154c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15154c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15154cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15154cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15154d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15154d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15154dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15154e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15154e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15154ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15154eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15154f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15154f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15154fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151550090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151550500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151550970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151550de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151551250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1515516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151551b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151551fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151552410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151552880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151552cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151553160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1515535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151553a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151553eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151554320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151554790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151554c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151555070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1515554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151555950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1515563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151556ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151557200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151557920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151557be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151558050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x151558650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151558c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.731s
user	0m0.277s
sys	0m0.317s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-save-load-state.log
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4643 (fa62da9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b70d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b70d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b70df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b70e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b70eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b70f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b70f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b70fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b710180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b710680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b711080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b711ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b712b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b713280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b7139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b7140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b7147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b714fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b7156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b715df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b716510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b716db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b7174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b717da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b718a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b718f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b7196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b71a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b71a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b71aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b71aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b71b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b71b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b71bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b71c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b71c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b71ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b71cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b71d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b71d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b71dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b71e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b71eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b71f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b71f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b71fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b7203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b7209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b721000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b7217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b721c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b722130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b7223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b722a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b7231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b7234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b723950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b723df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b724730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b724bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b725070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b725510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b7259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b725e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b7262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b726790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b727180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b7276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b727c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b728170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b7286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b728c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b729160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b7296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b729c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b72a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b72a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b72abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b72b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b72b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b72bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b72c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b72c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b72cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b72d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b72d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b72dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b72e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b72e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b72ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b71e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b72f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b72f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b72fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b730270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b7307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b730d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b731260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b7317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b731d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b732250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b7327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b732cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b733240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b733790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b733ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b734180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b734620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b734ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b734f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b735400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b7358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b735d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b7361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b736680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b736b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b737900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b738240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b7386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b7394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b73a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b73a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b73abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b73b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b73b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b73b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b73be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b73c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b73c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b73cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b73d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b73d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b73da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b73dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b73e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b73e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b73eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b73f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b73f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b73fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b73ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b7403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b740860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b7411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b742420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b7428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b7436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b744480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b745700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b745ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b7464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b746980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b746e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b7472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b747c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b7480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b7489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b749320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b7497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b74a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b74a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b74aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b74aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b74b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b74b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b74bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b74c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b74c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b74ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b74d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b74d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b74e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b74e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b74e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b74ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b74f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b74fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b750110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b7505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b750a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b751200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b751750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b751ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b7521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b752740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b7531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b753730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b753c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b7541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b754720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b754c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b7551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b755710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b755c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b7561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b756700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b756c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b7571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b7576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b757c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b758190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b7586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b758c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b759180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b7596d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b759c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b75a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b75a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b75ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b75b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b75b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b75bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b75c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b75c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b75cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b75d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b75d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b75dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b75e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b75e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b75ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b75f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b75f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b75fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b760110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b760660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b760bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b761100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b761650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b761ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b7620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b762640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b762b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b7630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b763630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b763b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b764020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b7644c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b764960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b764e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b7652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b765740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b765be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b766080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b766520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b7669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b766e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b767300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b7677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b7680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b768630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b768d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b769470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b769b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b76a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b76a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b76ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b76b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b76b630 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15c804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15c804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15c8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15c805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15c805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15c806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15c806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15c8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15c806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15c8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15c807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15c807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15c808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15c8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15c809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15c80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15c80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15c80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15c80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15c80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15c80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15c80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15c80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15c80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15c80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15c80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15c80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15c80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15c80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15c80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15c80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15c80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15c8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15c810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15c810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15c810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15c8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c8377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c8380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c8396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c83a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c83a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c83ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c83b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c83b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c83ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c83bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c83c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c83c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c83cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c83d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c83d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c83d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c83ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c83e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c83e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c83eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c83ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c83f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c83f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c83fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c8402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15c840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c8425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c8439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c8450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c8495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c84a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c84a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c84acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c84b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c84be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c84c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c84c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c84e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c84ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c84fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c8508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c8564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15c856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15c856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c85a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c85a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c85ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c85b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c85b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c85bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c85c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c85cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c85d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c85d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c85dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c85e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c85e880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15c904860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15c904cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15c905140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15c9055b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15c905a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15c905e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15c906300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15c906770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15c906be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15c907050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15c9074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15c907b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15c908680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15c908e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15c909640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15c909d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15c90a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15c90aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15c90b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15c90ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15c90c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15c90c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15c90cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15c90d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15c90de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15c90e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15c90e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15c90e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15c90ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15c90f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15c90f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15c90faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15c90ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15c9101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15c910640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15c910ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15c910f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c911390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c911800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c911c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c9120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c912550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c9129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c912e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c9132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c913710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c913b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c913ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c914460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c9148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c914d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c9151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c915620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c915a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c915f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c916370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c9168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c916de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c917250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c9176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c917b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c917fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c918410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c918880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c918cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c919160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c9195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c919a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c919eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c91a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c91a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c91ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c91b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c91b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c91b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c91bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c91c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c91c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c91cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c91cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c91d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c91d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c91dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c91e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c91e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c91ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c91ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c91f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c91f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c91fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c920050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c9204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c920930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c920da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c921210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c921680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c921af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c921f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c9223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c922840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c922cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c923120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c923590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c923e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c9240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c924550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c9249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c924e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c9252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c925710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c925b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c925ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c926460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c9268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c926d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c9271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c927620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c927a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c927f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c928370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c9287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c928c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c9290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c929530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c9299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c929e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c92a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c92a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c92ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c92afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c92b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c92b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c92bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c92c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c92c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c92ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c92cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c92d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c92d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c92dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c92e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c92e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c92e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c92edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c92f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c92f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c92fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c92ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c930420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c930890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c930d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c931170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c9315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c931a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c931ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c932330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c9327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c932c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c933080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c9334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c933960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c933dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c934240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c9346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c934b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c934f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c935400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c935870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c935ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c936150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c9365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c936a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c936ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c937310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c937780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c937bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c938060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c9384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c938940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c938db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c939220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c939690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c939b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c939f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c93a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c93a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c93acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c93b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c93b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c93ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c93be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c93c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c93c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c93cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c93d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c93d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c93d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c93dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c93e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c93e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c93eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c93ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c93f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c93f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c93fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15c940110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c940580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c9409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c940e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c9412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c941e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c942110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c9423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c942840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c942cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c943120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c943590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c943a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c943e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c9442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c944750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c944bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c945030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c9454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c945910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c945d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c9461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c946660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c946ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c946f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c9473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c947820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c947c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c948100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c948570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c9489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c948e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c9492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c949730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c949ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c94a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c94a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c94a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c94ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c94b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c94b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c94bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c94bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c94c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c94c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c94cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c94d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c94d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c94d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c94de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c94e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c94e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c94eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c94eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c94f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c94f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c94fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c9501b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c950620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c950a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c950f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c951370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15c9517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15c951c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c9520c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c952530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c9529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c952e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c953280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c9536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c953b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c953fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c954440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c9548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c954d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c955190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c955600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c955a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c9564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c956c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c957320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c957a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c957d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c958170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c958770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c958d80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.955s
user	0m0.236s
sys	0m0.186s
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-f16.log
++ grep '^\[1\]'
+ check_ppl f16 '[1]10.1498,'
+ qnt=f16
++ echo '[1]10.1498,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1498
++ echo '10.1498 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' f16 10.1498
+ return 0
  - f16 @ 10.1498 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q8_0.log
++ grep '^\[1\]'
+ check_ppl q8_0 '[1]10.1362,'
+ qnt=q8_0
++ echo '[1]10.1362,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1362
++ echo '10.1362 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q8_0 10.1362
+ return 0
  - q8_0 @ 10.1362 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_0.log
++ grep '^\[1\]'
+ check_ppl q4_0 '[1]11.1740,'
+ qnt=q4_0
++ echo '[1]11.1740,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=11.1740
++ echo '11.1740 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_0 11.1740
+ return 0
  - q4_0 @ 11.1740 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_1.log
++ grep '^\[1\]'
+ check_ppl q4_1 '[1]10.5507,'
+ qnt=q4_1
++ echo '[1]10.5507,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.5507
++ echo '10.5507 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_1 10.5507
+ return 0
  - q4_1 @ 10.5507 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_0.log
++ grep '^\[1\]'
+ check_ppl q5_0 '[1]10.0972,'
+ qnt=q5_0
++ echo '[1]10.0972,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.0972
++ echo '10.0972 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_0 10.0972
+ return 0
  - q5_0 @ 10.0972 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_1.log
++ grep '^\[1\]'
+ check_ppl q5_1 '[1]10.1971,'
+ qnt=q5_1
++ echo '[1]10.1971,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1971
++ echo '10.1971 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_1 10.1971
+ return 0
  - q5_1 @ 10.1971 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q3_k.log
++ grep '^\[1\]'
+ check_ppl q3_k '[1]12.0517,'
+ qnt=q3_k
++ echo '[1]12.0517,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=12.0517
++ echo '12.0517 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q3_k 12.0517
+ return 0
  - q3_k @ 12.0517 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q4_k.log
++ grep '^\[1\]'
+ check_ppl q4_k '[1]10.1031,'
+ qnt=q4_k
++ echo '[1]10.1031,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.1031
++ echo '10.1031 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q4_k 10.1031
+ return 0
  - q4_k @ 10.1031 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q5_k.log
++ grep '^\[1\]'
+ check_ppl q5_k '[1]10.2433,'
+ qnt=q5_k
++ echo '[1]10.2433,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.2433
++ echo '10.2433 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q5_k 10.2433
+ return 0
  - q5_k @ 10.2433 OK
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-ppl.log
++ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-tg-q6_k.log
++ grep '^\[1\]'
+ check_ppl q6_k '[1]10.3179,'
+ qnt=q6_k
++ echo '[1]10.3179,'
++ grep -oE '[0-9]+\.[0-9]+'
++ tail -n 1
+ ppl=10.3179
++ echo '10.3179 > 20.0'
++ bc
+ '[' 0 -eq 1 ']'
+ printf '  - %s @ %s OK\n' q6_k 10.3179
+ return 0
  - q6_k @ 10.3179 OK
+ cat /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/pythia_1_4b-imatrix.log
+ grep Final
+ set +e
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_debug
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_with_model_debug.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-debug
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_with_model_debug-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.72 sec*proc (2 tests)

Total Test time (real) =   1.73 sec
        1.75 real         0.52 user         0.21 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
+ gg_run_ctest_with_model_release
+ cd /Users/ggml/work/llama.cpp
+ local model
+ tee /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_with_model_release.log
++ gg_get_model
++ local gguf_0=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
++ local gguf_1=/Users/ggml/mnt/llama.cpp/models/pythia/2.8B/ggml-model-f16.gguf
++ local gguf_2=/Users/ggml/mnt/llama.cpp/models/open-llama/7B-v2/ggml-model-f16.gguf
++ [[ -s /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf ]]
++ echo -n /Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ model=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ cd build-ci-release
+ set -e
+ tee -a /Users/ggml/results/llama.cpp/fa/62da9b2dc03539a30f1306f59c4c6ffbe4f50a/ggml-100-mac-m4/ctest_with_model_release-ctest.log
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
+ set +e
+ cd ..
+ cur=0
+ echo 0
+ set +x
