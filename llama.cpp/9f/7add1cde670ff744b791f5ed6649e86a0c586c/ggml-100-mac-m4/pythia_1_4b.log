Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.541s
user	0m0.869s
sys	0m1.222s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-barrier
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Built target llama-batched-bench
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gguf-split
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Built target llama-imatrix
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-infill
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Built target llama-bench
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating index.html.gz.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-parallel
[ 85%] Built target llama-passkey
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 89%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Built target llama-quantize
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative-simple
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tokenize
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-run
[ 94%] Built target llama-tts
[ 94%] Built target llama-speculative
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.010s
user	0m6.041s
sys	0m9.289s

main: quantize time =  4635.02 ms
main:    total time =  4635.02 ms

main: quantize time =  3845.90 ms
main:    total time =  3845.90 ms

main: quantize time =  1890.79 ms
main:    total time =  1890.79 ms

main: quantize time =  2636.76 ms
main:    total time =  2636.76 ms

main: quantize time =  2917.33 ms
main:    total time =  2917.33 ms

main: quantize time =  4986.57 ms
main:    total time =  4986.57 ms

main: quantize time =  5760.45 ms
main:    total time =  5760.45 ms

main: quantize time =  6815.28 ms
main:    total time =  6815.28 ms

main: quantize time =  6118.96 ms
main:    total time =  6118.96 ms

main: quantize time =  4603.74 ms
main:    total time =  4603.74 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.178 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.294 I main: llama backend init
0.00.000.301 I main: load the model and apply lora adapter, if any
0.00.028.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.137 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.170 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.171 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.177 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.069 I llama_model_loader: - type  f32:  194 tensors
0.00.061.069 I llama_model_loader: - type  f16:   98 tensors
0.00.061.070 I print_info: file format = GGUF V3 (latest)
0.00.061.074 I print_info: file type   = all F32 (guessed)
0.00.061.076 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.985 I load: special tokens cache size = 25
0.00.096.834 I load: token to piece cache size = 0.2984 MB
0.00.096.838 I print_info: arch             = gptneox
0.00.096.838 I print_info: vocab_only       = 0
0.00.096.838 I print_info: n_ctx_train      = 2048
0.00.096.838 I print_info: n_embd           = 2048
0.00.096.838 I print_info: n_layer          = 24
0.00.096.842 I print_info: n_head           = 16
0.00.096.843 I print_info: n_head_kv        = 16
0.00.096.843 I print_info: n_rot            = 32
0.00.096.843 I print_info: n_swa            = 0
0.00.096.843 I print_info: n_embd_head_k    = 128
0.00.096.843 I print_info: n_embd_head_v    = 128
0.00.096.844 I print_info: n_gqa            = 1
0.00.096.845 I print_info: n_embd_k_gqa     = 2048
0.00.096.847 I print_info: n_embd_v_gqa     = 2048
0.00.096.847 I print_info: f_norm_eps       = 1.0e-05
0.00.096.848 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.848 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.848 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.848 I print_info: f_logit_scale    = 0.0e+00
0.00.096.849 I print_info: n_ff             = 8192
0.00.096.849 I print_info: n_expert         = 0
0.00.096.849 I print_info: n_expert_used    = 0
0.00.096.849 I print_info: causal attn      = 1
0.00.096.849 I print_info: pooling type     = 0
0.00.096.850 I print_info: rope type        = 2
0.00.096.851 I print_info: rope scaling     = linear
0.00.096.852 I print_info: freq_base_train  = 10000.0
0.00.096.852 I print_info: freq_scale_train = 1
0.00.096.852 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.852 I print_info: rope_finetuned   = unknown
0.00.096.853 I print_info: ssm_d_conv       = 0
0.00.096.853 I print_info: ssm_d_inner      = 0
0.00.096.853 I print_info: ssm_d_state      = 0
0.00.096.853 I print_info: ssm_dt_rank      = 0
0.00.096.853 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.853 I print_info: model type       = 1.4B
0.00.096.854 I print_info: model params     = 1.41 B
0.00.096.854 I print_info: general.name     = 1.4B
0.00.096.854 I print_info: vocab type       = BPE
0.00.096.854 I print_info: n_vocab          = 50304
0.00.096.854 I print_info: n_merges         = 50009
0.00.096.855 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.858 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.858 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.858 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.859 I print_info: LF token         = 128 'Ä'
0.00.096.859 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.860 I print_info: max token length = 1024
0.00.099.489 I load_tensors: offloading 24 repeating layers to GPU
0.00.099.490 I load_tensors: offloading output layer to GPU
0.00.099.490 I load_tensors: offloaded 25/25 layers to GPU
0.00.099.508 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.099.509 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.099.807 I llama_init_from_model: n_seq_max     = 1
0.00.099.808 I llama_init_from_model: n_ctx         = 2048
0.00.099.808 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.099.809 I llama_init_from_model: n_batch       = 2048
0.00.099.809 I llama_init_from_model: n_ubatch      = 512
0.00.099.809 I llama_init_from_model: flash_attn    = 0
0.00.099.809 I llama_init_from_model: freq_base     = 10000.0
0.00.099.810 I llama_init_from_model: freq_scale    = 1
0.00.099.810 I ggml_metal_init: allocating
0.00.099.813 I ggml_metal_init: found device: Apple M4
0.00.099.815 I ggml_metal_init: picking default device: Apple M4
0.00.100.526 I ggml_metal_init: using embedded metal library
0.00.103.276 I ggml_metal_init: GPU name:   Apple M4
0.00.103.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.279 I ggml_metal_init: simdgroup reduction   = true
0.00.103.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.279 I ggml_metal_init: has bfloat            = true
0.00.103.280 I ggml_metal_init: use bfloat            = true
0.00.103.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.766 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.131.885 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.131.891 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.131.911 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.132.874 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.132.876 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.132.876 I llama_init_from_model: graph nodes  = 967
0.00.132.876 I llama_init_from_model: graph splits = 2
0.00.132.880 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.133.008 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.009 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.212.469 I main: llama threadpool init, n_threads = 4
0.00.212.516 I 
0.00.212.557 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.212.559 I 
0.00.212.626 I sampler seed: 1234
0.00.212.630 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.212.655 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.212.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.212.657 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.054.462 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.02.054.462 I llama_perf_context_print:        load time =     183.73 ms
0.02.054.463 I llama_perf_context_print: prompt eval time =      43.59 ms /     7 tokens (    6.23 ms per token,   160.58 tokens per second)
0.02.054.465 I llama_perf_context_print:        eval time =    1795.32 ms /    63 runs   (   28.50 ms per token,    35.09 tokens per second)
0.02.054.465 I llama_perf_context_print:       total time =    1841.99 ms /    70 tokens
0.02.054.698 I ggml_metal_free: deallocating

real	0m2.361s
user	0m0.143s
sys	0m0.099s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.796 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.033 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.041 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.047 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.050 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.935 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.936 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.936 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.937 I llama_model_loader: - type  f32:  194 tensors
0.00.033.937 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.938 I print_info: file format = GGUF V3 (latest)
0.00.033.938 I print_info: file type   = Q8_0
0.00.033.940 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.054.373 I load: special tokens cache size = 25
0.00.060.355 I load: token to piece cache size = 0.2984 MB
0.00.060.360 I print_info: arch             = gptneox
0.00.060.360 I print_info: vocab_only       = 0
0.00.060.360 I print_info: n_ctx_train      = 2048
0.00.060.362 I print_info: n_embd           = 2048
0.00.060.362 I print_info: n_layer          = 24
0.00.060.369 I print_info: n_head           = 16
0.00.060.370 I print_info: n_head_kv        = 16
0.00.060.371 I print_info: n_rot            = 32
0.00.060.371 I print_info: n_swa            = 0
0.00.060.373 I print_info: n_embd_head_k    = 128
0.00.060.373 I print_info: n_embd_head_v    = 128
0.00.060.374 I print_info: n_gqa            = 1
0.00.060.375 I print_info: n_embd_k_gqa     = 2048
0.00.060.376 I print_info: n_embd_v_gqa     = 2048
0.00.060.378 I print_info: f_norm_eps       = 1.0e-05
0.00.060.378 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.379 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.379 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.379 I print_info: f_logit_scale    = 0.0e+00
0.00.060.380 I print_info: n_ff             = 8192
0.00.060.380 I print_info: n_expert         = 0
0.00.060.380 I print_info: n_expert_used    = 0
0.00.060.380 I print_info: causal attn      = 1
0.00.060.381 I print_info: pooling type     = 0
0.00.060.381 I print_info: rope type        = 2
0.00.060.381 I print_info: rope scaling     = linear
0.00.060.383 I print_info: freq_base_train  = 10000.0
0.00.060.383 I print_info: freq_scale_train = 1
0.00.060.383 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.384 I print_info: rope_finetuned   = unknown
0.00.060.384 I print_info: ssm_d_conv       = 0
0.00.060.384 I print_info: ssm_d_inner      = 0
0.00.060.384 I print_info: ssm_d_state      = 0
0.00.060.384 I print_info: ssm_dt_rank      = 0
0.00.060.385 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.385 I print_info: model type       = 1.4B
0.00.060.385 I print_info: model params     = 1.41 B
0.00.060.385 I print_info: general.name     = 1.4B
0.00.060.387 I print_info: vocab type       = BPE
0.00.060.387 I print_info: n_vocab          = 50304
0.00.060.387 I print_info: n_merges         = 50009
0.00.060.387 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.388 I print_info: LF token         = 128 'Ä'
0.00.060.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.389 I print_info: max token length = 1024
0.00.062.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.811 I load_tensors: offloading output layer to GPU
0.00.062.811 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.823 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.825 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.063.163 I llama_init_from_model: n_seq_max     = 1
0.00.063.164 I llama_init_from_model: n_ctx         = 2048
0.00.063.164 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.063.164 I llama_init_from_model: n_batch       = 2048
0.00.063.164 I llama_init_from_model: n_ubatch      = 512
0.00.063.164 I llama_init_from_model: flash_attn    = 0
0.00.063.165 I llama_init_from_model: freq_base     = 10000.0
0.00.063.165 I llama_init_from_model: freq_scale    = 1
0.00.063.166 I ggml_metal_init: allocating
0.00.063.170 I ggml_metal_init: found device: Apple M4
0.00.063.172 I ggml_metal_init: picking default device: Apple M4
0.00.063.913 I ggml_metal_init: using embedded metal library
0.00.066.528 I ggml_metal_init: GPU name:   Apple M4
0.00.066.530 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.531 I ggml_metal_init: simdgroup reduction   = true
0.00.066.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.532 I ggml_metal_init: has bfloat            = true
0.00.066.532 I ggml_metal_init: use bfloat            = true
0.00.066.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.975 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.262 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.271 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.298 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.626 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.629 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.629 I llama_init_from_model: graph nodes  = 967
0.00.104.629 I llama_init_from_model: graph splits = 2
0.00.104.634 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.300.149 I main: llama threadpool init, n_threads = 4
0.01.300.223 I 
0.01.300.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.300.301 I 
0.01.300.594 I sampler seed: 1234
0.01.300.602 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.300.671 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.300.677 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.300.677 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.385.543 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.02.385.544 I llama_perf_context_print:        load time =    1290.34 ms
0.02.385.544 I llama_perf_context_print: prompt eval time =      40.43 ms /     7 tokens (    5.78 ms per token,   173.14 tokens per second)
0.02.385.545 I llama_perf_context_print:        eval time =    1041.36 ms /    63 runs   (   16.53 ms per token,    60.50 tokens per second)
0.02.385.545 I llama_perf_context_print:       total time =    1085.40 ms /    70 tokens
0.02.385.772 I ggml_metal_free: deallocating

real	0m2.405s
user	0m0.123s
sys	0m0.240s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.408 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.422 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.177 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.225 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.944 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.944 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.945 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.946 I llama_model_loader: - type  f32:  194 tensors
0.00.026.946 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.946 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.947 I print_info: file format = GGUF V3 (latest)
0.00.026.948 I print_info: file type   = Q4_0
0.00.026.949 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.722 I load: special tokens cache size = 25
0.00.051.902 I load: token to piece cache size = 0.2984 MB
0.00.051.905 I print_info: arch             = gptneox
0.00.051.905 I print_info: vocab_only       = 0
0.00.051.905 I print_info: n_ctx_train      = 2048
0.00.051.905 I print_info: n_embd           = 2048
0.00.051.905 I print_info: n_layer          = 24
0.00.051.910 I print_info: n_head           = 16
0.00.051.911 I print_info: n_head_kv        = 16
0.00.051.914 I print_info: n_rot            = 32
0.00.051.914 I print_info: n_swa            = 0
0.00.051.915 I print_info: n_embd_head_k    = 128
0.00.051.915 I print_info: n_embd_head_v    = 128
0.00.051.916 I print_info: n_gqa            = 1
0.00.051.916 I print_info: n_embd_k_gqa     = 2048
0.00.051.917 I print_info: n_embd_v_gqa     = 2048
0.00.051.919 I print_info: f_norm_eps       = 1.0e-05
0.00.051.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.920 I print_info: f_logit_scale    = 0.0e+00
0.00.051.921 I print_info: n_ff             = 8192
0.00.051.922 I print_info: n_expert         = 0
0.00.051.922 I print_info: n_expert_used    = 0
0.00.051.922 I print_info: causal attn      = 1
0.00.051.922 I print_info: pooling type     = 0
0.00.051.922 I print_info: rope type        = 2
0.00.051.923 I print_info: rope scaling     = linear
0.00.051.924 I print_info: freq_base_train  = 10000.0
0.00.051.924 I print_info: freq_scale_train = 1
0.00.051.924 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.925 I print_info: rope_finetuned   = unknown
0.00.051.925 I print_info: ssm_d_conv       = 0
0.00.051.925 I print_info: ssm_d_inner      = 0
0.00.051.925 I print_info: ssm_d_state      = 0
0.00.051.925 I print_info: ssm_dt_rank      = 0
0.00.051.926 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.926 I print_info: model type       = 1.4B
0.00.051.927 I print_info: model params     = 1.41 B
0.00.051.927 I print_info: general.name     = 1.4B
0.00.051.927 I print_info: vocab type       = BPE
0.00.051.928 I print_info: n_vocab          = 50304
0.00.051.928 I print_info: n_merges         = 50009
0.00.051.928 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.929 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.929 I print_info: LF token         = 128 'Ä'
0.00.051.929 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.929 I print_info: max token length = 1024
0.00.054.250 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.250 I load_tensors: offloading output layer to GPU
0.00.054.250 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.262 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.263 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.604 I llama_init_from_model: n_seq_max     = 1
0.00.054.604 I llama_init_from_model: n_ctx         = 2048
0.00.054.604 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.605 I llama_init_from_model: n_batch       = 2048
0.00.054.605 I llama_init_from_model: n_ubatch      = 512
0.00.054.605 I llama_init_from_model: flash_attn    = 0
0.00.054.605 I llama_init_from_model: freq_base     = 10000.0
0.00.054.606 I llama_init_from_model: freq_scale    = 1
0.00.054.606 I ggml_metal_init: allocating
0.00.054.609 I ggml_metal_init: found device: Apple M4
0.00.054.611 I ggml_metal_init: picking default device: Apple M4
0.00.055.350 I ggml_metal_init: using embedded metal library
0.00.057.881 I ggml_metal_init: GPU name:   Apple M4
0.00.057.883 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.884 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.884 I ggml_metal_init: simdgroup reduction   = true
0.00.057.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.884 I ggml_metal_init: has bfloat            = true
0.00.057.884 I ggml_metal_init: use bfloat            = true
0.00.057.885 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.991 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.589 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.598 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.623 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.869 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.872 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.872 I llama_init_from_model: graph nodes  = 967
0.00.093.873 I llama_init_from_model: graph splits = 2
0.00.093.876 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.495 I main: llama threadpool init, n_threads = 4
0.00.658.538 I 
0.00.658.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.569 I 
0.00.658.797 I sampler seed: 1234
0.00.658.802 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.813 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.813 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.813 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.335.119 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.335.120 I llama_perf_context_print:        load time =     647.62 ms
0.01.335.121 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.08 tokens per second)
0.01.335.123 I llama_perf_context_print:        eval time =     629.59 ms /    63 runs   (    9.99 ms per token,   100.07 tokens per second)
0.01.335.123 I llama_perf_context_print:       total time =     676.63 ms /    70 tokens
0.01.335.347 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.110s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.783 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.392 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.403 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.411 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.412 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.940 I llama_model_loader: - type  f32:  194 tensors
0.00.026.940 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.940 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.941 I print_info: file format = GGUF V3 (latest)
0.00.026.941 I print_info: file type   = Q4_1
0.00.026.942 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.482 I load: special tokens cache size = 25
0.00.051.268 I load: token to piece cache size = 0.2984 MB
0.00.051.270 I print_info: arch             = gptneox
0.00.051.271 I print_info: vocab_only       = 0
0.00.051.271 I print_info: n_ctx_train      = 2048
0.00.051.271 I print_info: n_embd           = 2048
0.00.051.271 I print_info: n_layer          = 24
0.00.051.274 I print_info: n_head           = 16
0.00.051.275 I print_info: n_head_kv        = 16
0.00.051.275 I print_info: n_rot            = 32
0.00.051.276 I print_info: n_swa            = 0
0.00.051.276 I print_info: n_embd_head_k    = 128
0.00.051.276 I print_info: n_embd_head_v    = 128
0.00.051.277 I print_info: n_gqa            = 1
0.00.051.277 I print_info: n_embd_k_gqa     = 2048
0.00.051.278 I print_info: n_embd_v_gqa     = 2048
0.00.051.279 I print_info: f_norm_eps       = 1.0e-05
0.00.051.279 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.279 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.280 I print_info: f_logit_scale    = 0.0e+00
0.00.051.281 I print_info: n_ff             = 8192
0.00.051.282 I print_info: n_expert         = 0
0.00.051.282 I print_info: n_expert_used    = 0
0.00.051.282 I print_info: causal attn      = 1
0.00.051.282 I print_info: pooling type     = 0
0.00.051.282 I print_info: rope type        = 2
0.00.051.282 I print_info: rope scaling     = linear
0.00.051.285 I print_info: freq_base_train  = 10000.0
0.00.051.285 I print_info: freq_scale_train = 1
0.00.051.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.286 I print_info: rope_finetuned   = unknown
0.00.051.286 I print_info: ssm_d_conv       = 0
0.00.051.286 I print_info: ssm_d_inner      = 0
0.00.051.286 I print_info: ssm_d_state      = 0
0.00.051.286 I print_info: ssm_dt_rank      = 0
0.00.051.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.287 I print_info: model type       = 1.4B
0.00.051.287 I print_info: model params     = 1.41 B
0.00.051.287 I print_info: general.name     = 1.4B
0.00.051.288 I print_info: vocab type       = BPE
0.00.051.288 I print_info: n_vocab          = 50304
0.00.051.288 I print_info: n_merges         = 50009
0.00.051.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.292 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.293 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.293 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.293 I print_info: LF token         = 128 'Ä'
0.00.051.293 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.294 I print_info: max token length = 1024
0.00.053.248 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.248 I load_tensors: offloading output layer to GPU
0.00.053.248 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.259 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.260 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.543 I llama_init_from_model: n_seq_max     = 1
0.00.053.544 I llama_init_from_model: n_ctx         = 2048
0.00.053.544 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.544 I llama_init_from_model: n_batch       = 2048
0.00.053.544 I llama_init_from_model: n_ubatch      = 512
0.00.053.544 I llama_init_from_model: flash_attn    = 0
0.00.053.545 I llama_init_from_model: freq_base     = 10000.0
0.00.053.545 I llama_init_from_model: freq_scale    = 1
0.00.053.545 I ggml_metal_init: allocating
0.00.053.548 I ggml_metal_init: found device: Apple M4
0.00.053.550 I ggml_metal_init: picking default device: Apple M4
0.00.054.133 I ggml_metal_init: using embedded metal library
0.00.056.452 I ggml_metal_init: GPU name:   Apple M4
0.00.056.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.454 I ggml_metal_init: simdgroup reduction   = true
0.00.056.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.455 I ggml_metal_init: has bfloat            = true
0.00.056.455 I ggml_metal_init: use bfloat            = true
0.00.056.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.133 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.291 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.300 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.326 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.366 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.367 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.368 I llama_init_from_model: graph nodes  = 967
0.00.086.368 I llama_init_from_model: graph splits = 2
0.00.086.372 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.508 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.015 I main: llama threadpool init, n_threads = 4
0.00.688.060 I 
0.00.688.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.089 I 
0.00.688.311 I sampler seed: 1234
0.00.688.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.688.363 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.688.379 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.688.379 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.408.716 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62445.03 tokens per second)
0.01.408.716 I llama_perf_context_print:        load time =     677.22 ms
0.01.408.718 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.42 tokens per second)
0.01.408.719 I llama_perf_context_print:        eval time =     673.82 ms /    63 runs   (   10.70 ms per token,    93.50 tokens per second)
0.01.408.719 I llama_perf_context_print:       total time =     720.71 ms /    70 tokens
0.01.408.916 I ggml_metal_free: deallocating

real	0m1.426s
user	0m0.107s
sys	0m0.148s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.809 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.055 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.065 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.607 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.608 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.608 I llama_model_loader: - type  f32:  194 tensors
0.00.026.609 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.609 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.609 I print_info: file format = GGUF V3 (latest)
0.00.026.609 I print_info: file type   = Q5_0
0.00.026.610 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.211 I load: special tokens cache size = 25
0.00.051.126 I load: token to piece cache size = 0.2984 MB
0.00.051.129 I print_info: arch             = gptneox
0.00.051.130 I print_info: vocab_only       = 0
0.00.051.130 I print_info: n_ctx_train      = 2048
0.00.051.130 I print_info: n_embd           = 2048
0.00.051.130 I print_info: n_layer          = 24
0.00.051.133 I print_info: n_head           = 16
0.00.051.134 I print_info: n_head_kv        = 16
0.00.051.134 I print_info: n_rot            = 32
0.00.051.136 I print_info: n_swa            = 0
0.00.051.136 I print_info: n_embd_head_k    = 128
0.00.051.136 I print_info: n_embd_head_v    = 128
0.00.051.137 I print_info: n_gqa            = 1
0.00.051.138 I print_info: n_embd_k_gqa     = 2048
0.00.051.143 I print_info: n_embd_v_gqa     = 2048
0.00.051.143 I print_info: f_norm_eps       = 1.0e-05
0.00.051.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.146 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.146 I print_info: f_logit_scale    = 0.0e+00
0.00.051.147 I print_info: n_ff             = 8192
0.00.051.147 I print_info: n_expert         = 0
0.00.051.147 I print_info: n_expert_used    = 0
0.00.051.147 I print_info: causal attn      = 1
0.00.051.149 I print_info: pooling type     = 0
0.00.051.150 I print_info: rope type        = 2
0.00.051.151 I print_info: rope scaling     = linear
0.00.051.151 I print_info: freq_base_train  = 10000.0
0.00.051.151 I print_info: freq_scale_train = 1
0.00.051.151 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.152 I print_info: rope_finetuned   = unknown
0.00.051.152 I print_info: ssm_d_conv       = 0
0.00.051.152 I print_info: ssm_d_inner      = 0
0.00.051.152 I print_info: ssm_d_state      = 0
0.00.051.152 I print_info: ssm_dt_rank      = 0
0.00.051.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.152 I print_info: model type       = 1.4B
0.00.051.153 I print_info: model params     = 1.41 B
0.00.051.153 I print_info: general.name     = 1.4B
0.00.051.153 I print_info: vocab type       = BPE
0.00.051.154 I print_info: n_vocab          = 50304
0.00.051.154 I print_info: n_merges         = 50009
0.00.051.156 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.157 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.157 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.157 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.157 I print_info: LF token         = 128 'Ä'
0.00.051.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.159 I print_info: max token length = 1024
0.00.053.137 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.137 I load_tensors: offloading output layer to GPU
0.00.053.137 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.148 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.149 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.436 I llama_init_from_model: n_seq_max     = 1
0.00.053.437 I llama_init_from_model: n_ctx         = 2048
0.00.053.437 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.437 I llama_init_from_model: n_batch       = 2048
0.00.053.437 I llama_init_from_model: n_ubatch      = 512
0.00.053.437 I llama_init_from_model: flash_attn    = 0
0.00.053.438 I llama_init_from_model: freq_base     = 10000.0
0.00.053.438 I llama_init_from_model: freq_scale    = 1
0.00.053.439 I ggml_metal_init: allocating
0.00.053.442 I ggml_metal_init: found device: Apple M4
0.00.053.444 I ggml_metal_init: picking default device: Apple M4
0.00.054.051 I ggml_metal_init: using embedded metal library
0.00.056.387 I ggml_metal_init: GPU name:   Apple M4
0.00.056.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.390 I ggml_metal_init: simdgroup reduction   = true
0.00.056.390 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.390 I ggml_metal_init: has bfloat            = true
0.00.056.390 I ggml_metal_init: use bfloat            = true
0.00.056.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.242 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.571 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.576 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.594 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.732 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.734 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.734 I llama_init_from_model: graph nodes  = 967
0.00.086.735 I llama_init_from_model: graph splits = 2
0.00.086.737 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.919 I main: llama threadpool init, n_threads = 4
0.00.743.956 I 
0.00.744.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.003 I 
0.00.744.237 I sampler seed: 1234
0.00.744.243 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.292 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.296 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.297 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.523.019 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.523.020 I llama_perf_context_print:        load time =     733.11 ms
0.01.523.022 I llama_perf_context_print: prompt eval time =      43.12 ms /     7 tokens (    6.16 ms per token,   162.35 tokens per second)
0.01.523.022 I llama_perf_context_print:        eval time =     732.70 ms /    63 runs   (   11.63 ms per token,    85.98 tokens per second)
0.01.523.023 I llama_perf_context_print:       total time =     779.10 ms /    70 tokens
0.01.523.230 I ggml_metal_free: deallocating

real	0m1.541s
user	0m0.109s
sys	0m0.157s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.963 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.289 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.289 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.290 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.291 I llama_model_loader: - type  f32:  194 tensors
0.00.026.291 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.291 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.292 I print_info: file format = GGUF V3 (latest)
0.00.026.293 I print_info: file type   = Q5_1
0.00.026.293 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.701 I load: special tokens cache size = 25
0.00.051.626 I load: token to piece cache size = 0.2984 MB
0.00.051.629 I print_info: arch             = gptneox
0.00.051.629 I print_info: vocab_only       = 0
0.00.051.629 I print_info: n_ctx_train      = 2048
0.00.051.629 I print_info: n_embd           = 2048
0.00.051.629 I print_info: n_layer          = 24
0.00.051.633 I print_info: n_head           = 16
0.00.051.634 I print_info: n_head_kv        = 16
0.00.051.634 I print_info: n_rot            = 32
0.00.051.634 I print_info: n_swa            = 0
0.00.051.634 I print_info: n_embd_head_k    = 128
0.00.051.637 I print_info: n_embd_head_v    = 128
0.00.051.637 I print_info: n_gqa            = 1
0.00.051.638 I print_info: n_embd_k_gqa     = 2048
0.00.051.639 I print_info: n_embd_v_gqa     = 2048
0.00.051.640 I print_info: f_norm_eps       = 1.0e-05
0.00.051.640 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.640 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.640 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.641 I print_info: f_logit_scale    = 0.0e+00
0.00.051.643 I print_info: n_ff             = 8192
0.00.051.643 I print_info: n_expert         = 0
0.00.051.643 I print_info: n_expert_used    = 0
0.00.051.643 I print_info: causal attn      = 1
0.00.051.643 I print_info: pooling type     = 0
0.00.051.645 I print_info: rope type        = 2
0.00.051.645 I print_info: rope scaling     = linear
0.00.051.646 I print_info: freq_base_train  = 10000.0
0.00.051.646 I print_info: freq_scale_train = 1
0.00.051.646 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.646 I print_info: rope_finetuned   = unknown
0.00.051.647 I print_info: ssm_d_conv       = 0
0.00.051.647 I print_info: ssm_d_inner      = 0
0.00.051.647 I print_info: ssm_d_state      = 0
0.00.051.651 I print_info: ssm_dt_rank      = 0
0.00.051.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.651 I print_info: model type       = 1.4B
0.00.051.652 I print_info: model params     = 1.41 B
0.00.051.652 I print_info: general.name     = 1.4B
0.00.051.652 I print_info: vocab type       = BPE
0.00.051.652 I print_info: n_vocab          = 50304
0.00.051.653 I print_info: n_merges         = 50009
0.00.051.653 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.653 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: LF token         = 128 'Ä'
0.00.051.655 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.655 I print_info: max token length = 1024
0.00.053.419 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.419 I load_tensors: offloading output layer to GPU
0.00.053.419 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.428 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.429 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.690 I llama_init_from_model: n_seq_max     = 1
0.00.053.690 I llama_init_from_model: n_ctx         = 2048
0.00.053.690 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.691 I llama_init_from_model: n_batch       = 2048
0.00.053.691 I llama_init_from_model: n_ubatch      = 512
0.00.053.691 I llama_init_from_model: flash_attn    = 0
0.00.053.691 I llama_init_from_model: freq_base     = 10000.0
0.00.053.692 I llama_init_from_model: freq_scale    = 1
0.00.053.692 I ggml_metal_init: allocating
0.00.053.695 I ggml_metal_init: found device: Apple M4
0.00.053.697 I ggml_metal_init: picking default device: Apple M4
0.00.054.286 I ggml_metal_init: using embedded metal library
0.00.056.587 I ggml_metal_init: GPU name:   Apple M4
0.00.056.588 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.589 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.589 I ggml_metal_init: simdgroup reduction   = true
0.00.056.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.590 I ggml_metal_init: has bfloat            = true
0.00.056.590 I ggml_metal_init: use bfloat            = true
0.00.056.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.591 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.059 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.910 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.920 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.944 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.885 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.886 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.887 I llama_init_from_model: graph nodes  = 967
0.00.085.887 I llama_init_from_model: graph splits = 2
0.00.085.894 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.023 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.024 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.413 I main: llama threadpool init, n_threads = 4
0.00.821.451 I 
0.00.821.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.494 I 
0.00.821.726 I sampler seed: 1234
0.00.821.731 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.821.786 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.821.790 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.821.790 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.656.553 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.656.554 I llama_perf_context_print:        load time =     811.45 ms
0.01.656.558 I llama_perf_context_print: prompt eval time =      42.25 ms /     7 tokens (    6.04 ms per token,   165.67 tokens per second)
0.01.656.560 I llama_perf_context_print:        eval time =     789.56 ms /    63 runs   (   12.53 ms per token,    79.79 tokens per second)
0.01.656.561 I llama_perf_context_print:       total time =     835.14 ms /    70 tokens
0.01.656.827 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.109s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.151 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.961 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.966 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.972 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.973 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.973 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.974 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.978 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.814 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.530 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.531 I llama_model_loader: - type  f32:  194 tensors
0.00.025.531 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.532 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.532 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.533 I print_info: file format = GGUF V3 (latest)
0.00.025.533 I print_info: file type   = Q2_K - Medium
0.00.025.534 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.999 I load: special tokens cache size = 25
0.00.051.060 I load: token to piece cache size = 0.2984 MB
0.00.051.063 I print_info: arch             = gptneox
0.00.051.063 I print_info: vocab_only       = 0
0.00.051.063 I print_info: n_ctx_train      = 2048
0.00.051.064 I print_info: n_embd           = 2048
0.00.051.064 I print_info: n_layer          = 24
0.00.051.067 I print_info: n_head           = 16
0.00.051.067 I print_info: n_head_kv        = 16
0.00.051.068 I print_info: n_rot            = 32
0.00.051.068 I print_info: n_swa            = 0
0.00.051.068 I print_info: n_embd_head_k    = 128
0.00.051.068 I print_info: n_embd_head_v    = 128
0.00.051.069 I print_info: n_gqa            = 1
0.00.051.070 I print_info: n_embd_k_gqa     = 2048
0.00.051.070 I print_info: n_embd_v_gqa     = 2048
0.00.051.071 I print_info: f_norm_eps       = 1.0e-05
0.00.051.074 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.074 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.074 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.074 I print_info: f_logit_scale    = 0.0e+00
0.00.051.075 I print_info: n_ff             = 8192
0.00.051.075 I print_info: n_expert         = 0
0.00.051.075 I print_info: n_expert_used    = 0
0.00.051.075 I print_info: causal attn      = 1
0.00.051.077 I print_info: pooling type     = 0
0.00.051.078 I print_info: rope type        = 2
0.00.051.078 I print_info: rope scaling     = linear
0.00.051.079 I print_info: freq_base_train  = 10000.0
0.00.051.079 I print_info: freq_scale_train = 1
0.00.051.079 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.079 I print_info: rope_finetuned   = unknown
0.00.051.080 I print_info: ssm_d_conv       = 0
0.00.051.080 I print_info: ssm_d_inner      = 0
0.00.051.080 I print_info: ssm_d_state      = 0
0.00.051.080 I print_info: ssm_dt_rank      = 0
0.00.051.080 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.080 I print_info: model type       = 1.4B
0.00.051.081 I print_info: model params     = 1.41 B
0.00.051.081 I print_info: general.name     = 1.4B
0.00.051.082 I print_info: vocab type       = BPE
0.00.051.082 I print_info: n_vocab          = 50304
0.00.051.084 I print_info: n_merges         = 50009
0.00.051.084 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.084 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.084 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: LF token         = 128 'Ä'
0.00.051.085 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.085 I print_info: max token length = 1024
0.00.053.024 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.024 I load_tensors: offloading output layer to GPU
0.00.053.024 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.035 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.036 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.327 I llama_init_from_model: n_seq_max     = 1
0.00.053.328 I llama_init_from_model: n_ctx         = 2048
0.00.053.328 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.329 I llama_init_from_model: n_batch       = 2048
0.00.053.329 I llama_init_from_model: n_ubatch      = 512
0.00.053.329 I llama_init_from_model: flash_attn    = 0
0.00.053.329 I llama_init_from_model: freq_base     = 10000.0
0.00.053.330 I llama_init_from_model: freq_scale    = 1
0.00.053.330 I ggml_metal_init: allocating
0.00.053.333 I ggml_metal_init: found device: Apple M4
0.00.053.335 I ggml_metal_init: picking default device: Apple M4
0.00.053.943 I ggml_metal_init: using embedded metal library
0.00.056.320 I ggml_metal_init: GPU name:   Apple M4
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.323 I ggml_metal_init: simdgroup reduction   = true
0.00.056.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.323 I ggml_metal_init: has bfloat            = true
0.00.056.323 I ggml_metal_init: use bfloat            = true
0.00.056.323 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.058 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.067 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.087 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.120 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.122 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.122 I llama_init_from_model: graph nodes  = 967
0.00.086.122 I llama_init_from_model: graph splits = 2
0.00.086.125 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.241 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.543 I main: llama threadpool init, n_threads = 4
0.00.471.585 I 
0.00.471.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.616 I 
0.00.471.845 I sampler seed: 1234
0.00.471.850 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.471.892 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.471.892 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.471.892 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.150.458 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.150.458 I llama_perf_context_print:        load time =     461.39 ms
0.01.150.459 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.82 tokens per second)
0.01.150.460 I llama_perf_context_print:        eval time =     639.91 ms /    63 runs   (   10.16 ms per token,    98.45 tokens per second)
0.01.150.461 I llama_perf_context_print:       total time =     678.92 ms /    70 tokens
0.01.150.658 I ggml_metal_free: deallocating

real	0m1.170s
user	0m0.110s
sys	0m0.110s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.019.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.501 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.133 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.133 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.134 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.028.134 I llama_model_loader: - type  f32:  194 tensors
0.00.028.135 I llama_model_loader: - type q3_K:   25 tensors
0.00.028.135 I llama_model_loader: - type q4_K:   71 tensors
0.00.028.135 I llama_model_loader: - type q5_K:    1 tensors
0.00.028.135 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.136 I print_info: file format = GGUF V3 (latest)
0.00.028.137 I print_info: file type   = Q3_K - Medium
0.00.028.137 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.046.829 I load: special tokens cache size = 25
0.00.052.861 I load: token to piece cache size = 0.2984 MB
0.00.052.864 I print_info: arch             = gptneox
0.00.052.864 I print_info: vocab_only       = 0
0.00.052.865 I print_info: n_ctx_train      = 2048
0.00.052.865 I print_info: n_embd           = 2048
0.00.052.865 I print_info: n_layer          = 24
0.00.052.868 I print_info: n_head           = 16
0.00.052.869 I print_info: n_head_kv        = 16
0.00.052.869 I print_info: n_rot            = 32
0.00.052.869 I print_info: n_swa            = 0
0.00.052.870 I print_info: n_embd_head_k    = 128
0.00.052.870 I print_info: n_embd_head_v    = 128
0.00.052.871 I print_info: n_gqa            = 1
0.00.052.871 I print_info: n_embd_k_gqa     = 2048
0.00.052.872 I print_info: n_embd_v_gqa     = 2048
0.00.052.873 I print_info: f_norm_eps       = 1.0e-05
0.00.052.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.873 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.873 I print_info: f_logit_scale    = 0.0e+00
0.00.052.874 I print_info: n_ff             = 8192
0.00.052.874 I print_info: n_expert         = 0
0.00.052.874 I print_info: n_expert_used    = 0
0.00.052.876 I print_info: causal attn      = 1
0.00.052.878 I print_info: pooling type     = 0
0.00.052.878 I print_info: rope type        = 2
0.00.052.878 I print_info: rope scaling     = linear
0.00.052.879 I print_info: freq_base_train  = 10000.0
0.00.052.879 I print_info: freq_scale_train = 1
0.00.052.879 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.880 I print_info: rope_finetuned   = unknown
0.00.052.880 I print_info: ssm_d_conv       = 0
0.00.052.880 I print_info: ssm_d_inner      = 0
0.00.052.880 I print_info: ssm_d_state      = 0
0.00.052.880 I print_info: ssm_dt_rank      = 0
0.00.052.880 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.881 I print_info: model type       = 1.4B
0.00.052.881 I print_info: model params     = 1.41 B
0.00.052.881 I print_info: general.name     = 1.4B
0.00.052.882 I print_info: vocab type       = BPE
0.00.052.882 I print_info: n_vocab          = 50304
0.00.052.882 I print_info: n_merges         = 50009
0.00.052.882 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.883 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.883 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.883 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.883 I print_info: LF token         = 128 'Ä'
0.00.052.883 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.884 I print_info: max token length = 1024
0.00.054.792 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.792 I load_tensors: offloading output layer to GPU
0.00.054.792 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.803 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.804 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.055.101 I llama_init_from_model: n_seq_max     = 1
0.00.055.102 I llama_init_from_model: n_ctx         = 2048
0.00.055.102 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.103 I llama_init_from_model: n_batch       = 2048
0.00.055.103 I llama_init_from_model: n_ubatch      = 512
0.00.055.103 I llama_init_from_model: flash_attn    = 0
0.00.055.103 I llama_init_from_model: freq_base     = 10000.0
0.00.055.103 I llama_init_from_model: freq_scale    = 1
0.00.055.104 I ggml_metal_init: allocating
0.00.055.107 I ggml_metal_init: found device: Apple M4
0.00.055.109 I ggml_metal_init: picking default device: Apple M4
0.00.055.692 I ggml_metal_init: using embedded metal library
0.00.058.031 I ggml_metal_init: GPU name:   Apple M4
0.00.058.032 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.033 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.033 I ggml_metal_init: simdgroup reduction   = true
0.00.058.034 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.034 I ggml_metal_init: has bfloat            = true
0.00.058.034 I ggml_metal_init: use bfloat            = true
0.00.058.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.688 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.164 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.169 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.186 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.305 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.307 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.307 I llama_init_from_model: graph nodes  = 967
0.00.087.307 I llama_init_from_model: graph splits = 2
0.00.087.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.440 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.839 I main: llama threadpool init, n_threads = 4
0.00.532.879 I 
0.00.532.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.928 I 
0.00.533.150 I sampler seed: 1234
0.00.533.153 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.533.164 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.533.165 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.533.165 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.280.219 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.280.220 I llama_perf_context_print:        load time =     520.99 ms
0.01.280.221 I llama_perf_context_print: prompt eval time =      43.49 ms /     7 tokens (    6.21 ms per token,   160.94 tokens per second)
0.01.280.222 I llama_perf_context_print:        eval time =     700.53 ms /    63 runs   (   11.12 ms per token,    89.93 tokens per second)
0.01.280.222 I llama_perf_context_print:       total time =     747.38 ms /    70 tokens
0.01.280.441 I ggml_metal_free: deallocating

real	0m1.298s
user	0m0.108s
sys	0m0.126s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.323 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.324 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.324 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.325 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.326 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.327 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.327 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.328 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.313 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.208 I llama_model_loader: - type  f32:  194 tensors
0.00.025.209 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.209 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.209 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.210 I print_info: file format = GGUF V3 (latest)
0.00.025.210 I print_info: file type   = Q4_K - Medium
0.00.025.215 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.830 I load: special tokens cache size = 25
0.00.049.718 I load: token to piece cache size = 0.2984 MB
0.00.049.721 I print_info: arch             = gptneox
0.00.049.721 I print_info: vocab_only       = 0
0.00.049.721 I print_info: n_ctx_train      = 2048
0.00.049.722 I print_info: n_embd           = 2048
0.00.049.722 I print_info: n_layer          = 24
0.00.049.724 I print_info: n_head           = 16
0.00.049.725 I print_info: n_head_kv        = 16
0.00.049.725 I print_info: n_rot            = 32
0.00.049.725 I print_info: n_swa            = 0
0.00.049.725 I print_info: n_embd_head_k    = 128
0.00.049.726 I print_info: n_embd_head_v    = 128
0.00.049.727 I print_info: n_gqa            = 1
0.00.049.728 I print_info: n_embd_k_gqa     = 2048
0.00.049.730 I print_info: n_embd_v_gqa     = 2048
0.00.049.731 I print_info: f_norm_eps       = 1.0e-05
0.00.049.731 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.731 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.732 I print_info: f_logit_scale    = 0.0e+00
0.00.049.732 I print_info: n_ff             = 8192
0.00.049.733 I print_info: n_expert         = 0
0.00.049.733 I print_info: n_expert_used    = 0
0.00.049.733 I print_info: causal attn      = 1
0.00.049.733 I print_info: pooling type     = 0
0.00.049.733 I print_info: rope type        = 2
0.00.049.733 I print_info: rope scaling     = linear
0.00.049.734 I print_info: freq_base_train  = 10000.0
0.00.049.734 I print_info: freq_scale_train = 1
0.00.049.734 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.734 I print_info: rope_finetuned   = unknown
0.00.049.735 I print_info: ssm_d_conv       = 0
0.00.049.735 I print_info: ssm_d_inner      = 0
0.00.049.736 I print_info: ssm_d_state      = 0
0.00.049.736 I print_info: ssm_dt_rank      = 0
0.00.049.736 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.737 I print_info: model type       = 1.4B
0.00.049.737 I print_info: model params     = 1.41 B
0.00.049.737 I print_info: general.name     = 1.4B
0.00.049.738 I print_info: vocab type       = BPE
0.00.049.738 I print_info: n_vocab          = 50304
0.00.049.738 I print_info: n_merges         = 50009
0.00.049.738 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.739 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.739 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.739 I print_info: LF token         = 128 'Ä'
0.00.049.743 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.743 I print_info: max token length = 1024
0.00.051.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.550 I load_tensors: offloading output layer to GPU
0.00.051.550 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.555 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.555 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.838 I llama_init_from_model: n_seq_max     = 1
0.00.051.839 I llama_init_from_model: n_ctx         = 2048
0.00.051.839 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.839 I llama_init_from_model: n_batch       = 2048
0.00.051.839 I llama_init_from_model: n_ubatch      = 512
0.00.051.839 I llama_init_from_model: flash_attn    = 0
0.00.051.840 I llama_init_from_model: freq_base     = 10000.0
0.00.051.840 I llama_init_from_model: freq_scale    = 1
0.00.051.841 I ggml_metal_init: allocating
0.00.051.844 I ggml_metal_init: found device: Apple M4
0.00.051.846 I ggml_metal_init: picking default device: Apple M4
0.00.052.422 I ggml_metal_init: using embedded metal library
0.00.054.770 I ggml_metal_init: GPU name:   Apple M4
0.00.054.771 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.772 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.772 I ggml_metal_init: simdgroup reduction   = true
0.00.054.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.772 I ggml_metal_init: has bfloat            = true
0.00.054.773 I ggml_metal_init: use bfloat            = true
0.00.054.773 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.416 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.119 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.128 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.160 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.185 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.186 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.186 I llama_init_from_model: graph nodes  = 967
0.00.084.187 I llama_init_from_model: graph splits = 2
0.00.084.192 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.362 I main: llama threadpool init, n_threads = 4
0.00.603.407 I 
0.00.603.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.455 I 
0.00.603.675 I sampler seed: 1234
0.00.603.680 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.603.707 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.603.709 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.603.709 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.369.104 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55167.06 tokens per second)
0.01.369.104 I llama_perf_context_print:        load time =     594.40 ms
0.01.369.105 I llama_perf_context_print: prompt eval time =      51.06 ms /     7 tokens (    7.29 ms per token,   137.09 tokens per second)
0.01.369.108 I llama_perf_context_print:        eval time =     711.21 ms /    63 runs   (   11.29 ms per token,    88.58 tokens per second)
0.01.369.110 I llama_perf_context_print:       total time =     765.75 ms /    70 tokens
0.01.369.309 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.108s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.245 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.461 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.023.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.472 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.413 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.285 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.032.286 I llama_model_loader: - type  f32:  194 tensors
0.00.032.286 I llama_model_loader: - type q5_K:   61 tensors
0.00.032.286 I llama_model_loader: - type q6_K:   37 tensors
0.00.032.287 I print_info: file format = GGUF V3 (latest)
0.00.032.287 I print_info: file type   = Q5_K - Medium
0.00.032.289 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.052.349 I load: special tokens cache size = 25
0.00.058.095 I load: token to piece cache size = 0.2984 MB
0.00.058.099 I print_info: arch             = gptneox
0.00.058.099 I print_info: vocab_only       = 0
0.00.058.100 I print_info: n_ctx_train      = 2048
0.00.058.100 I print_info: n_embd           = 2048
0.00.058.100 I print_info: n_layer          = 24
0.00.058.104 I print_info: n_head           = 16
0.00.058.104 I print_info: n_head_kv        = 16
0.00.058.105 I print_info: n_rot            = 32
0.00.058.105 I print_info: n_swa            = 0
0.00.058.105 I print_info: n_embd_head_k    = 128
0.00.058.105 I print_info: n_embd_head_v    = 128
0.00.058.105 I print_info: n_gqa            = 1
0.00.058.106 I print_info: n_embd_k_gqa     = 2048
0.00.058.107 I print_info: n_embd_v_gqa     = 2048
0.00.058.107 I print_info: f_norm_eps       = 1.0e-05
0.00.058.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.107 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.108 I print_info: f_logit_scale    = 0.0e+00
0.00.058.108 I print_info: n_ff             = 8192
0.00.058.108 I print_info: n_expert         = 0
0.00.058.111 I print_info: n_expert_used    = 0
0.00.058.111 I print_info: causal attn      = 1
0.00.058.111 I print_info: pooling type     = 0
0.00.058.112 I print_info: rope type        = 2
0.00.058.113 I print_info: rope scaling     = linear
0.00.058.114 I print_info: freq_base_train  = 10000.0
0.00.058.114 I print_info: freq_scale_train = 1
0.00.058.114 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.114 I print_info: rope_finetuned   = unknown
0.00.058.115 I print_info: ssm_d_conv       = 0
0.00.058.115 I print_info: ssm_d_inner      = 0
0.00.058.115 I print_info: ssm_d_state      = 0
0.00.058.115 I print_info: ssm_dt_rank      = 0
0.00.058.115 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.115 I print_info: model type       = 1.4B
0.00.058.115 I print_info: model params     = 1.41 B
0.00.058.115 I print_info: general.name     = 1.4B
0.00.058.116 I print_info: vocab type       = BPE
0.00.058.117 I print_info: n_vocab          = 50304
0.00.058.117 I print_info: n_merges         = 50009
0.00.058.117 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.118 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.118 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.118 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.118 I print_info: LF token         = 128 'Ä'
0.00.058.118 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.118 I print_info: max token length = 1024
0.00.059.998 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.998 I load_tensors: offloading output layer to GPU
0.00.059.998 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.009 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.060.011 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.060.299 I llama_init_from_model: n_seq_max     = 1
0.00.060.300 I llama_init_from_model: n_ctx         = 2048
0.00.060.300 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.060.300 I llama_init_from_model: n_batch       = 2048
0.00.060.300 I llama_init_from_model: n_ubatch      = 512
0.00.060.301 I llama_init_from_model: flash_attn    = 0
0.00.060.301 I llama_init_from_model: freq_base     = 10000.0
0.00.060.301 I llama_init_from_model: freq_scale    = 1
0.00.060.302 I ggml_metal_init: allocating
0.00.060.306 I ggml_metal_init: found device: Apple M4
0.00.060.308 I ggml_metal_init: picking default device: Apple M4
0.00.060.941 I ggml_metal_init: using embedded metal library
0.00.063.306 I ggml_metal_init: GPU name:   Apple M4
0.00.063.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.308 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.308 I ggml_metal_init: simdgroup reduction   = true
0.00.063.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.309 I ggml_metal_init: has bfloat            = true
0.00.063.309 I ggml_metal_init: use bfloat            = true
0.00.063.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.606 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.385 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.396 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.414 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.093.404 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.093.405 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.093.406 I llama_init_from_model: graph nodes  = 967
0.00.093.406 I llama_init_from_model: graph splits = 2
0.00.093.409 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.541 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.278 I main: llama threadpool init, n_threads = 4
0.00.690.319 I 
0.00.690.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.346 I 
0.00.690.492 I sampler seed: 1234
0.00.690.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.507 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.507 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.540.504 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.01.540.505 I llama_perf_context_print:        load time =     679.03 ms
0.01.540.505 I llama_perf_context_print: prompt eval time =      51.20 ms /     7 tokens (    7.31 ms per token,   136.72 tokens per second)
0.01.540.506 I llama_perf_context_print:        eval time =     796.25 ms /    63 runs   (   12.64 ms per token,    79.12 tokens per second)
0.01.540.507 I llama_perf_context_print:       total time =     850.23 ms /    70 tokens
0.01.540.824 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.108s
sys	0m0.119s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.999 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.452 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.453 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.453 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.998 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.999 I llama_model_loader: - type  f32:  194 tensors
0.00.026.000 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.000 I print_info: file format = GGUF V3 (latest)
0.00.026.001 I print_info: file type   = Q6_K
0.00.026.001 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.968 I load: special tokens cache size = 25
0.00.051.005 I load: token to piece cache size = 0.2984 MB
0.00.051.008 I print_info: arch             = gptneox
0.00.051.008 I print_info: vocab_only       = 0
0.00.051.008 I print_info: n_ctx_train      = 2048
0.00.051.008 I print_info: n_embd           = 2048
0.00.051.009 I print_info: n_layer          = 24
0.00.051.011 I print_info: n_head           = 16
0.00.051.012 I print_info: n_head_kv        = 16
0.00.051.012 I print_info: n_rot            = 32
0.00.051.012 I print_info: n_swa            = 0
0.00.051.013 I print_info: n_embd_head_k    = 128
0.00.051.013 I print_info: n_embd_head_v    = 128
0.00.051.013 I print_info: n_gqa            = 1
0.00.051.014 I print_info: n_embd_k_gqa     = 2048
0.00.051.015 I print_info: n_embd_v_gqa     = 2048
0.00.051.015 I print_info: f_norm_eps       = 1.0e-05
0.00.051.016 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.016 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.016 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.016 I print_info: f_logit_scale    = 0.0e+00
0.00.051.017 I print_info: n_ff             = 8192
0.00.051.018 I print_info: n_expert         = 0
0.00.051.018 I print_info: n_expert_used    = 0
0.00.051.020 I print_info: causal attn      = 1
0.00.051.021 I print_info: pooling type     = 0
0.00.051.021 I print_info: rope type        = 2
0.00.051.021 I print_info: rope scaling     = linear
0.00.051.022 I print_info: freq_base_train  = 10000.0
0.00.051.022 I print_info: freq_scale_train = 1
0.00.051.022 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.022 I print_info: rope_finetuned   = unknown
0.00.051.022 I print_info: ssm_d_conv       = 0
0.00.051.022 I print_info: ssm_d_inner      = 0
0.00.051.022 I print_info: ssm_d_state      = 0
0.00.051.023 I print_info: ssm_dt_rank      = 0
0.00.051.023 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.023 I print_info: model type       = 1.4B
0.00.051.023 I print_info: model params     = 1.41 B
0.00.051.024 I print_info: general.name     = 1.4B
0.00.051.024 I print_info: vocab type       = BPE
0.00.051.024 I print_info: n_vocab          = 50304
0.00.051.024 I print_info: n_merges         = 50009
0.00.051.025 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.025 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.025 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.025 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.025 I print_info: LF token         = 128 'Ä'
0.00.051.029 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.029 I print_info: max token length = 1024
0.00.053.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.072 I load_tensors: offloading output layer to GPU
0.00.053.072 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.082 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.083 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.399 I llama_init_from_model: n_seq_max     = 1
0.00.053.399 I llama_init_from_model: n_ctx         = 2048
0.00.053.399 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.400 I llama_init_from_model: n_batch       = 2048
0.00.053.400 I llama_init_from_model: n_ubatch      = 512
0.00.053.400 I llama_init_from_model: flash_attn    = 0
0.00.053.400 I llama_init_from_model: freq_base     = 10000.0
0.00.053.401 I llama_init_from_model: freq_scale    = 1
0.00.053.401 I ggml_metal_init: allocating
0.00.053.404 I ggml_metal_init: found device: Apple M4
0.00.053.406 I ggml_metal_init: picking default device: Apple M4
0.00.053.986 I ggml_metal_init: using embedded metal library
0.00.056.381 I ggml_metal_init: GPU name:   Apple M4
0.00.056.384 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.385 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.385 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.385 I ggml_metal_init: simdgroup reduction   = true
0.00.056.386 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.386 I ggml_metal_init: has bfloat            = true
0.00.056.386 I ggml_metal_init: use bfloat            = true
0.00.056.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.786 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.544 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.552 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.578 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.591 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.594 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.594 I llama_init_from_model: graph nodes  = 967
0.00.088.594 I llama_init_from_model: graph splits = 2
0.00.088.597 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.110 I main: llama threadpool init, n_threads = 4
0.00.767.156 I 
0.00.767.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.189 I 
0.00.767.414 I sampler seed: 1234
0.00.767.419 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.433 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.433 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.639.501 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.639.502 I llama_perf_context_print:        load time =     757.10 ms
0.01.639.503 I llama_perf_context_print: prompt eval time =      54.34 ms /     7 tokens (    7.76 ms per token,   128.82 tokens per second)
0.01.639.503 I llama_perf_context_print:        eval time =     814.86 ms /    63 runs   (   12.93 ms per token,    77.31 tokens per second)
0.01.639.504 I llama_perf_context_print:       total time =     872.40 ms /    70 tokens
0.01.639.717 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.488 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.462 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.792 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.819 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.099 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.320 I llama_model_loader: - type  f32:  194 tensors
0.00.060.320 I llama_model_loader: - type  f16:   98 tensors
0.00.060.322 I print_info: file format = GGUF V3 (latest)
0.00.060.323 I print_info: file type   = all F32 (guessed)
0.00.060.327 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.089.417 I load: special tokens cache size = 25
0.00.096.228 I load: token to piece cache size = 0.2984 MB
0.00.096.232 I print_info: arch             = gptneox
0.00.096.232 I print_info: vocab_only       = 0
0.00.096.232 I print_info: n_ctx_train      = 2048
0.00.096.232 I print_info: n_embd           = 2048
0.00.096.232 I print_info: n_layer          = 24
0.00.096.235 I print_info: n_head           = 16
0.00.096.236 I print_info: n_head_kv        = 16
0.00.096.236 I print_info: n_rot            = 32
0.00.096.237 I print_info: n_swa            = 0
0.00.096.237 I print_info: n_embd_head_k    = 128
0.00.096.238 I print_info: n_embd_head_v    = 128
0.00.096.239 I print_info: n_gqa            = 1
0.00.096.239 I print_info: n_embd_k_gqa     = 2048
0.00.096.240 I print_info: n_embd_v_gqa     = 2048
0.00.096.240 I print_info: f_norm_eps       = 1.0e-05
0.00.096.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.096.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.096.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.096.241 I print_info: f_logit_scale    = 0.0e+00
0.00.096.242 I print_info: n_ff             = 8192
0.00.096.242 I print_info: n_expert         = 0
0.00.096.242 I print_info: n_expert_used    = 0
0.00.096.242 I print_info: causal attn      = 1
0.00.096.242 I print_info: pooling type     = 0
0.00.096.242 I print_info: rope type        = 2
0.00.096.243 I print_info: rope scaling     = linear
0.00.096.243 I print_info: freq_base_train  = 10000.0
0.00.096.243 I print_info: freq_scale_train = 1
0.00.096.243 I print_info: n_ctx_orig_yarn  = 2048
0.00.096.244 I print_info: rope_finetuned   = unknown
0.00.096.244 I print_info: ssm_d_conv       = 0
0.00.096.244 I print_info: ssm_d_inner      = 0
0.00.096.244 I print_info: ssm_d_state      = 0
0.00.096.244 I print_info: ssm_dt_rank      = 0
0.00.096.244 I print_info: ssm_dt_b_c_rms   = 0
0.00.096.244 I print_info: model type       = 1.4B
0.00.096.245 I print_info: model params     = 1.41 B
0.00.096.245 I print_info: general.name     = 1.4B
0.00.096.246 I print_info: vocab type       = BPE
0.00.096.246 I print_info: n_vocab          = 50304
0.00.096.246 I print_info: n_merges         = 50009
0.00.096.246 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.096.246 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.096.246 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.096.247 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.096.247 I print_info: LF token         = 128 'Ä'
0.00.096.249 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.096.249 I print_info: max token length = 1024
0.00.098.803 I load_tensors: offloading 24 repeating layers to GPU
0.00.098.803 I load_tensors: offloading output layer to GPU
0.00.098.803 I load_tensors: offloaded 25/25 layers to GPU
0.00.098.814 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.815 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.099.122 I llama_init_from_model: n_seq_max     = 1
0.00.099.123 I llama_init_from_model: n_ctx         = 128
0.00.099.123 I llama_init_from_model: n_ctx_per_seq = 128
0.00.099.123 I llama_init_from_model: n_batch       = 128
0.00.099.123 I llama_init_from_model: n_ubatch      = 128
0.00.099.123 I llama_init_from_model: flash_attn    = 0
0.00.099.124 I llama_init_from_model: freq_base     = 10000.0
0.00.099.124 I llama_init_from_model: freq_scale    = 1
0.00.099.124 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.099.125 I ggml_metal_init: allocating
0.00.099.128 I ggml_metal_init: found device: Apple M4
0.00.099.130 I ggml_metal_init: picking default device: Apple M4
0.00.099.764 I ggml_metal_init: using embedded metal library
0.00.102.364 I ggml_metal_init: GPU name:   Apple M4
0.00.102.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.367 I ggml_metal_init: simdgroup reduction   = true
0.00.102.367 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.367 I ggml_metal_init: has bfloat            = true
0.00.102.367 I ggml_metal_init: use bfloat            = true
0.00.102.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.837 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.114.143 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.114.158 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.944 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.114.945 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.114.945 I llama_init_from_model: graph nodes  = 967
0.00.114.945 I llama_init_from_model: graph splits = 2
0.00.114.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.114.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.912 I 
0.00.802.955 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.959 I perplexity: tokenizing the input ..
0.00.815.732 I perplexity: tokenization took 12.77 ms
0.00.815.738 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.935.784 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.937.623 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.937.673 I llama_perf_context_print:        load time =     775.44 ms
0.00.937.674 I llama_perf_context_print: prompt eval time =     119.48 ms /   128 tokens (    0.93 ms per token,  1071.27 tokens per second)
0.00.937.675 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.937.676 I llama_perf_context_print:       total time =     134.76 ms /   129 tokens
0.00.938.383 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.126s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.860 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.850 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.851 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.852 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.857 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.857 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.860 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.861 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.862 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.864 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.365 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.995 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.995 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.995 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.996 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.996 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.997 I llama_model_loader: - type  f32:  194 tensors
0.00.036.997 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.998 I print_info: file format = GGUF V3 (latest)
0.00.036.999 I print_info: file type   = Q8_0
0.00.037.001 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.062.345 I load: special tokens cache size = 25
0.00.068.710 I load: token to piece cache size = 0.2984 MB
0.00.068.713 I print_info: arch             = gptneox
0.00.068.714 I print_info: vocab_only       = 0
0.00.068.714 I print_info: n_ctx_train      = 2048
0.00.068.714 I print_info: n_embd           = 2048
0.00.068.714 I print_info: n_layer          = 24
0.00.068.719 I print_info: n_head           = 16
0.00.068.719 I print_info: n_head_kv        = 16
0.00.068.719 I print_info: n_rot            = 32
0.00.068.720 I print_info: n_swa            = 0
0.00.068.720 I print_info: n_embd_head_k    = 128
0.00.068.720 I print_info: n_embd_head_v    = 128
0.00.068.720 I print_info: n_gqa            = 1
0.00.068.721 I print_info: n_embd_k_gqa     = 2048
0.00.068.722 I print_info: n_embd_v_gqa     = 2048
0.00.068.724 I print_info: f_norm_eps       = 1.0e-05
0.00.068.724 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.725 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.725 I print_info: f_logit_scale    = 0.0e+00
0.00.068.725 I print_info: n_ff             = 8192
0.00.068.726 I print_info: n_expert         = 0
0.00.068.726 I print_info: n_expert_used    = 0
0.00.068.726 I print_info: causal attn      = 1
0.00.068.726 I print_info: pooling type     = 0
0.00.068.726 I print_info: rope type        = 2
0.00.068.726 I print_info: rope scaling     = linear
0.00.068.727 I print_info: freq_base_train  = 10000.0
0.00.068.727 I print_info: freq_scale_train = 1
0.00.068.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.727 I print_info: rope_finetuned   = unknown
0.00.068.727 I print_info: ssm_d_conv       = 0
0.00.068.727 I print_info: ssm_d_inner      = 0
0.00.068.729 I print_info: ssm_d_state      = 0
0.00.068.730 I print_info: ssm_dt_rank      = 0
0.00.068.730 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.730 I print_info: model type       = 1.4B
0.00.068.730 I print_info: model params     = 1.41 B
0.00.068.731 I print_info: general.name     = 1.4B
0.00.068.731 I print_info: vocab type       = BPE
0.00.068.731 I print_info: n_vocab          = 50304
0.00.068.731 I print_info: n_merges         = 50009
0.00.068.732 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.732 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.732 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.732 I print_info: LF token         = 128 'Ä'
0.00.068.733 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.733 I print_info: max token length = 1024
0.00.071.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.164 I load_tensors: offloading output layer to GPU
0.00.071.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.175 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.176 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.071.542 I llama_init_from_model: n_seq_max     = 1
0.00.071.543 I llama_init_from_model: n_ctx         = 128
0.00.071.543 I llama_init_from_model: n_ctx_per_seq = 128
0.00.071.543 I llama_init_from_model: n_batch       = 128
0.00.071.544 I llama_init_from_model: n_ubatch      = 128
0.00.071.544 I llama_init_from_model: flash_attn    = 0
0.00.071.544 I llama_init_from_model: freq_base     = 10000.0
0.00.071.544 I llama_init_from_model: freq_scale    = 1
0.00.071.545 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.071.545 I ggml_metal_init: allocating
0.00.071.548 I ggml_metal_init: found device: Apple M4
0.00.071.550 I ggml_metal_init: picking default device: Apple M4
0.00.072.254 I ggml_metal_init: using embedded metal library
0.00.075.054 I ggml_metal_init: GPU name:   Apple M4
0.00.075.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.056 I ggml_metal_init: simdgroup reduction   = true
0.00.075.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.056 I ggml_metal_init: has bfloat            = true
0.00.075.057 I ggml_metal_init: use bfloat            = true
0.00.075.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.058 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.179 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.185 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.200 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.225 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.087.226 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.087.227 I llama_init_from_model: graph nodes  = 967
0.00.087.227 I llama_init_from_model: graph splits = 2
0.00.087.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.087.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.853.902 I 
0.00.853.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.983 I perplexity: tokenizing the input ..
0.00.861.761 I perplexity: tokenization took 7.777 ms
0.00.861.764 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.986.159 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.987.430 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.987.455 I llama_perf_context_print:        load time =     841.03 ms
0.00.987.456 I llama_perf_context_print: prompt eval time =     124.16 ms /   128 tokens (    0.97 ms per token,  1030.97 tokens per second)
0.00.987.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.987.457 I llama_perf_context_print:       total time =     133.55 ms /   129 tokens
0.00.987.928 I ggml_metal_free: deallocating

real	0m1.006s
user	0m0.095s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.348 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.559 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.571 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.575 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.370 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.240 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.240 I llama_model_loader: - type  f32:  194 tensors
0.00.027.241 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.241 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.242 I print_info: file format = GGUF V3 (latest)
0.00.027.242 I print_info: file type   = Q4_0
0.00.027.243 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.548 I load: special tokens cache size = 25
0.00.052.505 I load: token to piece cache size = 0.2984 MB
0.00.052.509 I print_info: arch             = gptneox
0.00.052.509 I print_info: vocab_only       = 0
0.00.052.509 I print_info: n_ctx_train      = 2048
0.00.052.509 I print_info: n_embd           = 2048
0.00.052.509 I print_info: n_layer          = 24
0.00.052.513 I print_info: n_head           = 16
0.00.052.513 I print_info: n_head_kv        = 16
0.00.052.514 I print_info: n_rot            = 32
0.00.052.514 I print_info: n_swa            = 0
0.00.052.514 I print_info: n_embd_head_k    = 128
0.00.052.517 I print_info: n_embd_head_v    = 128
0.00.052.517 I print_info: n_gqa            = 1
0.00.052.518 I print_info: n_embd_k_gqa     = 2048
0.00.052.519 I print_info: n_embd_v_gqa     = 2048
0.00.052.519 I print_info: f_norm_eps       = 1.0e-05
0.00.052.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.520 I print_info: f_logit_scale    = 0.0e+00
0.00.052.521 I print_info: n_ff             = 8192
0.00.052.521 I print_info: n_expert         = 0
0.00.052.521 I print_info: n_expert_used    = 0
0.00.052.522 I print_info: causal attn      = 1
0.00.052.522 I print_info: pooling type     = 0
0.00.052.522 I print_info: rope type        = 2
0.00.052.523 I print_info: rope scaling     = linear
0.00.052.524 I print_info: freq_base_train  = 10000.0
0.00.052.524 I print_info: freq_scale_train = 1
0.00.052.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.526 I print_info: rope_finetuned   = unknown
0.00.052.526 I print_info: ssm_d_conv       = 0
0.00.052.526 I print_info: ssm_d_inner      = 0
0.00.052.526 I print_info: ssm_d_state      = 0
0.00.052.526 I print_info: ssm_dt_rank      = 0
0.00.052.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.527 I print_info: model type       = 1.4B
0.00.052.527 I print_info: model params     = 1.41 B
0.00.052.527 I print_info: general.name     = 1.4B
0.00.052.528 I print_info: vocab type       = BPE
0.00.052.528 I print_info: n_vocab          = 50304
0.00.052.528 I print_info: n_merges         = 50009
0.00.052.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.529 I print_info: LF token         = 128 'Ä'
0.00.052.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.530 I print_info: max token length = 1024
0.00.054.462 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.462 I load_tensors: offloading output layer to GPU
0.00.054.463 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.474 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.475 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.054.757 I llama_init_from_model: n_seq_max     = 1
0.00.054.758 I llama_init_from_model: n_ctx         = 128
0.00.054.758 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.758 I llama_init_from_model: n_batch       = 128
0.00.054.759 I llama_init_from_model: n_ubatch      = 128
0.00.054.759 I llama_init_from_model: flash_attn    = 0
0.00.054.759 I llama_init_from_model: freq_base     = 10000.0
0.00.054.759 I llama_init_from_model: freq_scale    = 1
0.00.054.760 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.760 I ggml_metal_init: allocating
0.00.054.763 I ggml_metal_init: found device: Apple M4
0.00.054.765 I ggml_metal_init: picking default device: Apple M4
0.00.055.357 I ggml_metal_init: using embedded metal library
0.00.057.722 I ggml_metal_init: GPU name:   Apple M4
0.00.057.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.725 I ggml_metal_init: simdgroup reduction   = true
0.00.057.725 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.725 I ggml_metal_init: has bfloat            = true
0.00.057.725 I ggml_metal_init: use bfloat            = true
0.00.057.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.726 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.012 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.021 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.043 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.927 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.928 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.929 I llama_init_from_model: graph nodes  = 967
0.00.069.929 I llama_init_from_model: graph splits = 2
0.00.069.930 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.025 I 
0.00.586.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.586.071 I perplexity: tokenizing the input ..
0.00.594.206 I perplexity: tokenization took 8.133 ms
0.00.594.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.837 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.718.000 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.718.034 I llama_perf_context_print:        load time =     574.67 ms
0.00.718.035 I llama_perf_context_print: prompt eval time =     122.40 ms /   128 tokens (    0.96 ms per token,  1045.76 tokens per second)
0.00.718.036 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.718.036 I llama_perf_context_print:       total time =     132.01 ms /   129 tokens
0.00.718.483 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.078s
sys	0m0.089s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.794 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.950 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.954 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.955 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.957 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.958 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.959 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.482 I llama_model_loader: - type  f32:  194 tensors
0.00.024.483 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.484 I print_info: file format = GGUF V3 (latest)
0.00.024.484 I print_info: file type   = Q4_1
0.00.024.485 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.107 I load: special tokens cache size = 25
0.00.049.023 I load: token to piece cache size = 0.2984 MB
0.00.049.026 I print_info: arch             = gptneox
0.00.049.026 I print_info: vocab_only       = 0
0.00.049.027 I print_info: n_ctx_train      = 2048
0.00.049.027 I print_info: n_embd           = 2048
0.00.049.027 I print_info: n_layer          = 24
0.00.049.030 I print_info: n_head           = 16
0.00.049.031 I print_info: n_head_kv        = 16
0.00.049.031 I print_info: n_rot            = 32
0.00.049.031 I print_info: n_swa            = 0
0.00.049.031 I print_info: n_embd_head_k    = 128
0.00.049.032 I print_info: n_embd_head_v    = 128
0.00.049.033 I print_info: n_gqa            = 1
0.00.049.034 I print_info: n_embd_k_gqa     = 2048
0.00.049.036 I print_info: n_embd_v_gqa     = 2048
0.00.049.037 I print_info: f_norm_eps       = 1.0e-05
0.00.049.037 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.037 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.038 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.038 I print_info: f_logit_scale    = 0.0e+00
0.00.049.039 I print_info: n_ff             = 8192
0.00.049.040 I print_info: n_expert         = 0
0.00.049.040 I print_info: n_expert_used    = 0
0.00.049.040 I print_info: causal attn      = 1
0.00.049.040 I print_info: pooling type     = 0
0.00.049.040 I print_info: rope type        = 2
0.00.049.041 I print_info: rope scaling     = linear
0.00.049.041 I print_info: freq_base_train  = 10000.0
0.00.049.041 I print_info: freq_scale_train = 1
0.00.049.042 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.042 I print_info: rope_finetuned   = unknown
0.00.049.042 I print_info: ssm_d_conv       = 0
0.00.049.042 I print_info: ssm_d_inner      = 0
0.00.049.042 I print_info: ssm_d_state      = 0
0.00.049.042 I print_info: ssm_dt_rank      = 0
0.00.049.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.043 I print_info: model type       = 1.4B
0.00.049.043 I print_info: model params     = 1.41 B
0.00.049.043 I print_info: general.name     = 1.4B
0.00.049.044 I print_info: vocab type       = BPE
0.00.049.044 I print_info: n_vocab          = 50304
0.00.049.044 I print_info: n_merges         = 50009
0.00.049.045 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.046 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.046 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.046 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.046 I print_info: LF token         = 128 'Ä'
0.00.049.047 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.047 I print_info: max token length = 1024
0.00.051.013 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.013 I load_tensors: offloading output layer to GPU
0.00.051.014 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.024 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.025 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.301 I llama_init_from_model: n_seq_max     = 1
0.00.051.302 I llama_init_from_model: n_ctx         = 128
0.00.051.302 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.302 I llama_init_from_model: n_batch       = 128
0.00.051.302 I llama_init_from_model: n_ubatch      = 128
0.00.051.302 I llama_init_from_model: flash_attn    = 0
0.00.051.303 I llama_init_from_model: freq_base     = 10000.0
0.00.051.303 I llama_init_from_model: freq_scale    = 1
0.00.051.303 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.304 I ggml_metal_init: allocating
0.00.051.306 I ggml_metal_init: found device: Apple M4
0.00.051.308 I ggml_metal_init: picking default device: Apple M4
0.00.051.887 I ggml_metal_init: using embedded metal library
0.00.054.222 I ggml_metal_init: GPU name:   Apple M4
0.00.054.223 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.223 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.224 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.224 I ggml_metal_init: simdgroup reduction   = true
0.00.054.224 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.224 I ggml_metal_init: has bfloat            = true
0.00.054.224 I ggml_metal_init: use bfloat            = true
0.00.054.225 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.226 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.758 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.039 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.042 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.058 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.940 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.941 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.942 I llama_init_from_model: graph nodes  = 967
0.00.065.942 I llama_init_from_model: graph splits = 2
0.00.065.943 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.348 I 
0.00.616.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.391 I perplexity: tokenizing the input ..
0.00.624.458 I perplexity: tokenization took 8.065 ms
0.00.624.461 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.689 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.748.941 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.748.977 I llama_perf_context_print:        load time =     607.55 ms
0.00.748.978 I llama_perf_context_print: prompt eval time =     122.99 ms /   128 tokens (    0.96 ms per token,  1040.75 tokens per second)
0.00.748.979 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.979 I llama_perf_context_print:       total time =     132.63 ms /   129 tokens
0.00.749.486 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.076s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.666 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.515 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.522 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.524 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.524 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.524 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.525 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.525 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.526 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.529 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.529 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.300 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.977 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.977 I llama_model_loader: - type  f32:  194 tensors
0.00.025.977 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.978 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.978 I print_info: file format = GGUF V3 (latest)
0.00.025.978 I print_info: file type   = Q5_0
0.00.025.979 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.736 I load: special tokens cache size = 25
0.00.050.759 I load: token to piece cache size = 0.2984 MB
0.00.050.762 I print_info: arch             = gptneox
0.00.050.762 I print_info: vocab_only       = 0
0.00.050.762 I print_info: n_ctx_train      = 2048
0.00.050.763 I print_info: n_embd           = 2048
0.00.050.763 I print_info: n_layer          = 24
0.00.050.765 I print_info: n_head           = 16
0.00.050.766 I print_info: n_head_kv        = 16
0.00.050.766 I print_info: n_rot            = 32
0.00.050.767 I print_info: n_swa            = 0
0.00.050.767 I print_info: n_embd_head_k    = 128
0.00.050.767 I print_info: n_embd_head_v    = 128
0.00.050.769 I print_info: n_gqa            = 1
0.00.050.770 I print_info: n_embd_k_gqa     = 2048
0.00.050.772 I print_info: n_embd_v_gqa     = 2048
0.00.050.773 I print_info: f_norm_eps       = 1.0e-05
0.00.050.773 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.773 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.774 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.774 I print_info: f_logit_scale    = 0.0e+00
0.00.050.774 I print_info: n_ff             = 8192
0.00.050.775 I print_info: n_expert         = 0
0.00.050.775 I print_info: n_expert_used    = 0
0.00.050.775 I print_info: causal attn      = 1
0.00.050.775 I print_info: pooling type     = 0
0.00.050.775 I print_info: rope type        = 2
0.00.050.776 I print_info: rope scaling     = linear
0.00.050.776 I print_info: freq_base_train  = 10000.0
0.00.050.776 I print_info: freq_scale_train = 1
0.00.050.776 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.778 I print_info: rope_finetuned   = unknown
0.00.050.778 I print_info: ssm_d_conv       = 0
0.00.050.778 I print_info: ssm_d_inner      = 0
0.00.050.779 I print_info: ssm_d_state      = 0
0.00.050.779 I print_info: ssm_dt_rank      = 0
0.00.050.779 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.779 I print_info: model type       = 1.4B
0.00.050.779 I print_info: model params     = 1.41 B
0.00.050.780 I print_info: general.name     = 1.4B
0.00.050.780 I print_info: vocab type       = BPE
0.00.050.780 I print_info: n_vocab          = 50304
0.00.050.780 I print_info: n_merges         = 50009
0.00.050.781 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.781 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.781 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.781 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.782 I print_info: LF token         = 128 'Ä'
0.00.050.782 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.784 I print_info: max token length = 1024
0.00.052.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.509 I load_tensors: offloading output layer to GPU
0.00.052.509 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.515 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.515 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.780 I llama_init_from_model: n_seq_max     = 1
0.00.052.781 I llama_init_from_model: n_ctx         = 128
0.00.052.781 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.781 I llama_init_from_model: n_batch       = 128
0.00.052.781 I llama_init_from_model: n_ubatch      = 128
0.00.052.781 I llama_init_from_model: flash_attn    = 0
0.00.052.782 I llama_init_from_model: freq_base     = 10000.0
0.00.052.782 I llama_init_from_model: freq_scale    = 1
0.00.052.782 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.783 I ggml_metal_init: allocating
0.00.052.786 I ggml_metal_init: found device: Apple M4
0.00.052.787 I ggml_metal_init: picking default device: Apple M4
0.00.053.359 I ggml_metal_init: using embedded metal library
0.00.055.682 I ggml_metal_init: GPU name:   Apple M4
0.00.055.684 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.685 I ggml_metal_init: simdgroup reduction   = true
0.00.055.685 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.685 I ggml_metal_init: has bfloat            = true
0.00.055.685 I ggml_metal_init: use bfloat            = true
0.00.055.686 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.686 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.134 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.425 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.427 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.441 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.302 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.303 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.303 I llama_init_from_model: graph nodes  = 967
0.00.067.304 I llama_init_from_model: graph splits = 2
0.00.067.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.072 I 
0.00.671.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.112 I perplexity: tokenizing the input ..
0.00.679.007 I perplexity: tokenization took 7.894 ms
0.00.679.011 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.065 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.815.256 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.815.280 I llama_perf_context_print:        load time =     660.40 ms
0.00.815.281 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.36 tokens per second)
0.00.815.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.283 I llama_perf_context_print:       total time =     144.21 ms /   129 tokens
0.00.815.785 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.009 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.014 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.017 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.019 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.022 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.532 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.534 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.534 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.534 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.535 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.535 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.535 I llama_model_loader: - type  f32:  194 tensors
0.00.024.536 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.536 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.537 I print_info: file format = GGUF V3 (latest)
0.00.024.537 I print_info: file type   = Q5_1
0.00.024.538 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.175 I load: special tokens cache size = 25
0.00.049.093 I load: token to piece cache size = 0.2984 MB
0.00.049.096 I print_info: arch             = gptneox
0.00.049.096 I print_info: vocab_only       = 0
0.00.049.096 I print_info: n_ctx_train      = 2048
0.00.049.097 I print_info: n_embd           = 2048
0.00.049.097 I print_info: n_layer          = 24
0.00.049.099 I print_info: n_head           = 16
0.00.049.100 I print_info: n_head_kv        = 16
0.00.049.100 I print_info: n_rot            = 32
0.00.049.100 I print_info: n_swa            = 0
0.00.049.101 I print_info: n_embd_head_k    = 128
0.00.049.102 I print_info: n_embd_head_v    = 128
0.00.049.103 I print_info: n_gqa            = 1
0.00.049.104 I print_info: n_embd_k_gqa     = 2048
0.00.049.106 I print_info: n_embd_v_gqa     = 2048
0.00.049.106 I print_info: f_norm_eps       = 1.0e-05
0.00.049.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.107 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.107 I print_info: f_logit_scale    = 0.0e+00
0.00.049.108 I print_info: n_ff             = 8192
0.00.049.109 I print_info: n_expert         = 0
0.00.049.109 I print_info: n_expert_used    = 0
0.00.049.109 I print_info: causal attn      = 1
0.00.049.109 I print_info: pooling type     = 0
0.00.049.109 I print_info: rope type        = 2
0.00.049.109 I print_info: rope scaling     = linear
0.00.049.110 I print_info: freq_base_train  = 10000.0
0.00.049.110 I print_info: freq_scale_train = 1
0.00.049.110 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.111 I print_info: rope_finetuned   = unknown
0.00.049.111 I print_info: ssm_d_conv       = 0
0.00.049.111 I print_info: ssm_d_inner      = 0
0.00.049.111 I print_info: ssm_d_state      = 0
0.00.049.111 I print_info: ssm_dt_rank      = 0
0.00.049.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.113 I print_info: model type       = 1.4B
0.00.049.113 I print_info: model params     = 1.41 B
0.00.049.113 I print_info: general.name     = 1.4B
0.00.049.113 I print_info: vocab type       = BPE
0.00.049.114 I print_info: n_vocab          = 50304
0.00.049.114 I print_info: n_merges         = 50009
0.00.049.114 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.114 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.114 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.115 I print_info: LF token         = 128 'Ä'
0.00.049.115 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.115 I print_info: max token length = 1024
0.00.051.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.104 I load_tensors: offloading output layer to GPU
0.00.051.105 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.115 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.116 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.397 I llama_init_from_model: n_seq_max     = 1
0.00.051.398 I llama_init_from_model: n_ctx         = 128
0.00.051.398 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.398 I llama_init_from_model: n_batch       = 128
0.00.051.398 I llama_init_from_model: n_ubatch      = 128
0.00.051.398 I llama_init_from_model: flash_attn    = 0
0.00.051.399 I llama_init_from_model: freq_base     = 10000.0
0.00.051.399 I llama_init_from_model: freq_scale    = 1
0.00.051.399 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.400 I ggml_metal_init: allocating
0.00.051.403 I ggml_metal_init: found device: Apple M4
0.00.051.405 I ggml_metal_init: picking default device: Apple M4
0.00.051.968 I ggml_metal_init: using embedded metal library
0.00.054.342 I ggml_metal_init: GPU name:   Apple M4
0.00.054.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.344 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.345 I ggml_metal_init: simdgroup reduction   = true
0.00.054.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.345 I ggml_metal_init: has bfloat            = true
0.00.054.345 I ggml_metal_init: use bfloat            = true
0.00.054.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.346 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.852 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.152 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.154 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.169 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.095 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.096 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.096 I llama_init_from_model: graph nodes  = 967
0.00.066.097 I llama_init_from_model: graph splits = 2
0.00.066.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.901 I 
0.00.802.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.940 I perplexity: tokenizing the input ..
0.00.811.076 I perplexity: tokenization took 8.135 ms
0.00.811.080 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.946.197 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.947.361 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.947.405 I llama_perf_context_print:        load time =     794.02 ms
0.00.947.406 I llama_perf_context_print: prompt eval time =     134.88 ms /   128 tokens (    1.05 ms per token,   948.96 tokens per second)
0.00.947.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.947.409 I llama_perf_context_print:       total time =     144.50 ms /   129 tokens
0.00.947.950 I ggml_metal_free: deallocating

real	0m0.962s
user	0m0.077s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.443 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.445 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.446 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.446 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.198 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.145 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.818 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.818 I llama_model_loader: - type  f32:  194 tensors
0.00.025.819 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.819 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.819 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.821 I print_info: file format = GGUF V3 (latest)
0.00.025.822 I print_info: file type   = Q2_K - Medium
0.00.025.822 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.232 I load: special tokens cache size = 25
0.00.051.623 I load: token to piece cache size = 0.2984 MB
0.00.051.626 I print_info: arch             = gptneox
0.00.051.627 I print_info: vocab_only       = 0
0.00.051.627 I print_info: n_ctx_train      = 2048
0.00.051.627 I print_info: n_embd           = 2048
0.00.051.627 I print_info: n_layer          = 24
0.00.051.630 I print_info: n_head           = 16
0.00.051.631 I print_info: n_head_kv        = 16
0.00.051.631 I print_info: n_rot            = 32
0.00.051.632 I print_info: n_swa            = 0
0.00.051.632 I print_info: n_embd_head_k    = 128
0.00.051.633 I print_info: n_embd_head_v    = 128
0.00.051.634 I print_info: n_gqa            = 1
0.00.051.635 I print_info: n_embd_k_gqa     = 2048
0.00.051.636 I print_info: n_embd_v_gqa     = 2048
0.00.051.636 I print_info: f_norm_eps       = 1.0e-05
0.00.051.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.637 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.637 I print_info: f_logit_scale    = 0.0e+00
0.00.051.638 I print_info: n_ff             = 8192
0.00.051.638 I print_info: n_expert         = 0
0.00.051.638 I print_info: n_expert_used    = 0
0.00.051.638 I print_info: causal attn      = 1
0.00.051.638 I print_info: pooling type     = 0
0.00.051.639 I print_info: rope type        = 2
0.00.051.639 I print_info: rope scaling     = linear
0.00.051.639 I print_info: freq_base_train  = 10000.0
0.00.051.641 I print_info: freq_scale_train = 1
0.00.051.642 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.642 I print_info: rope_finetuned   = unknown
0.00.051.642 I print_info: ssm_d_conv       = 0
0.00.051.642 I print_info: ssm_d_inner      = 0
0.00.051.642 I print_info: ssm_d_state      = 0
0.00.051.642 I print_info: ssm_dt_rank      = 0
0.00.051.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.643 I print_info: model type       = 1.4B
0.00.051.643 I print_info: model params     = 1.41 B
0.00.051.643 I print_info: general.name     = 1.4B
0.00.051.644 I print_info: vocab type       = BPE
0.00.051.644 I print_info: n_vocab          = 50304
0.00.051.644 I print_info: n_merges         = 50009
0.00.051.644 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.649 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.649 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.649 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.649 I print_info: LF token         = 128 'Ä'
0.00.051.650 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.650 I print_info: max token length = 1024
0.00.053.526 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.527 I load_tensors: offloading output layer to GPU
0.00.053.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.537 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.539 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.820 I llama_init_from_model: n_seq_max     = 1
0.00.053.821 I llama_init_from_model: n_ctx         = 128
0.00.053.821 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.821 I llama_init_from_model: n_batch       = 128
0.00.053.821 I llama_init_from_model: n_ubatch      = 128
0.00.053.821 I llama_init_from_model: flash_attn    = 0
0.00.053.822 I llama_init_from_model: freq_base     = 10000.0
0.00.053.822 I llama_init_from_model: freq_scale    = 1
0.00.053.822 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.823 I ggml_metal_init: allocating
0.00.053.826 I ggml_metal_init: found device: Apple M4
0.00.053.828 I ggml_metal_init: picking default device: Apple M4
0.00.054.395 I ggml_metal_init: using embedded metal library
0.00.056.803 I ggml_metal_init: GPU name:   Apple M4
0.00.056.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.805 I ggml_metal_init: simdgroup reduction   = true
0.00.056.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.805 I ggml_metal_init: has bfloat            = true
0.00.056.806 I ggml_metal_init: use bfloat            = true
0.00.056.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.446 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.891 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.893 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.917 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.845 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.846 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.846 I llama_init_from_model: graph nodes  = 967
0.00.068.846 I llama_init_from_model: graph splits = 2
0.00.068.848 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.409.691 I 
0.00.409.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.409.736 I perplexity: tokenizing the input ..
0.00.417.306 I perplexity: tokenization took 7.566 ms
0.00.417.309 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.549.824 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.550.988 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.551.023 I llama_perf_context_print:        load time =     399.06 ms
0.00.551.024 I llama_perf_context_print: prompt eval time =     132.29 ms /   128 tokens (    1.03 ms per token,   967.59 tokens per second)
0.00.551.024 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.551.025 I llama_perf_context_print:       total time =     141.33 ms /   129 tokens
0.00.551.549 I ggml_metal_free: deallocating

real	0m0.567s
user	0m0.078s
sys	0m0.068s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.710 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.443 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.450 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.451 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.452 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.452 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.453 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.453 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.454 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.454 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.455 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.455 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.455 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.456 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.456 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.756 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.756 I llama_model_loader: - type  f32:  194 tensors
0.00.023.756 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.757 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.757 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.757 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.758 I print_info: file format = GGUF V3 (latest)
0.00.023.758 I print_info: file type   = Q3_K - Medium
0.00.023.759 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.341 I load: special tokens cache size = 25
0.00.048.208 I load: token to piece cache size = 0.2984 MB
0.00.048.210 I print_info: arch             = gptneox
0.00.048.211 I print_info: vocab_only       = 0
0.00.048.211 I print_info: n_ctx_train      = 2048
0.00.048.211 I print_info: n_embd           = 2048
0.00.048.211 I print_info: n_layer          = 24
0.00.048.214 I print_info: n_head           = 16
0.00.048.215 I print_info: n_head_kv        = 16
0.00.048.215 I print_info: n_rot            = 32
0.00.048.215 I print_info: n_swa            = 0
0.00.048.216 I print_info: n_embd_head_k    = 128
0.00.048.216 I print_info: n_embd_head_v    = 128
0.00.048.216 I print_info: n_gqa            = 1
0.00.048.217 I print_info: n_embd_k_gqa     = 2048
0.00.048.218 I print_info: n_embd_v_gqa     = 2048
0.00.048.218 I print_info: f_norm_eps       = 1.0e-05
0.00.048.219 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.219 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.219 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.219 I print_info: f_logit_scale    = 0.0e+00
0.00.048.220 I print_info: n_ff             = 8192
0.00.048.220 I print_info: n_expert         = 0
0.00.048.220 I print_info: n_expert_used    = 0
0.00.048.221 I print_info: causal attn      = 1
0.00.048.221 I print_info: pooling type     = 0
0.00.048.221 I print_info: rope type        = 2
0.00.048.221 I print_info: rope scaling     = linear
0.00.048.221 I print_info: freq_base_train  = 10000.0
0.00.048.222 I print_info: freq_scale_train = 1
0.00.048.222 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.222 I print_info: rope_finetuned   = unknown
0.00.048.222 I print_info: ssm_d_conv       = 0
0.00.048.224 I print_info: ssm_d_inner      = 0
0.00.048.224 I print_info: ssm_d_state      = 0
0.00.048.224 I print_info: ssm_dt_rank      = 0
0.00.048.224 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.224 I print_info: model type       = 1.4B
0.00.048.225 I print_info: model params     = 1.41 B
0.00.048.225 I print_info: general.name     = 1.4B
0.00.048.225 I print_info: vocab type       = BPE
0.00.048.226 I print_info: n_vocab          = 50304
0.00.048.226 I print_info: n_merges         = 50009
0.00.048.226 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.226 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.226 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.227 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.227 I print_info: LF token         = 128 'Ä'
0.00.048.227 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.227 I print_info: max token length = 1024
0.00.050.093 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.093 I load_tensors: offloading output layer to GPU
0.00.050.094 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.104 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.106 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.391 I llama_init_from_model: n_seq_max     = 1
0.00.050.392 I llama_init_from_model: n_ctx         = 128
0.00.050.392 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.392 I llama_init_from_model: n_batch       = 128
0.00.050.393 I llama_init_from_model: n_ubatch      = 128
0.00.050.393 I llama_init_from_model: flash_attn    = 0
0.00.050.393 I llama_init_from_model: freq_base     = 10000.0
0.00.050.393 I llama_init_from_model: freq_scale    = 1
0.00.050.394 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.394 I ggml_metal_init: allocating
0.00.050.397 I ggml_metal_init: found device: Apple M4
0.00.050.399 I ggml_metal_init: picking default device: Apple M4
0.00.050.963 I ggml_metal_init: using embedded metal library
0.00.053.305 I ggml_metal_init: GPU name:   Apple M4
0.00.053.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.308 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.308 I ggml_metal_init: simdgroup reduction   = true
0.00.053.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.308 I ggml_metal_init: has bfloat            = true
0.00.053.308 I ggml_metal_init: use bfloat            = true
0.00.053.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.309 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.605 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.921 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.926 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.942 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.830 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.831 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.831 I llama_init_from_model: graph nodes  = 967
0.00.064.832 I llama_init_from_model: graph splits = 2
0.00.064.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.091 I 
0.00.475.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.142 I perplexity: tokenizing the input ..
0.00.482.992 I perplexity: tokenization took 7.847 ms
0.00.482.995 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.285 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.568 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.593 I llama_perf_context_print:        load time =     466.38 ms
0.00.616.594 I llama_perf_context_print: prompt eval time =     132.06 ms /   128 tokens (    1.03 ms per token,   969.28 tokens per second)
0.00.616.595 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.595 I llama_perf_context_print:       total time =     141.51 ms /   129 tokens
0.00.617.075 I ggml_metal_free: deallocating

real	0m0.631s
user	0m0.075s
sys	0m0.079s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.303 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.297 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.298 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.298 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.298 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.300 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.003 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.805 I llama_model_loader: - type  f32:  194 tensors
0.00.025.805 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.805 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.806 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.806 I print_info: file format = GGUF V3 (latest)
0.00.025.806 I print_info: file type   = Q4_K - Medium
0.00.025.807 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.478 I load: special tokens cache size = 25
0.00.050.444 I load: token to piece cache size = 0.2984 MB
0.00.050.447 I print_info: arch             = gptneox
0.00.050.447 I print_info: vocab_only       = 0
0.00.050.447 I print_info: n_ctx_train      = 2048
0.00.050.447 I print_info: n_embd           = 2048
0.00.050.448 I print_info: n_layer          = 24
0.00.050.450 I print_info: n_head           = 16
0.00.050.451 I print_info: n_head_kv        = 16
0.00.050.451 I print_info: n_rot            = 32
0.00.050.454 I print_info: n_swa            = 0
0.00.050.454 I print_info: n_embd_head_k    = 128
0.00.050.454 I print_info: n_embd_head_v    = 128
0.00.050.455 I print_info: n_gqa            = 1
0.00.050.455 I print_info: n_embd_k_gqa     = 2048
0.00.050.456 I print_info: n_embd_v_gqa     = 2048
0.00.050.456 I print_info: f_norm_eps       = 1.0e-05
0.00.050.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.457 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.457 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.457 I print_info: f_logit_scale    = 0.0e+00
0.00.050.458 I print_info: n_ff             = 8192
0.00.050.458 I print_info: n_expert         = 0
0.00.050.458 I print_info: n_expert_used    = 0
0.00.050.458 I print_info: causal attn      = 1
0.00.050.458 I print_info: pooling type     = 0
0.00.050.459 I print_info: rope type        = 2
0.00.050.459 I print_info: rope scaling     = linear
0.00.050.459 I print_info: freq_base_train  = 10000.0
0.00.050.460 I print_info: freq_scale_train = 1
0.00.050.460 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.460 I print_info: rope_finetuned   = unknown
0.00.050.460 I print_info: ssm_d_conv       = 0
0.00.050.460 I print_info: ssm_d_inner      = 0
0.00.050.461 I print_info: ssm_d_state      = 0
0.00.050.461 I print_info: ssm_dt_rank      = 0
0.00.050.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.461 I print_info: model type       = 1.4B
0.00.050.461 I print_info: model params     = 1.41 B
0.00.050.462 I print_info: general.name     = 1.4B
0.00.050.462 I print_info: vocab type       = BPE
0.00.050.462 I print_info: n_vocab          = 50304
0.00.050.463 I print_info: n_merges         = 50009
0.00.050.463 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.463 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.463 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.463 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.464 I print_info: LF token         = 128 'Ä'
0.00.050.464 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.464 I print_info: max token length = 1024
0.00.052.378 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.378 I load_tensors: offloading output layer to GPU
0.00.052.378 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.389 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.390 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.758 I llama_init_from_model: n_seq_max     = 1
0.00.052.759 I llama_init_from_model: n_ctx         = 128
0.00.052.759 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.759 I llama_init_from_model: n_batch       = 128
0.00.052.760 I llama_init_from_model: n_ubatch      = 128
0.00.052.760 I llama_init_from_model: flash_attn    = 0
0.00.052.760 I llama_init_from_model: freq_base     = 10000.0
0.00.052.760 I llama_init_from_model: freq_scale    = 1
0.00.052.761 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.761 I ggml_metal_init: allocating
0.00.052.764 I ggml_metal_init: found device: Apple M4
0.00.052.766 I ggml_metal_init: picking default device: Apple M4
0.00.053.360 I ggml_metal_init: using embedded metal library
0.00.055.733 I ggml_metal_init: GPU name:   Apple M4
0.00.055.734 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.734 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.735 I ggml_metal_init: simdgroup reduction   = true
0.00.055.735 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.735 I ggml_metal_init: has bfloat            = true
0.00.055.736 I ggml_metal_init: use bfloat            = true
0.00.055.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.343 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.568 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.586 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.485 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.487 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.487 I llama_init_from_model: graph nodes  = 967
0.00.067.487 I llama_init_from_model: graph splits = 2
0.00.067.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.496 I 
0.00.564.537 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.540 I perplexity: tokenizing the input ..
0.00.572.161 I perplexity: tokenization took 7.618 ms
0.00.572.165 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.549 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.706.707 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.706.729 I llama_perf_context_print:        load time =     554.19 ms
0.00.706.729 I llama_perf_context_print: prompt eval time =     133.15 ms /   128 tokens (    1.04 ms per token,   961.34 tokens per second)
0.00.706.731 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.706.731 I llama_perf_context_print:       total time =     142.24 ms /   129 tokens
0.00.707.078 I ggml_metal_free: deallocating

real	0m0.723s
user	0m0.077s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.469 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.589 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.597 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.599 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.599 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.600 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.600 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.603 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.603 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.400 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.276 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.277 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.277 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.277 I llama_model_loader: - type  f32:  194 tensors
0.00.024.278 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.278 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.278 I print_info: file format = GGUF V3 (latest)
0.00.024.279 I print_info: file type   = Q5_K - Medium
0.00.024.280 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.657 I load: special tokens cache size = 25
0.00.049.590 I load: token to piece cache size = 0.2984 MB
0.00.049.593 I print_info: arch             = gptneox
0.00.049.593 I print_info: vocab_only       = 0
0.00.049.594 I print_info: n_ctx_train      = 2048
0.00.049.594 I print_info: n_embd           = 2048
0.00.049.594 I print_info: n_layer          = 24
0.00.049.597 I print_info: n_head           = 16
0.00.049.598 I print_info: n_head_kv        = 16
0.00.049.598 I print_info: n_rot            = 32
0.00.049.598 I print_info: n_swa            = 0
0.00.049.599 I print_info: n_embd_head_k    = 128
0.00.049.599 I print_info: n_embd_head_v    = 128
0.00.049.602 I print_info: n_gqa            = 1
0.00.049.603 I print_info: n_embd_k_gqa     = 2048
0.00.049.605 I print_info: n_embd_v_gqa     = 2048
0.00.049.606 I print_info: f_norm_eps       = 1.0e-05
0.00.049.606 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.606 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.607 I print_info: f_logit_scale    = 0.0e+00
0.00.049.607 I print_info: n_ff             = 8192
0.00.049.607 I print_info: n_expert         = 0
0.00.049.608 I print_info: n_expert_used    = 0
0.00.049.608 I print_info: causal attn      = 1
0.00.049.608 I print_info: pooling type     = 0
0.00.049.608 I print_info: rope type        = 2
0.00.049.610 I print_info: rope scaling     = linear
0.00.049.610 I print_info: freq_base_train  = 10000.0
0.00.049.610 I print_info: freq_scale_train = 1
0.00.049.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.611 I print_info: rope_finetuned   = unknown
0.00.049.611 I print_info: ssm_d_conv       = 0
0.00.049.611 I print_info: ssm_d_inner      = 0
0.00.049.611 I print_info: ssm_d_state      = 0
0.00.049.611 I print_info: ssm_dt_rank      = 0
0.00.049.612 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.612 I print_info: model type       = 1.4B
0.00.049.612 I print_info: model params     = 1.41 B
0.00.049.614 I print_info: general.name     = 1.4B
0.00.049.614 I print_info: vocab type       = BPE
0.00.049.614 I print_info: n_vocab          = 50304
0.00.049.615 I print_info: n_merges         = 50009
0.00.049.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.616 I print_info: LF token         = 128 'Ä'
0.00.049.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.616 I print_info: max token length = 1024
0.00.051.554 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.554 I load_tensors: offloading output layer to GPU
0.00.051.554 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.565 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.566 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.852 I llama_init_from_model: n_seq_max     = 1
0.00.051.853 I llama_init_from_model: n_ctx         = 128
0.00.051.853 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.853 I llama_init_from_model: n_batch       = 128
0.00.051.853 I llama_init_from_model: n_ubatch      = 128
0.00.051.853 I llama_init_from_model: flash_attn    = 0
0.00.051.853 I llama_init_from_model: freq_base     = 10000.0
0.00.051.854 I llama_init_from_model: freq_scale    = 1
0.00.051.854 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.855 I ggml_metal_init: allocating
0.00.051.858 I ggml_metal_init: found device: Apple M4
0.00.051.860 I ggml_metal_init: picking default device: Apple M4
0.00.052.409 I ggml_metal_init: using embedded metal library
0.00.054.818 I ggml_metal_init: GPU name:   Apple M4
0.00.054.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.822 I ggml_metal_init: simdgroup reduction   = true
0.00.054.822 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.822 I ggml_metal_init: has bfloat            = true
0.00.054.822 I ggml_metal_init: use bfloat            = true
0.00.054.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.952 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.284 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.288 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.305 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.215 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.216 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.217 I llama_init_from_model: graph nodes  = 967
0.00.067.217 I llama_init_from_model: graph splits = 2
0.00.067.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.204 I 
0.00.637.249 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.258 I perplexity: tokenizing the input ..
0.00.645.032 I perplexity: tokenization took 7.773 ms
0.00.645.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.580 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.786.756 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.786.782 I llama_perf_context_print:        load time =     628.73 ms
0.00.786.783 I llama_perf_context_print: prompt eval time =     140.29 ms /   128 tokens (    1.10 ms per token,   912.39 tokens per second)
0.00.786.784 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.784 I llama_perf_context_print:       total time =     149.58 ms /   129 tokens
0.00.787.317 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.271 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.185 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.190 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.192 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.193 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.193 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.195 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.199 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.878 I llama_model_loader: - type  f32:  194 tensors
0.00.025.879 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.879 I print_info: file format = GGUF V3 (latest)
0.00.025.880 I print_info: file type   = Q6_K
0.00.025.881 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.087 I load: special tokens cache size = 25
0.00.050.964 I load: token to piece cache size = 0.2984 MB
0.00.050.967 I print_info: arch             = gptneox
0.00.050.968 I print_info: vocab_only       = 0
0.00.050.968 I print_info: n_ctx_train      = 2048
0.00.050.968 I print_info: n_embd           = 2048
0.00.050.968 I print_info: n_layer          = 24
0.00.050.971 I print_info: n_head           = 16
0.00.050.972 I print_info: n_head_kv        = 16
0.00.050.972 I print_info: n_rot            = 32
0.00.050.972 I print_info: n_swa            = 0
0.00.050.972 I print_info: n_embd_head_k    = 128
0.00.050.973 I print_info: n_embd_head_v    = 128
0.00.050.974 I print_info: n_gqa            = 1
0.00.050.975 I print_info: n_embd_k_gqa     = 2048
0.00.050.976 I print_info: n_embd_v_gqa     = 2048
0.00.050.976 I print_info: f_norm_eps       = 1.0e-05
0.00.050.978 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.978 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.978 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.979 I print_info: f_logit_scale    = 0.0e+00
0.00.050.979 I print_info: n_ff             = 8192
0.00.050.979 I print_info: n_expert         = 0
0.00.050.981 I print_info: n_expert_used    = 0
0.00.050.981 I print_info: causal attn      = 1
0.00.050.981 I print_info: pooling type     = 0
0.00.050.981 I print_info: rope type        = 2
0.00.050.981 I print_info: rope scaling     = linear
0.00.050.982 I print_info: freq_base_train  = 10000.0
0.00.050.982 I print_info: freq_scale_train = 1
0.00.050.982 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.982 I print_info: rope_finetuned   = unknown
0.00.050.982 I print_info: ssm_d_conv       = 0
0.00.050.983 I print_info: ssm_d_inner      = 0
0.00.050.983 I print_info: ssm_d_state      = 0
0.00.050.983 I print_info: ssm_dt_rank      = 0
0.00.050.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.983 I print_info: model type       = 1.4B
0.00.050.985 I print_info: model params     = 1.41 B
0.00.050.985 I print_info: general.name     = 1.4B
0.00.050.985 I print_info: vocab type       = BPE
0.00.050.985 I print_info: n_vocab          = 50304
0.00.050.986 I print_info: n_merges         = 50009
0.00.050.986 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.986 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.986 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.986 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.987 I print_info: LF token         = 128 'Ä'
0.00.050.987 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.987 I print_info: max token length = 1024
0.00.052.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.624 I load_tensors: offloading output layer to GPU
0.00.052.624 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.635 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.636 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.923 I llama_init_from_model: n_seq_max     = 1
0.00.052.924 I llama_init_from_model: n_ctx         = 128
0.00.052.924 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.924 I llama_init_from_model: n_batch       = 128
0.00.052.924 I llama_init_from_model: n_ubatch      = 128
0.00.052.925 I llama_init_from_model: flash_attn    = 0
0.00.052.925 I llama_init_from_model: freq_base     = 10000.0
0.00.052.925 I llama_init_from_model: freq_scale    = 1
0.00.052.925 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.926 I ggml_metal_init: allocating
0.00.052.929 I ggml_metal_init: found device: Apple M4
0.00.052.931 I ggml_metal_init: picking default device: Apple M4
0.00.053.497 I ggml_metal_init: using embedded metal library
0.00.055.872 I ggml_metal_init: GPU name:   Apple M4
0.00.055.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.874 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.874 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.875 I ggml_metal_init: simdgroup reduction   = true
0.00.055.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.875 I ggml_metal_init: has bfloat            = true
0.00.055.875 I ggml_metal_init: use bfloat            = true
0.00.055.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.876 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.565 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.884 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.802 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.803 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.803 I llama_init_from_model: graph nodes  = 967
0.00.067.803 I llama_init_from_model: graph splits = 2
0.00.067.805 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.134.635 I 
0.00.134.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.134.685 I perplexity: tokenizing the input ..
0.00.142.246 I perplexity: tokenization took 7.558 ms
0.00.142.251 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.281.346 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.282.465 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.282.483 I llama_perf_context_print:        load time =     124.36 ms
0.00.282.486 I llama_perf_context_print: prompt eval time =     138.83 ms /   128 tokens (    1.08 ms per token,   921.96 tokens per second)
0.00.282.486 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.282.487 I llama_perf_context_print:       total time =     147.85 ms /   129 tokens
0.00.282.855 I ggml_metal_free: deallocating

real	0m0.298s
user	0m0.078s
sys	0m0.038s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.297 I build: 4517 (9f7add1c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.155 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.165 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.166 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.155 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.157 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.158 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.158 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.159 I llama_model_loader: - type  f32:  194 tensors
0.00.055.160 I llama_model_loader: - type  f16:   98 tensors
0.00.055.161 I print_info: file format = GGUF V3 (latest)
0.00.055.162 I print_info: file type   = all F32 (guessed)
0.00.055.163 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.471 I load: special tokens cache size = 25
0.00.086.571 I load: token to piece cache size = 0.2984 MB
0.00.086.574 I print_info: arch             = gptneox
0.00.086.574 I print_info: vocab_only       = 0
0.00.086.574 I print_info: n_ctx_train      = 2048
0.00.086.574 I print_info: n_embd           = 2048
0.00.086.574 I print_info: n_layer          = 24
0.00.086.578 I print_info: n_head           = 16
0.00.086.579 I print_info: n_head_kv        = 16
0.00.086.579 I print_info: n_rot            = 32
0.00.086.579 I print_info: n_swa            = 0
0.00.086.579 I print_info: n_embd_head_k    = 128
0.00.086.580 I print_info: n_embd_head_v    = 128
0.00.086.580 I print_info: n_gqa            = 1
0.00.086.582 I print_info: n_embd_k_gqa     = 2048
0.00.086.583 I print_info: n_embd_v_gqa     = 2048
0.00.086.585 I print_info: f_norm_eps       = 1.0e-05
0.00.086.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.586 I print_info: f_logit_scale    = 0.0e+00
0.00.086.586 I print_info: n_ff             = 8192
0.00.086.587 I print_info: n_expert         = 0
0.00.086.587 I print_info: n_expert_used    = 0
0.00.086.587 I print_info: causal attn      = 1
0.00.086.587 I print_info: pooling type     = 0
0.00.086.587 I print_info: rope type        = 2
0.00.086.587 I print_info: rope scaling     = linear
0.00.086.588 I print_info: freq_base_train  = 10000.0
0.00.086.588 I print_info: freq_scale_train = 1
0.00.086.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.588 I print_info: rope_finetuned   = unknown
0.00.086.589 I print_info: ssm_d_conv       = 0
0.00.086.589 I print_info: ssm_d_inner      = 0
0.00.086.589 I print_info: ssm_d_state      = 0
0.00.086.589 I print_info: ssm_dt_rank      = 0
0.00.086.589 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.589 I print_info: model type       = 1.4B
0.00.086.590 I print_info: model params     = 1.41 B
0.00.086.590 I print_info: general.name     = 1.4B
0.00.086.590 I print_info: vocab type       = BPE
0.00.086.591 I print_info: n_vocab          = 50304
0.00.086.591 I print_info: n_merges         = 50009
0.00.086.591 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.591 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.591 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.591 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.592 I print_info: LF token         = 128 'Ä'
0.00.086.592 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.592 I print_info: max token length = 1024
0.00.088.505 I load_tensors: offloading 24 repeating layers to GPU
0.00.088.505 I load_tensors: offloading output layer to GPU
0.00.088.505 I load_tensors: offloaded 25/25 layers to GPU
0.00.088.515 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.517 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.804 I llama_init_from_model: n_seq_max     = 1
0.00.088.805 I llama_init_from_model: n_ctx         = 128
0.00.088.805 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.805 I llama_init_from_model: n_batch       = 128
0.00.088.805 I llama_init_from_model: n_ubatch      = 128
0.00.088.805 I llama_init_from_model: flash_attn    = 0
0.00.088.806 I llama_init_from_model: freq_base     = 10000.0
0.00.088.806 I llama_init_from_model: freq_scale    = 1
0.00.088.806 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.807 I ggml_metal_init: allocating
0.00.088.810 I ggml_metal_init: found device: Apple M4
0.00.088.812 I ggml_metal_init: picking default device: Apple M4
0.00.089.484 I ggml_metal_init: using embedded metal library
0.00.092.032 I ggml_metal_init: GPU name:   Apple M4
0.00.092.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.034 I ggml_metal_init: simdgroup reduction   = true
0.00.092.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.035 I ggml_metal_init: has bfloat            = true
0.00.092.035 I ggml_metal_init: use bfloat            = true
0.00.092.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.985 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.326 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.328 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.343 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.103.178 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.103.179 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.103.179 I llama_init_from_model: graph nodes  = 967
0.00.103.180 I llama_init_from_model: graph splits = 2
0.00.103.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.181 I 
0.00.103.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.103.217 I compute_imatrix: tokenizing the input ..
0.00.110.197 I compute_imatrix: tokenization took 6.978 ms
0.00.110.198 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.584.488 I compute_imatrix: 1.47 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.587.590 I llama_perf_context_print:        load time =    1560.60 ms
0.01.587.590 I llama_perf_context_print: prompt eval time =    1473.68 ms /   128 tokens (   11.51 ms per token,    86.86 tokens per second)
0.01.587.591 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.587.593 I llama_perf_context_print:       total time =    1563.69 ms /   129 tokens
0.01.588.207 I ggml_metal_free: deallocating

real	0m1.772s
user	0m0.177s
sys	0m0.250s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4517 (9f7add1c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106a07590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106a07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106a08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106a08800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106a08db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106a09360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106a09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106a09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106a0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106a0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106a0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106a0b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106a0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106a0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106a0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106a0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106a0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106a0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106a0ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106a0f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106a0f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106a100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106a10800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106a110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106a117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106a11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106a12090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106a12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106a13240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106a13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106a139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106a13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106a144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106a14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106a14cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106a15190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106a15630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106a15ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106a15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106a16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106a168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106a16d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106a171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106a17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106a17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106a17f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106a18570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106a18e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106a194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106a19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106a1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106a1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106a1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106a1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106a1bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106a1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106a1c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106a1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106a1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106a1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106a1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106a1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106a1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106a1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106a1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106a1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106a1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106a1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106a1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106a20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106a205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106a20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106a20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106a21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106a219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106a21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106a22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106a229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106a22f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106a23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106a239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106a23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106a24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106a24990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106a24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106a25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106a25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106a25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106a26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106a26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106a26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106a27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106a27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106a27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106a28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106a28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106a28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106a18b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106a29310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106a29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106a2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106a2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106a2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106a2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106a2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106a2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106a2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106a2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106a2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106a2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106a2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106a2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106a2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106a2e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106a2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106a2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106a2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106a2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106a2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106a30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106a304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106a30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106a30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106a312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106a31750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106a31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106a32090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106a32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106a329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106a32e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106a33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106a337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106a33c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106a340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106a34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106a34a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106a34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106a35370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106a35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106a35cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106a36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106a365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106a36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106a36f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106a373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106a37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106a37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106a381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106a38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106a38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106a38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106a39430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106a398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106a39d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106a3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106a3a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106a3ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106a3aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106a3b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106a3b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106a3bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106a3c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106a3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106a3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106a3d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106a3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106a3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106a3de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106a3e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106a3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106a3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106a3f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106a3f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106a3f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106a3fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106a40330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106a407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106a40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106a41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106a415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106a41a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106a41ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106a42390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106a42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106a42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106a43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106a43610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106a43ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106a43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106a443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106a44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106a44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106a451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106a45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106a45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106a461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106a46710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106a469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106a46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106a475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106a47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106a483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106a48890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106a48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106a49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106a49770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106a49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106a4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106a4a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106a4ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106a4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106a4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106a4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106a4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106a4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106a4d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106a4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106a4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106a4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106a4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106a4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106a4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106a4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106a4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106a504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106a509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106a50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106a51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106a519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106a51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106a52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106a529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106a52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106a53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106a539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106a53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106a54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106a549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106a54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106a55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106a559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106a55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106a56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106a56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106a56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106a57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106a57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106a57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106a58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106a58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106a58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106a59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106a59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106a59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106a5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106a5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106a5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106a5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106a5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106a5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106a5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106a5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106a5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106a5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106a5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106a5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106a5e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106a5e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106a5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106a5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106a5f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106a5fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106a5fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106a60370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106a60810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106a60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106a61150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106a615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106a61a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106a61f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106a623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106a62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106a63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106a63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106a63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106a645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106a64860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106a65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106a65310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106a65920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.149.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.149.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x103707a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x103707f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x103708370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1037087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x103708c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1037090c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x103709530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1037099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x103709e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10370a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10370a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10370ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10370b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10370c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10370c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10370cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10370d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10370ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10370e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10370ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10370f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10370faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x103710210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x103710930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x103711050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x103711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1037115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x103711a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x103711eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x103712320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x103712820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x103712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1037131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x103713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1037138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x103713d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1037142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1037147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x103714ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1037151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1037156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x103715ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1037160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1037165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x103716aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x103716f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x103717380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1037177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x103717c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1037180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x103718540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1037189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x103718e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x103719290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x103719700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x103719ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10371a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10371a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10371ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10371b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10371b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10371bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10371c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10371c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10371cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10371cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10371d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10371d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10371ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10371e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10371e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10371ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10371f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10371f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10371faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x103720040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x103720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x103720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x103721030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x103721580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x103721ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x103722020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x103722570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x103722ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x103723010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x103723560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x103723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x103724000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x103724550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x103724aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x103724ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x103725540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x103725a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x103725fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x103726530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x103726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x103726fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x103727520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x103727a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x103727fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x103728510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x103728a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x103728fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x103729500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x103729a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x103729fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10372a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10372aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10372af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10372b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10372ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10372bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10372c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10372c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10372ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10372d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10372d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10372dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10372e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10372e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10372e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10372ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10372f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10372f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10372fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1037300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x103730590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x103730a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x103730ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x103731370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x103731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x103731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x103732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1037325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x103732a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x103732f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1037333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x103733870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x103733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1037341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x103734650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x103734af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x103734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x103735430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1037358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x103735d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x103736210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1037366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x103736b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x103736ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x103737490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x103737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x103737dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x103738270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x103738710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x103738bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x103739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1037394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x103739990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x103739e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10373a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10373a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10373ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10373b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10373b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10373b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10373be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10373c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10373c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10373cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10373d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10373d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10373da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10373def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10373e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10373e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10373ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10373f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10373f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10373fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10373ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1037403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x103740890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x103740d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1037411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x103741670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x103741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x103741fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x103742450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1037428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x103742d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x103743230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1037436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x103743c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x103744170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1037446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x103744c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x103744ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1037454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x103745af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x103746100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1037468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x103746d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x103747050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x103747660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x103747c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x103748460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x103748900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x103748da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x103749240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1037499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x103749f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10374a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10374a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10374af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10374b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10374b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10374bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10374c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10374c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10374cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10374d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10374d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10374df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10374e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10374e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10374eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10374f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10374f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10374fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x103750430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x103750980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x103750ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x103751420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x103751970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x103751ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x103752410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x103752960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x103752eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x103753400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x103753950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x103753ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1037543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x103754940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x103754e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1037553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x103755930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x103755e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1037563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x103756920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x103756e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1037573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x103757910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x103757e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1037583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x103758900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x103758e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1037593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1037598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x103759e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10375a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10375a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10375ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10375b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10375b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10375be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10375c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10375c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10375ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10375d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10375d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10375da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10375df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10375e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10375e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10375ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10375f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10375f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10375faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10375ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x103760430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1037608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x103760e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x103761540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x103761c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x103762380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x103762aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x103762d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x103763550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x103763810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x103763e20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f706cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f707140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f7077e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f708300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f708ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f7092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f7099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f70a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f70a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f70af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f70b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f70be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f70c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f70cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f70d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f70dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f70dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f70e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f70e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f70e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f70ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f70f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f70f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f70fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f70fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f7102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f710730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f710ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f711010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f711480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f7118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f711d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f7121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f712ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f712f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f713390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f713800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f713c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f7140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f714550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f7149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f714e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f7152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f715710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f715b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f715ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f716a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f716ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f717340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f7177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f717c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f718090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f718500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f718970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f718de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f719250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f7196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f719b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f719fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f71a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f71a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f71acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f71b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f71b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f71ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f71beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f71c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f71c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f71cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f71d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f71d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f71d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f71ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f71e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f71e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f71eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f71ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f71f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f71f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f71fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f720140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f7205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f720a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f720e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f721300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f721770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f721be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f722050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f7224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f722930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f722da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f723210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f723aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f723d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f7241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f724ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f724f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f725390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f725800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f725c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f7260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f726550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f7269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f726e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f7272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f727710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f727b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f727ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f728460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f7288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f728d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f7291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f729620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f729a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f729f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f72a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f72a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f72ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f72b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f72b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f72b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f72be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f72c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f72c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f72cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f72cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f72d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f72d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f72dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f72e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f72e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f72ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f72eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f72f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f72f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f72fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f7300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f730510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f730980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f730df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f731260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f7316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f731b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f731fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f732420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f732890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f732d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f733170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f7335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f733a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f733ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f734330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f7347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f734c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f735080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f7354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f735960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f735dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f736240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f7366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f736b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f736f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f737400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f737870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f737ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f738150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f7385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f738a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f738ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f739310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f739780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f739bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f73a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f73a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f73a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f73adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f73b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f73b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f73bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f73bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f73c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f73c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f73ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f73d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f73d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f73da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f73de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f73e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f73e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f73ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f73f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f73f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f73f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f73fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f740200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f740670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f740ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f741ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f741d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f742050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f7424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f742930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f742da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f743210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f743680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f743af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f743f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f7443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f744840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f744cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f745120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f745590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f745a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f745e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f7462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f746750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f746bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f747030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f7474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f747910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f747d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f7481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f748660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f748ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f748f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f7493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f749820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f749c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f74a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f74a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f74a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f74ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f74b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f74b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f74bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f74c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f74c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f74c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f74cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f74d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f74d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f74dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f74df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f74e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f74e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f74ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f74f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f74f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f74f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f74fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f7502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f750710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f750b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f750ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f751460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f7518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f751d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f7521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f752620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f752a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f752f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f7537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f753c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f7540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f754530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f7549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f754e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f755280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f7556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f756160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f756880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f756fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f7576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f757df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f7583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f758a00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.801s
user	0m0.311s
sys	0m0.268s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4517 (9f7add1c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13590a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13590aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13590aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13590b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13590bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13590c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13590c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13590cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13590d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13590d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13590dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13590e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13590ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13590f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13590fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135910310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x135910a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x135911150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x135911870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x135912040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x135912760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x135912e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1359135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135913e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x135914560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x135914820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135914e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135915aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135915fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1359162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x135916740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135916a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135917290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1359177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135917a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135917f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1359183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135918870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135918d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1359191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135919650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135919af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135919f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13591a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13591a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13591ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13591b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13591bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13591c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13591c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13591ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13591d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13591da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13591e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13591e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13591ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13591f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13591f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13591fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135920280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135920540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1359209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135920e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135921320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1359217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135921c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135922100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1359225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135922a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135922ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x135923380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x135923820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x135923cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x135924210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x135924760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x135924cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x135925200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x135925750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x135925ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1359261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x135926740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x135926c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1359271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x135927730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x135927c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1359281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x135928720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x135928c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1359291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x135929710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135929c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13592a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13592a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13592ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13592b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13592b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13592bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13591b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13592c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13592c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13592cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13592d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13592d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13592dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13592e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13592e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13592ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13592f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13592f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13592fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1359302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135930820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135930d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135931210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1359316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135931b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135931ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135932490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135932930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135932dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135933270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135933710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135933bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135934050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1359344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135934990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135934e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1359352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x135935770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x135935c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1359360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x135936550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1359369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x135936e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x135937330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1359377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135937c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135938110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1359385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x135938a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135938ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135939390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x135939830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135939cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13593a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13593a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13593aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13593af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13593b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13593b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13593bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13593c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13593c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13593cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13593cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13593d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13593d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13593dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13593e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13593e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13593eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13593f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13593f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13593f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13593fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135940290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135940730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135940bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135941070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135941510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1359419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135941e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1359422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135942790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135942c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1359430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135943570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135943a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135943eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135944350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1359447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135944c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135945130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1359455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x135945a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x135945f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1359463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x135946850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x135946cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135947190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x135947630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x135947ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135947f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1359484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135948a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135948f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1359494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135949770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x135949d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13594a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13594a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13594b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13594b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13594b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13594bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13594c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13594cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13594d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13594d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13594dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13594e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13594e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13594ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13594f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13594f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13594fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135950270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1359507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135950d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135951260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1359517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135951d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135952250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1359527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135952cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135953240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135953790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135953ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135954230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135954780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135954cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135955220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135955770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135955cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135956210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135956760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135956cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135957200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135957750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135957ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1359581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x135958740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x135958c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1359591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x135959730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x135959c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13595a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13595a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13595ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13595b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13595b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13595bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13595c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13595c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13595cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13595d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13595d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13595dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13595e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13595e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13595ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13595f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13595f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13595fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135960170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1359606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135960c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1359610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135961550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1359619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135961e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135962330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1359627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135962c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135963110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1359635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135963a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135963ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135964390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135964830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135964cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135965170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1359656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135965de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135966500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135966c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135967340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135967600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135967df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1359680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1359686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.778 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13470ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13470b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13470b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13470b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13470bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13470c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13470c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13470cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13470cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13470d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13470d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13470dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13470eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13470f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13470fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1347101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1347108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134711010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134711730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134711f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134712620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134712d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134713b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1347142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134714560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134714820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134714c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134715100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134715570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1347159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134715f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134716380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134716640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134716ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134716f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134717390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134717c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1347180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134718550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1347189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134718e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1347192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134719710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134719b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134719ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13471a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13471a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13471ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13471b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13471b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13471ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13471bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13471c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13471c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13471cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13471d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13471d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13471db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13471dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13471e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13471e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13471ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13471f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13471f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13471fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13471feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1347214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134722230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1347226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134722b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1347233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134724140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1347245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134724e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134725770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134725be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134726050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1347264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134726da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134727210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134727680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134727af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134727f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1347283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134728840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134728cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134729120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134729590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134729a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134729e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13472a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13472a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13472abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13472b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13472b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13472b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13472bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13472c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13472c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13472cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13472cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13472d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13472d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13472dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13472e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13472e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13472e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13472ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13472f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13472f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13472fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134730010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134730480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1347308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134730d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1347311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134731640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134731ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134731f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134732390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134732800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134732c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1347330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1347339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134733e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1347342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134734710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134734b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134734ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134735460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1347358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134735d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1347361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134736620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134736a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134736f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134737370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1347377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134737c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1347380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134738530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1347389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134738e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134739280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1347396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134739b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134739fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13473a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13473a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13473ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13473b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13473bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13473c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13473c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13473c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13473cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13473d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13473d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13473d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13473dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13473e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13473e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13473eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13473efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13473f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13473f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13473fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134740160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1347405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134741320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134741790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134741c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134742070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1347424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134742950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134742dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134743230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1347436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134743b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134743f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1347443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134744860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134744cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134745140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1347455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134745b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134746020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134746490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134746900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134746d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1347471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134747700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134747c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134748780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134748a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1347495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13474a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13474a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13474acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13474b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13474b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13474be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13474c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13474c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13474cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13474d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13474dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13474e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13474e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13474ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13474f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13474f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13474fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134750300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1347508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134750e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134751440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134751a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134751fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134752580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134752b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134753100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1347536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134753c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134754240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134754800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134754dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134755380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134755940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134755f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1347564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134756a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134757040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134757600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134757bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134758180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134758740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134758d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1347592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134759880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134759e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13475a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13475a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13475af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13475b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13475bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13475c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13475c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13475cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13475d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13475d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13475db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13475e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13475e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13475ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13475ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13475f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13475f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13475fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134760340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134760840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134760d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134761240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134761740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134762150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134762870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134762f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1347636b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134763970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134764160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134764420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134764a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1358044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1358056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1358063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1358092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13580a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13580a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13580af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13580b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13580be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13580c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13580cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13580d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13580dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13580dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13580e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13580e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13580e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13580edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13580f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13580f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13580fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13580fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1358102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1358114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1358133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1358149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1358152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1358177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1358180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1358189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1358196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13581a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13581a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13581ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13581b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13581b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13581ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13581bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13581c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13581c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13581cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13581d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13581d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13581d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13581ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13581e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13581e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13581eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13581efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13581f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13581f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13581fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1358205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1358217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1358224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1358253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1358269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1358272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1358291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13582a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13582a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13582ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13582b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13582b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13582b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13582be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13582c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13582c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13582cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13582d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13582d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13582d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13582dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13582e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13582e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13582eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13582ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13582f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13582f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13582fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1358300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1358309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1358328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1358331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1358347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1358350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1358366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1358378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1358385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1358397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13583a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13583a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13583a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13583ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13583b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13583b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13583bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13583bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13583c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13583c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13583ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13583d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13583d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13583da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13583deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13583e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13583e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13583ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13583f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13583f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13583f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13583fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1358406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1358424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x135842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1358436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1358455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1358474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1358493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13584a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13584a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13584aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13584ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13584b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13584b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13584bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13584c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13584c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13584c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13584cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13584d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13584d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13584dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13584df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13584e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13584e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13584eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13584f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13584f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13584f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13584fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1358502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1358521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1358533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x135853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1358540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1358549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1358552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1358568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1358576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1358579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.916s
user	0m0.242s
sys	0m0.133s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
