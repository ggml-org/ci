Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.702s
user	0m0.699s
sys	0m1.023s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Linking CXX shared library libllama.dylib
[ 23%] Built target llama-gguf-hash
[ 23%] Built target llama-gguf
[ 23%] Built target llama
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 23%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 23%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking C executable ../bin/test-c
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-run
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Built target llava
[ 33%] Linking CXX static library libcommon.a
[ 33%] Built target llama-simple
[ 33%] Built target llama-simple-chat
[ 33%] Built target test-c
[ 33%] Built target llama-run
[ 33%] Linking CXX static library libllava_static.a
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target llama-quantize-stats
[ 34%] Built target common
[ 34%] Built target llava_static
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-sampling
[ 49%] Built target test-llama-grammar
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Built target test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Built target test-barrier
[ 63%] Built target test-model-load-cancel
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 63%] Built target test-chat-template
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Built target test-autorelease
[ 65%] Linking CXX executable ../bin/test-rope
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Built target test-quantize-perf
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Built target test-quantize-fns
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Built target test-rope
[ 67%] Built target llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Built target llama-batched-bench
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-imatrix
[ 74%] Built target llama-embedding
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-gbnf-validator
[ 74%] Built target llama-eval-callback
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Built target llama-gritlm
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-imatrix
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Built target llama-bench
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Generating loading.html.hpp
[ 84%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 85%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Built target llama-cli
[ 85%] Built target llama-lookup-create
[ 86%] Built target llama-lookup-stats
[ 86%] Generating index.html.hpp
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 87%] Linking CXX executable ../../bin/llama-perplexity
[ 87%] Built target llama-lookup-merge
[ 87%] Built target llama-parallel
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 88%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Built target llama-passkey
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Built target llama-perplexity
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Built target llama-retrieval
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 94%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Built target llama-speculative
[ 96%] Built target llama-save-load-state
[ 96%] Built target llama-speculative-simple
[ 96%] Built target llama-tokenize
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.425s
user	0m5.258s
sys	0m8.394s

main: quantize time =  5424.78 ms
main:    total time =  5424.78 ms

main: quantize time =  1828.46 ms
main:    total time =  1828.46 ms

main: quantize time =  2003.04 ms
main:    total time =  2003.04 ms

main: quantize time =  2652.68 ms
main:    total time =  2652.68 ms

main: quantize time =  2812.18 ms
main:    total time =  2812.18 ms

main: quantize time =  5203.02 ms
main:    total time =  5203.02 ms

main: quantize time =  5700.69 ms
main:    total time =  5700.69 ms

main: quantize time =  6929.22 ms
main:    total time =  6929.22 ms

main: quantize time =  6084.46 ms
main:    total time =  6084.46 ms

main: quantize time =  4559.32 ms
main:    total time =  4559.32 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.104 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.225 I main: llama backend init
0.00.000.232 I main: load the model and apply lora adapter, if any
0.00.045.842 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.056.811 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.056.824 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.056.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.056.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.056.834 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.056.834 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.056.835 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.056.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.056.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.056.838 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.056.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.056.839 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.056.839 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.056.840 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.056.846 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.056.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.056.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.065.984 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.075.012 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.075.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.075.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.075.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.075.021 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.075.021 I llama_model_loader: - type  f32:  194 tensors
0.00.075.022 I llama_model_loader: - type  f16:   98 tensors
0.00.109.775 I llm_load_vocab: special tokens cache size = 25
0.00.116.988 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.116.991 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.116.991 I llm_load_print_meta: arch             = gptneox
0.00.116.992 I llm_load_print_meta: vocab type       = BPE
0.00.116.992 I llm_load_print_meta: n_vocab          = 50304
0.00.116.992 I llm_load_print_meta: n_merges         = 50009
0.00.116.992 I llm_load_print_meta: vocab_only       = 0
0.00.116.992 I llm_load_print_meta: n_ctx_train      = 2048
0.00.116.992 I llm_load_print_meta: n_embd           = 2048
0.00.116.993 I llm_load_print_meta: n_layer          = 24
0.00.117.015 I llm_load_print_meta: n_head           = 16
0.00.117.017 I llm_load_print_meta: n_head_kv        = 16
0.00.117.017 I llm_load_print_meta: n_rot            = 32
0.00.117.017 I llm_load_print_meta: n_swa            = 0
0.00.117.017 I llm_load_print_meta: n_embd_head_k    = 128
0.00.117.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.117.018 I llm_load_print_meta: n_gqa            = 1
0.00.117.019 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.117.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.117.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.117.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.117.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.117.021 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.117.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.117.021 I llm_load_print_meta: n_ff             = 8192
0.00.117.022 I llm_load_print_meta: n_expert         = 0
0.00.117.022 I llm_load_print_meta: n_expert_used    = 0
0.00.117.022 I llm_load_print_meta: causal attn      = 1
0.00.117.022 I llm_load_print_meta: pooling type     = 0
0.00.117.022 I llm_load_print_meta: rope type        = 2
0.00.117.022 I llm_load_print_meta: rope scaling     = linear
0.00.117.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.117.023 I llm_load_print_meta: freq_scale_train = 1
0.00.117.023 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.117.023 I llm_load_print_meta: rope_finetuned   = unknown
0.00.117.023 I llm_load_print_meta: ssm_d_conv       = 0
0.00.117.024 I llm_load_print_meta: ssm_d_inner      = 0
0.00.117.024 I llm_load_print_meta: ssm_d_state      = 0
0.00.117.025 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.117.025 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.117.035 I llm_load_print_meta: model type       = 1.4B
0.00.117.035 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.117.036 I llm_load_print_meta: model params     = 1.41 B
0.00.117.036 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.117.036 I llm_load_print_meta: general.name     = 1.4B
0.00.117.038 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.117.038 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.117.038 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.117.038 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.117.039 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.117.039 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.117.039 I llm_load_print_meta: max token length = 1024
0.00.119.678 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.119.679 I llm_load_tensors: offloading output layer to GPU
0.00.119.679 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.119.698 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.119.699 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.120.673 I llama_new_context_with_model: n_seq_max     = 1
0.00.120.674 I llama_new_context_with_model: n_ctx         = 2048
0.00.120.674 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.120.674 I llama_new_context_with_model: n_batch       = 2048
0.00.120.674 I llama_new_context_with_model: n_ubatch      = 512
0.00.120.674 I llama_new_context_with_model: flash_attn    = 0
0.00.120.675 I llama_new_context_with_model: freq_base     = 10000.0
0.00.120.675 I llama_new_context_with_model: freq_scale    = 1
0.00.120.676 I ggml_metal_init: allocating
0.00.120.679 I ggml_metal_init: found device: Apple M4
0.00.120.681 I ggml_metal_init: picking default device: Apple M4
0.00.121.349 I ggml_metal_init: using embedded metal library
0.00.130.922 I ggml_metal_init: GPU name:   Apple M4
0.00.130.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.130.924 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.130.925 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.130.925 I ggml_metal_init: simdgroup reduction   = true
0.00.130.925 I ggml_metal_init: simdgroup matrix mul. = true
0.00.130.925 I ggml_metal_init: has bfloat            = true
0.00.130.925 I ggml_metal_init: use bfloat            = true
0.00.130.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.130.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.178.810 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.178.817 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.178.836 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.179.822 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.179.824 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.179.824 I llama_new_context_with_model: graph nodes  = 967
0.00.179.825 I llama_new_context_with_model: graph splits = 2
0.00.179.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.260.007 I main: llama threadpool init, n_threads = 4
0.00.260.041 I 
0.00.260.078 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.260.080 I 
0.00.260.159 I sampler seed: 1234
0.00.260.164 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.260.188 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.260.189 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.260.189 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.102.778 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.02.102.779 I llama_perf_context_print:        load time =     214.15 ms
0.02.102.779 I llama_perf_context_print: prompt eval time =      43.72 ms /     7 tokens (    6.25 ms per token,   160.10 tokens per second)
0.02.102.783 I llama_perf_context_print:        eval time =    1795.94 ms /    63 runs   (   28.51 ms per token,    35.08 tokens per second)
0.02.102.785 I llama_perf_context_print:       total time =    1842.77 ms /    70 tokens
0.02.102.973 I ggml_metal_free: deallocating

real	0m2.416s
user	0m0.147s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.815 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.555 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.468 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.300 I llama_model_loader: - type  f32:  194 tensors
0.00.025.301 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.361 I llm_load_vocab: special tokens cache size = 25
0.00.053.383 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.387 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.387 I llm_load_print_meta: arch             = gptneox
0.00.053.387 I llm_load_print_meta: vocab type       = BPE
0.00.053.387 I llm_load_print_meta: n_vocab          = 50304
0.00.053.388 I llm_load_print_meta: n_merges         = 50009
0.00.053.388 I llm_load_print_meta: vocab_only       = 0
0.00.053.388 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.388 I llm_load_print_meta: n_embd           = 2048
0.00.053.390 I llm_load_print_meta: n_layer          = 24
0.00.053.403 I llm_load_print_meta: n_head           = 16
0.00.053.403 I llm_load_print_meta: n_head_kv        = 16
0.00.053.404 I llm_load_print_meta: n_rot            = 32
0.00.053.406 I llm_load_print_meta: n_swa            = 0
0.00.053.406 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.406 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.407 I llm_load_print_meta: n_gqa            = 1
0.00.053.408 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.408 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.409 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.410 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.410 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.410 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.410 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.411 I llm_load_print_meta: n_ff             = 8192
0.00.053.411 I llm_load_print_meta: n_expert         = 0
0.00.053.411 I llm_load_print_meta: n_expert_used    = 0
0.00.053.411 I llm_load_print_meta: causal attn      = 1
0.00.053.412 I llm_load_print_meta: pooling type     = 0
0.00.053.412 I llm_load_print_meta: rope type        = 2
0.00.053.412 I llm_load_print_meta: rope scaling     = linear
0.00.053.413 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.414 I llm_load_print_meta: freq_scale_train = 1
0.00.053.414 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.414 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.414 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.415 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.415 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.415 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.415 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.420 I llm_load_print_meta: model type       = 1.4B
0.00.053.420 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.420 I llm_load_print_meta: model params     = 1.41 B
0.00.053.421 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.421 I llm_load_print_meta: general.name     = 1.4B
0.00.053.421 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.421 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.422 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.422 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.423 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.424 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.424 I llm_load_print_meta: max token length = 1024
0.00.055.447 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.447 I llm_load_tensors: offloading output layer to GPU
0.00.055.448 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.454 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.455 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.412 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.413 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.413 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.413 I llama_new_context_with_model: n_batch       = 2048
0.00.056.414 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.414 I llama_new_context_with_model: flash_attn    = 0
0.00.056.414 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.414 I llama_new_context_with_model: freq_scale    = 1
0.00.056.415 I ggml_metal_init: allocating
0.00.056.421 I ggml_metal_init: found device: Apple M4
0.00.056.424 I ggml_metal_init: picking default device: Apple M4
0.00.057.161 I ggml_metal_init: using embedded metal library
0.00.059.693 I ggml_metal_init: GPU name:   Apple M4
0.00.059.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.695 I ggml_metal_init: simdgroup reduction   = true
0.00.059.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.696 I ggml_metal_init: has bfloat            = true
0.00.059.696 I ggml_metal_init: use bfloat            = true
0.00.059.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.701 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.714 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.737 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.944 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.946 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.946 I llama_new_context_with_model: graph nodes  = 967
0.00.095.946 I llama_new_context_with_model: graph splits = 2
0.00.095.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.016.108 I main: llama threadpool init, n_threads = 4
0.01.016.143 I 
0.01.016.171 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.016.171 I 
0.01.016.406 I sampler seed: 1234
0.01.016.411 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.016.422 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.016.422 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.016.422 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.107.987 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61525.13 tokens per second)
0.02.107.987 I llama_perf_context_print:        load time =    1006.29 ms
0.02.107.988 I llama_perf_context_print: prompt eval time =      43.14 ms /     7 tokens (    6.16 ms per token,   162.27 tokens per second)
0.02.107.989 I llama_perf_context_print:        eval time =    1045.49 ms /    63 runs   (   16.60 ms per token,    60.26 tokens per second)
0.02.107.989 I llama_perf_context_print:       total time =    1091.88 ms /    70 tokens
0.02.108.189 I ggml_metal_free: deallocating

real	0m2.127s
user	0m0.113s
sys	0m0.201s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.621 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.349 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.351 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.352 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.354 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.178 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.318 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.319 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.320 I llama_model_loader: - type  f32:  194 tensors
0.00.026.320 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.321 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.832 I llm_load_vocab: special tokens cache size = 25
0.00.052.880 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.883 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.883 I llm_load_print_meta: arch             = gptneox
0.00.052.884 I llm_load_print_meta: vocab type       = BPE
0.00.052.884 I llm_load_print_meta: n_vocab          = 50304
0.00.052.884 I llm_load_print_meta: n_merges         = 50009
0.00.052.884 I llm_load_print_meta: vocab_only       = 0
0.00.052.885 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.885 I llm_load_print_meta: n_embd           = 2048
0.00.052.885 I llm_load_print_meta: n_layer          = 24
0.00.052.903 I llm_load_print_meta: n_head           = 16
0.00.052.904 I llm_load_print_meta: n_head_kv        = 16
0.00.052.904 I llm_load_print_meta: n_rot            = 32
0.00.052.904 I llm_load_print_meta: n_swa            = 0
0.00.052.905 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.905 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.906 I llm_load_print_meta: n_gqa            = 1
0.00.052.906 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.907 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.907 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.907 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.908 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.910 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.910 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.911 I llm_load_print_meta: n_ff             = 8192
0.00.052.911 I llm_load_print_meta: n_expert         = 0
0.00.052.911 I llm_load_print_meta: n_expert_used    = 0
0.00.052.912 I llm_load_print_meta: causal attn      = 1
0.00.052.912 I llm_load_print_meta: pooling type     = 0
0.00.052.912 I llm_load_print_meta: rope type        = 2
0.00.052.912 I llm_load_print_meta: rope scaling     = linear
0.00.052.912 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.913 I llm_load_print_meta: freq_scale_train = 1
0.00.052.913 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.913 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.913 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.913 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.913 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.913 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.924 I llm_load_print_meta: model type       = 1.4B
0.00.052.924 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.924 I llm_load_print_meta: model params     = 1.41 B
0.00.052.925 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.925 I llm_load_print_meta: general.name     = 1.4B
0.00.052.928 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.928 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.928 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.928 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.928 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.929 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.929 I llm_load_print_meta: max token length = 1024
0.00.055.174 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.174 I llm_load_tensors: offloading output layer to GPU
0.00.055.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.186 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.187 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.148 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.148 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.148 I llama_new_context_with_model: n_batch       = 2048
0.00.056.148 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.148 I llama_new_context_with_model: flash_attn    = 0
0.00.056.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.149 I llama_new_context_with_model: freq_scale    = 1
0.00.056.150 I ggml_metal_init: allocating
0.00.056.157 I ggml_metal_init: found device: Apple M4
0.00.056.160 I ggml_metal_init: picking default device: Apple M4
0.00.056.888 I ggml_metal_init: using embedded metal library
0.00.059.432 I ggml_metal_init: GPU name:   Apple M4
0.00.059.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.435 I ggml_metal_init: simdgroup reduction   = true
0.00.059.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.435 I ggml_metal_init: has bfloat            = true
0.00.059.435 I ggml_metal_init: use bfloat            = true
0.00.059.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.187 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.198 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.219 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.398 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.400 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.400 I llama_new_context_with_model: graph nodes  = 967
0.00.094.401 I llama_new_context_with_model: graph splits = 2
0.00.094.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.641 I main: llama threadpool init, n_threads = 4
0.00.675.691 I 
0.00.675.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.721 I 
0.00.675.946 I sampler seed: 1234
0.00.675.951 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.675.962 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.675.962 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.675.963 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.362.280 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.362.281 I llama_perf_context_print:        load time =     665.01 ms
0.01.362.282 I llama_perf_context_print: prompt eval time =      45.90 ms /     7 tokens (    6.56 ms per token,   152.51 tokens per second)
0.01.362.282 I llama_perf_context_print:        eval time =     637.44 ms /    63 runs   (   10.12 ms per token,    98.83 tokens per second)
0.01.362.283 I llama_perf_context_print:       total time =     686.64 ms /    70 tokens
0.01.362.512 I ggml_metal_free: deallocating

real	0m1.380s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.174 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.175 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.019 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.191 I llama_model_loader: - type  f32:  194 tensors
0.00.025.192 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.192 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.827 I llm_load_vocab: special tokens cache size = 25
0.00.052.927 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.932 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.932 I llm_load_print_meta: arch             = gptneox
0.00.052.932 I llm_load_print_meta: vocab type       = BPE
0.00.052.933 I llm_load_print_meta: n_vocab          = 50304
0.00.052.934 I llm_load_print_meta: n_merges         = 50009
0.00.052.935 I llm_load_print_meta: vocab_only       = 0
0.00.052.935 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.935 I llm_load_print_meta: n_embd           = 2048
0.00.052.935 I llm_load_print_meta: n_layer          = 24
0.00.052.953 I llm_load_print_meta: n_head           = 16
0.00.052.955 I llm_load_print_meta: n_head_kv        = 16
0.00.052.955 I llm_load_print_meta: n_rot            = 32
0.00.052.955 I llm_load_print_meta: n_swa            = 0
0.00.052.955 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.956 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.960 I llm_load_print_meta: n_gqa            = 1
0.00.052.961 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.961 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.962 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.963 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.965 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.965 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.965 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.966 I llm_load_print_meta: n_ff             = 8192
0.00.052.968 I llm_load_print_meta: n_expert         = 0
0.00.052.969 I llm_load_print_meta: n_expert_used    = 0
0.00.052.969 I llm_load_print_meta: causal attn      = 1
0.00.052.970 I llm_load_print_meta: pooling type     = 0
0.00.052.970 I llm_load_print_meta: rope type        = 2
0.00.052.970 I llm_load_print_meta: rope scaling     = linear
0.00.052.970 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.970 I llm_load_print_meta: freq_scale_train = 1
0.00.052.971 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.972 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.972 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.972 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.972 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.972 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.972 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.982 I llm_load_print_meta: model type       = 1.4B
0.00.052.983 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.983 I llm_load_print_meta: model params     = 1.41 B
0.00.052.983 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.984 I llm_load_print_meta: general.name     = 1.4B
0.00.052.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.984 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.984 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.984 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.984 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.985 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.985 I llm_load_print_meta: max token length = 1024
0.00.055.073 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.074 I llm_load_tensors: offloading output layer to GPU
0.00.055.074 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.085 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.086 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.984 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.985 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.985 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.985 I llama_new_context_with_model: n_batch       = 2048
0.00.055.985 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.985 I llama_new_context_with_model: flash_attn    = 0
0.00.055.986 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.986 I llama_new_context_with_model: freq_scale    = 1
0.00.055.987 I ggml_metal_init: allocating
0.00.055.991 I ggml_metal_init: found device: Apple M4
0.00.055.993 I ggml_metal_init: picking default device: Apple M4
0.00.056.613 I ggml_metal_init: using embedded metal library
0.00.058.979 I ggml_metal_init: GPU name:   Apple M4
0.00.058.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.982 I ggml_metal_init: simdgroup reduction   = true
0.00.058.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.982 I ggml_metal_init: has bfloat            = true
0.00.058.982 I ggml_metal_init: use bfloat            = true
0.00.058.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.126 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.133 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.153 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.216 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.218 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.219 I llama_new_context_with_model: graph nodes  = 967
0.00.091.219 I llama_new_context_with_model: graph splits = 2
0.00.091.232 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.198 I main: llama threadpool init, n_threads = 4
0.00.688.245 I 
0.00.688.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.275 I 
0.00.688.503 I sampler seed: 1234
0.00.688.508 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.688.519 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.688.521 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.688.521 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.413.126 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.413.127 I llama_perf_context_print:        load time =     678.52 ms
0.01.413.127 I llama_perf_context_print: prompt eval time =      43.25 ms /     7 tokens (    6.18 ms per token,   161.85 tokens per second)
0.01.413.129 I llama_perf_context_print:        eval time =     678.64 ms /    63 runs   (   10.77 ms per token,    92.83 tokens per second)
0.01.413.130 I llama_perf_context_print:       total time =     724.93 ms /    70 tokens
0.01.413.288 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.112s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.174 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.961 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.967 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.968 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.972 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.973 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.929 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.835 I llama_model_loader: - type  f32:  194 tensors
0.00.026.835 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.835 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.091 I llm_load_vocab: special tokens cache size = 25
0.00.053.065 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.068 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.068 I llm_load_print_meta: arch             = gptneox
0.00.053.068 I llm_load_print_meta: vocab type       = BPE
0.00.053.069 I llm_load_print_meta: n_vocab          = 50304
0.00.053.069 I llm_load_print_meta: n_merges         = 50009
0.00.053.069 I llm_load_print_meta: vocab_only       = 0
0.00.053.069 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.069 I llm_load_print_meta: n_embd           = 2048
0.00.053.070 I llm_load_print_meta: n_layer          = 24
0.00.053.079 I llm_load_print_meta: n_head           = 16
0.00.053.079 I llm_load_print_meta: n_head_kv        = 16
0.00.053.079 I llm_load_print_meta: n_rot            = 32
0.00.053.080 I llm_load_print_meta: n_swa            = 0
0.00.053.080 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.080 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.081 I llm_load_print_meta: n_gqa            = 1
0.00.053.081 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.082 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.083 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.087 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.087 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.087 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.088 I llm_load_print_meta: n_ff             = 8192
0.00.053.088 I llm_load_print_meta: n_expert         = 0
0.00.053.088 I llm_load_print_meta: n_expert_used    = 0
0.00.053.088 I llm_load_print_meta: causal attn      = 1
0.00.053.088 I llm_load_print_meta: pooling type     = 0
0.00.053.088 I llm_load_print_meta: rope type        = 2
0.00.053.089 I llm_load_print_meta: rope scaling     = linear
0.00.053.089 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.089 I llm_load_print_meta: freq_scale_train = 1
0.00.053.089 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.090 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.090 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.090 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.090 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.090 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.090 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.096 I llm_load_print_meta: model type       = 1.4B
0.00.053.096 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.097 I llm_load_print_meta: model params     = 1.41 B
0.00.053.097 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.097 I llm_load_print_meta: general.name     = 1.4B
0.00.053.098 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.098 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.098 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.098 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.098 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.099 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.099 I llm_load_print_meta: max token length = 1024
0.00.054.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.881 I llm_load_tensors: offloading output layer to GPU
0.00.054.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.887 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.887 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.779 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.779 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.780 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.780 I llama_new_context_with_model: n_batch       = 2048
0.00.055.780 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.780 I llama_new_context_with_model: flash_attn    = 0
0.00.055.781 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.781 I llama_new_context_with_model: freq_scale    = 1
0.00.055.782 I ggml_metal_init: allocating
0.00.055.789 I ggml_metal_init: found device: Apple M4
0.00.055.791 I ggml_metal_init: picking default device: Apple M4
0.00.056.421 I ggml_metal_init: using embedded metal library
0.00.058.743 I ggml_metal_init: GPU name:   Apple M4
0.00.058.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.747 I ggml_metal_init: simdgroup reduction   = true
0.00.058.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.748 I ggml_metal_init: has bfloat            = true
0.00.058.748 I ggml_metal_init: use bfloat            = true
0.00.058.748 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.749 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.922 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.927 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.946 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.982 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.984 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.984 I llama_new_context_with_model: graph nodes  = 967
0.00.088.984 I llama_new_context_with_model: graph splits = 2
0.00.088.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.029 I main: llama threadpool init, n_threads = 4
0.00.738.065 I 
0.00.738.092 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.094 I 
0.00.738.280 I sampler seed: 1234
0.00.738.284 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.738.293 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.738.294 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.738.294 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.527.872 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.527.873 I llama_perf_context_print:        load time =     728.85 ms
0.01.527.874 I llama_perf_context_print: prompt eval time =      42.99 ms /     7 tokens (    6.14 ms per token,   162.84 tokens per second)
0.01.527.874 I llama_perf_context_print:        eval time =     743.54 ms /    63 runs   (   11.80 ms per token,    84.73 tokens per second)
0.01.527.875 I llama_perf_context_print:       total time =     789.85 ms /    70 tokens
0.01.528.065 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.109s
sys	0m0.140s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.009.533 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.170 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.172 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.172 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.175 I llama_model_loader: - type  f32:  194 tensors
0.00.025.175 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.297 I llm_load_vocab: special tokens cache size = 25
0.00.051.273 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.275 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.276 I llm_load_print_meta: arch             = gptneox
0.00.051.276 I llm_load_print_meta: vocab type       = BPE
0.00.051.276 I llm_load_print_meta: n_vocab          = 50304
0.00.051.276 I llm_load_print_meta: n_merges         = 50009
0.00.051.276 I llm_load_print_meta: vocab_only       = 0
0.00.051.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.277 I llm_load_print_meta: n_embd           = 2048
0.00.051.277 I llm_load_print_meta: n_layer          = 24
0.00.051.286 I llm_load_print_meta: n_head           = 16
0.00.051.287 I llm_load_print_meta: n_head_kv        = 16
0.00.051.287 I llm_load_print_meta: n_rot            = 32
0.00.051.287 I llm_load_print_meta: n_swa            = 0
0.00.051.287 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.288 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.291 I llm_load_print_meta: n_gqa            = 1
0.00.051.291 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.292 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.293 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.293 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.293 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.293 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.293 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.294 I llm_load_print_meta: n_ff             = 8192
0.00.051.294 I llm_load_print_meta: n_expert         = 0
0.00.051.294 I llm_load_print_meta: n_expert_used    = 0
0.00.051.296 I llm_load_print_meta: causal attn      = 1
0.00.051.296 I llm_load_print_meta: pooling type     = 0
0.00.051.296 I llm_load_print_meta: rope type        = 2
0.00.051.297 I llm_load_print_meta: rope scaling     = linear
0.00.051.297 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.297 I llm_load_print_meta: freq_scale_train = 1
0.00.051.297 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.298 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.298 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.298 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.298 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.298 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.298 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.302 I llm_load_print_meta: model type       = 1.4B
0.00.051.303 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.303 I llm_load_print_meta: model params     = 1.41 B
0.00.051.304 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.304 I llm_load_print_meta: general.name     = 1.4B
0.00.051.304 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.304 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.304 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.304 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.305 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.305 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.305 I llm_load_print_meta: max token length = 1024
0.00.053.039 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.040 I llm_load_tensors: offloading output layer to GPU
0.00.053.040 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.045 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.046 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.977 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.978 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.978 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.978 I llama_new_context_with_model: n_batch       = 2048
0.00.053.978 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.978 I llama_new_context_with_model: flash_attn    = 0
0.00.053.979 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.979 I llama_new_context_with_model: freq_scale    = 1
0.00.053.980 I ggml_metal_init: allocating
0.00.053.986 I ggml_metal_init: found device: Apple M4
0.00.053.988 I ggml_metal_init: picking default device: Apple M4
0.00.054.574 I ggml_metal_init: using embedded metal library
0.00.056.932 I ggml_metal_init: GPU name:   Apple M4
0.00.056.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.934 I ggml_metal_init: simdgroup reduction   = true
0.00.056.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.935 I ggml_metal_init: has bfloat            = true
0.00.056.936 I ggml_metal_init: use bfloat            = true
0.00.056.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.325 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.337 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.358 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.460 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.461 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.462 I llama_new_context_with_model: graph nodes  = 967
0.00.087.462 I llama_new_context_with_model: graph splits = 2
0.00.087.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.476 I main: llama threadpool init, n_threads = 4
0.00.828.519 I 
0.00.828.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.550 I 
0.00.828.774 I sampler seed: 1234
0.00.828.778 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.828.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.828.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.828.832 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.703.498 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57770.55 tokens per second)
0.01.703.498 I llama_perf_context_print:        load time =     818.94 ms
0.01.703.499 I llama_perf_context_print: prompt eval time =      46.18 ms /     7 tokens (    6.60 ms per token,   151.58 tokens per second)
0.01.703.500 I llama_perf_context_print:        eval time =     825.44 ms /    63 runs   (   13.10 ms per token,    76.32 tokens per second)
0.01.703.500 I llama_perf_context_print:       total time =     875.02 ms /    70 tokens
0.01.703.698 I ggml_metal_free: deallocating

real	0m1.720s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.479 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.997 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.003 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.004 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.006 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.009 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.845 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.778 I llama_model_loader: - type  f32:  194 tensors
0.00.023.778 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.779 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.779 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.815 I llm_load_vocab: special tokens cache size = 25
0.00.049.841 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.844 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.844 I llm_load_print_meta: arch             = gptneox
0.00.049.844 I llm_load_print_meta: vocab type       = BPE
0.00.049.845 I llm_load_print_meta: n_vocab          = 50304
0.00.049.845 I llm_load_print_meta: n_merges         = 50009
0.00.049.845 I llm_load_print_meta: vocab_only       = 0
0.00.049.845 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.845 I llm_load_print_meta: n_embd           = 2048
0.00.049.845 I llm_load_print_meta: n_layer          = 24
0.00.049.860 I llm_load_print_meta: n_head           = 16
0.00.049.861 I llm_load_print_meta: n_head_kv        = 16
0.00.049.861 I llm_load_print_meta: n_rot            = 32
0.00.049.861 I llm_load_print_meta: n_swa            = 0
0.00.049.862 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.862 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.862 I llm_load_print_meta: n_gqa            = 1
0.00.049.863 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.864 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.864 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.865 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.865 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.866 I llm_load_print_meta: n_ff             = 8192
0.00.049.866 I llm_load_print_meta: n_expert         = 0
0.00.049.866 I llm_load_print_meta: n_expert_used    = 0
0.00.049.866 I llm_load_print_meta: causal attn      = 1
0.00.049.866 I llm_load_print_meta: pooling type     = 0
0.00.049.867 I llm_load_print_meta: rope type        = 2
0.00.049.867 I llm_load_print_meta: rope scaling     = linear
0.00.049.867 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.867 I llm_load_print_meta: freq_scale_train = 1
0.00.049.868 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.868 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.868 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.868 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.868 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.869 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.869 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.878 I llm_load_print_meta: model type       = 1.4B
0.00.049.878 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.879 I llm_load_print_meta: model params     = 1.41 B
0.00.049.879 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.880 I llm_load_print_meta: general.name     = 1.4B
0.00.049.880 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.881 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.881 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.881 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.881 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.881 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.882 I llm_load_print_meta: max token length = 1024
0.00.051.479 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.479 I llm_load_tensors: offloading output layer to GPU
0.00.051.479 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.489 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.490 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.338 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.339 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.339 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.340 I llama_new_context_with_model: n_batch       = 2048
0.00.052.340 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.340 I llama_new_context_with_model: flash_attn    = 0
0.00.052.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.341 I llama_new_context_with_model: freq_scale    = 1
0.00.052.341 I ggml_metal_init: allocating
0.00.052.347 I ggml_metal_init: found device: Apple M4
0.00.052.350 I ggml_metal_init: picking default device: Apple M4
0.00.052.931 I ggml_metal_init: using embedded metal library
0.00.055.291 I ggml_metal_init: GPU name:   Apple M4
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.292 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.293 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.293 I ggml_metal_init: simdgroup reduction   = true
0.00.055.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.293 I ggml_metal_init: has bfloat            = true
0.00.055.294 I ggml_metal_init: use bfloat            = true
0.00.055.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.295 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.517 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.522 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.540 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.620 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.622 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.622 I llama_new_context_with_model: graph nodes  = 967
0.00.085.623 I llama_new_context_with_model: graph splits = 2
0.00.085.637 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.489.238 I main: llama threadpool init, n_threads = 4
0.00.489.280 I 
0.00.489.311 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.489.311 I 
0.00.489.482 I sampler seed: 1234
0.00.489.487 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.489.506 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.489.507 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.489.507 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.178.664 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.178.665 I llama_perf_context_print:        load time =     479.75 ms
0.01.178.666 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.17 tokens per second)
0.01.178.667 I llama_perf_context_print:        eval time =     650.31 ms /    63 runs   (   10.32 ms per token,    96.88 tokens per second)
0.01.178.668 I llama_perf_context_print:       total time =     689.43 ms /    70 tokens
0.01.178.855 I ggml_metal_free: deallocating

real	0m1.195s
user	0m0.109s
sys	0m0.108s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.997 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.364 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.372 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.375 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.379 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.380 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.327 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.397 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.238 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.240 I llama_model_loader: - type  f32:  194 tensors
0.00.027.240 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.240 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.241 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.241 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.396 I llm_load_vocab: special tokens cache size = 25
0.00.053.403 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.406 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.407 I llm_load_print_meta: arch             = gptneox
0.00.053.407 I llm_load_print_meta: vocab type       = BPE
0.00.053.407 I llm_load_print_meta: n_vocab          = 50304
0.00.053.407 I llm_load_print_meta: n_merges         = 50009
0.00.053.408 I llm_load_print_meta: vocab_only       = 0
0.00.053.408 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.408 I llm_load_print_meta: n_embd           = 2048
0.00.053.408 I llm_load_print_meta: n_layer          = 24
0.00.053.422 I llm_load_print_meta: n_head           = 16
0.00.053.422 I llm_load_print_meta: n_head_kv        = 16
0.00.053.423 I llm_load_print_meta: n_rot            = 32
0.00.053.423 I llm_load_print_meta: n_swa            = 0
0.00.053.423 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.423 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.424 I llm_load_print_meta: n_gqa            = 1
0.00.053.425 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.425 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.426 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.426 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.426 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.427 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.427 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.427 I llm_load_print_meta: n_ff             = 8192
0.00.053.429 I llm_load_print_meta: n_expert         = 0
0.00.053.431 I llm_load_print_meta: n_expert_used    = 0
0.00.053.431 I llm_load_print_meta: causal attn      = 1
0.00.053.431 I llm_load_print_meta: pooling type     = 0
0.00.053.431 I llm_load_print_meta: rope type        = 2
0.00.053.431 I llm_load_print_meta: rope scaling     = linear
0.00.053.432 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.432 I llm_load_print_meta: freq_scale_train = 1
0.00.053.432 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.432 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.432 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.432 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.443 I llm_load_print_meta: model type       = 1.4B
0.00.053.443 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.444 I llm_load_print_meta: model params     = 1.41 B
0.00.053.444 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.444 I llm_load_print_meta: general.name     = 1.4B
0.00.053.445 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.445 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.449 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.449 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.450 I llm_load_print_meta: max token length = 1024
0.00.055.161 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.161 I llm_load_tensors: offloading output layer to GPU
0.00.055.161 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.171 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.172 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.041 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.042 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.042 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.042 I llama_new_context_with_model: n_batch       = 2048
0.00.056.042 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.043 I llama_new_context_with_model: flash_attn    = 0
0.00.056.043 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.043 I llama_new_context_with_model: freq_scale    = 1
0.00.056.044 I ggml_metal_init: allocating
0.00.056.047 I ggml_metal_init: found device: Apple M4
0.00.056.049 I ggml_metal_init: picking default device: Apple M4
0.00.056.645 I ggml_metal_init: using embedded metal library
0.00.058.990 I ggml_metal_init: GPU name:   Apple M4
0.00.058.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.994 I ggml_metal_init: simdgroup reduction   = true
0.00.058.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.994 I ggml_metal_init: has bfloat            = true
0.00.058.994 I ggml_metal_init: use bfloat            = true
0.00.058.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.305 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.326 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.376 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.378 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.378 I llama_new_context_with_model: graph nodes  = 967
0.00.089.379 I llama_new_context_with_model: graph splits = 2
0.00.089.393 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.543.150 I main: llama threadpool init, n_threads = 4
0.00.543.195 I 
0.00.543.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.543.242 I 
0.00.543.417 I sampler seed: 1234
0.00.543.422 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.543.432 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.543.433 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.543.434 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.310.779 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.310.780 I llama_perf_context_print:        load time =     531.15 ms
0.01.310.780 I llama_perf_context_print: prompt eval time =      40.63 ms /     7 tokens (    5.80 ms per token,   172.29 tokens per second)
0.01.310.782 I llama_perf_context_print:        eval time =     723.79 ms /    63 runs   (   11.49 ms per token,    87.04 tokens per second)
0.01.310.782 I llama_perf_context_print:       total time =     767.63 ms /    70 tokens
0.01.310.982 I ggml_metal_free: deallocating

real	0m1.325s
user	0m0.109s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.012.569 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.923 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.924 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.925 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.928 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.928 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.929 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.929 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.931 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.932 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.932 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.819 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.820 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.821 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.821 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.822 I llama_model_loader: - type  f32:  194 tensors
0.00.027.822 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.822 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.822 I llama_model_loader: - type q6_K:   13 tensors
0.00.048.787 I llm_load_vocab: special tokens cache size = 25
0.00.054.788 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.791 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.791 I llm_load_print_meta: arch             = gptneox
0.00.054.792 I llm_load_print_meta: vocab type       = BPE
0.00.054.792 I llm_load_print_meta: n_vocab          = 50304
0.00.054.792 I llm_load_print_meta: n_merges         = 50009
0.00.054.792 I llm_load_print_meta: vocab_only       = 0
0.00.054.792 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.793 I llm_load_print_meta: n_embd           = 2048
0.00.054.793 I llm_load_print_meta: n_layer          = 24
0.00.054.806 I llm_load_print_meta: n_head           = 16
0.00.054.807 I llm_load_print_meta: n_head_kv        = 16
0.00.054.808 I llm_load_print_meta: n_rot            = 32
0.00.054.808 I llm_load_print_meta: n_swa            = 0
0.00.054.810 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.810 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.811 I llm_load_print_meta: n_gqa            = 1
0.00.054.812 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.812 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.813 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.813 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.813 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.814 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.814 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.814 I llm_load_print_meta: n_ff             = 8192
0.00.054.814 I llm_load_print_meta: n_expert         = 0
0.00.054.815 I llm_load_print_meta: n_expert_used    = 0
0.00.054.815 I llm_load_print_meta: causal attn      = 1
0.00.054.815 I llm_load_print_meta: pooling type     = 0
0.00.054.815 I llm_load_print_meta: rope type        = 2
0.00.054.816 I llm_load_print_meta: rope scaling     = linear
0.00.054.816 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.817 I llm_load_print_meta: freq_scale_train = 1
0.00.054.817 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.817 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.818 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.818 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.818 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.819 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.819 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.828 I llm_load_print_meta: model type       = 1.4B
0.00.054.828 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.054.829 I llm_load_print_meta: model params     = 1.41 B
0.00.054.829 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.054.829 I llm_load_print_meta: general.name     = 1.4B
0.00.054.830 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.830 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.830 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.830 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.830 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.831 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.831 I llm_load_print_meta: max token length = 1024
0.00.056.463 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.463 I llm_load_tensors: offloading output layer to GPU
0.00.056.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.474 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.056.475 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.057.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.324 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.324 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.324 I llama_new_context_with_model: n_batch       = 2048
0.00.057.324 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.324 I llama_new_context_with_model: flash_attn    = 0
0.00.057.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.325 I llama_new_context_with_model: freq_scale    = 1
0.00.057.326 I ggml_metal_init: allocating
0.00.057.329 I ggml_metal_init: found device: Apple M4
0.00.057.331 I ggml_metal_init: picking default device: Apple M4
0.00.057.956 I ggml_metal_init: using embedded metal library
0.00.060.267 I ggml_metal_init: GPU name:   Apple M4
0.00.060.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.270 I ggml_metal_init: simdgroup reduction   = true
0.00.060.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.270 I ggml_metal_init: has bfloat            = true
0.00.060.270 I ggml_metal_init: use bfloat            = true
0.00.060.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.517 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.524 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.543 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.671 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.673 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.673 I llama_new_context_with_model: graph nodes  = 967
0.00.092.673 I llama_new_context_with_model: graph splits = 2
0.00.092.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.362 I main: llama threadpool init, n_threads = 4
0.00.650.404 I 
0.00.650.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.434 I 
0.00.650.665 I sampler seed: 1234
0.00.650.670 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.650.681 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.650.681 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.650.681 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.407.306 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.407.307 I llama_perf_context_print:        load time =     637.79 ms
0.01.407.308 I llama_perf_context_print: prompt eval time =      47.15 ms /     7 tokens (    6.74 ms per token,   148.46 tokens per second)
0.01.407.308 I llama_perf_context_print:        eval time =     706.36 ms /    63 runs   (   11.21 ms per token,    89.19 tokens per second)
0.01.407.309 I llama_perf_context_print:       total time =     756.95 ms /    70 tokens
0.01.407.495 I ggml_metal_free: deallocating

real	0m1.427s
user	0m0.111s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.854 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.550 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.551 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.551 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.552 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.553 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.553 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.554 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.555 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.556 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.556 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.411 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.318 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.319 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.320 I llama_model_loader: - type  f32:  194 tensors
0.00.026.321 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.321 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.617 I llm_load_vocab: special tokens cache size = 25
0.00.052.593 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.595 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.596 I llm_load_print_meta: arch             = gptneox
0.00.052.596 I llm_load_print_meta: vocab type       = BPE
0.00.052.596 I llm_load_print_meta: n_vocab          = 50304
0.00.052.596 I llm_load_print_meta: n_merges         = 50009
0.00.052.597 I llm_load_print_meta: vocab_only       = 0
0.00.052.597 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.597 I llm_load_print_meta: n_embd           = 2048
0.00.052.597 I llm_load_print_meta: n_layer          = 24
0.00.052.612 I llm_load_print_meta: n_head           = 16
0.00.052.612 I llm_load_print_meta: n_head_kv        = 16
0.00.052.612 I llm_load_print_meta: n_rot            = 32
0.00.052.613 I llm_load_print_meta: n_swa            = 0
0.00.052.613 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.613 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.614 I llm_load_print_meta: n_gqa            = 1
0.00.052.614 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.615 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.616 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.616 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.616 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.617 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.617 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.617 I llm_load_print_meta: n_ff             = 8192
0.00.052.618 I llm_load_print_meta: n_expert         = 0
0.00.052.618 I llm_load_print_meta: n_expert_used    = 0
0.00.052.618 I llm_load_print_meta: causal attn      = 1
0.00.052.618 I llm_load_print_meta: pooling type     = 0
0.00.052.618 I llm_load_print_meta: rope type        = 2
0.00.052.618 I llm_load_print_meta: rope scaling     = linear
0.00.052.619 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.619 I llm_load_print_meta: freq_scale_train = 1
0.00.052.619 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.619 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.619 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.620 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.620 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.629 I llm_load_print_meta: model type       = 1.4B
0.00.052.629 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.630 I llm_load_print_meta: model params     = 1.41 B
0.00.052.630 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.630 I llm_load_print_meta: general.name     = 1.4B
0.00.052.631 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.631 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.631 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.631 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.631 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.632 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.632 I llm_load_print_meta: max token length = 1024
0.00.054.715 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.715 I llm_load_tensors: offloading output layer to GPU
0.00.054.715 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.726 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.727 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.722 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.723 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.723 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.723 I llama_new_context_with_model: n_batch       = 2048
0.00.055.723 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.723 I llama_new_context_with_model: flash_attn    = 0
0.00.055.724 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.724 I llama_new_context_with_model: freq_scale    = 1
0.00.055.724 I ggml_metal_init: allocating
0.00.055.727 I ggml_metal_init: found device: Apple M4
0.00.055.729 I ggml_metal_init: picking default device: Apple M4
0.00.056.322 I ggml_metal_init: using embedded metal library
0.00.058.594 I ggml_metal_init: GPU name:   Apple M4
0.00.058.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.596 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.596 I ggml_metal_init: simdgroup reduction   = true
0.00.058.596 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.596 I ggml_metal_init: has bfloat            = true
0.00.058.597 I ggml_metal_init: use bfloat            = true
0.00.058.597 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.598 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.920 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.925 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.944 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.913 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.914 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.914 I llama_new_context_with_model: graph nodes  = 967
0.00.088.914 I llama_new_context_with_model: graph splits = 2
0.00.088.923 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.918 I main: llama threadpool init, n_threads = 4
0.00.686.957 I 
0.00.686.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.998 I 
0.00.687.153 I sampler seed: 1234
0.00.687.158 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.188 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.190 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.190 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.536.925 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.01.536.926 I llama_perf_context_print:        load time =     676.06 ms
0.01.536.927 I llama_perf_context_print: prompt eval time =      51.50 ms /     7 tokens (    7.36 ms per token,   135.91 tokens per second)
0.01.536.927 I llama_perf_context_print:        eval time =     795.59 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.536.928 I llama_perf_context_print:       total time =     850.01 ms /    70 tokens
0.01.537.164 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.802 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.468 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.473 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.473 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.475 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.475 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.478 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.478 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.479 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.666 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.711 I llama_model_loader: - type  f32:  194 tensors
0.00.025.712 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.709 I llm_load_vocab: special tokens cache size = 25
0.00.052.659 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.662 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.662 I llm_load_print_meta: arch             = gptneox
0.00.052.662 I llm_load_print_meta: vocab type       = BPE
0.00.052.662 I llm_load_print_meta: n_vocab          = 50304
0.00.052.663 I llm_load_print_meta: n_merges         = 50009
0.00.052.663 I llm_load_print_meta: vocab_only       = 0
0.00.052.663 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.663 I llm_load_print_meta: n_embd           = 2048
0.00.052.663 I llm_load_print_meta: n_layer          = 24
0.00.052.678 I llm_load_print_meta: n_head           = 16
0.00.052.680 I llm_load_print_meta: n_head_kv        = 16
0.00.052.680 I llm_load_print_meta: n_rot            = 32
0.00.052.681 I llm_load_print_meta: n_swa            = 0
0.00.052.681 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.681 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.682 I llm_load_print_meta: n_gqa            = 1
0.00.052.682 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.683 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.683 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.684 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.684 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.684 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.684 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.685 I llm_load_print_meta: n_ff             = 8192
0.00.052.685 I llm_load_print_meta: n_expert         = 0
0.00.052.685 I llm_load_print_meta: n_expert_used    = 0
0.00.052.685 I llm_load_print_meta: causal attn      = 1
0.00.052.685 I llm_load_print_meta: pooling type     = 0
0.00.052.685 I llm_load_print_meta: rope type        = 2
0.00.052.686 I llm_load_print_meta: rope scaling     = linear
0.00.052.686 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.686 I llm_load_print_meta: freq_scale_train = 1
0.00.052.686 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.686 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.687 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.687 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.687 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.687 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.687 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.697 I llm_load_print_meta: model type       = 1.4B
0.00.052.697 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.697 I llm_load_print_meta: model params     = 1.41 B
0.00.052.698 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.698 I llm_load_print_meta: general.name     = 1.4B
0.00.052.698 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.698 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.699 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.699 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.699 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.699 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.699 I llm_load_print_meta: max token length = 1024
0.00.054.796 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.796 I llm_load_tensors: offloading output layer to GPU
0.00.054.797 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.808 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.809 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.771 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.771 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.772 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.772 I llama_new_context_with_model: n_batch       = 2048
0.00.055.772 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.772 I llama_new_context_with_model: flash_attn    = 0
0.00.055.773 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.773 I llama_new_context_with_model: freq_scale    = 1
0.00.055.773 I ggml_metal_init: allocating
0.00.055.777 I ggml_metal_init: found device: Apple M4
0.00.055.779 I ggml_metal_init: picking default device: Apple M4
0.00.056.395 I ggml_metal_init: using embedded metal library
0.00.058.733 I ggml_metal_init: GPU name:   Apple M4
0.00.058.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.735 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.735 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.736 I ggml_metal_init: simdgroup reduction   = true
0.00.058.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.736 I ggml_metal_init: has bfloat            = true
0.00.058.736 I ggml_metal_init: use bfloat            = true
0.00.058.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.737 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.527 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.536 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.626 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.627 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.628 I llama_new_context_with_model: graph nodes  = 967
0.00.090.628 I llama_new_context_with_model: graph splits = 2
0.00.090.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.977 I main: llama threadpool init, n_threads = 4
0.00.752.024 I 
0.00.752.055 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.056 I 
0.00.752.286 I sampler seed: 1234
0.00.752.291 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.321 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.352 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.354 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.633.944 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.633.945 I llama_perf_context_print:        load time =     742.17 ms
0.01.633.945 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.58 tokens per second)
0.01.633.946 I llama_perf_context_print:        eval time =     824.11 ms /    63 runs   (   13.08 ms per token,    76.45 tokens per second)
0.01.633.946 I llama_perf_context_print:       total time =     881.97 ms /    70 tokens
0.01.634.128 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.112s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.542 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.806 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.458 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.477 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.481 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.481 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.486 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.487 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.488 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.488 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.489 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.498 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.499 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.852 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.852 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.854 I llama_model_loader: - type  f32:  194 tensors
0.00.055.854 I llama_model_loader: - type  f16:   98 tensors
0.00.085.687 I llm_load_vocab: special tokens cache size = 25
0.00.092.628 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.633 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.633 I llm_load_print_meta: arch             = gptneox
0.00.092.633 I llm_load_print_meta: vocab type       = BPE
0.00.092.634 I llm_load_print_meta: n_vocab          = 50304
0.00.092.634 I llm_load_print_meta: n_merges         = 50009
0.00.092.634 I llm_load_print_meta: vocab_only       = 0
0.00.092.634 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.634 I llm_load_print_meta: n_embd           = 2048
0.00.092.634 I llm_load_print_meta: n_layer          = 24
0.00.092.648 I llm_load_print_meta: n_head           = 16
0.00.092.650 I llm_load_print_meta: n_head_kv        = 16
0.00.092.650 I llm_load_print_meta: n_rot            = 32
0.00.092.650 I llm_load_print_meta: n_swa            = 0
0.00.092.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.651 I llm_load_print_meta: n_gqa            = 1
0.00.092.651 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.652 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.652 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.653 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.653 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.653 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.655 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.656 I llm_load_print_meta: n_ff             = 8192
0.00.092.656 I llm_load_print_meta: n_expert         = 0
0.00.092.657 I llm_load_print_meta: n_expert_used    = 0
0.00.092.659 I llm_load_print_meta: causal attn      = 1
0.00.092.659 I llm_load_print_meta: pooling type     = 0
0.00.092.659 I llm_load_print_meta: rope type        = 2
0.00.092.659 I llm_load_print_meta: rope scaling     = linear
0.00.092.659 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.660 I llm_load_print_meta: freq_scale_train = 1
0.00.092.660 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.660 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.660 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.660 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.660 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.660 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.661 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.670 I llm_load_print_meta: model type       = 1.4B
0.00.092.671 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.671 I llm_load_print_meta: model params     = 1.41 B
0.00.092.672 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.672 I llm_load_print_meta: general.name     = 1.4B
0.00.092.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.672 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.673 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.674 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.675 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.675 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.675 I llm_load_print_meta: max token length = 1024
0.00.095.276 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.276 I llm_load_tensors: offloading output layer to GPU
0.00.095.276 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.287 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.288 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.279 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.280 I llama_new_context_with_model: n_ctx         = 128
0.00.096.280 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.281 I llama_new_context_with_model: n_batch       = 128
0.00.096.281 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.281 I llama_new_context_with_model: flash_attn    = 0
0.00.096.281 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.282 I llama_new_context_with_model: freq_scale    = 1
0.00.096.282 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.283 I ggml_metal_init: allocating
0.00.096.292 I ggml_metal_init: found device: Apple M4
0.00.096.294 I ggml_metal_init: picking default device: Apple M4
0.00.096.950 I ggml_metal_init: using embedded metal library
0.00.099.560 I ggml_metal_init: GPU name:   Apple M4
0.00.099.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.564 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.564 I ggml_metal_init: simdgroup reduction   = true
0.00.099.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.564 I ggml_metal_init: has bfloat            = true
0.00.099.565 I ggml_metal_init: use bfloat            = true
0.00.099.565 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.566 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.430 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.111.436 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.111.450 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.325 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.112.326 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.112.326 I llama_new_context_with_model: graph nodes  = 967
0.00.112.326 I llama_new_context_with_model: graph splits = 2
0.00.112.339 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.967.314 I 
0.00.967.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.967.392 I perplexity: tokenizing the input ..
0.00.980.989 I perplexity: tokenization took 13.594 ms
0.00.981.027 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.103.138 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.104.985 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.105.022 I llama_perf_context_print:        load time =     942.49 ms
0.01.105.024 I llama_perf_context_print: prompt eval time =     121.21 ms /   128 tokens (    0.95 ms per token,  1056.00 tokens per second)
0.01.105.026 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.105.027 I llama_perf_context_print:       total time =     137.71 ms /   129 tokens
0.01.105.780 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.125s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.126 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.720 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.916 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.923 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.927 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.932 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.001 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.284 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.288 I llama_model_loader: - type  f32:  194 tensors
0.00.032.289 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.342 I llm_load_vocab: special tokens cache size = 25
0.00.062.535 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.538 I llm_load_print_meta: arch             = gptneox
0.00.062.539 I llm_load_print_meta: vocab type       = BPE
0.00.062.539 I llm_load_print_meta: n_vocab          = 50304
0.00.062.539 I llm_load_print_meta: n_merges         = 50009
0.00.062.539 I llm_load_print_meta: vocab_only       = 0
0.00.062.539 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.539 I llm_load_print_meta: n_embd           = 2048
0.00.062.540 I llm_load_print_meta: n_layer          = 24
0.00.062.555 I llm_load_print_meta: n_head           = 16
0.00.062.556 I llm_load_print_meta: n_head_kv        = 16
0.00.062.556 I llm_load_print_meta: n_rot            = 32
0.00.062.557 I llm_load_print_meta: n_swa            = 0
0.00.062.557 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.557 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.558 I llm_load_print_meta: n_gqa            = 1
0.00.062.558 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.559 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.560 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.560 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.560 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.560 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.560 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.561 I llm_load_print_meta: n_ff             = 8192
0.00.062.561 I llm_load_print_meta: n_expert         = 0
0.00.062.561 I llm_load_print_meta: n_expert_used    = 0
0.00.062.561 I llm_load_print_meta: causal attn      = 1
0.00.062.561 I llm_load_print_meta: pooling type     = 0
0.00.062.561 I llm_load_print_meta: rope type        = 2
0.00.062.562 I llm_load_print_meta: rope scaling     = linear
0.00.062.562 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.562 I llm_load_print_meta: freq_scale_train = 1
0.00.062.562 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.563 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.565 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.565 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.575 I llm_load_print_meta: model type       = 1.4B
0.00.062.575 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.576 I llm_load_print_meta: model params     = 1.41 B
0.00.062.576 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.577 I llm_load_print_meta: general.name     = 1.4B
0.00.062.577 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.577 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.578 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.578 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.578 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.579 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.579 I llm_load_print_meta: max token length = 1024
0.00.064.727 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.728 I llm_load_tensors: offloading output layer to GPU
0.00.064.728 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.738 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.739 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.686 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.687 I llama_new_context_with_model: n_ctx         = 128
0.00.065.687 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.065.687 I llama_new_context_with_model: n_batch       = 128
0.00.065.687 I llama_new_context_with_model: n_ubatch      = 128
0.00.065.688 I llama_new_context_with_model: flash_attn    = 0
0.00.065.688 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.688 I llama_new_context_with_model: freq_scale    = 1
0.00.065.689 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.689 I ggml_metal_init: allocating
0.00.065.696 I ggml_metal_init: found device: Apple M4
0.00.065.698 I ggml_metal_init: picking default device: Apple M4
0.00.066.300 I ggml_metal_init: using embedded metal library
0.00.068.796 I ggml_metal_init: GPU name:   Apple M4
0.00.068.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.798 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.799 I ggml_metal_init: simdgroup reduction   = true
0.00.068.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.799 I ggml_metal_init: has bfloat            = true
0.00.068.799 I ggml_metal_init: use bfloat            = true
0.00.068.799 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.800 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.873 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.890 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.823 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.824 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.825 I llama_new_context_with_model: graph nodes  = 967
0.00.080.825 I llama_new_context_with_model: graph splits = 2
0.00.080.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.149 I 
0.00.829.179 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.829.187 I perplexity: tokenizing the input ..
0.00.837.242 I perplexity: tokenization took 8.054 ms
0.00.837.252 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.960.653 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.961.945 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.961.959 I llama_perf_context_print:        load time =     817.43 ms
0.00.961.960 I llama_perf_context_print: prompt eval time =     123.18 ms /   128 tokens (    0.96 ms per token,  1039.16 tokens per second)
0.00.961.961 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.961.961 I llama_perf_context_print:       total time =     132.81 ms /   129 tokens
0.00.962.371 I ggml_metal_free: deallocating

real	0m0.978s
user	0m0.091s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.970 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.768 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.774 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.775 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.775 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.777 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.778 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.571 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.572 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.572 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.573 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.573 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.573 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.574 I llama_model_loader: - type  f32:  194 tensors
0.00.025.574 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.574 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.765 I llm_load_vocab: special tokens cache size = 25
0.00.051.652 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.655 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.656 I llm_load_print_meta: arch             = gptneox
0.00.051.656 I llm_load_print_meta: vocab type       = BPE
0.00.051.656 I llm_load_print_meta: n_vocab          = 50304
0.00.051.656 I llm_load_print_meta: n_merges         = 50009
0.00.051.657 I llm_load_print_meta: vocab_only       = 0
0.00.051.657 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.657 I llm_load_print_meta: n_embd           = 2048
0.00.051.657 I llm_load_print_meta: n_layer          = 24
0.00.051.671 I llm_load_print_meta: n_head           = 16
0.00.051.672 I llm_load_print_meta: n_head_kv        = 16
0.00.051.672 I llm_load_print_meta: n_rot            = 32
0.00.051.672 I llm_load_print_meta: n_swa            = 0
0.00.051.672 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.672 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.673 I llm_load_print_meta: n_gqa            = 1
0.00.051.674 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.674 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.675 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.675 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.675 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.676 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.676 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.677 I llm_load_print_meta: n_ff             = 8192
0.00.051.677 I llm_load_print_meta: n_expert         = 0
0.00.051.677 I llm_load_print_meta: n_expert_used    = 0
0.00.051.677 I llm_load_print_meta: causal attn      = 1
0.00.051.677 I llm_load_print_meta: pooling type     = 0
0.00.051.677 I llm_load_print_meta: rope type        = 2
0.00.051.678 I llm_load_print_meta: rope scaling     = linear
0.00.051.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.678 I llm_load_print_meta: freq_scale_train = 1
0.00.051.678 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.678 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.679 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.688 I llm_load_print_meta: model type       = 1.4B
0.00.051.689 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.691 I llm_load_print_meta: model params     = 1.41 B
0.00.051.691 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.691 I llm_load_print_meta: general.name     = 1.4B
0.00.051.691 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.693 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.693 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.693 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.693 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.693 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.694 I llm_load_print_meta: max token length = 1024
0.00.053.675 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.675 I llm_load_tensors: offloading output layer to GPU
0.00.053.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.686 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.687 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.637 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.638 I llama_new_context_with_model: n_ctx         = 128
0.00.054.638 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.638 I llama_new_context_with_model: n_batch       = 128
0.00.054.639 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.639 I llama_new_context_with_model: flash_attn    = 0
0.00.054.639 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.640 I llama_new_context_with_model: freq_scale    = 1
0.00.054.640 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.640 I ggml_metal_init: allocating
0.00.054.646 I ggml_metal_init: found device: Apple M4
0.00.054.648 I ggml_metal_init: picking default device: Apple M4
0.00.055.205 I ggml_metal_init: using embedded metal library
0.00.057.569 I ggml_metal_init: GPU name:   Apple M4
0.00.057.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.572 I ggml_metal_init: simdgroup reduction   = true
0.00.057.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.572 I ggml_metal_init: has bfloat            = true
0.00.057.572 I ggml_metal_init: use bfloat            = true
0.00.057.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.397 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.399 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.414 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.303 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.304 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.304 I llama_new_context_with_model: graph nodes  = 967
0.00.069.304 I llama_new_context_with_model: graph splits = 2
0.00.069.311 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.185 I 
0.00.618.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.270 I perplexity: tokenizing the input ..
0.00.626.455 I perplexity: tokenization took 8.184 ms
0.00.626.472 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.748.901 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.118 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.137 I llama_perf_context_print:        load time =     607.21 ms
0.00.750.139 I llama_perf_context_print: prompt eval time =     122.19 ms /   128 tokens (    0.95 ms per token,  1047.55 tokens per second)
0.00.750.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.141 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.750.654 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.078s
sys	0m0.102s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.894 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.731 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.732 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.735 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.736 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.739 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.685 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.547 I llama_model_loader: - type  f32:  194 tensors
0.00.023.548 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.548 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.580 I llm_load_vocab: special tokens cache size = 25
0.00.050.686 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.689 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.689 I llm_load_print_meta: arch             = gptneox
0.00.050.689 I llm_load_print_meta: vocab type       = BPE
0.00.050.690 I llm_load_print_meta: n_vocab          = 50304
0.00.050.690 I llm_load_print_meta: n_merges         = 50009
0.00.050.690 I llm_load_print_meta: vocab_only       = 0
0.00.050.690 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.690 I llm_load_print_meta: n_embd           = 2048
0.00.050.691 I llm_load_print_meta: n_layer          = 24
0.00.050.705 I llm_load_print_meta: n_head           = 16
0.00.050.706 I llm_load_print_meta: n_head_kv        = 16
0.00.050.706 I llm_load_print_meta: n_rot            = 32
0.00.050.706 I llm_load_print_meta: n_swa            = 0
0.00.050.706 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.706 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.707 I llm_load_print_meta: n_gqa            = 1
0.00.050.708 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.708 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.709 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.709 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.709 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.710 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.710 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.710 I llm_load_print_meta: n_ff             = 8192
0.00.050.711 I llm_load_print_meta: n_expert         = 0
0.00.050.711 I llm_load_print_meta: n_expert_used    = 0
0.00.050.711 I llm_load_print_meta: causal attn      = 1
0.00.050.711 I llm_load_print_meta: pooling type     = 0
0.00.050.711 I llm_load_print_meta: rope type        = 2
0.00.050.711 I llm_load_print_meta: rope scaling     = linear
0.00.050.712 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.713 I llm_load_print_meta: freq_scale_train = 1
0.00.050.713 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.713 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.713 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.713 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.713 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.713 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.714 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.723 I llm_load_print_meta: model type       = 1.4B
0.00.050.724 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.725 I llm_load_print_meta: model params     = 1.41 B
0.00.050.725 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.726 I llm_load_print_meta: general.name     = 1.4B
0.00.050.727 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.728 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.728 I llm_load_print_meta: max token length = 1024
0.00.052.660 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.660 I llm_load_tensors: offloading output layer to GPU
0.00.052.660 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.671 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.672 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.533 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.534 I llama_new_context_with_model: n_ctx         = 128
0.00.053.534 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.534 I llama_new_context_with_model: n_batch       = 128
0.00.053.534 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.534 I llama_new_context_with_model: flash_attn    = 0
0.00.053.535 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.535 I llama_new_context_with_model: freq_scale    = 1
0.00.053.536 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.536 I ggml_metal_init: allocating
0.00.053.542 I ggml_metal_init: found device: Apple M4
0.00.053.544 I ggml_metal_init: picking default device: Apple M4
0.00.054.120 I ggml_metal_init: using embedded metal library
0.00.056.408 I ggml_metal_init: GPU name:   Apple M4
0.00.056.409 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.410 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.410 I ggml_metal_init: simdgroup reduction   = true
0.00.056.410 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.411 I ggml_metal_init: has bfloat            = true
0.00.056.411 I ggml_metal_init: use bfloat            = true
0.00.056.411 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.093 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.095 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.114 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.967 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.968 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.968 I llama_new_context_with_model: graph nodes  = 967
0.00.067.969 I llama_new_context_with_model: graph splits = 2
0.00.067.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.640.562 I 
0.00.640.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.640.596 I perplexity: tokenizing the input ..
0.00.648.886 I perplexity: tokenization took 8.289 ms
0.00.648.896 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.771.030 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.772.450 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.772.466 I llama_perf_context_print:        load time =     631.67 ms
0.00.772.467 I llama_perf_context_print: prompt eval time =     121.89 ms /   128 tokens (    0.95 ms per token,  1050.16 tokens per second)
0.00.772.468 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.772.469 I llama_perf_context_print:       total time =     131.90 ms /   129 tokens
0.00.772.838 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.079s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.317 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.499 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.506 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.506 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.507 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.508 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.508 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.509 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.509 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.510 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.510 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.513 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.578 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.553 I llama_model_loader: - type  f32:  194 tensors
0.00.024.554 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.554 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.061 I llm_load_vocab: special tokens cache size = 25
0.00.052.068 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.074 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.074 I llm_load_print_meta: arch             = gptneox
0.00.052.075 I llm_load_print_meta: vocab type       = BPE
0.00.052.075 I llm_load_print_meta: n_vocab          = 50304
0.00.052.075 I llm_load_print_meta: n_merges         = 50009
0.00.052.075 I llm_load_print_meta: vocab_only       = 0
0.00.052.075 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.075 I llm_load_print_meta: n_embd           = 2048
0.00.052.075 I llm_load_print_meta: n_layer          = 24
0.00.052.093 I llm_load_print_meta: n_head           = 16
0.00.052.094 I llm_load_print_meta: n_head_kv        = 16
0.00.052.094 I llm_load_print_meta: n_rot            = 32
0.00.052.094 I llm_load_print_meta: n_swa            = 0
0.00.052.095 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.095 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.095 I llm_load_print_meta: n_gqa            = 1
0.00.052.096 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.096 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.097 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.097 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.097 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.098 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.098 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.098 I llm_load_print_meta: n_ff             = 8192
0.00.052.098 I llm_load_print_meta: n_expert         = 0
0.00.052.099 I llm_load_print_meta: n_expert_used    = 0
0.00.052.099 I llm_load_print_meta: causal attn      = 1
0.00.052.099 I llm_load_print_meta: pooling type     = 0
0.00.052.099 I llm_load_print_meta: rope type        = 2
0.00.052.099 I llm_load_print_meta: rope scaling     = linear
0.00.052.099 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.100 I llm_load_print_meta: freq_scale_train = 1
0.00.052.100 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.100 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.100 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.100 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.100 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.101 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.101 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.110 I llm_load_print_meta: model type       = 1.4B
0.00.052.111 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.111 I llm_load_print_meta: model params     = 1.41 B
0.00.052.112 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.112 I llm_load_print_meta: general.name     = 1.4B
0.00.052.112 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.113 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.113 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.115 I llm_load_print_meta: max token length = 1024
0.00.054.204 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.205 I llm_load_tensors: offloading output layer to GPU
0.00.054.205 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.216 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.217 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.139 I llama_new_context_with_model: n_ctx         = 128
0.00.055.139 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.139 I llama_new_context_with_model: n_batch       = 128
0.00.055.139 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.139 I llama_new_context_with_model: flash_attn    = 0
0.00.055.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.140 I llama_new_context_with_model: freq_scale    = 1
0.00.055.141 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.141 I ggml_metal_init: allocating
0.00.055.145 I ggml_metal_init: found device: Apple M4
0.00.055.147 I ggml_metal_init: picking default device: Apple M4
0.00.055.744 I ggml_metal_init: using embedded metal library
0.00.058.167 I ggml_metal_init: GPU name:   Apple M4
0.00.058.168 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.170 I ggml_metal_init: simdgroup reduction   = true
0.00.058.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.170 I ggml_metal_init: has bfloat            = true
0.00.058.170 I ggml_metal_init: use bfloat            = true
0.00.058.171 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.469 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.474 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.490 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.389 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.390 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.390 I llama_new_context_with_model: graph nodes  = 967
0.00.069.391 I llama_new_context_with_model: graph splits = 2
0.00.069.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.747 I 
0.00.707.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.788 I perplexity: tokenizing the input ..
0.00.715.445 I perplexity: tokenization took 7.656 ms
0.00.715.457 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.761 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.850.927 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.850.937 I llama_perf_context_print:        load time =     698.43 ms
0.00.850.938 I llama_perf_context_print: prompt eval time =     134.05 ms /   128 tokens (    1.05 ms per token,   954.85 tokens per second)
0.00.850.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.939 I llama_perf_context_print:       total time =     143.19 ms /   129 tokens
0.00.851.348 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.080s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.505 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.585 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.600 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.411 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.412 I llama_model_loader: - type  f32:  194 tensors
0.00.028.412 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.399 I llm_load_vocab: special tokens cache size = 25
0.00.055.356 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.359 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.359 I llm_load_print_meta: arch             = gptneox
0.00.055.359 I llm_load_print_meta: vocab type       = BPE
0.00.055.360 I llm_load_print_meta: n_vocab          = 50304
0.00.055.360 I llm_load_print_meta: n_merges         = 50009
0.00.055.360 I llm_load_print_meta: vocab_only       = 0
0.00.055.360 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.360 I llm_load_print_meta: n_embd           = 2048
0.00.055.361 I llm_load_print_meta: n_layer          = 24
0.00.055.375 I llm_load_print_meta: n_head           = 16
0.00.055.377 I llm_load_print_meta: n_head_kv        = 16
0.00.055.377 I llm_load_print_meta: n_rot            = 32
0.00.055.377 I llm_load_print_meta: n_swa            = 0
0.00.055.377 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.377 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.378 I llm_load_print_meta: n_gqa            = 1
0.00.055.379 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.380 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.380 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.381 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.381 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.381 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.381 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.382 I llm_load_print_meta: n_ff             = 8192
0.00.055.382 I llm_load_print_meta: n_expert         = 0
0.00.055.384 I llm_load_print_meta: n_expert_used    = 0
0.00.055.384 I llm_load_print_meta: causal attn      = 1
0.00.055.384 I llm_load_print_meta: pooling type     = 0
0.00.055.384 I llm_load_print_meta: rope type        = 2
0.00.055.384 I llm_load_print_meta: rope scaling     = linear
0.00.055.384 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.385 I llm_load_print_meta: freq_scale_train = 1
0.00.055.385 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.385 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.385 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.385 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.386 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.387 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.387 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.396 I llm_load_print_meta: model type       = 1.4B
0.00.055.397 I llm_load_print_meta: model ftype      = Q5_1
0.00.055.397 I llm_load_print_meta: model params     = 1.41 B
0.00.055.397 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.055.398 I llm_load_print_meta: general.name     = 1.4B
0.00.055.398 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.398 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.398 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.398 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.399 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.399 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.399 I llm_load_print_meta: max token length = 1024
0.00.057.365 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.365 I llm_load_tensors: offloading output layer to GPU
0.00.057.365 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.375 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.057.377 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.058.298 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.299 I llama_new_context_with_model: n_ctx         = 128
0.00.058.299 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.058.299 I llama_new_context_with_model: n_batch       = 128
0.00.058.299 I llama_new_context_with_model: n_ubatch      = 128
0.00.058.299 I llama_new_context_with_model: flash_attn    = 0
0.00.058.300 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.300 I llama_new_context_with_model: freq_scale    = 1
0.00.058.300 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.058.301 I ggml_metal_init: allocating
0.00.058.304 I ggml_metal_init: found device: Apple M4
0.00.058.307 I ggml_metal_init: picking default device: Apple M4
0.00.058.847 I ggml_metal_init: using embedded metal library
0.00.061.226 I ggml_metal_init: GPU name:   Apple M4
0.00.061.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.228 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.230 I ggml_metal_init: simdgroup reduction   = true
0.00.061.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.230 I ggml_metal_init: has bfloat            = true
0.00.061.230 I ggml_metal_init: use bfloat            = true
0.00.061.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.233 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.567 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.584 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.442 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.443 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.443 I llama_new_context_with_model: graph nodes  = 967
0.00.073.443 I llama_new_context_with_model: graph splits = 2
0.00.073.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.682 I 
0.00.750.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.717 I perplexity: tokenizing the input ..
0.00.758.863 I perplexity: tokenization took 8.145 ms
0.00.758.875 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.893.799 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.894.994 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.895.046 I llama_perf_context_print:        load time =     742.17 ms
0.00.895.046 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.31 tokens per second)
0.00.895.048 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.048 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.895.509 I ggml_metal_free: deallocating

real	0m0.908s
user	0m0.080s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.175 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.745 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.023.750 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.752 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.755 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.756 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.758 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.758 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.758 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.758 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.759 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.762 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.762 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.762 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.034.967 I llama_model_loader: - type  f32:  194 tensors
0.00.034.968 I llama_model_loader: - type q2_K:   49 tensors
0.00.034.968 I llama_model_loader: - type q3_K:   48 tensors
0.00.034.968 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.327 I llm_load_vocab: special tokens cache size = 25
0.00.077.224 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.229 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.229 I llm_load_print_meta: arch             = gptneox
0.00.077.230 I llm_load_print_meta: vocab type       = BPE
0.00.077.230 I llm_load_print_meta: n_vocab          = 50304
0.00.077.230 I llm_load_print_meta: n_merges         = 50009
0.00.077.231 I llm_load_print_meta: vocab_only       = 0
0.00.077.231 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.231 I llm_load_print_meta: n_embd           = 2048
0.00.077.234 I llm_load_print_meta: n_layer          = 24
0.00.077.248 I llm_load_print_meta: n_head           = 16
0.00.077.249 I llm_load_print_meta: n_head_kv        = 16
0.00.077.250 I llm_load_print_meta: n_rot            = 32
0.00.077.250 I llm_load_print_meta: n_swa            = 0
0.00.077.250 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.250 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.251 I llm_load_print_meta: n_gqa            = 1
0.00.077.252 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.253 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.254 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.255 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.256 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.257 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.257 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.258 I llm_load_print_meta: n_ff             = 8192
0.00.077.258 I llm_load_print_meta: n_expert         = 0
0.00.077.258 I llm_load_print_meta: n_expert_used    = 0
0.00.077.258 I llm_load_print_meta: causal attn      = 1
0.00.077.260 I llm_load_print_meta: pooling type     = 0
0.00.077.260 I llm_load_print_meta: rope type        = 2
0.00.077.261 I llm_load_print_meta: rope scaling     = linear
0.00.077.261 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.262 I llm_load_print_meta: freq_scale_train = 1
0.00.077.262 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.262 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.262 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.263 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.263 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.263 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.263 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.275 I llm_load_print_meta: model type       = 1.4B
0.00.077.275 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.077.277 I llm_load_print_meta: model params     = 1.41 B
0.00.077.278 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.077.278 I llm_load_print_meta: general.name     = 1.4B
0.00.077.279 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.279 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.279 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.279 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.280 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.077.280 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.280 I llm_load_print_meta: max token length = 1024
0.00.080.073 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.074 I llm_load_tensors: offloading output layer to GPU
0.00.080.074 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.085 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.080.087 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.081.631 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.632 I llama_new_context_with_model: n_ctx         = 128
0.00.081.632 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.081.633 I llama_new_context_with_model: n_batch       = 128
0.00.081.633 I llama_new_context_with_model: n_ubatch      = 128
0.00.081.633 I llama_new_context_with_model: flash_attn    = 0
0.00.081.634 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.634 I llama_new_context_with_model: freq_scale    = 1
0.00.081.635 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.081.635 I ggml_metal_init: allocating
0.00.081.640 I ggml_metal_init: found device: Apple M4
0.00.081.643 I ggml_metal_init: picking default device: Apple M4
0.00.082.512 I ggml_metal_init: using embedded metal library
0.00.086.283 I ggml_metal_init: GPU name:   Apple M4
0.00.086.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.287 I ggml_metal_init: simdgroup reduction   = true
0.00.086.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.287 I ggml_metal_init: has bfloat            = true
0.00.086.287 I ggml_metal_init: use bfloat            = true
0.00.086.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.437 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.445 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.466 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.555 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.556 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.557 I llama_new_context_with_model: graph nodes  = 967
0.00.100.557 I llama_new_context_with_model: graph splits = 2
0.00.100.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.496 I 
0.00.509.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.569 I perplexity: tokenizing the input ..
0.00.522.568 I perplexity: tokenization took 12.997 ms
0.00.522.592 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.656.528 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.657.800 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.657.823 I llama_perf_context_print:        load time =     495.32 ms
0.00.657.824 I llama_perf_context_print: prompt eval time =     133.40 ms /   128 tokens (    1.04 ms per token,   959.49 tokens per second)
0.00.657.827 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.657.828 I llama_perf_context_print:       total time =     148.33 ms /   129 tokens
0.00.658.471 I ggml_metal_free: deallocating

real	0m0.688s
user	0m0.108s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.538 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.539 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.539 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.541 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.541 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.542 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.542 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.542 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.545 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.545 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.546 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.493 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.538 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.538 I llama_model_loader: - type  f32:  194 tensors
0.00.026.539 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.539 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.539 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.539 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.485 I llm_load_vocab: special tokens cache size = 25
0.00.053.539 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.542 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.542 I llm_load_print_meta: arch             = gptneox
0.00.053.543 I llm_load_print_meta: vocab type       = BPE
0.00.053.543 I llm_load_print_meta: n_vocab          = 50304
0.00.053.543 I llm_load_print_meta: n_merges         = 50009
0.00.053.543 I llm_load_print_meta: vocab_only       = 0
0.00.053.543 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.543 I llm_load_print_meta: n_embd           = 2048
0.00.053.544 I llm_load_print_meta: n_layer          = 24
0.00.053.558 I llm_load_print_meta: n_head           = 16
0.00.053.558 I llm_load_print_meta: n_head_kv        = 16
0.00.053.559 I llm_load_print_meta: n_rot            = 32
0.00.053.561 I llm_load_print_meta: n_swa            = 0
0.00.053.561 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.561 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.562 I llm_load_print_meta: n_gqa            = 1
0.00.053.562 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.563 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.564 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.564 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.564 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.564 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.564 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.565 I llm_load_print_meta: n_ff             = 8192
0.00.053.565 I llm_load_print_meta: n_expert         = 0
0.00.053.566 I llm_load_print_meta: n_expert_used    = 0
0.00.053.566 I llm_load_print_meta: causal attn      = 1
0.00.053.566 I llm_load_print_meta: pooling type     = 0
0.00.053.568 I llm_load_print_meta: rope type        = 2
0.00.053.569 I llm_load_print_meta: rope scaling     = linear
0.00.053.569 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.570 I llm_load_print_meta: freq_scale_train = 1
0.00.053.570 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.570 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.570 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.570 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.570 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.570 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.570 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.580 I llm_load_print_meta: model type       = 1.4B
0.00.053.580 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.581 I llm_load_print_meta: model params     = 1.41 B
0.00.053.581 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.581 I llm_load_print_meta: general.name     = 1.4B
0.00.053.582 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.582 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.582 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.582 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.582 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.582 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.583 I llm_load_print_meta: max token length = 1024
0.00.055.576 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.576 I llm_load_tensors: offloading output layer to GPU
0.00.055.576 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.587 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.588 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.572 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.573 I llama_new_context_with_model: n_ctx         = 128
0.00.056.573 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.573 I llama_new_context_with_model: n_batch       = 128
0.00.056.574 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.574 I llama_new_context_with_model: flash_attn    = 0
0.00.056.574 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.574 I llama_new_context_with_model: freq_scale    = 1
0.00.056.575 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.576 I ggml_metal_init: allocating
0.00.056.582 I ggml_metal_init: found device: Apple M4
0.00.056.584 I ggml_metal_init: picking default device: Apple M4
0.00.057.164 I ggml_metal_init: using embedded metal library
0.00.059.544 I ggml_metal_init: GPU name:   Apple M4
0.00.059.545 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.546 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.546 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.547 I ggml_metal_init: simdgroup reduction   = true
0.00.059.548 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.548 I ggml_metal_init: has bfloat            = true
0.00.059.548 I ggml_metal_init: use bfloat            = true
0.00.059.548 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.549 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.155 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.158 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.172 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.059 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.060 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.061 I llama_new_context_with_model: graph nodes  = 967
0.00.071.061 I llama_new_context_with_model: graph splits = 2
0.00.071.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.694 I 
0.00.537.759 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.772 I perplexity: tokenizing the input ..
0.00.545.968 I perplexity: tokenization took 8.195 ms
0.00.545.979 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.678.067 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.679.232 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.679.248 I llama_perf_context_print:        load time =     528.87 ms
0.00.679.249 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.71 tokens per second)
0.00.679.250 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.679.252 I llama_perf_context_print:       total time =     141.56 ms /   129 tokens
0.00.679.784 I ggml_metal_free: deallocating

real	0m0.693s
user	0m0.079s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.074 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.735 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.441 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.022.446 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.447 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.451 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.453 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.454 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.820 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.032.821 I llama_model_loader: - type  f32:  194 tensors
0.00.032.821 I llama_model_loader: - type q4_K:   61 tensors
0.00.032.821 I llama_model_loader: - type q5_K:   24 tensors
0.00.032.821 I llama_model_loader: - type q6_K:   13 tensors
0.00.061.806 I llm_load_vocab: special tokens cache size = 25
0.00.072.489 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.493 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.494 I llm_load_print_meta: arch             = gptneox
0.00.072.494 I llm_load_print_meta: vocab type       = BPE
0.00.072.494 I llm_load_print_meta: n_vocab          = 50304
0.00.072.495 I llm_load_print_meta: n_merges         = 50009
0.00.072.495 I llm_load_print_meta: vocab_only       = 0
0.00.072.495 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.495 I llm_load_print_meta: n_embd           = 2048
0.00.072.496 I llm_load_print_meta: n_layer          = 24
0.00.072.512 I llm_load_print_meta: n_head           = 16
0.00.072.513 I llm_load_print_meta: n_head_kv        = 16
0.00.072.513 I llm_load_print_meta: n_rot            = 32
0.00.072.513 I llm_load_print_meta: n_swa            = 0
0.00.072.513 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.514 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.515 I llm_load_print_meta: n_gqa            = 1
0.00.072.517 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.518 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.519 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.522 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.522 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.522 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.522 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.523 I llm_load_print_meta: n_ff             = 8192
0.00.072.523 I llm_load_print_meta: n_expert         = 0
0.00.072.524 I llm_load_print_meta: n_expert_used    = 0
0.00.072.524 I llm_load_print_meta: causal attn      = 1
0.00.072.524 I llm_load_print_meta: pooling type     = 0
0.00.072.524 I llm_load_print_meta: rope type        = 2
0.00.072.524 I llm_load_print_meta: rope scaling     = linear
0.00.072.525 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.525 I llm_load_print_meta: freq_scale_train = 1
0.00.072.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.526 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.526 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.527 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.527 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.527 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.527 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.537 I llm_load_print_meta: model type       = 1.4B
0.00.072.538 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.072.539 I llm_load_print_meta: model params     = 1.41 B
0.00.072.540 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.072.540 I llm_load_print_meta: general.name     = 1.4B
0.00.072.540 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.540 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.541 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.541 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.543 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.543 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.544 I llm_load_print_meta: max token length = 1024
0.00.075.360 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.360 I llm_load_tensors: offloading output layer to GPU
0.00.075.360 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.372 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.075.373 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.076.827 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.828 I llama_new_context_with_model: n_ctx         = 128
0.00.076.828 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.076.828 I llama_new_context_with_model: n_batch       = 128
0.00.076.829 I llama_new_context_with_model: n_ubatch      = 128
0.00.076.829 I llama_new_context_with_model: flash_attn    = 0
0.00.076.830 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.830 I llama_new_context_with_model: freq_scale    = 1
0.00.076.831 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.831 I ggml_metal_init: allocating
0.00.076.840 I ggml_metal_init: found device: Apple M4
0.00.076.843 I ggml_metal_init: picking default device: Apple M4
0.00.077.714 I ggml_metal_init: using embedded metal library
0.00.081.818 I ggml_metal_init: GPU name:   Apple M4
0.00.081.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.821 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.821 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.822 I ggml_metal_init: simdgroup reduction   = true
0.00.081.822 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.822 I ggml_metal_init: has bfloat            = true
0.00.081.822 I ggml_metal_init: use bfloat            = true
0.00.081.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.823 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.793 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.095.800 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.095.818 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.853 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.096.855 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.096.855 I llama_new_context_with_model: graph nodes  = 967
0.00.096.855 I llama_new_context_with_model: graph splits = 2
0.00.096.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.946 I 
0.00.661.985 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.993 I perplexity: tokenizing the input ..
0.00.673.245 I perplexity: tokenization took 11.25 ms
0.00.673.260 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.656 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.808.806 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.808.819 I llama_perf_context_print:        load time =     651.21 ms
0.00.808.820 I llama_perf_context_print: prompt eval time =     134.18 ms /   128 tokens (    1.05 ms per token,   953.97 tokens per second)
0.00.808.820 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.821 I llama_perf_context_print:       total time =     146.88 ms /   129 tokens
0.00.809.137 I ggml_metal_free: deallocating

real	0m0.837s
user	0m0.102s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.077 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.905 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.021.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.931 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.932 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.933 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.933 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.934 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.934 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.939 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.658 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.665 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.610 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.610 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.611 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.611 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.611 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.030.612 I llama_model_loader: - type  f32:  194 tensors
0.00.030.612 I llama_model_loader: - type q5_K:   61 tensors
0.00.030.612 I llama_model_loader: - type q6_K:   37 tensors
0.00.052.336 I llm_load_vocab: special tokens cache size = 25
0.00.058.327 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.330 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.330 I llm_load_print_meta: arch             = gptneox
0.00.058.331 I llm_load_print_meta: vocab type       = BPE
0.00.058.331 I llm_load_print_meta: n_vocab          = 50304
0.00.058.331 I llm_load_print_meta: n_merges         = 50009
0.00.058.331 I llm_load_print_meta: vocab_only       = 0
0.00.058.331 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.332 I llm_load_print_meta: n_embd           = 2048
0.00.058.332 I llm_load_print_meta: n_layer          = 24
0.00.058.346 I llm_load_print_meta: n_head           = 16
0.00.058.347 I llm_load_print_meta: n_head_kv        = 16
0.00.058.347 I llm_load_print_meta: n_rot            = 32
0.00.058.347 I llm_load_print_meta: n_swa            = 0
0.00.058.347 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.348 I llm_load_print_meta: n_gqa            = 1
0.00.058.349 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.349 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.350 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.351 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.352 I llm_load_print_meta: n_ff             = 8192
0.00.058.352 I llm_load_print_meta: n_expert         = 0
0.00.058.352 I llm_load_print_meta: n_expert_used    = 0
0.00.058.352 I llm_load_print_meta: causal attn      = 1
0.00.058.352 I llm_load_print_meta: pooling type     = 0
0.00.058.353 I llm_load_print_meta: rope type        = 2
0.00.058.353 I llm_load_print_meta: rope scaling     = linear
0.00.058.354 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.356 I llm_load_print_meta: freq_scale_train = 1
0.00.058.356 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.357 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.357 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.357 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.357 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.368 I llm_load_print_meta: model type       = 1.4B
0.00.058.368 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.058.369 I llm_load_print_meta: model params     = 1.41 B
0.00.058.369 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.058.370 I llm_load_print_meta: general.name     = 1.4B
0.00.058.370 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.370 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.370 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.370 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.370 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.058.371 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.371 I llm_load_print_meta: max token length = 1024
0.00.060.296 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.296 I llm_load_tensors: offloading output layer to GPU
0.00.060.296 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.307 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.060.308 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.061.207 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.208 I llama_new_context_with_model: n_ctx         = 128
0.00.061.208 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.061.208 I llama_new_context_with_model: n_batch       = 128
0.00.061.209 I llama_new_context_with_model: n_ubatch      = 128
0.00.061.209 I llama_new_context_with_model: flash_attn    = 0
0.00.061.209 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.209 I llama_new_context_with_model: freq_scale    = 1
0.00.061.210 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.210 I ggml_metal_init: allocating
0.00.061.213 I ggml_metal_init: found device: Apple M4
0.00.061.215 I ggml_metal_init: picking default device: Apple M4
0.00.061.786 I ggml_metal_init: using embedded metal library
0.00.064.131 I ggml_metal_init: GPU name:   Apple M4
0.00.064.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.134 I ggml_metal_init: simdgroup reduction   = true
0.00.064.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.134 I ggml_metal_init: has bfloat            = true
0.00.064.134 I ggml_metal_init: use bfloat            = true
0.00.064.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.123 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.125 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.140 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.056 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.057 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.057 I llama_new_context_with_model: graph nodes  = 967
0.00.076.058 I llama_new_context_with_model: graph splits = 2
0.00.076.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.880 I 
0.00.735.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.917 I perplexity: tokenizing the input ..
0.00.743.706 I perplexity: tokenization took 7.787 ms
0.00.743.716 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.643 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.885.833 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.885.852 I llama_perf_context_print:        load time =     726.97 ms
0.00.885.853 I llama_perf_context_print: prompt eval time =     140.68 ms /   128 tokens (    1.10 ms per token,   909.86 tokens per second)
0.00.885.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.885.855 I llama_perf_context_print:       total time =     149.97 ms /   129 tokens
0.00.886.355 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.080s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.296 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.077 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.081 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.083 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.084 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.084 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.084 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.086 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.907 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.905 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.661 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.661 I llama_model_loader: - type  f32:  194 tensors
0.00.023.661 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.496 I llm_load_vocab: special tokens cache size = 25
0.00.050.600 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.603 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.603 I llm_load_print_meta: arch             = gptneox
0.00.050.604 I llm_load_print_meta: vocab type       = BPE
0.00.050.604 I llm_load_print_meta: n_vocab          = 50304
0.00.050.604 I llm_load_print_meta: n_merges         = 50009
0.00.050.604 I llm_load_print_meta: vocab_only       = 0
0.00.050.604 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.605 I llm_load_print_meta: n_embd           = 2048
0.00.050.605 I llm_load_print_meta: n_layer          = 24
0.00.050.619 I llm_load_print_meta: n_head           = 16
0.00.050.620 I llm_load_print_meta: n_head_kv        = 16
0.00.050.621 I llm_load_print_meta: n_rot            = 32
0.00.050.621 I llm_load_print_meta: n_swa            = 0
0.00.050.621 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.622 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.623 I llm_load_print_meta: n_gqa            = 1
0.00.050.624 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.625 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.626 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.627 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.629 I llm_load_print_meta: n_ff             = 8192
0.00.050.629 I llm_load_print_meta: n_expert         = 0
0.00.050.629 I llm_load_print_meta: n_expert_used    = 0
0.00.050.629 I llm_load_print_meta: causal attn      = 1
0.00.050.629 I llm_load_print_meta: pooling type     = 0
0.00.050.633 I llm_load_print_meta: rope type        = 2
0.00.050.634 I llm_load_print_meta: rope scaling     = linear
0.00.050.634 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.634 I llm_load_print_meta: freq_scale_train = 1
0.00.050.636 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.636 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.636 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.636 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.636 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.636 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.636 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.646 I llm_load_print_meta: model type       = 1.4B
0.00.050.646 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.647 I llm_load_print_meta: model params     = 1.41 B
0.00.050.647 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.647 I llm_load_print_meta: general.name     = 1.4B
0.00.050.647 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.649 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.649 I llm_load_print_meta: max token length = 1024
0.00.052.658 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.658 I llm_load_tensors: offloading output layer to GPU
0.00.052.658 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.669 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.670 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.568 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.569 I llama_new_context_with_model: n_ctx         = 128
0.00.053.569 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.569 I llama_new_context_with_model: n_batch       = 128
0.00.053.569 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.569 I llama_new_context_with_model: flash_attn    = 0
0.00.053.570 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.570 I llama_new_context_with_model: freq_scale    = 1
0.00.053.570 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.571 I ggml_metal_init: allocating
0.00.053.577 I ggml_metal_init: found device: Apple M4
0.00.053.579 I ggml_metal_init: picking default device: Apple M4
0.00.054.125 I ggml_metal_init: using embedded metal library
0.00.056.440 I ggml_metal_init: GPU name:   Apple M4
0.00.056.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.442 I ggml_metal_init: simdgroup reduction   = true
0.00.056.442 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.442 I ggml_metal_init: has bfloat            = true
0.00.056.442 I ggml_metal_init: use bfloat            = true
0.00.056.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.946 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.956 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.974 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.803 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.804 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.804 I llama_new_context_with_model: graph nodes  = 967
0.00.067.805 I llama_new_context_with_model: graph splits = 2
0.00.067.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.274.767 I 
0.00.274.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.274.809 I perplexity: tokenizing the input ..
0.00.282.880 I perplexity: tokenization took 8.068 ms
0.00.282.890 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.422.969 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.424.252 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.424.269 I llama_perf_context_print:        load time =     265.47 ms
0.00.424.272 I llama_perf_context_print: prompt eval time =     139.85 ms /   128 tokens (    1.09 ms per token,   915.25 tokens per second)
0.00.424.273 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.424.273 I llama_perf_context_print:       total time =     149.50 ms /   129 tokens
0.00.424.635 I ggml_metal_free: deallocating

real	0m0.441s
user	0m0.078s
sys	0m0.062s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.268 I build: 4321 (9f35e445) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.985 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.976 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.984 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.986 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.987 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.988 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.988 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.997 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.997 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.001 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.001 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.002 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.420 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.241 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.770 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.772 I llama_model_loader: - type  f32:  194 tensors
0.00.050.772 I llama_model_loader: - type  f16:   98 tensors
0.00.078.239 I llm_load_vocab: special tokens cache size = 25
0.00.084.611 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.613 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.614 I llm_load_print_meta: arch             = gptneox
0.00.084.614 I llm_load_print_meta: vocab type       = BPE
0.00.084.614 I llm_load_print_meta: n_vocab          = 50304
0.00.084.614 I llm_load_print_meta: n_merges         = 50009
0.00.084.614 I llm_load_print_meta: vocab_only       = 0
0.00.084.615 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.615 I llm_load_print_meta: n_embd           = 2048
0.00.084.615 I llm_load_print_meta: n_layer          = 24
0.00.084.629 I llm_load_print_meta: n_head           = 16
0.00.084.630 I llm_load_print_meta: n_head_kv        = 16
0.00.084.630 I llm_load_print_meta: n_rot            = 32
0.00.084.631 I llm_load_print_meta: n_swa            = 0
0.00.084.631 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.631 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.631 I llm_load_print_meta: n_gqa            = 1
0.00.084.633 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.634 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.634 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.635 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.636 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.636 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.636 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.637 I llm_load_print_meta: n_ff             = 8192
0.00.084.637 I llm_load_print_meta: n_expert         = 0
0.00.084.637 I llm_load_print_meta: n_expert_used    = 0
0.00.084.637 I llm_load_print_meta: causal attn      = 1
0.00.084.637 I llm_load_print_meta: pooling type     = 0
0.00.084.637 I llm_load_print_meta: rope type        = 2
0.00.084.638 I llm_load_print_meta: rope scaling     = linear
0.00.084.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.638 I llm_load_print_meta: freq_scale_train = 1
0.00.084.638 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.639 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.639 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.639 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.639 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.639 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.639 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.649 I llm_load_print_meta: model type       = 1.4B
0.00.084.649 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.650 I llm_load_print_meta: model params     = 1.41 B
0.00.084.650 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.651 I llm_load_print_meta: general.name     = 1.4B
0.00.084.651 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.651 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.652 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.652 I llm_load_print_meta: max token length = 1024
0.00.087.213 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.213 I llm_load_tensors: offloading output layer to GPU
0.00.087.214 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.224 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.225 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.151 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.151 I llama_new_context_with_model: n_ctx         = 128
0.00.088.152 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.152 I llama_new_context_with_model: n_batch       = 128
0.00.088.152 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.152 I llama_new_context_with_model: flash_attn    = 0
0.00.088.153 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.153 I llama_new_context_with_model: freq_scale    = 1
0.00.088.153 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.154 I ggml_metal_init: allocating
0.00.088.157 I ggml_metal_init: found device: Apple M4
0.00.088.159 I ggml_metal_init: picking default device: Apple M4
0.00.088.737 I ggml_metal_init: using embedded metal library
0.00.091.216 I ggml_metal_init: GPU name:   Apple M4
0.00.091.218 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.219 I ggml_metal_init: simdgroup reduction   = true
0.00.091.219 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.219 I ggml_metal_init: has bfloat            = true
0.00.091.219 I ggml_metal_init: use bfloat            = true
0.00.091.220 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.352 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.354 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.379 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.288 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.289 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.289 I llama_new_context_with_model: graph nodes  = 967
0.00.102.290 I llama_new_context_with_model: graph splits = 2
0.00.102.295 I 
0.00.102.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.102.317 I compute_imatrix: tokenizing the input ..
0.00.109.372 I compute_imatrix: tokenization took 7.055 ms
0.00.109.374 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.542.004 I compute_imatrix: 1.43 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.546.532 I llama_perf_context_print:        load time =    1520.02 ms
0.01.546.533 I llama_perf_context_print: prompt eval time =    1432.01 ms /   128 tokens (   11.19 ms per token,    89.39 tokens per second)
0.01.546.534 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.546.534 I llama_perf_context_print:       total time =    1524.54 ms /   129 tokens
0.01.547.007 I ggml_metal_free: deallocating

real	0m1.729s
user	0m0.171s
sys	0m0.219s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4321 (9f35e445)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15510a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15510acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15510b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15510b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15510bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15510c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15510c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15510cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15510d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15510d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15510de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15510e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15510eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15510f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15510fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155110590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155110cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1551113d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155111af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1551122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1551129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155113100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155113820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1551140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1551147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155114aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1551150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155115d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155116260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155116520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1551169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155116c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155117510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155117a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155117d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1551181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155118650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155118af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155118f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155119430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1551198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155119d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15511a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15511a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15511a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15511af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15511b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15511beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15511c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15511cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15511d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15511d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15511dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15511e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15511eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15511efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15511f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15511f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15511fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155120500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1551207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155120c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155121100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1551215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155121a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155121ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155122380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155122820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155122cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155123160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155123600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155123aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155123f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155124490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1551249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155124f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155125480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1551259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155125f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155126470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1551269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155126f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155127460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1551279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155127f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155128450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1551289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155128ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155129440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155129990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155129ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15512a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15512a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15512aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15512b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15512b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15512bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15511bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15512c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15512cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15512d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15512d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15512dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15512e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15512e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15512eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15512f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15512f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15512fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155130000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155130550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155130aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155130ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155131490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155131930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155131dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155132270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155132710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155132bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155133050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1551334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155133990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155133e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1551342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155134770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155134c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1551350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155135550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1551359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155135e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155136330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1551367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155136c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155137110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1551375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155137a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155137ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155138390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155138830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155138cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155139170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155139610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155139ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155139f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15513a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15513a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15513ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15513b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15513b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15513bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15513bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15513c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15513c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15513cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15513d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15513d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15513db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15513e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15513e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15513e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15513edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15513f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15513f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15513fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155140070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155140510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1551409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155140e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1551412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155141790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155141c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1551420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155142570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155142a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155142eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155143350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1551437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155143c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155144130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1551445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155144a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155144f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1551453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155145850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155145cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155146190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155146630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155146ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155146f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155147410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1551478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155147d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1551481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155148740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155148c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1551491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155149730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1551499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15514a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15514a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15514ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15514b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15514b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15514bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15514c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15514c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15514cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15514d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15514d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15514dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15514e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15514ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15514efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15514f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15514fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15514ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1551504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155150a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155150f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1551514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155151a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155151f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1551524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155152a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155152f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1551534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155153a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155153f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1551544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155154a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155154f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1551554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1551559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155155f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155156490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1551569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155156f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155157480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1551579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155157f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155158470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1551589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155158f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155159460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1551599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155159f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15515a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15515a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15515aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15515b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15515b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15515bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15515c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15515c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15515ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15515d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15515d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15515dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15515e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15515e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15515eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15515f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15515f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15515fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1551603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155160940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155160e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155161330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1551617d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155161c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155162110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1551625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155162a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155162ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155163390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155163830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155163cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155164170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155164610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155164ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155164f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1551653f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155165940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155166060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155166780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155166ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1551675c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155167880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155168070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155168330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155168940 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.133.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155204ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155205150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1552055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155205a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155205ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155206310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155206780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155206bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155207060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1552074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155207940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155207fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155208ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155209280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155209a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15520a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15520a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15520aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15520b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15520bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15520c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15520cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15520d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15520db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15520e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15520e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15520e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15520ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15520f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15520f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15520f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15520fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155210360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155210620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155210a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155210f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155211370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1552117e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155211c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1552120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155212530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1552129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155212e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155213280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1552136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155213b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155213fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155214440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1552148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155214d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155215190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155215600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155215a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155215ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155216350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1552167c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155216d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155217230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1552176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155217b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155217f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1552183f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155218860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155218cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155219140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1552195b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155219a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155219e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15521a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15521a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15521abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15521b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15521b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15521b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15521bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15521c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15521c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15521caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15521cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15521d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15521d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15521dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15521e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15521e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15521ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15521ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15521f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15521f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15521fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155220030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1552204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155220910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155220d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1552211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155221660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155221ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155221f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1552223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155222820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155222c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155223100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155223570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1552239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155223e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1552242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155224730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155224ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155225010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155225480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1552258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155225d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1552261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155226640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155226ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155226f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155227390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155227800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155227c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1552280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155228550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1552289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155228e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1552292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155229710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155229b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155229ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15522a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15522a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15522ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15522b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15522b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15522ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15522bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15522c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15522c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15522cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15522d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15522d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15522d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15522de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15522e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15522e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15522eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15522efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15522f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15522f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15522fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155230190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155230600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155230a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155230ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155231350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1552317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155231c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1552320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155232510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155232980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155232df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155233260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1552336d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155233b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155233fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155234420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155234890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155234d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155235170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1552355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155235a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155235ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155236330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1552367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155236c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155237080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1552374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155237960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155237dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155238240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1552386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155238b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155238f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155239400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155239870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155239ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15523a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15523a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15523aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15523aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15523b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15523b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15523bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15523c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15523c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15523c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15523cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15523d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15523d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15523db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15523df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15523e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15523e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15523ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15523f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15523f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15523fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15523fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1552402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155240760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155240cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155241160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1552415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155242120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1552423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1552426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155242b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155242f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1552433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155243860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155243cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155244140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1552445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155244a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155244e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155245300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155245770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155245be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155246050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1552464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155246930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155246da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155247210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155247680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155247af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155247f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1552483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155248840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155248cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155249120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155249590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155249a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155249e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15524a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15524a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15524abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15524b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15524b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15524b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15524bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15524c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15524c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15524cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15524cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15524d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15524d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15524dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15524e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15524e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15524e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15524ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15524f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15524f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15524fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155250010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155250480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1552508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155250d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1552511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155251640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155251ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155251f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155252390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155252800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155252c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1552530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155253550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1552539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155253e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1552542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155254710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155254b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155254ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155255460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1552558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155255d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1552567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155256ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1552575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155257d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155257fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155258440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155258a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155259050 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147b044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147b04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147b04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147b05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147b056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147b05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147b05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147b063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147b06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147b06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147b07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147b07870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147b08390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147b08b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147b09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147b09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147b0a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147b0a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147b0afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147b0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147b0be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147b0c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147b0cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147b0d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147b0daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147b0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147b0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147b0e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147b0e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147b0ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147b0f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147b0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147b0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147b0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147b102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147b10720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147b10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147b11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147b11470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147b118e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147b11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147b121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147b12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147b12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147b12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147b13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147b137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147b13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147b140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147b14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147b149b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147b14e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147b15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147b15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147b15b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147b15fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147b16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147b16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147b16ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147b17330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147b177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147b17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147b18080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147b184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147b18960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147b18dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147b19240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147b196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147b19b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147b19f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147b1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147b1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147b1ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147b1b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147b1b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147b1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147b1bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147b1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147b1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147b1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147b1d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147b1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147b1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147b1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147b1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147b1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147b1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147b1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147b1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147b1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147b1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147b20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147b205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147b20a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147b20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147b212f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147b21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147b21bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147b22040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147b224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147b22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147b22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147b23200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147b23670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147b23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147b23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147b243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147b24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147b24ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147b25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147b25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147b259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147b25e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147b262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147b26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147b26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147b27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147b27490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147b27900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147b27d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147b281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147b28650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147b28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147b28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147b293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147b29810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147b29c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147b2a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147b2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147b2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147b2ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147b2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147b2b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147b2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147b2c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147b2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147b2c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147b2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147b2d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147b2d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147b2daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147b2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147b2e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147b2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147b2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147b2f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147b2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147b2f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147b2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147b30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147b30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147b30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147b30fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147b31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147b318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147b31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147b321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147b32610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147b32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147b32ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147b33360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147b337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147b33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147b340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147b34520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147b34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147b34e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147b35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147b356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147b35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147b35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147b36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147b368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147b36d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147b37180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147b375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147b37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147b37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147b38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147b387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147b38c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147b39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147b39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147b39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147b39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147b3a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147b3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147b3ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147b3afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147b3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147b3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147b3bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147b3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147b3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147b3ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147b3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147b3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147b3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147b3dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147b3e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147b3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147b3e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147b3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147b3f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147b3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147b3fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147b3ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147b40510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147b40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147b40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147b41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147b41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147b41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147b42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147b427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147b42c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147b43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147b434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147b43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147b43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147b44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147b446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147b44b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147b44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147b45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147b45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147b45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147b46150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147b465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147b46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147b46ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147b47310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147b47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147b47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147b48060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147b484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147b48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147b48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147b49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147b49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147b49b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147b49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147b4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147b4a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147b4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147b4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147b4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147b4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147b4c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147b4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147b4cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147b4cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147b4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147b4d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147b4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147b4e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147b4e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147b4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147b4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147b4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147b4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147b4fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147b4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147b50460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147b508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147b50d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147b511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147b51620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147b51a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147b51f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147b52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147b527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147b52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147b530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147b53530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147b539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147b53e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147b54280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147b546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147b54b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147b54fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147b55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147b558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147b56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147b56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147b57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147b57880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147b57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147b57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147b585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147b58bc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.783s
user	0m0.286s
sys	0m0.303s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4321 (9f35e445)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145f0ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145f0f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145f0ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145f10500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145f10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145f11060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145f11610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145f11bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145f120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145f125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145f12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145f135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145f13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145f145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145f14cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145f153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145f15b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145f16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145f169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145f17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145f17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145f17f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145f187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145f18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145f191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145f197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145f1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145f1a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145f1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145f1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145f1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145f1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145f1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145f1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145f1d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145f1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145f1db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145f1e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145f1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145f1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145f1ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145f1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145f1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145f1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145f205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145f20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145f21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145f21810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145f21e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145f22430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145f22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145f23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145f236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145f23b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145f23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145f24440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145f24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145f24ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145f25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145f25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145f25cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145f26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145f26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145f26ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145f26f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145f27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145f27d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145f281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145f28670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145f28bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145f29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145f29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145f29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145f2a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145f2a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145f2aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145f2b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145f2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145f2bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145f2c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145f2c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145f2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145f2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145f2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145f2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145f2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145f2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145f2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145f2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145f2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145f300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145f305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145f202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145f30a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145f31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145f31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145f31cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145f32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145f32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145f32ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145f331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145f33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145f33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145f341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145f34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145f34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145f35720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145f36060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145f36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145f369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145f36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145f372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145f37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145f37c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145f380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145f38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145f38a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145f38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145f39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145f397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145f39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145f3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145f3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145f3aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145f3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145f3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145f3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145f3bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145f3c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145f3c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145f3cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145f3cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145f3d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145f3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145f3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145f3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145f3e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145f3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145f3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145f3f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145f3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145f3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145f40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145f406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145f40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145f41020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145f414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145f41960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145f41e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145f422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145f42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145f42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145f43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145f43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145f439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145f44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145f447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145f44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145f450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145f45580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145f45a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145f45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145f46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145f46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145f46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145f47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145f475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145f47a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145f47f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145f483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145f48860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145f48d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145f491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145f49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145f49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145f49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145f4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145f4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145f4ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145f4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145f4b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145f4bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145f4bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145f4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145f4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145f4d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145f4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145f4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145f4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145f4e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145f4ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145f4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145f4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145f502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145f508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145f50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145f516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145f51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145f51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145f52490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145f52c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145f53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145f536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145f53c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145f546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145f54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145f55170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145f556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145f55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145f56160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145f566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145f56c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145f57150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145f576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145f57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145f58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145f58690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145f58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145f59130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145f59680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145f59bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145f5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145f5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145f5abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145f5b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145f5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145f5bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145f5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145f5c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145f5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145f5d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145f5d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145f5db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145f5e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145f5e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145f5eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145f5f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145f5f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145f5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145f600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145f60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145f60b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145f610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145f61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145f61b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145f620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145f625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145f62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145f63090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145f635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145f63b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145f64080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145f645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145f64b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145f65070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145f655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145f65a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145f65f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145f663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145f66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145f66ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145f67180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145f67620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145f67ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145f67f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145f68400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145f688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145f68d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145f691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145f69680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145f69b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145f6a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145f6a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145f6aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145f6b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145f6bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145f6bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145f6c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145f6ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145f6d070 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.225 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145c04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145c04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145c05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145c05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145c05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145c06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145c065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145c06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145c06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145c07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145c07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145c07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145c08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145c09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145c09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145c0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145c0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145c0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145c0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145c0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145c0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145c0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145c0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145c0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145c0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145c0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145c0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145c0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145c0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145c0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145c0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145c0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145c10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145c10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145c108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145c10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145c11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145c11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145c11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145c11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145c12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145c127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145c12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145c130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145c13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145c13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145c13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145c14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145c146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145c14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145c14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145c15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145c15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145c15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145c16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145c165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145c16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145c17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145c174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145c17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145c17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145c18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145c18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145c18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145c18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145c193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145c19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145c19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145c1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145c1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145c1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145c1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145c1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145c1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145c1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145c1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145c1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145c1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145c1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145c1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145c1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145c1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145c1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145c1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145c1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145c1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145c1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145c1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145c1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145c1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145c202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145c20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145c20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145c21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145c21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145c218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145c21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145c221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145c22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145c22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145c22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145c23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145c23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145c23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145c240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145c24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145c249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145c24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145c252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145c25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145c25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145c25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145c26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145c268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145c26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145c271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145c27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145c27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145c27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145c28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145c287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145c28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145c290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145c29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145c299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145c29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145c2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145c2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145c2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145c2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145c2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145c2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145c2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145c2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145c2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145c2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145c2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145c2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145c2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145c2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145c2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145c2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145c2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145c2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145c2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145c2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145c2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145c2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145c30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145c30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145c30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145c31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145c315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145c31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145c31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145c32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145c327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145c32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145c33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145c334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145c33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145c33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145c34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145c346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145c34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145c34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145c35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145c35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145c35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145c36150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145c365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145c36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145c36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145c37310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145c37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145c37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145c38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145c384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145c38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145c38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145c39220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145c39690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145c39b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145c39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145c3a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145c3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145c3acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145c3b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145c3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145c3ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145c3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145c3c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145c3c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145c3cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145c3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145c3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145c3d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145c3dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145c3e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145c3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145c3eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145c3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145c3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145c3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145c3fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145c40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145c40580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145c40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145c40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145c413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145c41f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145c42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145c424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145c42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145c42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145c43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145c43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145c43af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145c43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145c443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145c44840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145c44cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145c45120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145c45590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145c45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145c45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145c462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145c46750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145c46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145c47030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145c474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145c47910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145c47d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145c481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145c48660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145c48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145c48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145c493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145c49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145c49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145c4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145c4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145c4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145c4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145c4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145c4b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145c4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145c4c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145c4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145c4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145c4cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145c4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145c4d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145c4dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145c4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145c4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145c4e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145c4ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145c4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145c4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145c4f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145c4fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145c502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145c50710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145c50b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145c50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145c51460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145c518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145c51d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145c521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145c52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145c52a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145c52f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145c53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145c537e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145c53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145c540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145c54530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145c549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145c54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145c55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145c556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145c55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145c565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145c56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145c57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145c57b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145c57df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145c58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145c58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145c58e70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145e053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145e05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145e05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145e06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145e06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145e069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145e06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145e072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145e07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145e07cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145e08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145e087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145e092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145e09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145e0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145e0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145e0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145e0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145e0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145e0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145e0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145e0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145e0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145e0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145e0ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145e0ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145e0eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145e0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145e0f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145e0fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145e101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145e106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145e10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145e11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145e116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145e11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145e11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145e12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145e128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145e12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145e13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145e13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145e13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145e13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145e14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145e147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145e14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145e150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145e15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145e15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145e16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145e166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145e16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145e16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145e17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145e17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145e17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145e18300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145e18770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145e18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145e19050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145e194c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145e19930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145e19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145e1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145e1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145e1aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145e1af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145e1b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145e1b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145e1bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145e1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145e1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145e1ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145e1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145e1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145e1d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145e1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145e1e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145e1e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145e1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145e1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145e1f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145e1f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145e1fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145e1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145e20820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145e20c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145e21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145e21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145e219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145e21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145e222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145e22730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145e22ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145e23010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145e23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145e238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145e23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145e241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145e24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145e24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145e24f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145e25390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145e25800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145e25c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145e260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145e26550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145e269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145e26e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145e272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145e27710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145e27b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145e27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145e28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145e28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145e291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145e29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145e29a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145e29f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145e2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145e2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145e2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145e2b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145e2b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145e2b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145e2be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145e2c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145e2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145e2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145e2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145e2d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145e2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145e2dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145e2e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145e2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145e2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145e2eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145e2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145e2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145e2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145e300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145e30510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145e30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145e30df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145e31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145e316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145e31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145e31fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145e32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145e32890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145e32d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145e33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145e335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145e33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145e33ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145e34330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145e347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145e34c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145e35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145e354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145e35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145e35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145e36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145e366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145e36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145e36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145e37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145e37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145e37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145e38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145e385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145e38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145e38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145e39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145e39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145e3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145e3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145e3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145e3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145e3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145e3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145e3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145e3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145e3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145e3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145e3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145e3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145e3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145e3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145e3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145e3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145e3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145e3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145e3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145e3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145e3f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145e3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145e40200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145e40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145e40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145e414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145e41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145e41dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145e42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145e42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145e43300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145e43770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145e43be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145e44050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145e444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145e44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145e44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145e45210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145e45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145e45af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145e45f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145e463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145e46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145e46cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145e47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145e47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145e47a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145e47e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145e482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145e48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145e48bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145e49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145e494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145e49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145e49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145e4a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145e4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145e4aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145e4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145e4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145e4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145e4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145e4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145e4c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145e4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145e4d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145e4d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145e4d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145e4ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145e4e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145e4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145e4ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145e4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145e4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145e50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145e505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145e50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145e50e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145e51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145e51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145e52050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145e524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145e52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145e53310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145e53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145e53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145e54060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145e544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145e54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145e54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145e55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145e55690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145e55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145e55f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145e563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145e56850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145e573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145e57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145e581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145e58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145e58bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145e58e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145e592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145e59760 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.245s
sys	0m0.149s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
