Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:42 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:104 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.591s
user	0m0.723s
sys	0m0.956s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[  9%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX shared library libllama.dylib
[ 21%] Built target llama-gguf-hash
[ 21%] Built target llama-gguf
[ 21%] Built target llama
[ 21%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 22%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 22%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking C executable ../bin/test-c
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Linking CXX executable ../../bin/llama-run
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 31%] Built target test-c
[ 31%] Built target llama-simple
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-run
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target llama-simple-chat
[ 32%] Built target llama-quantize-stats
[ 32%] Built target common
[ 32%] Built target llava_static
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Built target llava_shared
[ 37%] Linking CXX executable ../bin/test-tokenizer-0
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-arg-parser
[ 44%] Linking CXX executable ../bin/test-log
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-chat-template
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-0
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-tokenizer-1-bpe
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-arg-parser
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 48%] Built target test-chat-template
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Built target test-grammar-integration
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-model-load-cancel
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-autorelease
[ 54%] Built target test-llama-grammar
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 59%] Linking CXX executable ../bin/test-rope
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-perf
[ 60%] Built target test-autorelease
[ 60%] Built target test-backend-ops
[ 60%] Built target test-model-load-cancel
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 61%] Built target test-barrier
[ 61%] Built target test-quantize-fns
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 62%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 62%] Built target test-quantize-perf
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 62%] Built target test-rope
[ 63%] Linking CXX executable ../../bin/llama-embedding
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Built target test-json-schema-to-grammar
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched-bench
[ 70%] Built target llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-batched
[ 70%] Built target llama-eval-callback
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 70%] Built target llama-infill
[ 70%] Built target llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-lookup-merge
[ 73%] Linking CXX executable ../../bin/llama-lookup-create
[ 73%] Built target llama-imatrix
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-lookup-stats
[ 75%] Built target llama-bench
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Built target llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Generating loading.html.hpp
[ 80%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 81%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 81%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 82%] Generating completion.js.hpp
[ 82%] Built target llama-parallel
[ 82%] Built target llama-passkey
[ 82%] Built target llama-cli
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Generating deps_markdown-it.js.hpp
[ 84%] Generating deps_daisyui.min.css.hpp
[ 85%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Built target llama-perplexity
[ 87%] Generating deps_tailwindcss.js.hpp
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 89%] Built target llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Generating deps_vue.esm-browser.js.hpp
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Generating index.html.hpp
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-tokenize
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Built target llama-cvector-generator
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 99%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.546s
user	0m5.751s
sys	0m8.508s

main: quantize time =  3625.64 ms
main:    total time =  3625.64 ms

main: quantize time =  1398.22 ms
main:    total time =  1398.22 ms

main: quantize time =  1360.53 ms
main:    total time =  1360.53 ms

main: quantize time =  2635.62 ms
main:    total time =  2635.62 ms

main: quantize time =  2875.63 ms
main:    total time =  2875.63 ms

main: quantize time =  5064.98 ms
main:    total time =  5064.98 ms

main: quantize time =  5751.27 ms
main:    total time =  5751.27 ms

main: quantize time =  6828.39 ms
main:    total time =  6828.39 ms

main: quantize time =  5914.34 ms
main:    total time =  5914.34 ms

main: quantize time =  4552.87 ms
main:    total time =  4552.87 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.146 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.308 I main: llama backend init
0.00.000.337 I main: load the model and apply lora adapter, if any
0.00.045.927 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.057.251 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.275 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.277 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.277 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.278 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.279 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.296 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.366 I llama_model_loader: - type  f32:  194 tensors
0.00.076.366 I llama_model_loader: - type  f16:   98 tensors
0.00.109.056 I llm_load_vocab: special tokens cache size = 25
0.00.116.344 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.116.347 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.116.348 I llm_load_print_meta: arch             = gptneox
0.00.116.348 I llm_load_print_meta: vocab type       = BPE
0.00.116.348 I llm_load_print_meta: n_vocab          = 50304
0.00.116.349 I llm_load_print_meta: n_merges         = 50009
0.00.116.349 I llm_load_print_meta: vocab_only       = 0
0.00.116.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.116.349 I llm_load_print_meta: n_embd           = 2048
0.00.116.349 I llm_load_print_meta: n_layer          = 24
0.00.116.352 I llm_load_print_meta: n_head           = 16
0.00.116.355 I llm_load_print_meta: n_head_kv        = 16
0.00.116.355 I llm_load_print_meta: n_rot            = 32
0.00.116.355 I llm_load_print_meta: n_swa            = 0
0.00.116.355 I llm_load_print_meta: n_embd_head_k    = 128
0.00.116.355 I llm_load_print_meta: n_embd_head_v    = 128
0.00.116.356 I llm_load_print_meta: n_gqa            = 1
0.00.116.357 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.116.357 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.116.358 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.116.358 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.116.358 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.116.359 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.116.359 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.116.359 I llm_load_print_meta: n_ff             = 8192
0.00.116.360 I llm_load_print_meta: n_expert         = 0
0.00.116.360 I llm_load_print_meta: n_expert_used    = 0
0.00.116.360 I llm_load_print_meta: causal attn      = 1
0.00.116.360 I llm_load_print_meta: pooling type     = 0
0.00.116.361 I llm_load_print_meta: rope type        = 2
0.00.116.361 I llm_load_print_meta: rope scaling     = linear
0.00.116.361 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.116.362 I llm_load_print_meta: freq_scale_train = 1
0.00.116.362 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.116.362 I llm_load_print_meta: rope_finetuned   = unknown
0.00.116.362 I llm_load_print_meta: ssm_d_conv       = 0
0.00.116.364 I llm_load_print_meta: ssm_d_inner      = 0
0.00.116.364 I llm_load_print_meta: ssm_d_state      = 0
0.00.116.364 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.116.364 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.116.376 I llm_load_print_meta: model type       = 1.4B
0.00.116.377 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.116.378 I llm_load_print_meta: model params     = 1.41 B
0.00.116.378 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.116.378 I llm_load_print_meta: general.name     = 1.4B
0.00.116.378 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.116.379 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.116.379 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.116.379 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.116.379 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.116.380 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.116.380 I llm_load_print_meta: max token length = 1024
0.00.119.006 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.119.007 I llm_load_tensors: offloading output layer to GPU
0.00.119.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.119.024 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.119.025 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.120.003 I llama_new_context_with_model: n_seq_max     = 1
0.00.120.004 I llama_new_context_with_model: n_ctx         = 2048
0.00.120.005 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.120.005 I llama_new_context_with_model: n_batch       = 2048
0.00.120.005 I llama_new_context_with_model: n_ubatch      = 512
0.00.120.005 I llama_new_context_with_model: flash_attn    = 0
0.00.120.006 I llama_new_context_with_model: freq_base     = 10000.0
0.00.120.006 I llama_new_context_with_model: freq_scale    = 1
0.00.120.006 I ggml_metal_init: allocating
0.00.120.009 I ggml_metal_init: found device: Apple M4
0.00.120.011 I ggml_metal_init: picking default device: Apple M4
0.00.120.660 I ggml_metal_init: using embedded metal library
0.00.128.164 I ggml_metal_init: GPU name:   Apple M4
0.00.128.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.128.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.128.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.128.167 I ggml_metal_init: simdgroup reduction   = true
0.00.128.167 I ggml_metal_init: simdgroup matrix mul. = true
0.00.128.167 I ggml_metal_init: has bfloat            = true
0.00.128.167 I ggml_metal_init: use bfloat            = true
0.00.128.168 I ggml_metal_init: hasUnifiedMemory      = true
0.00.128.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.164.390 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.164.395 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.164.413 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.165.315 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.165.316 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.165.316 I llama_new_context_with_model: graph nodes  = 967
0.00.165.316 I llama_new_context_with_model: graph splits = 2
0.00.165.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.491 I main: llama threadpool init, n_threads = 4
0.00.239.523 I 
0.00.239.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.239.554 I 
0.00.239.630 I sampler seed: 1234
0.00.239.634 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.239.658 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.239.660 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.239.660 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.088.471 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.02.088.471 I llama_perf_context_print:        load time =     193.55 ms
0.02.088.473 I llama_perf_context_print: prompt eval time =      37.88 ms /     7 tokens (    5.41 ms per token,   184.77 tokens per second)
0.02.088.473 I llama_perf_context_print:        eval time =    1808.00 ms /    63 runs   (   28.70 ms per token,    34.85 tokens per second)
0.02.088.474 I llama_perf_context_print:       total time =    1848.98 ms /    70 tokens
0.02.088.645 I ggml_metal_free: deallocating

real	0m2.819s
user	0m0.148s
sys	0m0.097s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.487 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.720 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.725 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.727 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.730 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.730 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.730 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.732 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.732 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.732 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.733 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.733 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.733 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.734 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.633 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.770 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.006 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.009 I llama_model_loader: - type  f32:  194 tensors
0.00.039.009 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.402 I llm_load_vocab: special tokens cache size = 25
0.00.072.787 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.792 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.792 I llm_load_print_meta: arch             = gptneox
0.00.072.793 I llm_load_print_meta: vocab type       = BPE
0.00.072.793 I llm_load_print_meta: n_vocab          = 50304
0.00.072.793 I llm_load_print_meta: n_merges         = 50009
0.00.072.794 I llm_load_print_meta: vocab_only       = 0
0.00.072.794 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.794 I llm_load_print_meta: n_embd           = 2048
0.00.072.794 I llm_load_print_meta: n_layer          = 24
0.00.072.798 I llm_load_print_meta: n_head           = 16
0.00.072.799 I llm_load_print_meta: n_head_kv        = 16
0.00.072.799 I llm_load_print_meta: n_rot            = 32
0.00.072.799 I llm_load_print_meta: n_swa            = 0
0.00.072.799 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.799 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.800 I llm_load_print_meta: n_gqa            = 1
0.00.072.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.802 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.803 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.803 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.804 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.804 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.804 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.805 I llm_load_print_meta: n_ff             = 8192
0.00.072.805 I llm_load_print_meta: n_expert         = 0
0.00.072.805 I llm_load_print_meta: n_expert_used    = 0
0.00.072.805 I llm_load_print_meta: causal attn      = 1
0.00.072.805 I llm_load_print_meta: pooling type     = 0
0.00.072.806 I llm_load_print_meta: rope type        = 2
0.00.072.806 I llm_load_print_meta: rope scaling     = linear
0.00.072.806 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.807 I llm_load_print_meta: freq_scale_train = 1
0.00.072.807 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.807 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.808 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.808 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.808 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.808 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.822 I llm_load_print_meta: model type       = 1.4B
0.00.072.822 I llm_load_print_meta: model ftype      = Q8_0
0.00.072.823 I llm_load_print_meta: model params     = 1.41 B
0.00.072.823 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.072.824 I llm_load_print_meta: general.name     = 1.4B
0.00.072.824 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.824 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.827 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.827 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.827 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.827 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.828 I llm_load_print_meta: max token length = 1024
0.00.075.733 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.733 I llm_load_tensors: offloading output layer to GPU
0.00.075.733 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.744 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.746 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.077.068 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.069 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.070 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.070 I llama_new_context_with_model: n_batch       = 2048
0.00.077.070 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.070 I llama_new_context_with_model: flash_attn    = 0
0.00.077.071 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.071 I llama_new_context_with_model: freq_scale    = 1
0.00.077.071 I ggml_metal_init: allocating
0.00.077.080 I ggml_metal_init: found device: Apple M4
0.00.077.083 I ggml_metal_init: picking default device: Apple M4
0.00.077.926 I ggml_metal_init: using embedded metal library
0.00.080.579 I ggml_metal_init: GPU name:   Apple M4
0.00.080.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.582 I ggml_metal_init: simdgroup reduction   = true
0.00.080.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.583 I ggml_metal_init: has bfloat            = true
0.00.080.583 I ggml_metal_init: use bfloat            = true
0.00.080.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.964 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.975 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.997 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.065 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.116.066 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.116.067 I llama_new_context_with_model: graph nodes  = 967
0.00.116.067 I llama_new_context_with_model: graph splits = 2
0.00.116.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.401.223 I main: llama threadpool init, n_threads = 4
0.01.401.257 I 
0.01.401.276 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.401.277 I 
0.01.401.542 I sampler seed: 1234
0.01.401.547 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.401.592 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.401.594 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.401.594 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.495.561 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.02.495.561 I llama_perf_context_print:        load time =    1391.73 ms
0.02.495.562 I llama_perf_context_print: prompt eval time =      40.00 ms /     7 tokens (    5.71 ms per token,   175.01 tokens per second)
0.02.495.563 I llama_perf_context_print:        eval time =    1051.05 ms /    63 runs   (   16.68 ms per token,    59.94 tokens per second)
0.02.495.563 I llama_perf_context_print:       total time =    1094.34 ms /    70 tokens
0.02.495.738 I ggml_metal_free: deallocating

real	0m2.514s
user	0m0.122s
sys	0m0.242s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.012.702 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.218 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.219 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.219 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.221 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.222 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.222 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.137 I llama_model_loader: - type  f32:  194 tensors
0.00.028.138 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.045 I llm_load_vocab: special tokens cache size = 25
0.00.055.028 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.032 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.032 I llm_load_print_meta: arch             = gptneox
0.00.055.032 I llm_load_print_meta: vocab type       = BPE
0.00.055.032 I llm_load_print_meta: n_vocab          = 50304
0.00.055.033 I llm_load_print_meta: n_merges         = 50009
0.00.055.033 I llm_load_print_meta: vocab_only       = 0
0.00.055.033 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.033 I llm_load_print_meta: n_embd           = 2048
0.00.055.033 I llm_load_print_meta: n_layer          = 24
0.00.055.036 I llm_load_print_meta: n_head           = 16
0.00.055.037 I llm_load_print_meta: n_head_kv        = 16
0.00.055.037 I llm_load_print_meta: n_rot            = 32
0.00.055.038 I llm_load_print_meta: n_swa            = 0
0.00.055.039 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.041 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.041 I llm_load_print_meta: n_gqa            = 1
0.00.055.042 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.043 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.043 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.045 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.046 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.046 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.047 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.048 I llm_load_print_meta: n_ff             = 8192
0.00.055.048 I llm_load_print_meta: n_expert         = 0
0.00.055.048 I llm_load_print_meta: n_expert_used    = 0
0.00.055.048 I llm_load_print_meta: causal attn      = 1
0.00.055.048 I llm_load_print_meta: pooling type     = 0
0.00.055.049 I llm_load_print_meta: rope type        = 2
0.00.055.049 I llm_load_print_meta: rope scaling     = linear
0.00.055.049 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.049 I llm_load_print_meta: freq_scale_train = 1
0.00.055.050 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.050 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.050 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.050 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.050 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.051 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.064 I llm_load_print_meta: model type       = 1.4B
0.00.055.064 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.065 I llm_load_print_meta: model params     = 1.41 B
0.00.055.065 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.067 I llm_load_print_meta: general.name     = 1.4B
0.00.055.067 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.067 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.068 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.068 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.068 I llm_load_print_meta: max token length = 1024
0.00.057.422 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.422 I llm_load_tensors: offloading output layer to GPU
0.00.057.422 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.433 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.435 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.498 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.499 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.499 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.499 I llama_new_context_with_model: n_batch       = 2048
0.00.058.499 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.499 I llama_new_context_with_model: flash_attn    = 0
0.00.058.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.500 I llama_new_context_with_model: freq_scale    = 1
0.00.058.501 I ggml_metal_init: allocating
0.00.058.507 I ggml_metal_init: found device: Apple M4
0.00.058.510 I ggml_metal_init: picking default device: Apple M4
0.00.059.162 I ggml_metal_init: using embedded metal library
0.00.061.302 I ggml_metal_init: GPU name:   Apple M4
0.00.061.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.304 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.305 I ggml_metal_init: simdgroup reduction   = true
0.00.061.305 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.305 I ggml_metal_init: has bfloat            = true
0.00.061.305 I ggml_metal_init: use bfloat            = true
0.00.061.306 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.228 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.235 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.259 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.361 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.363 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.363 I llama_new_context_with_model: graph nodes  = 967
0.00.096.364 I llama_new_context_with_model: graph splits = 2
0.00.096.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.152 I main: llama threadpool init, n_threads = 4
0.00.679.189 I 
0.00.679.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.679.209 I 
0.00.679.432 I sampler seed: 1234
0.00.679.437 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.448 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.449 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.450 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.354.951 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.354.953 I llama_perf_context_print:        load time =     666.45 ms
0.01.354.953 I llama_perf_context_print: prompt eval time =      36.52 ms /     7 tokens (    5.22 ms per token,   191.70 tokens per second)
0.01.354.954 I llama_perf_context_print:        eval time =     636.00 ms /    63 runs   (   10.10 ms per token,    99.06 tokens per second)
0.01.354.955 I llama_perf_context_print:       total time =     675.80 ms /    70 tokens
0.01.355.131 I ggml_metal_free: deallocating

real	0m1.375s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.928 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.943 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.944 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.770 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.615 I llama_model_loader: - type  f32:  194 tensors
0.00.023.615 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.615 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.357 I llm_load_vocab: special tokens cache size = 25
0.00.050.546 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.549 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.549 I llm_load_print_meta: arch             = gptneox
0.00.050.550 I llm_load_print_meta: vocab type       = BPE
0.00.050.550 I llm_load_print_meta: n_vocab          = 50304
0.00.050.550 I llm_load_print_meta: n_merges         = 50009
0.00.050.550 I llm_load_print_meta: vocab_only       = 0
0.00.050.550 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.551 I llm_load_print_meta: n_embd           = 2048
0.00.050.551 I llm_load_print_meta: n_layer          = 24
0.00.050.553 I llm_load_print_meta: n_head           = 16
0.00.050.554 I llm_load_print_meta: n_head_kv        = 16
0.00.050.554 I llm_load_print_meta: n_rot            = 32
0.00.050.554 I llm_load_print_meta: n_swa            = 0
0.00.050.555 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.555 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.555 I llm_load_print_meta: n_gqa            = 1
0.00.050.556 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.557 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.557 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.558 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.558 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.559 I llm_load_print_meta: n_ff             = 8192
0.00.050.559 I llm_load_print_meta: n_expert         = 0
0.00.050.559 I llm_load_print_meta: n_expert_used    = 0
0.00.050.559 I llm_load_print_meta: causal attn      = 1
0.00.050.559 I llm_load_print_meta: pooling type     = 0
0.00.050.559 I llm_load_print_meta: rope type        = 2
0.00.050.560 I llm_load_print_meta: rope scaling     = linear
0.00.050.560 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.560 I llm_load_print_meta: freq_scale_train = 1
0.00.050.561 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.562 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.562 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.562 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.563 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.563 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.574 I llm_load_print_meta: model type       = 1.4B
0.00.050.574 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.575 I llm_load_print_meta: model params     = 1.41 B
0.00.050.575 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.575 I llm_load_print_meta: general.name     = 1.4B
0.00.050.575 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.577 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.578 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.578 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.578 I llm_load_print_meta: max token length = 1024
0.00.052.100 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.100 I llm_load_tensors: offloading output layer to GPU
0.00.052.100 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.110 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.111 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.965 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.966 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.966 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.966 I llama_new_context_with_model: n_batch       = 2048
0.00.052.966 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.967 I llama_new_context_with_model: flash_attn    = 0
0.00.052.967 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.967 I llama_new_context_with_model: freq_scale    = 1
0.00.052.967 I ggml_metal_init: allocating
0.00.052.970 I ggml_metal_init: found device: Apple M4
0.00.052.972 I ggml_metal_init: picking default device: Apple M4
0.00.053.497 I ggml_metal_init: using embedded metal library
0.00.055.409 I ggml_metal_init: GPU name:   Apple M4
0.00.055.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.412 I ggml_metal_init: simdgroup reduction   = true
0.00.055.412 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.412 I ggml_metal_init: has bfloat            = true
0.00.055.412 I ggml_metal_init: use bfloat            = true
0.00.055.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.413 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.926 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.932 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.953 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.857 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.858 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.858 I llama_new_context_with_model: graph nodes  = 967
0.00.083.858 I llama_new_context_with_model: graph splits = 2
0.00.083.880 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.318 I main: llama threadpool init, n_threads = 4
0.00.734.354 I 
0.00.734.371 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.734.371 I 
0.00.734.597 I sampler seed: 1234
0.00.734.601 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.643 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.643 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.643 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.454.150 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.454.151 I llama_perf_context_print:        load time =     725.57 ms
0.01.454.152 I llama_perf_context_print: prompt eval time =      35.08 ms /     7 tokens (    5.01 ms per token,   199.57 tokens per second)
0.01.454.153 I llama_perf_context_print:        eval time =     681.45 ms /    63 runs   (   10.82 ms per token,    92.45 tokens per second)
0.01.454.153 I llama_perf_context_print:       total time =     719.83 ms /    70 tokens
0.01.454.328 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.109s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.509 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.935 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.940 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.948 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.949 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.952 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.952 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.423 I llama_model_loader: - type  f32:  194 tensors
0.00.025.424 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.735 I llm_load_vocab: special tokens cache size = 25
0.00.051.805 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.807 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.807 I llm_load_print_meta: arch             = gptneox
0.00.051.808 I llm_load_print_meta: vocab type       = BPE
0.00.051.808 I llm_load_print_meta: n_vocab          = 50304
0.00.051.808 I llm_load_print_meta: n_merges         = 50009
0.00.051.808 I llm_load_print_meta: vocab_only       = 0
0.00.051.809 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.809 I llm_load_print_meta: n_embd           = 2048
0.00.051.809 I llm_load_print_meta: n_layer          = 24
0.00.051.811 I llm_load_print_meta: n_head           = 16
0.00.051.812 I llm_load_print_meta: n_head_kv        = 16
0.00.051.813 I llm_load_print_meta: n_rot            = 32
0.00.051.813 I llm_load_print_meta: n_swa            = 0
0.00.051.813 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.815 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.816 I llm_load_print_meta: n_gqa            = 1
0.00.051.816 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.817 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.818 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.818 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.818 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.818 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.819 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.819 I llm_load_print_meta: n_ff             = 8192
0.00.051.819 I llm_load_print_meta: n_expert         = 0
0.00.051.820 I llm_load_print_meta: n_expert_used    = 0
0.00.051.820 I llm_load_print_meta: causal attn      = 1
0.00.051.820 I llm_load_print_meta: pooling type     = 0
0.00.051.820 I llm_load_print_meta: rope type        = 2
0.00.051.820 I llm_load_print_meta: rope scaling     = linear
0.00.051.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.825 I llm_load_print_meta: freq_scale_train = 1
0.00.051.826 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.826 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.826 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.826 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.828 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.828 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.828 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.840 I llm_load_print_meta: model type       = 1.4B
0.00.051.840 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.840 I llm_load_print_meta: model params     = 1.41 B
0.00.051.841 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.841 I llm_load_print_meta: general.name     = 1.4B
0.00.051.841 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.842 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.842 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.843 I llm_load_print_meta: max token length = 1024
0.00.053.844 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.844 I llm_load_tensors: offloading output layer to GPU
0.00.053.844 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.854 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.855 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.833 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.833 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.834 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.834 I llama_new_context_with_model: n_batch       = 2048
0.00.054.834 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.834 I llama_new_context_with_model: flash_attn    = 0
0.00.054.835 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.835 I llama_new_context_with_model: freq_scale    = 1
0.00.054.835 I ggml_metal_init: allocating
0.00.054.838 I ggml_metal_init: found device: Apple M4
0.00.054.840 I ggml_metal_init: picking default device: Apple M4
0.00.055.401 I ggml_metal_init: using embedded metal library
0.00.057.288 I ggml_metal_init: GPU name:   Apple M4
0.00.057.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.290 I ggml_metal_init: simdgroup reduction   = true
0.00.057.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.290 I ggml_metal_init: has bfloat            = true
0.00.057.290 I ggml_metal_init: use bfloat            = true
0.00.057.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.360 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.366 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.384 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.460 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.461 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.461 I llama_new_context_with_model: graph nodes  = 967
0.00.085.462 I llama_new_context_with_model: graph splits = 2
0.00.085.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.074 I main: llama threadpool init, n_threads = 4
0.00.752.108 I 
0.00.752.126 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.752.126 I 
0.00.752.263 I sampler seed: 1234
0.00.752.267 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.752.303 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.752.303 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.752.303 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.537.769 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.01.537.769 I llama_perf_context_print:        load time =     741.56 ms
0.01.537.770 I llama_perf_context_print: prompt eval time =      36.58 ms /     7 tokens (    5.23 ms per token,   191.38 tokens per second)
0.01.537.772 I llama_perf_context_print:        eval time =     745.83 ms /    63 runs   (   11.84 ms per token,    84.47 tokens per second)
0.01.537.772 I llama_perf_context_print:       total time =     785.70 ms /    70 tokens
0.01.537.938 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.108s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.741 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.233 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.236 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.239 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.240 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.240 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.240 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.241 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.241 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.244 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.244 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.184 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.208 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.209 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.210 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.210 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.210 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.210 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.211 I llama_model_loader: - type  f32:  194 tensors
0.00.024.211 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.212 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.460 I llm_load_vocab: special tokens cache size = 25
0.00.050.496 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.499 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.499 I llm_load_print_meta: arch             = gptneox
0.00.050.500 I llm_load_print_meta: vocab type       = BPE
0.00.050.500 I llm_load_print_meta: n_vocab          = 50304
0.00.050.500 I llm_load_print_meta: n_merges         = 50009
0.00.050.500 I llm_load_print_meta: vocab_only       = 0
0.00.050.500 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.500 I llm_load_print_meta: n_embd           = 2048
0.00.050.501 I llm_load_print_meta: n_layer          = 24
0.00.050.504 I llm_load_print_meta: n_head           = 16
0.00.050.504 I llm_load_print_meta: n_head_kv        = 16
0.00.050.505 I llm_load_print_meta: n_rot            = 32
0.00.050.505 I llm_load_print_meta: n_swa            = 0
0.00.050.505 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.505 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.506 I llm_load_print_meta: n_gqa            = 1
0.00.050.506 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.507 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.508 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.511 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.511 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.511 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.512 I llm_load_print_meta: n_ff             = 8192
0.00.050.512 I llm_load_print_meta: n_expert         = 0
0.00.050.512 I llm_load_print_meta: n_expert_used    = 0
0.00.050.516 I llm_load_print_meta: causal attn      = 1
0.00.050.516 I llm_load_print_meta: pooling type     = 0
0.00.050.516 I llm_load_print_meta: rope type        = 2
0.00.050.516 I llm_load_print_meta: rope scaling     = linear
0.00.050.516 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.517 I llm_load_print_meta: freq_scale_train = 1
0.00.050.517 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.517 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.517 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.517 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.518 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.518 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.518 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.530 I llm_load_print_meta: model type       = 1.4B
0.00.050.530 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.530 I llm_load_print_meta: model params     = 1.41 B
0.00.050.531 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.531 I llm_load_print_meta: general.name     = 1.4B
0.00.050.531 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.531 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.532 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.532 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.532 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.533 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.533 I llm_load_print_meta: max token length = 1024
0.00.052.559 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.559 I llm_load_tensors: offloading output layer to GPU
0.00.052.559 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.569 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.570 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.518 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.518 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.518 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.519 I llama_new_context_with_model: n_batch       = 2048
0.00.053.519 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.519 I llama_new_context_with_model: flash_attn    = 0
0.00.053.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.520 I llama_new_context_with_model: freq_scale    = 1
0.00.053.520 I ggml_metal_init: allocating
0.00.053.523 I ggml_metal_init: found device: Apple M4
0.00.053.525 I ggml_metal_init: picking default device: Apple M4
0.00.054.078 I ggml_metal_init: using embedded metal library
0.00.056.583 I ggml_metal_init: GPU name:   Apple M4
0.00.056.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.585 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.585 I ggml_metal_init: simdgroup reduction   = true
0.00.056.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.587 I ggml_metal_init: has bfloat            = true
0.00.056.587 I ggml_metal_init: use bfloat            = true
0.00.056.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.000 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.046 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.047 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.048 I llama_new_context_with_model: graph nodes  = 967
0.00.086.048 I llama_new_context_with_model: graph splits = 2
0.00.086.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.352 I main: llama threadpool init, n_threads = 4
0.00.710.391 I 
0.00.710.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.710.411 I 
0.00.710.650 I sampler seed: 1234
0.00.710.655 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.687 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.687 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.543.497 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61101.55 tokens per second)
0.01.543.498 I llama_perf_context_print:        load time =     701.61 ms
0.01.543.498 I llama_perf_context_print: prompt eval time =      36.58 ms /     7 tokens (    5.23 ms per token,   191.36 tokens per second)
0.01.543.499 I llama_perf_context_print:        eval time =     793.31 ms /    63 runs   (   12.59 ms per token,    79.41 tokens per second)
0.01.543.499 I llama_perf_context_print:       total time =     833.15 ms /    70 tokens
0.01.543.665 I ggml_metal_free: deallocating

real	0m1.559s
user	0m0.109s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.868 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.467 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.470 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.472 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.352 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.195 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.198 I llama_model_loader: - type  f32:  194 tensors
0.00.024.198 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.199 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.204 I llm_load_vocab: special tokens cache size = 25
0.00.051.292 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.295 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.296 I llm_load_print_meta: arch             = gptneox
0.00.051.296 I llm_load_print_meta: vocab type       = BPE
0.00.051.296 I llm_load_print_meta: n_vocab          = 50304
0.00.051.296 I llm_load_print_meta: n_merges         = 50009
0.00.051.297 I llm_load_print_meta: vocab_only       = 0
0.00.051.297 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.297 I llm_load_print_meta: n_embd           = 2048
0.00.051.297 I llm_load_print_meta: n_layer          = 24
0.00.051.299 I llm_load_print_meta: n_head           = 16
0.00.051.300 I llm_load_print_meta: n_head_kv        = 16
0.00.051.300 I llm_load_print_meta: n_rot            = 32
0.00.051.300 I llm_load_print_meta: n_swa            = 0
0.00.051.300 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.301 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.301 I llm_load_print_meta: n_gqa            = 1
0.00.051.302 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.304 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.305 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.305 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.310 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.312 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.312 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.313 I llm_load_print_meta: n_ff             = 8192
0.00.051.313 I llm_load_print_meta: n_expert         = 0
0.00.051.313 I llm_load_print_meta: n_expert_used    = 0
0.00.051.313 I llm_load_print_meta: causal attn      = 1
0.00.051.313 I llm_load_print_meta: pooling type     = 0
0.00.051.313 I llm_load_print_meta: rope type        = 2
0.00.051.314 I llm_load_print_meta: rope scaling     = linear
0.00.051.314 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.317 I llm_load_print_meta: freq_scale_train = 1
0.00.051.318 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.318 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.318 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.318 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.318 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.318 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.318 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.331 I llm_load_print_meta: model type       = 1.4B
0.00.051.331 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.331 I llm_load_print_meta: model params     = 1.41 B
0.00.051.332 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.332 I llm_load_print_meta: general.name     = 1.4B
0.00.051.332 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.332 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.333 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.333 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.333 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: max token length = 1024
0.00.053.275 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.275 I llm_load_tensors: offloading output layer to GPU
0.00.053.275 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.285 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.286 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.258 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.259 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.259 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.260 I llama_new_context_with_model: n_batch       = 2048
0.00.054.260 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.260 I llama_new_context_with_model: flash_attn    = 0
0.00.054.260 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.261 I llama_new_context_with_model: freq_scale    = 1
0.00.054.261 I ggml_metal_init: allocating
0.00.054.264 I ggml_metal_init: found device: Apple M4
0.00.054.266 I ggml_metal_init: picking default device: Apple M4
0.00.054.821 I ggml_metal_init: using embedded metal library
0.00.056.752 I ggml_metal_init: GPU name:   Apple M4
0.00.056.753 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.753 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.754 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.754 I ggml_metal_init: simdgroup reduction   = true
0.00.056.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.754 I ggml_metal_init: has bfloat            = true
0.00.056.754 I ggml_metal_init: use bfloat            = true
0.00.056.755 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.801 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.807 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.827 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.784 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.785 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.785 I llama_new_context_with_model: graph nodes  = 967
0.00.085.785 I llama_new_context_with_model: graph splits = 2
0.00.085.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.447.603 I main: llama threadpool init, n_threads = 4
0.00.447.642 I 
0.00.447.659 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.447.660 I 
0.00.447.885 I sampler seed: 1234
0.00.447.890 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.447.920 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.447.922 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.447.922 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.127.723 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.01.127.723 I llama_perf_context_print:        load time =     437.73 ms
0.01.127.724 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.47 tokens per second)
0.01.127.725 I llama_perf_context_print:        eval time =     640.95 ms /    63 runs   (   10.17 ms per token,    98.29 tokens per second)
0.01.127.725 I llama_perf_context_print:       total time =     680.12 ms /    70 tokens
0.01.127.900 I ggml_metal_free: deallocating

real	0m1.145s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.119 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.493 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.499 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.503 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.504 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.505 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.505 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.506 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.511 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.511 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.330 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.192 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.194 I llama_model_loader: - type  f32:  194 tensors
0.00.024.194 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.194 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.195 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.359 I llm_load_vocab: special tokens cache size = 25
0.00.050.481 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.484 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.484 I llm_load_print_meta: arch             = gptneox
0.00.050.485 I llm_load_print_meta: vocab type       = BPE
0.00.050.485 I llm_load_print_meta: n_vocab          = 50304
0.00.050.485 I llm_load_print_meta: n_merges         = 50009
0.00.050.485 I llm_load_print_meta: vocab_only       = 0
0.00.050.486 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.486 I llm_load_print_meta: n_embd           = 2048
0.00.050.486 I llm_load_print_meta: n_layer          = 24
0.00.050.488 I llm_load_print_meta: n_head           = 16
0.00.050.489 I llm_load_print_meta: n_head_kv        = 16
0.00.050.491 I llm_load_print_meta: n_rot            = 32
0.00.050.491 I llm_load_print_meta: n_swa            = 0
0.00.050.491 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.491 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.492 I llm_load_print_meta: n_gqa            = 1
0.00.050.493 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.495 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.495 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.496 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.496 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.497 I llm_load_print_meta: n_ff             = 8192
0.00.050.497 I llm_load_print_meta: n_expert         = 0
0.00.050.497 I llm_load_print_meta: n_expert_used    = 0
0.00.050.497 I llm_load_print_meta: causal attn      = 1
0.00.050.497 I llm_load_print_meta: pooling type     = 0
0.00.050.497 I llm_load_print_meta: rope type        = 2
0.00.050.498 I llm_load_print_meta: rope scaling     = linear
0.00.050.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.498 I llm_load_print_meta: freq_scale_train = 1
0.00.050.498 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.499 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.499 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.499 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.499 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.499 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.499 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.511 I llm_load_print_meta: model type       = 1.4B
0.00.050.512 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.512 I llm_load_print_meta: model params     = 1.41 B
0.00.050.512 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.513 I llm_load_print_meta: general.name     = 1.4B
0.00.050.513 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.513 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.514 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.514 I llm_load_print_meta: max token length = 1024
0.00.052.455 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.455 I llm_load_tensors: offloading output layer to GPU
0.00.052.456 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.466 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.467 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.345 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.346 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.346 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.346 I llama_new_context_with_model: n_batch       = 2048
0.00.053.346 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.346 I llama_new_context_with_model: flash_attn    = 0
0.00.053.348 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.348 I llama_new_context_with_model: freq_scale    = 1
0.00.053.349 I ggml_metal_init: allocating
0.00.053.355 I ggml_metal_init: found device: Apple M4
0.00.053.357 I ggml_metal_init: picking default device: Apple M4
0.00.053.928 I ggml_metal_init: using embedded metal library
0.00.056.077 I ggml_metal_init: GPU name:   Apple M4
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.080 I ggml_metal_init: simdgroup reduction   = true
0.00.056.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.080 I ggml_metal_init: has bfloat            = true
0.00.056.080 I ggml_metal_init: use bfloat            = true
0.00.056.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.680 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.700 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.565 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.566 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.566 I llama_new_context_with_model: graph nodes  = 967
0.00.084.566 I llama_new_context_with_model: graph splits = 2
0.00.084.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.809 I main: llama threadpool init, n_threads = 4
0.00.545.846 I 
0.00.545.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.545.880 I 
0.00.546.127 I sampler seed: 1234
0.00.546.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.546.152 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.546.153 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.546.153 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.290.930 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58340.18 tokens per second)
0.01.290.930 I llama_perf_context_print:        load time =     536.69 ms
0.01.290.931 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.62 tokens per second)
0.01.290.932 I llama_perf_context_print:        eval time =     702.13 ms /    63 runs   (   11.14 ms per token,    89.73 tokens per second)
0.01.290.932 I llama_perf_context_print:       total time =     745.12 ms /    70 tokens
0.01.291.096 I ggml_metal_free: deallocating

real	0m1.306s
user	0m0.108s
sys	0m0.131s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.407 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.291 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.343 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.187 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.188 I llama_model_loader: - type  f32:  194 tensors
0.00.026.188 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.189 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.189 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.313 I llm_load_vocab: special tokens cache size = 25
0.00.053.502 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.504 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.505 I llm_load_print_meta: arch             = gptneox
0.00.053.505 I llm_load_print_meta: vocab type       = BPE
0.00.053.505 I llm_load_print_meta: n_vocab          = 50304
0.00.053.506 I llm_load_print_meta: n_merges         = 50009
0.00.053.506 I llm_load_print_meta: vocab_only       = 0
0.00.053.506 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.506 I llm_load_print_meta: n_embd           = 2048
0.00.053.506 I llm_load_print_meta: n_layer          = 24
0.00.053.509 I llm_load_print_meta: n_head           = 16
0.00.053.510 I llm_load_print_meta: n_head_kv        = 16
0.00.053.510 I llm_load_print_meta: n_rot            = 32
0.00.053.510 I llm_load_print_meta: n_swa            = 0
0.00.053.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.511 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.511 I llm_load_print_meta: n_gqa            = 1
0.00.053.512 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.515 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.515 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.515 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.516 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.516 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.516 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.517 I llm_load_print_meta: n_ff             = 8192
0.00.053.517 I llm_load_print_meta: n_expert         = 0
0.00.053.517 I llm_load_print_meta: n_expert_used    = 0
0.00.053.518 I llm_load_print_meta: causal attn      = 1
0.00.053.518 I llm_load_print_meta: pooling type     = 0
0.00.053.518 I llm_load_print_meta: rope type        = 2
0.00.053.518 I llm_load_print_meta: rope scaling     = linear
0.00.053.518 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.519 I llm_load_print_meta: freq_scale_train = 1
0.00.053.519 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.519 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.519 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.519 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.521 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.533 I llm_load_print_meta: model type       = 1.4B
0.00.053.534 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.534 I llm_load_print_meta: model params     = 1.41 B
0.00.053.535 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.535 I llm_load_print_meta: general.name     = 1.4B
0.00.053.535 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.535 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.535 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.535 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.536 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.536 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.536 I llm_load_print_meta: max token length = 1024
0.00.055.555 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.555 I llm_load_tensors: offloading output layer to GPU
0.00.055.555 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.565 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.566 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.551 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.552 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.552 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.552 I llama_new_context_with_model: n_batch       = 2048
0.00.056.553 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.553 I llama_new_context_with_model: flash_attn    = 0
0.00.056.553 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.553 I llama_new_context_with_model: freq_scale    = 1
0.00.056.554 I ggml_metal_init: allocating
0.00.056.559 I ggml_metal_init: found device: Apple M4
0.00.056.561 I ggml_metal_init: picking default device: Apple M4
0.00.057.110 I ggml_metal_init: using embedded metal library
0.00.059.072 I ggml_metal_init: GPU name:   Apple M4
0.00.059.074 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.075 I ggml_metal_init: simdgroup reduction   = true
0.00.059.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.075 I ggml_metal_init: has bfloat            = true
0.00.059.075 I ggml_metal_init: use bfloat            = true
0.00.059.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.076 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.767 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.773 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.789 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.789 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.790 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.790 I llama_new_context_with_model: graph nodes  = 967
0.00.086.791 I llama_new_context_with_model: graph splits = 2
0.00.086.812 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.788 I main: llama threadpool init, n_threads = 4
0.00.619.828 I 
0.00.619.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.619.845 I 
0.00.620.054 I sampler seed: 1234
0.00.620.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.620.083 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.620.084 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.620.084 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.375.129 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.375.130 I llama_perf_context_print:        load time =     608.99 ms
0.01.375.131 I llama_perf_context_print: prompt eval time =      36.50 ms /     7 tokens (    5.21 ms per token,   191.78 tokens per second)
0.01.375.131 I llama_perf_context_print:        eval time =     715.46 ms /    63 runs   (   11.36 ms per token,    88.06 tokens per second)
0.01.375.134 I llama_perf_context_print:       total time =     755.34 ms /    70 tokens
0.01.375.312 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.012.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.122 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.122 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.123 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.123 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.126 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.967 I llama_model_loader: - type  f32:  194 tensors
0.00.027.968 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.968 I llama_model_loader: - type q6_K:   37 tensors
0.00.048.081 I llm_load_vocab: special tokens cache size = 25
0.00.053.990 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.993 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.993 I llm_load_print_meta: arch             = gptneox
0.00.053.994 I llm_load_print_meta: vocab type       = BPE
0.00.053.994 I llm_load_print_meta: n_vocab          = 50304
0.00.053.994 I llm_load_print_meta: n_merges         = 50009
0.00.053.994 I llm_load_print_meta: vocab_only       = 0
0.00.053.995 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.995 I llm_load_print_meta: n_embd           = 2048
0.00.053.995 I llm_load_print_meta: n_layer          = 24
0.00.053.997 I llm_load_print_meta: n_head           = 16
0.00.053.998 I llm_load_print_meta: n_head_kv        = 16
0.00.053.998 I llm_load_print_meta: n_rot            = 32
0.00.053.999 I llm_load_print_meta: n_swa            = 0
0.00.054.001 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.001 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.001 I llm_load_print_meta: n_gqa            = 1
0.00.054.002 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.003 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.003 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.009 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.009 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.009 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.009 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.010 I llm_load_print_meta: n_ff             = 8192
0.00.054.010 I llm_load_print_meta: n_expert         = 0
0.00.054.010 I llm_load_print_meta: n_expert_used    = 0
0.00.054.011 I llm_load_print_meta: causal attn      = 1
0.00.054.011 I llm_load_print_meta: pooling type     = 0
0.00.054.012 I llm_load_print_meta: rope type        = 2
0.00.054.013 I llm_load_print_meta: rope scaling     = linear
0.00.054.013 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.013 I llm_load_print_meta: freq_scale_train = 1
0.00.054.013 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.014 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.014 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.014 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.014 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.014 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.014 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.021 I llm_load_print_meta: model type       = 1.4B
0.00.054.022 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.054.022 I llm_load_print_meta: model params     = 1.41 B
0.00.054.023 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.054.023 I llm_load_print_meta: general.name     = 1.4B
0.00.054.023 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.024 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.025 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.025 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.026 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.054.026 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.026 I llm_load_print_meta: max token length = 1024
0.00.055.784 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.784 I llm_load_tensors: offloading output layer to GPU
0.00.055.784 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.789 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.789 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.651 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.652 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.652 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.653 I llama_new_context_with_model: n_batch       = 2048
0.00.056.653 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.653 I llama_new_context_with_model: flash_attn    = 0
0.00.056.653 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.654 I llama_new_context_with_model: freq_scale    = 1
0.00.056.654 I ggml_metal_init: allocating
0.00.056.657 I ggml_metal_init: found device: Apple M4
0.00.056.659 I ggml_metal_init: picking default device: Apple M4
0.00.057.197 I ggml_metal_init: using embedded metal library
0.00.059.132 I ggml_metal_init: GPU name:   Apple M4
0.00.059.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.134 I ggml_metal_init: simdgroup reduction   = true
0.00.059.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.134 I ggml_metal_init: has bfloat            = true
0.00.059.134 I ggml_metal_init: use bfloat            = true
0.00.059.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.010 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.015 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.040 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.063 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.064 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.065 I llama_new_context_with_model: graph nodes  = 967
0.00.087.065 I llama_new_context_with_model: graph splits = 2
0.00.087.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.823 I main: llama threadpool init, n_threads = 4
0.00.717.862 I 
0.00.717.880 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.717.881 I 
0.00.718.107 I sampler seed: 1234
0.00.718.112 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.718.122 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.718.123 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.718.123 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.553.649 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.553.650 I llama_perf_context_print:        load time =     704.98 ms
0.01.553.651 I llama_perf_context_print: prompt eval time =      38.64 ms /     7 tokens (    5.52 ms per token,   181.16 tokens per second)
0.01.553.652 I llama_perf_context_print:        eval time =     793.90 ms /    63 runs   (   12.60 ms per token,    79.36 tokens per second)
0.01.553.652 I llama_perf_context_print:       total time =     835.83 ms /    70 tokens
0.01.553.832 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.107s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.544 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.779 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.786 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.786 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.787 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.787 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.788 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.789 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.793 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.798 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.654 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.655 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.656 I llama_model_loader: - type  f32:  194 tensors
0.00.025.656 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.192 I llm_load_vocab: special tokens cache size = 25
0.00.052.193 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.195 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.196 I llm_load_print_meta: arch             = gptneox
0.00.052.196 I llm_load_print_meta: vocab type       = BPE
0.00.052.197 I llm_load_print_meta: n_vocab          = 50304
0.00.052.197 I llm_load_print_meta: n_merges         = 50009
0.00.052.197 I llm_load_print_meta: vocab_only       = 0
0.00.052.197 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.197 I llm_load_print_meta: n_embd           = 2048
0.00.052.197 I llm_load_print_meta: n_layer          = 24
0.00.052.200 I llm_load_print_meta: n_head           = 16
0.00.052.201 I llm_load_print_meta: n_head_kv        = 16
0.00.052.201 I llm_load_print_meta: n_rot            = 32
0.00.052.201 I llm_load_print_meta: n_swa            = 0
0.00.052.202 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.202 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.205 I llm_load_print_meta: n_gqa            = 1
0.00.052.206 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.206 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.207 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.208 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.208 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.208 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.209 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.209 I llm_load_print_meta: n_ff             = 8192
0.00.052.210 I llm_load_print_meta: n_expert         = 0
0.00.052.210 I llm_load_print_meta: n_expert_used    = 0
0.00.052.210 I llm_load_print_meta: causal attn      = 1
0.00.052.212 I llm_load_print_meta: pooling type     = 0
0.00.052.213 I llm_load_print_meta: rope type        = 2
0.00.052.213 I llm_load_print_meta: rope scaling     = linear
0.00.052.213 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.214 I llm_load_print_meta: freq_scale_train = 1
0.00.052.214 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.214 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.214 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.214 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.215 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.215 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.215 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.226 I llm_load_print_meta: model type       = 1.4B
0.00.052.228 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.228 I llm_load_print_meta: model params     = 1.41 B
0.00.052.228 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.228 I llm_load_print_meta: general.name     = 1.4B
0.00.052.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.229 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.229 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.229 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.229 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.230 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.230 I llm_load_print_meta: max token length = 1024
0.00.054.260 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.260 I llm_load_tensors: offloading output layer to GPU
0.00.054.260 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.270 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.271 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.284 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.285 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.285 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.285 I llama_new_context_with_model: n_batch       = 2048
0.00.055.285 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.285 I llama_new_context_with_model: flash_attn    = 0
0.00.055.286 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.286 I llama_new_context_with_model: freq_scale    = 1
0.00.055.286 I ggml_metal_init: allocating
0.00.055.292 I ggml_metal_init: found device: Apple M4
0.00.055.295 I ggml_metal_init: picking default device: Apple M4
0.00.055.854 I ggml_metal_init: using embedded metal library
0.00.057.815 I ggml_metal_init: GPU name:   Apple M4
0.00.057.817 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.817 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.818 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.818 I ggml_metal_init: simdgroup reduction   = true
0.00.057.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.820 I ggml_metal_init: has bfloat            = true
0.00.057.820 I ggml_metal_init: use bfloat            = true
0.00.057.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.230 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.238 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.258 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.377 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.378 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.378 I llama_new_context_with_model: graph nodes  = 967
0.00.086.379 I llama_new_context_with_model: graph splits = 2
0.00.086.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.298 I main: llama threadpool init, n_threads = 4
0.00.763.330 I 
0.00.763.349 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.763.349 I 
0.00.763.572 I sampler seed: 1234
0.00.763.576 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.763.587 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.763.587 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.763.587 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.632.098 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.632.098 I llama_perf_context_print:        load time =     753.75 ms
0.01.632.099 I llama_perf_context_print: prompt eval time =      38.59 ms /     7 tokens (    5.51 ms per token,   181.40 tokens per second)
0.01.632.100 I llama_perf_context_print:        eval time =     826.74 ms /    63 runs   (   13.12 ms per token,    76.20 tokens per second)
0.01.632.100 I llama_perf_context_print:       total time =     868.80 ms /    70 tokens
0.01.632.267 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.109s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.893 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.306 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.148 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.173 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.399 I llama_model_loader: - type  f32:  194 tensors
0.00.055.400 I llama_model_loader: - type  f16:   98 tensors
0.00.084.991 I llm_load_vocab: special tokens cache size = 25
0.00.091.667 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.670 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.670 I llm_load_print_meta: arch             = gptneox
0.00.091.670 I llm_load_print_meta: vocab type       = BPE
0.00.091.670 I llm_load_print_meta: n_vocab          = 50304
0.00.091.671 I llm_load_print_meta: n_merges         = 50009
0.00.091.671 I llm_load_print_meta: vocab_only       = 0
0.00.091.671 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.671 I llm_load_print_meta: n_embd           = 2048
0.00.091.671 I llm_load_print_meta: n_layer          = 24
0.00.091.674 I llm_load_print_meta: n_head           = 16
0.00.091.675 I llm_load_print_meta: n_head_kv        = 16
0.00.091.675 I llm_load_print_meta: n_rot            = 32
0.00.091.675 I llm_load_print_meta: n_swa            = 0
0.00.091.675 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.675 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.676 I llm_load_print_meta: n_gqa            = 1
0.00.091.676 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.677 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.677 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.678 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.678 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.678 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.678 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.679 I llm_load_print_meta: n_ff             = 8192
0.00.091.679 I llm_load_print_meta: n_expert         = 0
0.00.091.679 I llm_load_print_meta: n_expert_used    = 0
0.00.091.679 I llm_load_print_meta: causal attn      = 1
0.00.091.679 I llm_load_print_meta: pooling type     = 0
0.00.091.679 I llm_load_print_meta: rope type        = 2
0.00.091.680 I llm_load_print_meta: rope scaling     = linear
0.00.091.680 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.680 I llm_load_print_meta: freq_scale_train = 1
0.00.091.680 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.681 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.681 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.681 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.681 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.681 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.681 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.693 I llm_load_print_meta: model type       = 1.4B
0.00.091.693 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.693 I llm_load_print_meta: model params     = 1.41 B
0.00.091.695 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.695 I llm_load_print_meta: general.name     = 1.4B
0.00.091.695 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.695 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.696 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.696 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.696 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.696 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.696 I llm_load_print_meta: max token length = 1024
0.00.094.218 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.219 I llm_load_tensors: offloading output layer to GPU
0.00.094.219 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.229 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.230 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.142 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.143 I llama_new_context_with_model: n_ctx         = 128
0.00.095.143 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.143 I llama_new_context_with_model: n_batch       = 128
0.00.095.143 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.144 I llama_new_context_with_model: flash_attn    = 0
0.00.095.144 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.144 I llama_new_context_with_model: freq_scale    = 1
0.00.095.145 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.145 I ggml_metal_init: allocating
0.00.095.149 I ggml_metal_init: found device: Apple M4
0.00.095.151 I ggml_metal_init: picking default device: Apple M4
0.00.095.708 I ggml_metal_init: using embedded metal library
0.00.097.785 I ggml_metal_init: GPU name:   Apple M4
0.00.097.786 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.787 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.787 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.787 I ggml_metal_init: simdgroup reduction   = true
0.00.097.787 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.788 I ggml_metal_init: has bfloat            = true
0.00.097.788 I ggml_metal_init: use bfloat            = true
0.00.097.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.965 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.967 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.980 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.874 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.875 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.875 I llama_new_context_with_model: graph nodes  = 967
0.00.107.876 I llama_new_context_with_model: graph splits = 2
0.00.107.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.407.867 I 
0.01.407.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.407.920 I perplexity: tokenizing the input ..
0.01.419.738 I perplexity: tokenization took 11.814 ms
0.01.419.764 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.540.127 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.541.946 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.541.965 I llama_perf_context_print:        load time =    1382.54 ms
0.01.541.967 I llama_perf_context_print: prompt eval time =     119.99 ms /   128 tokens (    0.94 ms per token,  1066.79 tokens per second)
0.01.541.968 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.541.969 I llama_perf_context_print:       total time =     134.11 ms /   129 tokens
0.01.542.709 I ggml_metal_free: deallocating

real	0m1.740s
user	0m0.125s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.143 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.929 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.438 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.441 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.443 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.443 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.443 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.444 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.444 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.445 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.446 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.447 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.447 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.840 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.653 I llama_model_loader: - type  f32:  194 tensors
0.00.031.654 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.001 I llm_load_vocab: special tokens cache size = 25
0.00.063.249 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.252 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.252 I llm_load_print_meta: arch             = gptneox
0.00.063.253 I llm_load_print_meta: vocab type       = BPE
0.00.063.253 I llm_load_print_meta: n_vocab          = 50304
0.00.063.253 I llm_load_print_meta: n_merges         = 50009
0.00.063.253 I llm_load_print_meta: vocab_only       = 0
0.00.063.253 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.253 I llm_load_print_meta: n_embd           = 2048
0.00.063.253 I llm_load_print_meta: n_layer          = 24
0.00.063.256 I llm_load_print_meta: n_head           = 16
0.00.063.257 I llm_load_print_meta: n_head_kv        = 16
0.00.063.257 I llm_load_print_meta: n_rot            = 32
0.00.063.258 I llm_load_print_meta: n_swa            = 0
0.00.063.258 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.258 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.259 I llm_load_print_meta: n_gqa            = 1
0.00.063.259 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.260 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.261 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.261 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.261 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.261 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.261 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.262 I llm_load_print_meta: n_ff             = 8192
0.00.063.262 I llm_load_print_meta: n_expert         = 0
0.00.063.262 I llm_load_print_meta: n_expert_used    = 0
0.00.063.262 I llm_load_print_meta: causal attn      = 1
0.00.063.263 I llm_load_print_meta: pooling type     = 0
0.00.063.263 I llm_load_print_meta: rope type        = 2
0.00.063.263 I llm_load_print_meta: rope scaling     = linear
0.00.063.263 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.264 I llm_load_print_meta: freq_scale_train = 1
0.00.063.264 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.264 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.267 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.267 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.267 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.267 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.267 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.273 I llm_load_print_meta: model type       = 1.4B
0.00.063.274 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.274 I llm_load_print_meta: model params     = 1.41 B
0.00.063.275 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.275 I llm_load_print_meta: general.name     = 1.4B
0.00.063.275 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.275 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.275 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.275 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.276 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.063.276 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.277 I llm_load_print_meta: max token length = 1024
0.00.065.165 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.166 I llm_load_tensors: offloading output layer to GPU
0.00.065.166 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.171 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.171 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.152 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.153 I llama_new_context_with_model: n_ctx         = 128
0.00.066.153 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.153 I llama_new_context_with_model: n_batch       = 128
0.00.066.153 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.153 I llama_new_context_with_model: flash_attn    = 0
0.00.066.154 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.154 I llama_new_context_with_model: freq_scale    = 1
0.00.066.155 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.155 I ggml_metal_init: allocating
0.00.066.161 I ggml_metal_init: found device: Apple M4
0.00.066.163 I ggml_metal_init: picking default device: Apple M4
0.00.066.731 I ggml_metal_init: using embedded metal library
0.00.068.781 I ggml_metal_init: GPU name:   Apple M4
0.00.068.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.783 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.784 I ggml_metal_init: simdgroup reduction   = true
0.00.068.784 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.784 I ggml_metal_init: has bfloat            = true
0.00.068.784 I ggml_metal_init: use bfloat            = true
0.00.068.785 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.785 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.824 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.831 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.849 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.755 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.756 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.757 I llama_new_context_with_model: graph nodes  = 967
0.00.078.757 I llama_new_context_with_model: graph splits = 2
0.00.078.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.936.470 I 
0.00.936.490 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.936.493 I perplexity: tokenizing the input ..
0.00.944.688 I perplexity: tokenization took 8.194 ms
0.00.944.704 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.066.922 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.068.180 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.068.192 I llama_perf_context_print:        load time =     925.54 ms
0.01.068.193 I llama_perf_context_print: prompt eval time =     121.96 ms /   128 tokens (    0.95 ms per token,  1049.53 tokens per second)
0.01.068.194 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.068.195 I llama_perf_context_print:       total time =     131.72 ms /   129 tokens
0.01.068.612 I ggml_metal_free: deallocating

real	0m1.088s
user	0m0.092s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.922 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.926 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.927 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.928 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.928 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.929 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.930 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.931 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.932 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.806 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.677 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.678 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.679 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.680 I llama_model_loader: - type  f32:  194 tensors
0.00.025.680 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.680 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.503 I llm_load_vocab: special tokens cache size = 25
0.00.052.759 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.761 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.762 I llm_load_print_meta: arch             = gptneox
0.00.052.762 I llm_load_print_meta: vocab type       = BPE
0.00.052.762 I llm_load_print_meta: n_vocab          = 50304
0.00.052.763 I llm_load_print_meta: n_merges         = 50009
0.00.052.763 I llm_load_print_meta: vocab_only       = 0
0.00.052.763 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.763 I llm_load_print_meta: n_embd           = 2048
0.00.052.763 I llm_load_print_meta: n_layer          = 24
0.00.052.766 I llm_load_print_meta: n_head           = 16
0.00.052.767 I llm_load_print_meta: n_head_kv        = 16
0.00.052.767 I llm_load_print_meta: n_rot            = 32
0.00.052.767 I llm_load_print_meta: n_swa            = 0
0.00.052.767 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.767 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.769 I llm_load_print_meta: n_gqa            = 1
0.00.052.770 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.771 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.771 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.772 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.772 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.775 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.775 I llm_load_print_meta: n_ff             = 8192
0.00.052.776 I llm_load_print_meta: n_expert         = 0
0.00.052.776 I llm_load_print_meta: n_expert_used    = 0
0.00.052.776 I llm_load_print_meta: causal attn      = 1
0.00.052.776 I llm_load_print_meta: pooling type     = 0
0.00.052.776 I llm_load_print_meta: rope type        = 2
0.00.052.776 I llm_load_print_meta: rope scaling     = linear
0.00.052.777 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.777 I llm_load_print_meta: freq_scale_train = 1
0.00.052.777 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.778 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.778 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.778 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.778 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.778 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.778 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.783 I llm_load_print_meta: model type       = 1.4B
0.00.052.783 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.784 I llm_load_print_meta: model params     = 1.41 B
0.00.052.784 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.784 I llm_load_print_meta: general.name     = 1.4B
0.00.052.784 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.785 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.786 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.786 I llm_load_print_meta: max token length = 1024
0.00.054.564 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.564 I llm_load_tensors: offloading output layer to GPU
0.00.054.565 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.570 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.570 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.543 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.543 I llama_new_context_with_model: n_ctx         = 128
0.00.055.543 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.544 I llama_new_context_with_model: n_batch       = 128
0.00.055.544 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.544 I llama_new_context_with_model: flash_attn    = 0
0.00.055.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.545 I llama_new_context_with_model: freq_scale    = 1
0.00.055.545 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.545 I ggml_metal_init: allocating
0.00.055.551 I ggml_metal_init: found device: Apple M4
0.00.055.553 I ggml_metal_init: picking default device: Apple M4
0.00.056.106 I ggml_metal_init: using embedded metal library
0.00.058.027 I ggml_metal_init: GPU name:   Apple M4
0.00.058.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.029 I ggml_metal_init: simdgroup reduction   = true
0.00.058.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.029 I ggml_metal_init: has bfloat            = true
0.00.058.030 I ggml_metal_init: use bfloat            = true
0.00.058.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.280 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.283 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.298 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.173 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.174 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.175 I llama_new_context_with_model: graph nodes  = 967
0.00.068.175 I llama_new_context_with_model: graph splits = 2
0.00.068.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.474 I 
0.00.622.494 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.622.497 I perplexity: tokenizing the input ..
0.00.630.423 I perplexity: tokenization took 7.924 ms
0.00.630.435 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.987 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.754.173 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.754.201 I llama_perf_context_print:        load time =     611.58 ms
0.00.754.203 I llama_perf_context_print: prompt eval time =     122.31 ms /   128 tokens (    0.96 ms per token,  1046.54 tokens per second)
0.00.754.204 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.204 I llama_perf_context_print:       total time =     131.73 ms /   129 tokens
0.00.754.742 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.078s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.345 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.346 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.347 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.353 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.353 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.354 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.197 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.002 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.004 I llama_model_loader: - type  f32:  194 tensors
0.00.024.004 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.954 I llm_load_vocab: special tokens cache size = 25
0.00.050.015 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.018 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.018 I llm_load_print_meta: arch             = gptneox
0.00.050.018 I llm_load_print_meta: vocab type       = BPE
0.00.050.019 I llm_load_print_meta: n_vocab          = 50304
0.00.050.019 I llm_load_print_meta: n_merges         = 50009
0.00.050.019 I llm_load_print_meta: vocab_only       = 0
0.00.050.019 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.019 I llm_load_print_meta: n_embd           = 2048
0.00.050.020 I llm_load_print_meta: n_layer          = 24
0.00.050.022 I llm_load_print_meta: n_head           = 16
0.00.050.023 I llm_load_print_meta: n_head_kv        = 16
0.00.050.023 I llm_load_print_meta: n_rot            = 32
0.00.050.023 I llm_load_print_meta: n_swa            = 0
0.00.050.024 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.024 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.025 I llm_load_print_meta: n_gqa            = 1
0.00.050.025 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.026 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.027 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.027 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.027 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.028 I llm_load_print_meta: n_ff             = 8192
0.00.050.030 I llm_load_print_meta: n_expert         = 0
0.00.050.030 I llm_load_print_meta: n_expert_used    = 0
0.00.050.030 I llm_load_print_meta: causal attn      = 1
0.00.050.030 I llm_load_print_meta: pooling type     = 0
0.00.050.030 I llm_load_print_meta: rope type        = 2
0.00.050.031 I llm_load_print_meta: rope scaling     = linear
0.00.050.031 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.031 I llm_load_print_meta: freq_scale_train = 1
0.00.050.031 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.032 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.032 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.032 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.032 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.032 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.032 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.044 I llm_load_print_meta: model type       = 1.4B
0.00.050.044 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.045 I llm_load_print_meta: model params     = 1.41 B
0.00.050.045 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.045 I llm_load_print_meta: general.name     = 1.4B
0.00.050.045 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.046 I llm_load_print_meta: max token length = 1024
0.00.051.985 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.986 I llm_load_tensors: offloading output layer to GPU
0.00.051.986 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.996 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.997 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.845 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.846 I llama_new_context_with_model: n_ctx         = 128
0.00.052.846 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.846 I llama_new_context_with_model: n_batch       = 128
0.00.052.846 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.847 I llama_new_context_with_model: flash_attn    = 0
0.00.052.847 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.847 I llama_new_context_with_model: freq_scale    = 1
0.00.052.848 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.848 I ggml_metal_init: allocating
0.00.052.854 I ggml_metal_init: found device: Apple M4
0.00.052.856 I ggml_metal_init: picking default device: Apple M4
0.00.053.387 I ggml_metal_init: using embedded metal library
0.00.055.330 I ggml_metal_init: GPU name:   Apple M4
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.332 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.332 I ggml_metal_init: simdgroup reduction   = true
0.00.055.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.332 I ggml_metal_init: has bfloat            = true
0.00.055.332 I ggml_metal_init: use bfloat            = true
0.00.055.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.417 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.420 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.435 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.346 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.347 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.348 I llama_new_context_with_model: graph nodes  = 967
0.00.065.348 I llama_new_context_with_model: graph splits = 2
0.00.065.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.079 I 
0.00.686.112 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.686.121 I perplexity: tokenizing the input ..
0.00.695.335 I perplexity: tokenization took 9.211 ms
0.00.695.347 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.634 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.821.801 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.821.821 I llama_perf_context_print:        load time =     676.29 ms
0.00.821.823 I llama_perf_context_print: prompt eval time =     125.06 ms /   128 tokens (    0.98 ms per token,  1023.47 tokens per second)
0.00.821.824 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.824 I llama_perf_context_print:       total time =     135.75 ms /   129 tokens
0.00.822.305 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.078s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.614 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.500 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.501 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.502 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.502 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.503 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.504 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.506 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.507 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.306 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.236 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.237 I llama_model_loader: - type  f32:  194 tensors
0.00.025.237 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.238 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.851 I llm_load_vocab: special tokens cache size = 25
0.00.051.897 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.899 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.900 I llm_load_print_meta: arch             = gptneox
0.00.051.900 I llm_load_print_meta: vocab type       = BPE
0.00.051.900 I llm_load_print_meta: n_vocab          = 50304
0.00.051.901 I llm_load_print_meta: n_merges         = 50009
0.00.051.901 I llm_load_print_meta: vocab_only       = 0
0.00.051.901 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.901 I llm_load_print_meta: n_embd           = 2048
0.00.051.901 I llm_load_print_meta: n_layer          = 24
0.00.051.904 I llm_load_print_meta: n_head           = 16
0.00.051.905 I llm_load_print_meta: n_head_kv        = 16
0.00.051.905 I llm_load_print_meta: n_rot            = 32
0.00.051.905 I llm_load_print_meta: n_swa            = 0
0.00.051.908 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.908 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.909 I llm_load_print_meta: n_gqa            = 1
0.00.051.911 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.912 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.912 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.914 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.915 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.915 I llm_load_print_meta: n_ff             = 8192
0.00.051.915 I llm_load_print_meta: n_expert         = 0
0.00.051.916 I llm_load_print_meta: n_expert_used    = 0
0.00.051.916 I llm_load_print_meta: causal attn      = 1
0.00.051.916 I llm_load_print_meta: pooling type     = 0
0.00.051.916 I llm_load_print_meta: rope type        = 2
0.00.051.916 I llm_load_print_meta: rope scaling     = linear
0.00.051.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.917 I llm_load_print_meta: freq_scale_train = 1
0.00.051.917 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.922 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.922 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.922 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.922 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.922 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.925 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.936 I llm_load_print_meta: model type       = 1.4B
0.00.051.937 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.937 I llm_load_print_meta: model params     = 1.41 B
0.00.051.937 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.938 I llm_load_print_meta: general.name     = 1.4B
0.00.051.938 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.938 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.938 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.938 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.939 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.939 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.939 I llm_load_print_meta: max token length = 1024
0.00.053.490 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.490 I llm_load_tensors: offloading output layer to GPU
0.00.053.490 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.500 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.501 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.328 I llama_new_context_with_model: n_ctx         = 128
0.00.054.329 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.329 I llama_new_context_with_model: n_batch       = 128
0.00.054.329 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.329 I llama_new_context_with_model: flash_attn    = 0
0.00.054.329 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.330 I llama_new_context_with_model: freq_scale    = 1
0.00.054.330 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.330 I ggml_metal_init: allocating
0.00.054.333 I ggml_metal_init: found device: Apple M4
0.00.054.335 I ggml_metal_init: picking default device: Apple M4
0.00.054.886 I ggml_metal_init: using embedded metal library
0.00.056.777 I ggml_metal_init: GPU name:   Apple M4
0.00.056.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.779 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.780 I ggml_metal_init: simdgroup reduction   = true
0.00.056.780 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.780 I ggml_metal_init: has bfloat            = true
0.00.056.780 I ggml_metal_init: use bfloat            = true
0.00.056.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.781 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.929 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.931 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.946 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.822 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.823 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.824 I llama_new_context_with_model: graph nodes  = 967
0.00.066.824 I llama_new_context_with_model: graph splits = 2
0.00.066.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.393 I 
0.00.721.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.721.417 I perplexity: tokenizing the input ..
0.00.729.006 I perplexity: tokenization took 7.587 ms
0.00.729.017 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.863.434 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.864.568 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.864.580 I llama_perf_context_print:        load time =     710.77 ms
0.00.864.581 I llama_perf_context_print: prompt eval time =     134.19 ms /   128 tokens (    1.05 ms per token,   953.86 tokens per second)
0.00.864.582 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.582 I llama_perf_context_print:       total time =     143.19 ms /   129 tokens
0.00.865.056 I ggml_metal_free: deallocating

real	0m0.881s
user	0m0.077s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.315 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.013.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.917 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.917 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.919 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.920 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.922 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.922 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.801 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.631 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.632 I llama_model_loader: - type  f32:  194 tensors
0.00.022.633 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.633 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.628 I llm_load_vocab: special tokens cache size = 25
0.00.048.756 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.758 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.759 I llm_load_print_meta: arch             = gptneox
0.00.048.759 I llm_load_print_meta: vocab type       = BPE
0.00.048.759 I llm_load_print_meta: n_vocab          = 50304
0.00.048.759 I llm_load_print_meta: n_merges         = 50009
0.00.048.759 I llm_load_print_meta: vocab_only       = 0
0.00.048.760 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.760 I llm_load_print_meta: n_embd           = 2048
0.00.048.760 I llm_load_print_meta: n_layer          = 24
0.00.048.763 I llm_load_print_meta: n_head           = 16
0.00.048.763 I llm_load_print_meta: n_head_kv        = 16
0.00.048.763 I llm_load_print_meta: n_rot            = 32
0.00.048.764 I llm_load_print_meta: n_swa            = 0
0.00.048.764 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.764 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.765 I llm_load_print_meta: n_gqa            = 1
0.00.048.765 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.766 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.767 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.767 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.769 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.769 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.769 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.770 I llm_load_print_meta: n_ff             = 8192
0.00.048.770 I llm_load_print_meta: n_expert         = 0
0.00.048.770 I llm_load_print_meta: n_expert_used    = 0
0.00.048.770 I llm_load_print_meta: causal attn      = 1
0.00.048.770 I llm_load_print_meta: pooling type     = 0
0.00.048.770 I llm_load_print_meta: rope type        = 2
0.00.048.771 I llm_load_print_meta: rope scaling     = linear
0.00.048.771 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.771 I llm_load_print_meta: freq_scale_train = 1
0.00.048.771 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.772 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.772 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.772 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.772 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.772 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.774 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.785 I llm_load_print_meta: model type       = 1.4B
0.00.048.785 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.786 I llm_load_print_meta: model params     = 1.41 B
0.00.048.786 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.787 I llm_load_print_meta: general.name     = 1.4B
0.00.048.787 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.787 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.787 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.787 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.788 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.788 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.788 I llm_load_print_meta: max token length = 1024
0.00.050.258 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.258 I llm_load_tensors: offloading output layer to GPU
0.00.050.259 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.268 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.269 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.102 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.103 I llama_new_context_with_model: n_ctx         = 128
0.00.051.103 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.103 I llama_new_context_with_model: n_batch       = 128
0.00.051.103 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.104 I llama_new_context_with_model: flash_attn    = 0
0.00.051.104 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.104 I llama_new_context_with_model: freq_scale    = 1
0.00.051.105 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.105 I ggml_metal_init: allocating
0.00.051.111 I ggml_metal_init: found device: Apple M4
0.00.051.113 I ggml_metal_init: picking default device: Apple M4
0.00.051.634 I ggml_metal_init: using embedded metal library
0.00.053.567 I ggml_metal_init: GPU name:   Apple M4
0.00.053.569 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.569 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.569 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.570 I ggml_metal_init: simdgroup reduction   = true
0.00.053.570 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.570 I ggml_metal_init: has bfloat            = true
0.00.053.570 I ggml_metal_init: use bfloat            = true
0.00.053.571 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.571 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.636 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.639 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.654 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.529 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.530 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.530 I llama_new_context_with_model: graph nodes  = 967
0.00.063.531 I llama_new_context_with_model: graph splits = 2
0.00.063.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.129 I 
0.00.663.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.663.157 I perplexity: tokenizing the input ..
0.00.671.063 I perplexity: tokenization took 7.905 ms
0.00.671.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.530 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.806.676 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.806.693 I llama_perf_context_print:        load time =     654.81 ms
0.00.806.694 I llama_perf_context_print: prompt eval time =     134.23 ms /   128 tokens (    1.05 ms per token,   953.60 tokens per second)
0.00.806.695 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.695 I llama_perf_context_print:       total time =     143.56 ms /   129 tokens
0.00.807.041 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.076s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.762 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.203 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.208 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.209 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.210 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.210 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.211 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.997 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.752 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.753 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.753 I llama_model_loader: - type  f32:  194 tensors
0.00.023.754 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.754 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.754 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.623 I llm_load_vocab: special tokens cache size = 25
0.00.049.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.499 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.500 I llm_load_print_meta: arch             = gptneox
0.00.049.500 I llm_load_print_meta: vocab type       = BPE
0.00.049.500 I llm_load_print_meta: n_vocab          = 50304
0.00.049.500 I llm_load_print_meta: n_merges         = 50009
0.00.049.501 I llm_load_print_meta: vocab_only       = 0
0.00.049.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.501 I llm_load_print_meta: n_embd           = 2048
0.00.049.501 I llm_load_print_meta: n_layer          = 24
0.00.049.504 I llm_load_print_meta: n_head           = 16
0.00.049.505 I llm_load_print_meta: n_head_kv        = 16
0.00.049.505 I llm_load_print_meta: n_rot            = 32
0.00.049.505 I llm_load_print_meta: n_swa            = 0
0.00.049.505 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.505 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.506 I llm_load_print_meta: n_gqa            = 1
0.00.049.507 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.507 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.508 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.511 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.511 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.511 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.512 I llm_load_print_meta: n_ff             = 8192
0.00.049.512 I llm_load_print_meta: n_expert         = 0
0.00.049.512 I llm_load_print_meta: n_expert_used    = 0
0.00.049.512 I llm_load_print_meta: causal attn      = 1
0.00.049.512 I llm_load_print_meta: pooling type     = 0
0.00.049.513 I llm_load_print_meta: rope type        = 2
0.00.049.513 I llm_load_print_meta: rope scaling     = linear
0.00.049.513 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.513 I llm_load_print_meta: freq_scale_train = 1
0.00.049.514 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.515 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.515 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.516 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.516 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.516 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.516 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.527 I llm_load_print_meta: model type       = 1.4B
0.00.049.527 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.528 I llm_load_print_meta: model params     = 1.41 B
0.00.049.528 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.528 I llm_load_print_meta: general.name     = 1.4B
0.00.049.529 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.529 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.530 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.530 I llm_load_print_meta: max token length = 1024
0.00.051.015 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.015 I llm_load_tensors: offloading output layer to GPU
0.00.051.015 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.024 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.025 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.811 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.812 I llama_new_context_with_model: n_ctx         = 128
0.00.051.812 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.813 I llama_new_context_with_model: n_batch       = 128
0.00.051.813 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.813 I llama_new_context_with_model: flash_attn    = 0
0.00.051.813 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.814 I llama_new_context_with_model: freq_scale    = 1
0.00.051.814 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.814 I ggml_metal_init: allocating
0.00.051.817 I ggml_metal_init: found device: Apple M4
0.00.051.819 I ggml_metal_init: picking default device: Apple M4
0.00.052.339 I ggml_metal_init: using embedded metal library
0.00.054.249 I ggml_metal_init: GPU name:   Apple M4
0.00.054.251 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.252 I ggml_metal_init: simdgroup reduction   = true
0.00.054.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.252 I ggml_metal_init: has bfloat            = true
0.00.054.252 I ggml_metal_init: use bfloat            = true
0.00.054.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.345 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.348 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.371 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.264 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.265 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.265 I llama_new_context_with_model: graph nodes  = 967
0.00.064.265 I llama_new_context_with_model: graph splits = 2
0.00.064.278 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.400.236 I 
0.00.400.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.400.274 I perplexity: tokenizing the input ..
0.00.408.075 I perplexity: tokenization took 7.799 ms
0.00.408.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.540.805 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.541.970 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.541.983 I llama_perf_context_print:        load time =     390.47 ms
0.00.541.984 I llama_perf_context_print: prompt eval time =     132.49 ms /   128 tokens (    1.04 ms per token,   966.10 tokens per second)
0.00.541.984 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.541.985 I llama_perf_context_print:       total time =     141.75 ms /   129 tokens
0.00.542.520 I ggml_metal_free: deallocating

real	0m0.559s
user	0m0.076s
sys	0m0.082s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.675 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.654 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.666 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.667 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.667 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.669 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.670 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.670 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.559 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.360 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.360 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.360 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.361 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.361 I llama_model_loader: - type  f32:  194 tensors
0.00.023.361 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.362 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.362 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.362 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.041 I llm_load_vocab: special tokens cache size = 25
0.00.050.207 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.210 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.210 I llm_load_print_meta: arch             = gptneox
0.00.050.210 I llm_load_print_meta: vocab type       = BPE
0.00.050.211 I llm_load_print_meta: n_vocab          = 50304
0.00.050.211 I llm_load_print_meta: n_merges         = 50009
0.00.050.211 I llm_load_print_meta: vocab_only       = 0
0.00.050.211 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.211 I llm_load_print_meta: n_embd           = 2048
0.00.050.211 I llm_load_print_meta: n_layer          = 24
0.00.050.214 I llm_load_print_meta: n_head           = 16
0.00.050.215 I llm_load_print_meta: n_head_kv        = 16
0.00.050.215 I llm_load_print_meta: n_rot            = 32
0.00.050.216 I llm_load_print_meta: n_swa            = 0
0.00.050.216 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.216 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.217 I llm_load_print_meta: n_gqa            = 1
0.00.050.218 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.218 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.219 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.219 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.219 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.220 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.220 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.221 I llm_load_print_meta: n_ff             = 8192
0.00.050.221 I llm_load_print_meta: n_expert         = 0
0.00.050.221 I llm_load_print_meta: n_expert_used    = 0
0.00.050.221 I llm_load_print_meta: causal attn      = 1
0.00.050.229 I llm_load_print_meta: pooling type     = 0
0.00.050.231 I llm_load_print_meta: rope type        = 2
0.00.050.231 I llm_load_print_meta: rope scaling     = linear
0.00.050.232 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.232 I llm_load_print_meta: freq_scale_train = 1
0.00.050.232 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.232 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.232 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.233 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.233 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.233 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.233 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.244 I llm_load_print_meta: model type       = 1.4B
0.00.050.245 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.245 I llm_load_print_meta: model params     = 1.41 B
0.00.050.246 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.246 I llm_load_print_meta: general.name     = 1.4B
0.00.050.246 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.246 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.247 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.248 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: max token length = 1024
0.00.051.765 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.765 I llm_load_tensors: offloading output layer to GPU
0.00.051.765 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.775 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.776 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.574 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.574 I llama_new_context_with_model: n_ctx         = 128
0.00.052.575 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.575 I llama_new_context_with_model: n_batch       = 128
0.00.052.575 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.575 I llama_new_context_with_model: flash_attn    = 0
0.00.052.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.576 I llama_new_context_with_model: freq_scale    = 1
0.00.052.576 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.576 I ggml_metal_init: allocating
0.00.052.579 I ggml_metal_init: found device: Apple M4
0.00.052.581 I ggml_metal_init: picking default device: Apple M4
0.00.053.097 I ggml_metal_init: using embedded metal library
0.00.055.039 I ggml_metal_init: GPU name:   Apple M4
0.00.055.040 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.041 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.041 I ggml_metal_init: simdgroup reduction   = true
0.00.055.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.041 I ggml_metal_init: has bfloat            = true
0.00.055.041 I ggml_metal_init: use bfloat            = true
0.00.055.042 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.934 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.937 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.957 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.804 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.805 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.805 I llama_new_context_with_model: graph nodes  = 967
0.00.064.806 I llama_new_context_with_model: graph splits = 2
0.00.064.817 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.501.435 I 
0.00.501.457 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.501.462 I perplexity: tokenizing the input ..
0.00.509.527 I perplexity: tokenization took 8.063 ms
0.00.509.540 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.641.412 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.642.553 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.642.573 I llama_perf_context_print:        load time =     492.75 ms
0.00.642.574 I llama_perf_context_print: prompt eval time =     131.65 ms /   128 tokens (    1.03 ms per token,   972.30 tokens per second)
0.00.642.575 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.642.575 I llama_perf_context_print:       total time =     141.14 ms /   129 tokens
0.00.643.029 I ggml_metal_free: deallocating

real	0m0.656s
user	0m0.077s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.475 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.083 I llama_model_loader: - type  f32:  194 tensors
0.00.023.084 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.084 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.084 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.200 I llm_load_vocab: special tokens cache size = 25
0.00.049.263 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.266 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.266 I llm_load_print_meta: arch             = gptneox
0.00.049.267 I llm_load_print_meta: vocab type       = BPE
0.00.049.267 I llm_load_print_meta: n_vocab          = 50304
0.00.049.267 I llm_load_print_meta: n_merges         = 50009
0.00.049.267 I llm_load_print_meta: vocab_only       = 0
0.00.049.267 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.268 I llm_load_print_meta: n_embd           = 2048
0.00.049.268 I llm_load_print_meta: n_layer          = 24
0.00.049.271 I llm_load_print_meta: n_head           = 16
0.00.049.271 I llm_load_print_meta: n_head_kv        = 16
0.00.049.272 I llm_load_print_meta: n_rot            = 32
0.00.049.272 I llm_load_print_meta: n_swa            = 0
0.00.049.273 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.273 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.276 I llm_load_print_meta: n_gqa            = 1
0.00.049.276 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.277 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.278 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.278 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.280 I llm_load_print_meta: n_ff             = 8192
0.00.049.281 I llm_load_print_meta: n_expert         = 0
0.00.049.281 I llm_load_print_meta: n_expert_used    = 0
0.00.049.281 I llm_load_print_meta: causal attn      = 1
0.00.049.281 I llm_load_print_meta: pooling type     = 0
0.00.049.281 I llm_load_print_meta: rope type        = 2
0.00.049.281 I llm_load_print_meta: rope scaling     = linear
0.00.049.282 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.282 I llm_load_print_meta: freq_scale_train = 1
0.00.049.282 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.283 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.283 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.283 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.284 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.284 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.284 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.295 I llm_load_print_meta: model type       = 1.4B
0.00.049.296 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.296 I llm_load_print_meta: model params     = 1.41 B
0.00.049.297 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.297 I llm_load_print_meta: general.name     = 1.4B
0.00.049.297 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.297 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.298 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.298 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.298 I llm_load_print_meta: max token length = 1024
0.00.050.811 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.811 I llm_load_tensors: offloading output layer to GPU
0.00.050.812 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.821 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.822 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.655 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.656 I llama_new_context_with_model: n_ctx         = 128
0.00.051.656 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.656 I llama_new_context_with_model: n_batch       = 128
0.00.051.657 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.657 I llama_new_context_with_model: flash_attn    = 0
0.00.051.657 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.657 I llama_new_context_with_model: freq_scale    = 1
0.00.051.658 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.658 I ggml_metal_init: allocating
0.00.051.662 I ggml_metal_init: found device: Apple M4
0.00.051.665 I ggml_metal_init: picking default device: Apple M4
0.00.052.211 I ggml_metal_init: using embedded metal library
0.00.054.090 I ggml_metal_init: GPU name:   Apple M4
0.00.054.092 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.092 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.092 I ggml_metal_init: simdgroup reduction   = true
0.00.054.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.093 I ggml_metal_init: has bfloat            = true
0.00.054.093 I ggml_metal_init: use bfloat            = true
0.00.054.093 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.072 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.074 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.088 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.954 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.956 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.956 I llama_new_context_with_model: graph nodes  = 967
0.00.063.956 I llama_new_context_with_model: graph splits = 2
0.00.063.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.911 I 
0.00.573.934 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.573.937 I perplexity: tokenizing the input ..
0.00.581.889 I perplexity: tokenization took 7.95 ms
0.00.581.905 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.715.959 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.135 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.149 I llama_perf_context_print:        load time =     565.16 ms
0.00.717.151 I llama_perf_context_print: prompt eval time =     133.82 ms /   128 tokens (    1.05 ms per token,   956.53 tokens per second)
0.00.717.151 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.152 I llama_perf_context_print:       total time =     143.24 ms /   129 tokens
0.00.717.553 I ggml_metal_free: deallocating

real	0m0.731s
user	0m0.076s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.728 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.401 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.402 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.403 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.403 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.403 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.404 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.404 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.404 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.297 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.176 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.176 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.177 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.177 I llama_model_loader: - type  f32:  194 tensors
0.00.024.178 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.178 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.053 I llm_load_vocab: special tokens cache size = 25
0.00.050.070 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.072 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.073 I llm_load_print_meta: arch             = gptneox
0.00.050.073 I llm_load_print_meta: vocab type       = BPE
0.00.050.073 I llm_load_print_meta: n_vocab          = 50304
0.00.050.073 I llm_load_print_meta: n_merges         = 50009
0.00.050.073 I llm_load_print_meta: vocab_only       = 0
0.00.050.074 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.074 I llm_load_print_meta: n_embd           = 2048
0.00.050.074 I llm_load_print_meta: n_layer          = 24
0.00.050.076 I llm_load_print_meta: n_head           = 16
0.00.050.077 I llm_load_print_meta: n_head_kv        = 16
0.00.050.080 I llm_load_print_meta: n_rot            = 32
0.00.050.080 I llm_load_print_meta: n_swa            = 0
0.00.050.080 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.080 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.081 I llm_load_print_meta: n_gqa            = 1
0.00.050.082 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.082 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.083 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.083 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.083 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.083 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.084 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.084 I llm_load_print_meta: n_ff             = 8192
0.00.050.085 I llm_load_print_meta: n_expert         = 0
0.00.050.085 I llm_load_print_meta: n_expert_used    = 0
0.00.050.085 I llm_load_print_meta: causal attn      = 1
0.00.050.089 I llm_load_print_meta: pooling type     = 0
0.00.050.089 I llm_load_print_meta: rope type        = 2
0.00.050.089 I llm_load_print_meta: rope scaling     = linear
0.00.050.090 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.090 I llm_load_print_meta: freq_scale_train = 1
0.00.050.090 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.091 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.091 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.091 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.091 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.092 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.093 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.104 I llm_load_print_meta: model type       = 1.4B
0.00.050.104 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.104 I llm_load_print_meta: model params     = 1.41 B
0.00.050.105 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.105 I llm_load_print_meta: general.name     = 1.4B
0.00.050.105 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.105 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.106 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.106 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.106 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.106 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.106 I llm_load_print_meta: max token length = 1024
0.00.051.638 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.638 I llm_load_tensors: offloading output layer to GPU
0.00.051.638 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.648 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.649 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.457 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.458 I llama_new_context_with_model: n_ctx         = 128
0.00.052.458 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.458 I llama_new_context_with_model: n_batch       = 128
0.00.052.459 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.459 I llama_new_context_with_model: flash_attn    = 0
0.00.052.459 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.460 I llama_new_context_with_model: freq_scale    = 1
0.00.052.460 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.460 I ggml_metal_init: allocating
0.00.052.466 I ggml_metal_init: found device: Apple M4
0.00.052.468 I ggml_metal_init: picking default device: Apple M4
0.00.052.995 I ggml_metal_init: using embedded metal library
0.00.054.944 I ggml_metal_init: GPU name:   Apple M4
0.00.054.945 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.946 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.946 I ggml_metal_init: simdgroup reduction   = true
0.00.054.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.947 I ggml_metal_init: has bfloat            = true
0.00.054.947 I ggml_metal_init: use bfloat            = true
0.00.054.947 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.948 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.891 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.893 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.917 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.777 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.778 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.778 I llama_new_context_with_model: graph nodes  = 967
0.00.064.778 I llama_new_context_with_model: graph splits = 2
0.00.064.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.471 I 
0.00.660.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.660.503 I perplexity: tokenizing the input ..
0.00.668.468 I perplexity: tokenization took 7.964 ms
0.00.668.482 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.629 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.797 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.812 I llama_perf_context_print:        load time =     650.74 ms
0.00.809.813 I llama_perf_context_print: prompt eval time =     139.92 ms /   128 tokens (    1.09 ms per token,   914.80 tokens per second)
0.00.809.814 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.815 I llama_perf_context_print:       total time =     149.35 ms /   129 tokens
0.00.810.163 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.076s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.459 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.329 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.143 I llama_model_loader: - type  f32:  194 tensors
0.00.023.144 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.718 I llm_load_vocab: special tokens cache size = 25
0.00.049.791 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.793 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.794 I llm_load_print_meta: arch             = gptneox
0.00.049.794 I llm_load_print_meta: vocab type       = BPE
0.00.049.794 I llm_load_print_meta: n_vocab          = 50304
0.00.049.795 I llm_load_print_meta: n_merges         = 50009
0.00.049.795 I llm_load_print_meta: vocab_only       = 0
0.00.049.795 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.795 I llm_load_print_meta: n_embd           = 2048
0.00.049.795 I llm_load_print_meta: n_layer          = 24
0.00.049.798 I llm_load_print_meta: n_head           = 16
0.00.049.799 I llm_load_print_meta: n_head_kv        = 16
0.00.049.799 I llm_load_print_meta: n_rot            = 32
0.00.049.799 I llm_load_print_meta: n_swa            = 0
0.00.049.799 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.799 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.800 I llm_load_print_meta: n_gqa            = 1
0.00.049.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.803 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.804 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.804 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.805 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.805 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.805 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.806 I llm_load_print_meta: n_ff             = 8192
0.00.049.806 I llm_load_print_meta: n_expert         = 0
0.00.049.806 I llm_load_print_meta: n_expert_used    = 0
0.00.049.806 I llm_load_print_meta: causal attn      = 1
0.00.049.806 I llm_load_print_meta: pooling type     = 0
0.00.049.806 I llm_load_print_meta: rope type        = 2
0.00.049.807 I llm_load_print_meta: rope scaling     = linear
0.00.049.807 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.807 I llm_load_print_meta: freq_scale_train = 1
0.00.049.808 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.808 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.808 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.808 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.808 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.808 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.820 I llm_load_print_meta: model type       = 1.4B
0.00.049.821 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.821 I llm_load_print_meta: model params     = 1.41 B
0.00.049.822 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.823 I llm_load_print_meta: general.name     = 1.4B
0.00.049.823 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.823 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.823 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.823 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.824 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.824 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.824 I llm_load_print_meta: max token length = 1024
0.00.051.391 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.391 I llm_load_tensors: offloading output layer to GPU
0.00.051.391 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.400 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.402 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.250 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.250 I llama_new_context_with_model: n_ctx         = 128
0.00.052.251 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.251 I llama_new_context_with_model: n_batch       = 128
0.00.052.251 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.251 I llama_new_context_with_model: flash_attn    = 0
0.00.052.251 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.252 I llama_new_context_with_model: freq_scale    = 1
0.00.052.252 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.252 I ggml_metal_init: allocating
0.00.052.257 I ggml_metal_init: found device: Apple M4
0.00.052.259 I ggml_metal_init: picking default device: Apple M4
0.00.052.797 I ggml_metal_init: using embedded metal library
0.00.054.711 I ggml_metal_init: GPU name:   Apple M4
0.00.054.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.712 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.713 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.713 I ggml_metal_init: simdgroup reduction   = true
0.00.054.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.713 I ggml_metal_init: has bfloat            = true
0.00.054.713 I ggml_metal_init: use bfloat            = true
0.00.054.714 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.714 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.658 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.674 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.531 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.532 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.532 I llama_new_context_with_model: graph nodes  = 967
0.00.064.533 I llama_new_context_with_model: graph splits = 2
0.00.064.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.330 I 
0.00.590.364 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.590.373 I perplexity: tokenizing the input ..
0.00.598.466 I perplexity: tokenization took 8.092 ms
0.00.598.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.127 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.739.322 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.739.335 I llama_perf_context_print:        load time =     581.65 ms
0.00.739.336 I llama_perf_context_print: prompt eval time =     139.42 ms /   128 tokens (    1.09 ms per token,   918.09 tokens per second)
0.00.739.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.337 I llama_perf_context_print:       total time =     149.01 ms /   129 tokens
0.00.739.716 I ggml_metal_free: deallocating

real	0m0.752s
user	0m0.077s
sys	0m0.121s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.317 I build: 4202 (9f912511) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.470 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.477 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.478 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.480 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.480 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.484 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.852 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.853 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.854 I llama_model_loader: - type  f32:  194 tensors
0.00.051.854 I llama_model_loader: - type  f16:   98 tensors
0.00.079.789 I llm_load_vocab: special tokens cache size = 25
0.00.086.337 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.341 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.341 I llm_load_print_meta: arch             = gptneox
0.00.086.341 I llm_load_print_meta: vocab type       = BPE
0.00.086.341 I llm_load_print_meta: n_vocab          = 50304
0.00.086.341 I llm_load_print_meta: n_merges         = 50009
0.00.086.342 I llm_load_print_meta: vocab_only       = 0
0.00.086.342 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.342 I llm_load_print_meta: n_embd           = 2048
0.00.086.342 I llm_load_print_meta: n_layer          = 24
0.00.086.346 I llm_load_print_meta: n_head           = 16
0.00.086.346 I llm_load_print_meta: n_head_kv        = 16
0.00.086.346 I llm_load_print_meta: n_rot            = 32
0.00.086.347 I llm_load_print_meta: n_swa            = 0
0.00.086.347 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.347 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.347 I llm_load_print_meta: n_gqa            = 1
0.00.086.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.349 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.349 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.350 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.350 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.350 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.350 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.351 I llm_load_print_meta: n_ff             = 8192
0.00.086.351 I llm_load_print_meta: n_expert         = 0
0.00.086.351 I llm_load_print_meta: n_expert_used    = 0
0.00.086.352 I llm_load_print_meta: causal attn      = 1
0.00.086.352 I llm_load_print_meta: pooling type     = 0
0.00.086.352 I llm_load_print_meta: rope type        = 2
0.00.086.353 I llm_load_print_meta: rope scaling     = linear
0.00.086.353 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.353 I llm_load_print_meta: freq_scale_train = 1
0.00.086.353 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.354 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.354 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.354 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.366 I llm_load_print_meta: model type       = 1.4B
0.00.086.366 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.367 I llm_load_print_meta: model params     = 1.41 B
0.00.086.367 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.367 I llm_load_print_meta: general.name     = 1.4B
0.00.086.368 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.368 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.368 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.368 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.368 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.369 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.369 I llm_load_print_meta: max token length = 1024
0.00.088.180 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.180 I llm_load_tensors: offloading output layer to GPU
0.00.088.181 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.190 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.191 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.116 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.117 I llama_new_context_with_model: n_ctx         = 128
0.00.089.117 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.118 I llama_new_context_with_model: n_batch       = 128
0.00.089.118 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.118 I llama_new_context_with_model: flash_attn    = 0
0.00.089.118 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.119 I llama_new_context_with_model: freq_scale    = 1
0.00.089.119 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.119 I ggml_metal_init: allocating
0.00.089.123 I ggml_metal_init: found device: Apple M4
0.00.089.125 I ggml_metal_init: picking default device: Apple M4
0.00.089.726 I ggml_metal_init: using embedded metal library
0.00.091.839 I ggml_metal_init: GPU name:   Apple M4
0.00.091.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.841 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.841 I ggml_metal_init: simdgroup reduction   = true
0.00.091.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.842 I ggml_metal_init: has bfloat            = true
0.00.091.842 I ggml_metal_init: use bfloat            = true
0.00.091.842 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.418 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.100.420 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.100.435 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.337 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.338 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.338 I llama_new_context_with_model: graph nodes  = 967
0.00.101.338 I llama_new_context_with_model: graph splits = 2
0.00.101.351 I 
0.00.101.384 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.101.385 I compute_imatrix: tokenizing the input ..
0.00.108.477 I compute_imatrix: tokenization took 7.092 ms
0.00.108.479 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.741.842 I compute_imatrix: 1.63 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.744.741 I llama_perf_context_print:        load time =    1719.15 ms
0.01.744.742 I llama_perf_context_print: prompt eval time =    1632.75 ms /   128 tokens (   12.76 ms per token,    78.40 tokens per second)
0.01.744.746 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.744.746 I llama_perf_context_print:       total time =    1722.04 ms /   129 tokens
0.01.745.342 I ggml_metal_free: deallocating

real	0m1.937s
user	0m0.173s
sys	0m0.281s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4202 (9f912511)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12dc0a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12dc0a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12dc0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12dc0b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12dc0b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12dc0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12dc0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12dc0c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12dc0cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12dc0d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12dc0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12dc0dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12dc0e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12dc0f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12dc0f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12dc100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12dc107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12dc10ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12dc11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12dc11dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12dc124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12dc12c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12dc13330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12dc13bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12dc142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12dc145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12dc14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12dc15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12dc15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12dc16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12dc164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12dc16790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12dc17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12dc17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12dc17820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12dc17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12dc18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12dc18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12dc18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12dc18f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12dc193e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12dc19880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12dc19d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12dc1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12dc1a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12dc1aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12dc1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12dc1b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12dc1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12dc1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12dc1cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12dc1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12dc1d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12dc1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12dc1e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12dc1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12dc1ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12dc1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12dc1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12dc20010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12dc202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12dc20770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12dc20c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12dc210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12dc21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12dc219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12dc21e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12dc22330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12dc227d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12dc22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12dc23110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12dc235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12dc23a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12dc23ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12dc24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12dc24830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12dc24cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12dc25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12dc25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12dc25ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12dc25f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12dc263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12dc26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12dc26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dc271d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12dc27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dc27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12dc27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12dc28450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12dc288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dc28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dc29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12dc296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12dc29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12dc2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12dc2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12dc2a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12dc1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dc2afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12dc2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12dc2b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12dc2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12dc2c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12dc2c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12dc2cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12dc2d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12dc2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12dc2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12dc2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12dc2e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12dc2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12dc2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12dc2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12dc2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12dc2f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12dc2fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12dc302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12dc30780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12dc30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12dc310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12dc31560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dc31a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dc31ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12dc32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12dc327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12dc32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12dc33120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12dc335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12dc33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dc33f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12dc343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12dc34840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12dc34ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12dc35180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dc35620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12dc35ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dc35f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12dc36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12dc368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12dc36d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12dc371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12dc37680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12dc37b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12dc37fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12dc38460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12dc38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12dc38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12dc39240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12dc396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12dc39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12dc3a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12dc3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dc3a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dc3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dc3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12dc3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12dc3bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12dc3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12dc3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12dc3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12dc3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12dc3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12dc3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dc3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12dc3ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12dc3f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12dc3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12dc3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12dc40280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12dc407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12dc40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12dc41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12dc417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12dc41d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12dc42260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12dc427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12dc42d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12dc43250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12dc437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12dc43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12dc44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12dc44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12dc44ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12dc45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12dc45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dc45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12dc46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12dc46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dc46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12dc47210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12dc47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12dc47cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12dc48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12dc48750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12dc48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12dc491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12dc49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12dc49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12dc4a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12dc4a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12dc4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12dc4b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12dc4b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12dc4bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12dc4c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12dc4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12dc4cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12dc4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12dc4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dc4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dc4e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12dc4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12dc4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12dc4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12dc4f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12dc4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12dc50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12dc506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12dc50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12dc51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12dc516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12dc51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12dc52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12dc526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12dc52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12dc52ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12dc53490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12dc53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12dc53dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12dc54270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12dc54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dc54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12dc55050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dc554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12dc55990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12dc55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12dc562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12dc56820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12dc56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12dc57660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12dc57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12dc584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12dc58760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12dc58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12dc59380 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.151.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12db062d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12db06740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12db06bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12db07020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12db07490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12db07900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12db07d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12db081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12db08650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12db08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12db09010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12db09690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12db0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12db0a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12db0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12db0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12db0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12db0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12db0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12db0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12db0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12db0e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12db0eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12db0f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12db0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12db0fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12db0fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12db10350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12db107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12db10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12db110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12db115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12db11a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12db11d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12db12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12db125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12db12a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12db12ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12db13330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12db137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12db13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12db14080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12db144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12db14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12db14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12db15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12db156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12db15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12db15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12db16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12db16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12db16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12db17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12db175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12db17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12db17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12db18410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12db18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12db18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12db191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12db19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12db19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12db19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12db1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12db1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12db1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12db1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12db1b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12db1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12db1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12db1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12db1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12db1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12db1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12db1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12db1d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12db1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12db1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12db1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12db1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12db1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12db1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12db1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12db1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12db200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12db20550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12db209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12db20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12db212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12db21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12db21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12db21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12db22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12db228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12db22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12db231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12db23620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12db23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12db23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12db24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12db247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12db24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12db250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12db25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12db259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12db25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12db26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12db266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12db26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12db26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12db27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12db278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12db27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12db28190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12db28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12db28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12db28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12db29350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12db297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12db29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12db2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12db2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12db2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12db2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12db2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12db2b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12db2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12db2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12db2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12db2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12db2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12db2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12db2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12db2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12db2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12db2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12db2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12db2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12db2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12db2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12db2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12db2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12db30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12db306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12db30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12db30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12db31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12db31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12db31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12db32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12db325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12db32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12db32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12db33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12db33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12db33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12db34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12db344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12db34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12db34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12db35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12db35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12db35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12db35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12db363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12db36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12db36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12db37850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12db37b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12db37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12db38240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12db386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12db38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12db38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12db39400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12db39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12db39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12db3a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12db3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12db3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12db3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12db3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12db3b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12db3bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12db3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12db3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12db3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12db3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12db3d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12db3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12db3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12db3df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12db3e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12db3e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12db3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12db3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12db3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12db3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12db3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12db402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12db40760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12db40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12db41040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12db414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12db41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12db41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12db42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12db42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12db42ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12db42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12db433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12db43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12db43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12db44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12db44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12db449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12db44e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12db452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12db45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12db45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12db46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12db46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12db46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12db46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12db471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12db47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12db47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12db47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12db483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12db48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12db48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12db490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12db49560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12db499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12db49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12db4a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12db4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12db4ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12db4b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12db4bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12db4c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12db4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12db4cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12db4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12db4d620 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da05840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da05cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da06120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da06590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da06a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da06e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da072e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da07750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da07bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da08560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da08be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da09700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da09eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da0a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da0ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da0b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da0c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da0cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da0d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da0e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da0e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da0eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da0f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da0f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da0f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da10680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da10b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da11000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da11ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da12100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da13500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da14400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da18490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da19730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da1a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da1b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da1bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da1ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da1d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da1dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da1e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da1e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da1ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da1ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da1fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da20630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da21410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da23470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da25970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da27530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da27e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12da28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12da287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12da28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12da290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12da29590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12da29a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12da29ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12da2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12da2a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12da2acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12da2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12da2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12da2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12da2bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12da2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12da2c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12da2cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12da2d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12da2d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12da2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12da2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12da2e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12da2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12da2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12da2f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12da2f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12da2fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12da2fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12da30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12da30930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12da30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12da31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12da31710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12da31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12da32050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12da324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12da32990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12da32e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12da332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12da33770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12da33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12da340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12da34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12da34b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12da350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12da355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12da358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12da35ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12da364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12da36ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12da370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12da37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12da37ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12da38390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12da38830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12da38cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12da39480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12da399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12da39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12da3a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12da3a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12da3af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12da3b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12da3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12da3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12da3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12da3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12da3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12da3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12da3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12da3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12da3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12da3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12da3eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12da3f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12da3f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12da3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12da40410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12da40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12da40eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12da41400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12da41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12da41ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12da423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12da42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12da42e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12da433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12da43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12da43e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12da443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12da44920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12da44e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12da453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12da45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12da45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12da463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12da46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12da46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12da473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12da478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12da47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12da48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12da488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12da48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12da49380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12da498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12da49e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12da4a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12da4a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12da4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12da4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12da4b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12da4be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12da4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12da4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12da4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12da4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12da4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12da4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12da4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12da4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12da4e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12da4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12da4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12da4f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12da4fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12da4ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12da50690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12da50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12da514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12da51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12da51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12da524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12da52ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.853s
user	0m0.291s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4202 (9f912511)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15870af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15870b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15870bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15870c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15870c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15870cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15870d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15870d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15870de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15870e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15870e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15870ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15870f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158710040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158710850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158710f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158711690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158711db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1587124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158712ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1587133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158714200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158714aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1587151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158715a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158716700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158716c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158716f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1587173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158717660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158717ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1587186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158718b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158719030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1587194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158719e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15871a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15871a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15871abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15871b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15871b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15871b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15871bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15871c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15871cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15871d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15871dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15871e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15871e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15871ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15871f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15871f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15871fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1587200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1587206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158720ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1587211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158721640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158721ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158721f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1587228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158723200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1587236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158723b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158723fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158724480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158724920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158724dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158725260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158725700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158725ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158726040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1587264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158726980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158726e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1587272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158727760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158727c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1587280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158728540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1587289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158728e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158729320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1587297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15872a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15872a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15872aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15872aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15872b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15872b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15871c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15872be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15872c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15872c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15872cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15872d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15872d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15872da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15872ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15872e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15872e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15872ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15872f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15872f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15872fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15872ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1587303d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158730870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158730d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1587311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158731650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158731af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158731f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158732430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1587328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158732d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158733210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1587336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158733b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158734490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158734930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158735710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158735bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158736050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1587364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158736990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158736e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1587372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158737c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1587380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1587389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158738e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158739330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1587397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158739c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15873a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15873a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15873aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15873aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15873b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15873b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15873bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15873c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15873c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15873cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15873d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15873d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15873dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15873e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15873e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15873ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15873f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15873fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15873ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158740450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158740c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158741150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1587416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158741bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158742be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158743130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158743680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158743bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158744670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158744bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158745110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158745660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158745bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158746100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158746650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158746ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1587470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158747640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1587480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158748630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158748b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1587490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158749b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15874a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15874a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15874ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15874b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15874b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15874bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15874c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15874c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15874cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15874d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15874d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15874db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15874e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15874e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15874eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15874f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15874f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15874fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158750060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1587505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158750b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158751050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1587515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158751af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158752040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158752590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158752ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158753030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158753580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158753a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158753ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158754360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158754800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158754ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158755140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1587555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158755a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158755f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1587563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158756860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158756d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1587571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1587576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158757e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158758530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158758c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158759370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158759630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158759c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15875a250 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.095.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158606770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158606be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158607050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1586074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158607930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158607da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158608210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158608680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158608af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158609020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158609490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158609b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15860a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15860ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15860b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15860bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15860c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15860cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15860d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15860da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15860e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15860e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15860efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15860f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15860fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1586100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158610360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1586107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158610c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1586110b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158611520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158611a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158611ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158612180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1586125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158612a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158612ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158613340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1586137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158613c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158614090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158614500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158614970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158614de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158615250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1586156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158615b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158615fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158616880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158616cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1586175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158617a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158617eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158618890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158619200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15861a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15861a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15861aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15861b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15861b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15861b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15861be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15861c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15861c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15861cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15861d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15861d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15861d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15861dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15861e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15861e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15861eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15861ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15861f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15861f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15861fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1586200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158620560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1586209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158620e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1586212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158621720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158621b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158622470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1586228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158622d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1586231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158623630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158623aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158623f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1586247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1586250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158625540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1586259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158626290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158626700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158626b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158626fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158627450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1586278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158627d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1586281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1586297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158629c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15862a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15862a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15862a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15862ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15862b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15862b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15862bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15862bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15862c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15862c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15862cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15862d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15862d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15862da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15862ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15862e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15862e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15862ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15862f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15862f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15862f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15862fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1586306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158630b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158630fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158631410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158631880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158631cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158632160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1586325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158632a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158632eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158633320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158633790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158633c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1586344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158634950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158634dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158635230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1586356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158635f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1586363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158636860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158636cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158637140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158637cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158637f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158638250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1586386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158639880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158639cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15863a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15863a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15863aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15863aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15863b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15863b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15863bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15863c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15863c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15863c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15863cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15863d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15863d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15863db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15863df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15863e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15863e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15863ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15863f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15863f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15863fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15863fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158640300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158640770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158640be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158641050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1586414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158641da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158642210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158642680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158642af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158642f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1586433d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158643840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158643cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158644120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158644a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158644e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1586452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158645750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158645bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158646030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1586464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158646910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158646d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1586471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1586483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158648c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158649100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158649570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1586499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158649e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15864a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15864a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15864aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15864b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15864bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15864c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15864c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15864d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15864d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15864d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15864daa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bb046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bb04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bb04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bb05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bb058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bb05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bb06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bb065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bb06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bb06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bb07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bb07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bb08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bb08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bb09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bb09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bb0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bb0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bb0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bb0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bb0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bb0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bb0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bb0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bb0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bb0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bb0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bb0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bb0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bb0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bb0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bb0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bb0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bb10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bb104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bb10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bb10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bb111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bb11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bb11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bb11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bb123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bb12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bb12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bb13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bb13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bb139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bb13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bb142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bb14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bb14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bb15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bb15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bb158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bb15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bb161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bb16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bb16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bb170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bb17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bb17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bb17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bb18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bb186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bb18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bb18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bb19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bb198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bb19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bb1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bb1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bb1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bb1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bb1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bb1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bb1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bb1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bb1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bb1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bb1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bb1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bb1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bb1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bb1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bb1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bb1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bb1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bb1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bb1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bb1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bb1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bb20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bb20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bb20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bb21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bb214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bb21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bb21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bb22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bb226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bb22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bb22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bb233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bb23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bb23cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bb24140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bb245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bb24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bb24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bb25300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bb25770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bb25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bb26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bb264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bb26930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bb26da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bb27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bb27680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bb27af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bb27f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bb283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bb28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bb28cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bb29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bb29590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bb29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bb29e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bb2a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bb2a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bb2abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bb2b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bb2b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bb2b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bb2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bb2c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bb2c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bb2cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bb2cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bb2d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bb2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bb2dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bb2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bb2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bb2e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bb2ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bb2f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bb2f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bb2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bb30010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bb30480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bb308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bb30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bb311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bb31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bb31ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bb31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bb32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bb32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bb32c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bb330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bb33550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bb339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bb33e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bb342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bb34710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bb34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bb34ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bb35b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bb35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bb36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bb36570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bb369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bb36e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bb372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bb37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bb37ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bb38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bb38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bb388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bb38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bb391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bb39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bb39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bb39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bb3a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bb3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bb3ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bb3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bb3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bb3b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bb3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bb3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bb3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bb3cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bb3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bb3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bb3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bb3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bb3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bb3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bb3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bb3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bb3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bb3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bb3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bb400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bb40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bb409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bb40e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bb41280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bb416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bb41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bb41fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bb42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bb428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bb42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bb43190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bb43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bb43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bb43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bb44350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bb447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bb44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bb450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bb45510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bb45980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bb45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bb46260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bb466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bb46b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bb46fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bb47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bb47890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bb47d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bb48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bb485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bb48a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bb48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bb49a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bb4a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bb4a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bb4af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bb4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bb4b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bb4b950 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.936s
user	0m0.241s
sys	0m0.139s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
