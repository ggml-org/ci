### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.31 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.65 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.47 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.86 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.94 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  194.05 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.79 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 254.55 sec*proc (29 tests)

Total Test time (real) = 254.56 sec

real	4m14.874s
user	8m37.978s
sys	0m7.142s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.85 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.35 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.48 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.84 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.82 sec*proc (29 tests)

Total Test time (real) =  54.83 sec

real	0m54.840s
user	1m16.991s
sys	0m6.343s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.194 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.442 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.088 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.092 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.094 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.030.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.095 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.030.095 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.030.098 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.030.099 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.030.101 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.030.101 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.030.105 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.030.106 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.030.108 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.108 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.109 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.030.109 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.030.109 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.110 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.030.110 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.707 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.708 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.708 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.709 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.709 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.709 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.710 I llama_model_loader: - type  f32:  124 tensors
0.00.033.710 I llama_model_loader: - type  f16:   73 tensors
0.00.033.711 I print_info: file format = GGUF V3 (latest)
0.00.033.711 I print_info: file type   = F16
0.00.033.712 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.036.684 I load: special tokens cache size = 5
0.00.038.180 I load: token to piece cache size = 0.2032 MB
0.00.038.183 I print_info: arch             = bert
0.00.038.183 I print_info: vocab_only       = 0
0.00.038.184 I print_info: n_ctx_train      = 512
0.00.038.184 I print_info: n_embd           = 384
0.00.038.184 I print_info: n_layer          = 12
0.00.038.187 I print_info: n_head           = 12
0.00.038.188 I print_info: n_head_kv        = 12
0.00.038.188 I print_info: n_rot            = 32
0.00.038.188 I print_info: n_swa            = 0
0.00.038.188 I print_info: n_embd_head_k    = 32
0.00.038.190 I print_info: n_embd_head_v    = 32
0.00.038.190 I print_info: n_gqa            = 1
0.00.038.191 I print_info: n_embd_k_gqa     = 384
0.00.038.191 I print_info: n_embd_v_gqa     = 384
0.00.038.192 I print_info: f_norm_eps       = 1.0e-12
0.00.038.193 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.193 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.194 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.194 I print_info: f_logit_scale    = 0.0e+00
0.00.038.194 I print_info: n_ff             = 1536
0.00.038.194 I print_info: n_expert         = 0
0.00.038.195 I print_info: n_expert_used    = 0
0.00.038.196 I print_info: causal attn      = 0
0.00.038.196 I print_info: pooling type     = 2
0.00.038.196 I print_info: rope type        = 2
0.00.038.197 I print_info: rope scaling     = linear
0.00.038.197 I print_info: freq_base_train  = 10000.0
0.00.038.197 I print_info: freq_scale_train = 1
0.00.038.197 I print_info: n_ctx_orig_yarn  = 512
0.00.038.198 I print_info: rope_finetuned   = unknown
0.00.038.198 I print_info: ssm_d_conv       = 0
0.00.038.198 I print_info: ssm_d_inner      = 0
0.00.038.198 I print_info: ssm_d_state      = 0
0.00.038.198 I print_info: ssm_dt_rank      = 0
0.00.038.199 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.199 I print_info: model type       = 33M
0.00.038.199 I print_info: model params     = 33.21 M
0.00.038.199 I print_info: general.name     = Bge Small
0.00.038.200 I print_info: vocab type       = WPM
0.00.038.200 I print_info: n_vocab          = 30522
0.00.038.200 I print_info: n_merges         = 0
0.00.038.200 I print_info: BOS token        = 101 '[CLS]'
0.00.038.200 I print_info: UNK token        = 100 '[UNK]'
0.00.038.201 I print_info: SEP token        = 102 '[SEP]'
0.00.038.201 I print_info: PAD token        = 0 '[PAD]'
0.00.038.201 I print_info: MASK token       = 103 '[MASK]'
0.00.038.201 I print_info: LF token         = 0 '[PAD]'
0.00.038.202 I print_info: max token length = 21
0.00.038.202 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.040.409 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.410 I load_tensors: offloading output layer to GPU
0.00.040.411 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.429 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.431 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.620 I llama_init_from_model: n_seq_max     = 1
0.00.040.621 I llama_init_from_model: n_ctx         = 512
0.00.040.621 I llama_init_from_model: n_ctx_per_seq = 512
0.00.040.621 I llama_init_from_model: n_batch       = 2048
0.00.040.621 I llama_init_from_model: n_ubatch      = 2048
0.00.040.622 I llama_init_from_model: flash_attn    = 0
0.00.040.622 I llama_init_from_model: freq_base     = 10000.0
0.00.040.622 I llama_init_from_model: freq_scale    = 1
0.00.040.623 I ggml_metal_init: allocating
0.00.040.635 I ggml_metal_init: found device: Apple M4
0.00.040.644 I ggml_metal_init: picking default device: Apple M4
0.00.041.243 I ggml_metal_init: using embedded metal library
0.00.044.098 I ggml_metal_init: GPU name:   Apple M4
0.00.044.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.101 I ggml_metal_init: simdgroup reduction   = true
0.00.044.101 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.102 I ggml_metal_init: has residency sets    = true
0.00.044.102 I ggml_metal_init: has bfloat            = true
0.00.044.102 I ggml_metal_init: use bfloat            = true
0.00.044.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.103 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.836 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.445 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.448 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.470 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.055.526 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.055.527 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.055.534 I llama_init_from_model: graph nodes  = 429
0.00.055.535 I llama_init_from_model: graph splits = 2
0.00.055.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.055.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.004 I 
0.00.060.021 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.572 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.896 I llama_perf_context_print:        load time =      34.54 ms
0.00.064.897 I llama_perf_context_print: prompt eval time =       4.19 ms /     9 tokens (    0.47 ms per token,  2149.00 tokens per second)
0.00.064.898 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.898 I llama_perf_context_print:       total time =       4.89 ms /    10 tokens
0.00.065.098 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.042s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.337 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.342 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.343 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.346 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.346 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.347 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.348 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.348 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.348 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.349 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.351 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.352 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.352 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.352 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.353 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.353 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.541 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.156 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.157 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.157 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.158 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.158 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.158 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.159 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.159 I llama_model_loader: - type  f32:  124 tensors
0.00.014.159 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.160 I print_info: file format = GGUF V3 (latest)
0.00.014.161 I print_info: file type   = Q8_0
0.00.014.162 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.392 I load: special tokens cache size = 5
0.00.017.591 I load: token to piece cache size = 0.2032 MB
0.00.017.594 I print_info: arch             = bert
0.00.017.594 I print_info: vocab_only       = 0
0.00.017.595 I print_info: n_ctx_train      = 512
0.00.017.595 I print_info: n_embd           = 384
0.00.017.595 I print_info: n_layer          = 12
0.00.017.598 I print_info: n_head           = 12
0.00.017.599 I print_info: n_head_kv        = 12
0.00.017.599 I print_info: n_rot            = 32
0.00.017.600 I print_info: n_swa            = 0
0.00.017.601 I print_info: n_embd_head_k    = 32
0.00.017.603 I print_info: n_embd_head_v    = 32
0.00.017.603 I print_info: n_gqa            = 1
0.00.017.604 I print_info: n_embd_k_gqa     = 384
0.00.017.605 I print_info: n_embd_v_gqa     = 384
0.00.017.605 I print_info: f_norm_eps       = 1.0e-12
0.00.017.606 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.622 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.622 I print_info: f_logit_scale    = 0.0e+00
0.00.017.624 I print_info: n_ff             = 1536
0.00.017.625 I print_info: n_expert         = 0
0.00.017.625 I print_info: n_expert_used    = 0
0.00.017.625 I print_info: causal attn      = 0
0.00.017.625 I print_info: pooling type     = 2
0.00.017.625 I print_info: rope type        = 2
0.00.017.625 I print_info: rope scaling     = linear
0.00.017.626 I print_info: freq_base_train  = 10000.0
0.00.017.626 I print_info: freq_scale_train = 1
0.00.017.626 I print_info: n_ctx_orig_yarn  = 512
0.00.017.626 I print_info: rope_finetuned   = unknown
0.00.017.626 I print_info: ssm_d_conv       = 0
0.00.017.626 I print_info: ssm_d_inner      = 0
0.00.017.627 I print_info: ssm_d_state      = 0
0.00.017.627 I print_info: ssm_dt_rank      = 0
0.00.017.627 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.627 I print_info: model type       = 33M
0.00.017.628 I print_info: model params     = 33.21 M
0.00.017.628 I print_info: general.name     = Bge Small
0.00.017.628 I print_info: vocab type       = WPM
0.00.017.628 I print_info: n_vocab          = 30522
0.00.017.629 I print_info: n_merges         = 0
0.00.017.629 I print_info: BOS token        = 101 '[CLS]'
0.00.017.629 I print_info: UNK token        = 100 '[UNK]'
0.00.017.629 I print_info: SEP token        = 102 '[SEP]'
0.00.017.629 I print_info: PAD token        = 0 '[PAD]'
0.00.017.629 I print_info: MASK token       = 103 '[MASK]'
0.00.017.630 I print_info: LF token         = 0 '[PAD]'
0.00.017.630 I print_info: max token length = 21
0.00.017.630 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.294 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.295 I load_tensors: offloading output layer to GPU
0.00.019.296 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.302 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.302 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.471 I llama_init_from_model: n_seq_max     = 1
0.00.019.472 I llama_init_from_model: n_ctx         = 512
0.00.019.472 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.472 I llama_init_from_model: n_batch       = 2048
0.00.019.472 I llama_init_from_model: n_ubatch      = 2048
0.00.019.473 I llama_init_from_model: flash_attn    = 0
0.00.019.473 I llama_init_from_model: freq_base     = 10000.0
0.00.019.473 I llama_init_from_model: freq_scale    = 1
0.00.019.474 I ggml_metal_init: allocating
0.00.019.477 I ggml_metal_init: found device: Apple M4
0.00.019.481 I ggml_metal_init: picking default device: Apple M4
0.00.020.009 I ggml_metal_init: using embedded metal library
0.00.022.331 I ggml_metal_init: GPU name:   Apple M4
0.00.022.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.334 I ggml_metal_init: simdgroup reduction   = true
0.00.022.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.334 I ggml_metal_init: has residency sets    = true
0.00.022.334 I ggml_metal_init: has bfloat            = true
0.00.022.335 I ggml_metal_init: use bfloat            = true
0.00.022.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.460 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.058 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.060 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.074 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.044 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.045 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.045 I llama_init_from_model: graph nodes  = 429
0.00.034.046 I llama_init_from_model: graph splits = 2
0.00.034.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.111 I 
0.00.038.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.665 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.044 I llama_perf_context_print:        load time =      29.09 ms
0.00.043.045 I llama_perf_context_print: prompt eval time =       4.25 ms /     9 tokens (    0.47 ms per token,  2116.15 tokens per second)
0.00.043.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.046 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.043.260 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.279 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.488 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.062 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.067 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.070 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.070 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.071 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.072 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.072 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.074 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.075 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.075 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.076 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.077 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.080 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.080 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.081 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.082 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.082 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.692 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.770 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.237 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.238 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.238 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.239 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.239 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.239 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.240 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.240 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.241 I llama_model_loader: - type  f32:   40 tensors
0.00.050.241 I llama_model_loader: - type  f16:   30 tensors
0.00.050.242 I print_info: file format = GGUF V3 (latest)
0.00.050.242 I print_info: file type   = F16
0.00.050.243 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.424 W load: empty token at index 5
0.00.059.675 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.272 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.306 I load: special tokens cache size = 5
0.00.322.551 I load: token to piece cache size = 1.5060 MB
0.00.322.558 I print_info: arch             = jina-bert-v2
0.00.322.558 I print_info: vocab_only       = 0
0.00.322.558 I print_info: n_ctx_train      = 8192
0.00.322.559 I print_info: n_embd           = 384
0.00.322.559 I print_info: n_layer          = 4
0.00.322.566 I print_info: n_head           = 12
0.00.322.566 I print_info: n_head_kv        = 12
0.00.322.567 I print_info: n_rot            = 32
0.00.322.567 I print_info: n_swa            = 0
0.00.322.567 I print_info: n_embd_head_k    = 32
0.00.322.570 I print_info: n_embd_head_v    = 32
0.00.322.573 I print_info: n_gqa            = 1
0.00.322.573 I print_info: n_embd_k_gqa     = 384
0.00.322.574 I print_info: n_embd_v_gqa     = 384
0.00.322.575 I print_info: f_norm_eps       = 1.0e-12
0.00.322.575 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.576 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.576 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.576 I print_info: f_logit_scale    = 0.0e+00
0.00.322.577 I print_info: n_ff             = 1536
0.00.322.577 I print_info: n_expert         = 0
0.00.322.577 I print_info: n_expert_used    = 0
0.00.322.577 I print_info: causal attn      = 0
0.00.322.578 I print_info: pooling type     = -1
0.00.322.578 I print_info: rope type        = -1
0.00.322.578 I print_info: rope scaling     = linear
0.00.322.578 I print_info: freq_base_train  = 10000.0
0.00.322.579 I print_info: freq_scale_train = 1
0.00.322.579 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.579 I print_info: rope_finetuned   = unknown
0.00.322.580 I print_info: ssm_d_conv       = 0
0.00.322.580 I print_info: ssm_d_inner      = 0
0.00.322.580 I print_info: ssm_d_state      = 0
0.00.322.580 I print_info: ssm_dt_rank      = 0
0.00.322.580 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.581 I print_info: model type       = 33M
0.00.322.581 I print_info: model params     = 32.90 M
0.00.322.581 I print_info: general.name     = Jina Bert Implementation
0.00.322.583 I print_info: vocab type       = BPE
0.00.322.583 I print_info: n_vocab          = 61056
0.00.322.583 I print_info: n_merges         = 39382
0.00.322.583 I print_info: BOS token        = 0 '<s>'
0.00.322.583 I print_info: EOS token        = 2 '</s>'
0.00.322.584 I print_info: UNK token        = 3 '<unk>'
0.00.322.584 I print_info: SEP token        = 2 '</s>'
0.00.322.584 I print_info: PAD token        = 1 '<pad>'
0.00.322.590 I print_info: MASK token       = 4 '<mask>'
0.00.322.590 I print_info: EOG token        = 2 '</s>'
0.00.322.590 I print_info: max token length = 45
0.00.322.591 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.519 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.520 I load_tensors: offloading output layer to GPU
0.00.324.520 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.543 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.544 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.868 I llama_init_from_model: n_seq_max     = 1
0.00.324.869 I llama_init_from_model: n_ctx         = 8192
0.00.324.869 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.870 I llama_init_from_model: n_batch       = 2048
0.00.324.870 I llama_init_from_model: n_ubatch      = 2048
0.00.324.870 I llama_init_from_model: flash_attn    = 0
0.00.324.871 I llama_init_from_model: freq_base     = 10000.0
0.00.324.871 I llama_init_from_model: freq_scale    = 1
0.00.324.872 I ggml_metal_init: allocating
0.00.324.876 I ggml_metal_init: found device: Apple M4
0.00.324.880 I ggml_metal_init: picking default device: Apple M4
0.00.325.816 I ggml_metal_init: using embedded metal library
0.00.328.676 I ggml_metal_init: GPU name:   Apple M4
0.00.328.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.679 I ggml_metal_init: simdgroup reduction   = true
0.00.328.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.679 I ggml_metal_init: has residency sets    = true
0.00.328.679 I ggml_metal_init: has bfloat            = true
0.00.328.679 I ggml_metal_init: use bfloat            = true
0.00.328.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.338.198 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.341.380 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.341.382 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.402 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.646 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.648 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.648 I llama_init_from_model: graph nodes  = 154
0.00.347.648 I llama_init_from_model: graph splits = 2
0.00.347.650 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.271 I 
0.00.358.291 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.393 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.394 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.397 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.397 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.400 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.400 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.953 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.534 I llama_perf_context_print:        load time =     334.75 ms
0.00.362.535 I llama_perf_context_print: prompt eval time =       3.57 ms /    62 tokens (    0.06 ms per token, 17357.22 tokens per second)
0.00.362.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.536 I llama_perf_context_print:       total time =       4.26 ms /    63 tokens
0.00.362.789 I ggml_metal_free: deallocating

real	0m1.062s
user	0m0.342s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.167 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.312 I main: llama backend init
0.00.000.318 I main: load the model and apply lora adapter, if any
0.00.066.301 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.079.010 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.079.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.079.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.079.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.079.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.079.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.079.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.079.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.079.048 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.079.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.079.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.079.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.079.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.079.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.079.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.079.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.079.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.086.090 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.088.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.095.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.095.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.095.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.095.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.095.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.095.689 I llama_model_loader: - type  f32:  194 tensors
0.00.095.689 I llama_model_loader: - type  f16:   98 tensors
0.00.095.691 I print_info: file format = GGUF V3 (latest)
0.00.095.693 I print_info: file type   = all F32 (guessed)
0.00.095.696 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.113.605 I load: special tokens cache size = 25
0.00.123.582 I load: token to piece cache size = 0.2984 MB
0.00.123.586 I print_info: arch             = gptneox
0.00.123.586 I print_info: vocab_only       = 0
0.00.123.587 I print_info: n_ctx_train      = 2048
0.00.123.587 I print_info: n_embd           = 2048
0.00.123.587 I print_info: n_layer          = 24
0.00.123.591 I print_info: n_head           = 16
0.00.123.592 I print_info: n_head_kv        = 16
0.00.123.593 I print_info: n_rot            = 32
0.00.123.593 I print_info: n_swa            = 0
0.00.123.593 I print_info: n_embd_head_k    = 128
0.00.123.593 I print_info: n_embd_head_v    = 128
0.00.123.594 I print_info: n_gqa            = 1
0.00.123.595 I print_info: n_embd_k_gqa     = 2048
0.00.123.596 I print_info: n_embd_v_gqa     = 2048
0.00.123.597 I print_info: f_norm_eps       = 1.0e-05
0.00.123.597 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.123.597 I print_info: f_clamp_kqv      = 0.0e+00
0.00.123.597 I print_info: f_max_alibi_bias = 0.0e+00
0.00.123.598 I print_info: f_logit_scale    = 0.0e+00
0.00.123.598 I print_info: n_ff             = 8192
0.00.123.599 I print_info: n_expert         = 0
0.00.123.599 I print_info: n_expert_used    = 0
0.00.123.599 I print_info: causal attn      = 1
0.00.123.599 I print_info: pooling type     = 0
0.00.123.603 I print_info: rope type        = 2
0.00.123.604 I print_info: rope scaling     = linear
0.00.123.604 I print_info: freq_base_train  = 10000.0
0.00.123.604 I print_info: freq_scale_train = 1
0.00.123.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.123.606 I print_info: rope_finetuned   = unknown
0.00.123.607 I print_info: ssm_d_conv       = 0
0.00.123.607 I print_info: ssm_d_inner      = 0
0.00.123.607 I print_info: ssm_d_state      = 0
0.00.123.607 I print_info: ssm_dt_rank      = 0
0.00.123.607 I print_info: ssm_dt_b_c_rms   = 0
0.00.123.608 I print_info: model type       = 1.4B
0.00.123.608 I print_info: model params     = 1.41 B
0.00.123.608 I print_info: general.name     = 1.4B
0.00.123.609 I print_info: vocab type       = BPE
0.00.123.609 I print_info: n_vocab          = 50304
0.00.123.609 I print_info: n_merges         = 50009
0.00.123.609 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.123.614 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.123.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.123.615 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.123.615 I print_info: LF token         = 187 ''
0.00.123.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.123.616 I print_info: max token length = 1024
0.00.123.616 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.194.111 I load_tensors: offloading 24 repeating layers to GPU
0.00.194.115 I load_tensors: offloading output layer to GPU
0.00.194.115 I load_tensors: offloaded 25/25 layers to GPU
0.00.194.142 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.194.144 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.194.758 I llama_init_from_model: n_seq_max     = 1
0.00.194.759 I llama_init_from_model: n_ctx         = 2048
0.00.194.759 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.194.760 I llama_init_from_model: n_batch       = 2048
0.00.194.760 I llama_init_from_model: n_ubatch      = 512
0.00.194.760 I llama_init_from_model: flash_attn    = 0
0.00.194.761 I llama_init_from_model: freq_base     = 10000.0
0.00.194.761 I llama_init_from_model: freq_scale    = 1
0.00.194.762 I ggml_metal_init: allocating
0.00.194.806 I ggml_metal_init: found device: Apple M4
0.00.194.814 I ggml_metal_init: picking default device: Apple M4
0.00.195.534 I ggml_metal_init: using embedded metal library
0.00.205.426 I ggml_metal_init: GPU name:   Apple M4
0.00.205.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.205.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.205.428 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.205.429 I ggml_metal_init: simdgroup reduction   = true
0.00.205.429 I ggml_metal_init: simdgroup matrix mul. = true
0.00.205.429 I ggml_metal_init: has residency sets    = true
0.00.205.429 I ggml_metal_init: has bfloat            = true
0.00.205.429 I ggml_metal_init: use bfloat            = true
0.00.205.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.205.430 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.232.438 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.262.364 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.262.370 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.262.415 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.266.547 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.266.549 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.266.549 I llama_init_from_model: graph nodes  = 967
0.00.266.550 I llama_init_from_model: graph splits = 2
0.00.266.556 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.266.681 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.266.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.331.545 I main: llama threadpool init, n_threads = 4
0.00.331.588 I 
0.00.331.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.331.605 I 
0.00.331.777 I sampler seed: 1234
0.00.331.781 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.331.805 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.331.807 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.331.807 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.162.507 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.02.162.508 I llama_perf_context_print:        load time =     264.35 ms
0.02.162.509 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.18 tokens per second)
0.02.162.509 I llama_perf_context_print:        eval time =    1784.03 ms /    63 runs   (   28.32 ms per token,    35.31 tokens per second)
0.02.162.510 I llama_perf_context_print:       total time =    1831.83 ms /    70 tokens
0.02.162.702 I ggml_metal_free: deallocating

real	0m2.806s
user	0m0.134s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.558 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.933 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.941 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.950 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.951 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.952 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.956 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.667 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.241 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.243 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.244 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.244 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.245 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.245 I llama_model_loader: - type  f32:  194 tensors
0.00.058.246 I llama_model_loader: - type  f16:   98 tensors
0.00.058.246 I print_info: file format = GGUF V3 (latest)
0.00.058.248 I print_info: file type   = all F32 (guessed)
0.00.058.250 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.629 I load: special tokens cache size = 25
0.00.078.846 I load: token to piece cache size = 0.2984 MB
0.00.078.849 I print_info: arch             = gptneox
0.00.078.850 I print_info: vocab_only       = 0
0.00.078.850 I print_info: n_ctx_train      = 2048
0.00.078.850 I print_info: n_embd           = 2048
0.00.078.850 I print_info: n_layer          = 24
0.00.078.854 I print_info: n_head           = 16
0.00.078.855 I print_info: n_head_kv        = 16
0.00.078.855 I print_info: n_rot            = 32
0.00.078.855 I print_info: n_swa            = 0
0.00.078.855 I print_info: n_embd_head_k    = 128
0.00.078.855 I print_info: n_embd_head_v    = 128
0.00.078.856 I print_info: n_gqa            = 1
0.00.078.857 I print_info: n_embd_k_gqa     = 2048
0.00.078.858 I print_info: n_embd_v_gqa     = 2048
0.00.078.858 I print_info: f_norm_eps       = 1.0e-05
0.00.078.859 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.859 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.859 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.859 I print_info: f_logit_scale    = 0.0e+00
0.00.078.860 I print_info: n_ff             = 8192
0.00.078.860 I print_info: n_expert         = 0
0.00.078.860 I print_info: n_expert_used    = 0
0.00.078.860 I print_info: causal attn      = 1
0.00.078.861 I print_info: pooling type     = 0
0.00.078.861 I print_info: rope type        = 2
0.00.078.861 I print_info: rope scaling     = linear
0.00.078.863 I print_info: freq_base_train  = 10000.0
0.00.078.864 I print_info: freq_scale_train = 1
0.00.078.864 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.864 I print_info: rope_finetuned   = unknown
0.00.078.864 I print_info: ssm_d_conv       = 0
0.00.078.864 I print_info: ssm_d_inner      = 0
0.00.078.865 I print_info: ssm_d_state      = 0
0.00.078.865 I print_info: ssm_dt_rank      = 0
0.00.078.865 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.865 I print_info: model type       = 1.4B
0.00.078.866 I print_info: model params     = 1.41 B
0.00.078.866 I print_info: general.name     = 1.4B
0.00.078.866 I print_info: vocab type       = BPE
0.00.078.866 I print_info: n_vocab          = 50304
0.00.078.867 I print_info: n_merges         = 50009
0.00.078.867 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.868 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.868 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.870 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.870 I print_info: LF token         = 187 ''
0.00.078.870 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.870 I print_info: max token length = 1024
0.00.078.871 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.467.018 I load_tensors: offloading 24 repeating layers to GPU
0.01.467.023 I load_tensors: offloading output layer to GPU
0.01.467.023 I load_tensors: offloaded 25/25 layers to GPU
0.01.467.055 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.467.057 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.467.807 I llama_init_from_model: n_seq_max     = 1
0.01.467.808 I llama_init_from_model: n_ctx         = 128
0.01.467.808 I llama_init_from_model: n_ctx_per_seq = 128
0.01.467.809 I llama_init_from_model: n_batch       = 128
0.01.467.809 I llama_init_from_model: n_ubatch      = 128
0.01.467.809 I llama_init_from_model: flash_attn    = 0
0.01.467.810 I llama_init_from_model: freq_base     = 10000.0
0.01.467.810 I llama_init_from_model: freq_scale    = 1
0.01.467.811 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.467.814 I ggml_metal_init: allocating
0.01.467.890 I ggml_metal_init: found device: Apple M4
0.01.467.896 I ggml_metal_init: picking default device: Apple M4
0.01.469.048 I ggml_metal_init: using embedded metal library
0.01.472.841 I ggml_metal_init: GPU name:   Apple M4
0.01.472.843 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.472.844 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.472.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.472.844 I ggml_metal_init: simdgroup reduction   = true
0.01.472.844 I ggml_metal_init: simdgroup matrix mul. = true
0.01.472.845 I ggml_metal_init: has residency sets    = true
0.01.472.845 I ggml_metal_init: has bfloat            = true
0.01.472.845 I ggml_metal_init: use bfloat            = true
0.01.472.846 I ggml_metal_init: hasUnifiedMemory      = true
0.01.472.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.483.934 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.485.704 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.485.708 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.485.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.487.495 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.487.496 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.487.496 I llama_init_from_model: graph nodes  = 967
0.01.487.497 I llama_init_from_model: graph splits = 2
0.01.487.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.487.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.522.544 I 
0.01.522.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.522.594 I perplexity: tokenizing the input ..
0.01.527.714 I perplexity: tokenization took 5.118 ms
0.01.527.719 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.646.272 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.647.612 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.647.648 I llama_perf_context_print:        load time =    1496.67 ms
0.01.647.649 I llama_perf_context_print: prompt eval time =     118.25 ms /   128 tokens (    0.92 ms per token,  1082.48 tokens per second)
0.01.647.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.647.650 I llama_perf_context_print:       total time =     125.10 ms /   129 tokens
0.01.648.021 I ggml_metal_free: deallocating

real	0m1.832s
user	0m0.099s
sys	0m0.282s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.673 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.675 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.677 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.678 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.678 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.678 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.679 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.681 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.897 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.509 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.510 I llama_model_loader: - type  f32:  194 tensors
0.00.040.511 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.511 I print_info: file format = GGUF V3 (latest)
0.00.040.512 I print_info: file type   = Q8_0
0.00.040.513 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.050.416 I load: special tokens cache size = 25
0.00.058.282 I load: token to piece cache size = 0.2984 MB
0.00.058.287 I print_info: arch             = gptneox
0.00.058.288 I print_info: vocab_only       = 0
0.00.058.288 I print_info: n_ctx_train      = 2048
0.00.058.292 I print_info: n_embd           = 2048
0.00.058.292 I print_info: n_layer          = 24
0.00.058.297 I print_info: n_head           = 16
0.00.058.298 I print_info: n_head_kv        = 16
0.00.058.298 I print_info: n_rot            = 32
0.00.058.298 I print_info: n_swa            = 0
0.00.058.298 I print_info: n_embd_head_k    = 128
0.00.058.298 I print_info: n_embd_head_v    = 128
0.00.058.299 I print_info: n_gqa            = 1
0.00.058.300 I print_info: n_embd_k_gqa     = 2048
0.00.058.301 I print_info: n_embd_v_gqa     = 2048
0.00.058.301 I print_info: f_norm_eps       = 1.0e-05
0.00.058.302 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.302 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.302 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.302 I print_info: f_logit_scale    = 0.0e+00
0.00.058.303 I print_info: n_ff             = 8192
0.00.058.303 I print_info: n_expert         = 0
0.00.058.304 I print_info: n_expert_used    = 0
0.00.058.304 I print_info: causal attn      = 1
0.00.058.304 I print_info: pooling type     = 0
0.00.058.304 I print_info: rope type        = 2
0.00.058.305 I print_info: rope scaling     = linear
0.00.058.306 I print_info: freq_base_train  = 10000.0
0.00.058.306 I print_info: freq_scale_train = 1
0.00.058.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.306 I print_info: rope_finetuned   = unknown
0.00.058.307 I print_info: ssm_d_conv       = 0
0.00.058.307 I print_info: ssm_d_inner      = 0
0.00.058.307 I print_info: ssm_d_state      = 0
0.00.058.307 I print_info: ssm_dt_rank      = 0
0.00.058.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.308 I print_info: model type       = 1.4B
0.00.058.308 I print_info: model params     = 1.41 B
0.00.058.308 I print_info: general.name     = 1.4B
0.00.058.309 I print_info: vocab type       = BPE
0.00.058.309 I print_info: n_vocab          = 50304
0.00.058.310 I print_info: n_merges         = 50009
0.00.058.310 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.311 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.311 I print_info: LF token         = 187 ''
0.00.058.311 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.311 I print_info: max token length = 1024
0.00.058.314 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.179.527 I load_tensors: offloading 24 repeating layers to GPU
0.01.179.533 I load_tensors: offloading output layer to GPU
0.01.179.534 I load_tensors: offloaded 25/25 layers to GPU
0.01.179.559 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.179.561 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.180.411 I llama_init_from_model: n_seq_max     = 1
0.01.180.412 I llama_init_from_model: n_ctx         = 2048
0.01.180.413 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.180.413 I llama_init_from_model: n_batch       = 2048
0.01.180.414 I llama_init_from_model: n_ubatch      = 512
0.01.180.414 I llama_init_from_model: flash_attn    = 0
0.01.180.415 I llama_init_from_model: freq_base     = 10000.0
0.01.180.415 I llama_init_from_model: freq_scale    = 1
0.01.180.416 I ggml_metal_init: allocating
0.01.180.425 I ggml_metal_init: found device: Apple M4
0.01.180.433 I ggml_metal_init: picking default device: Apple M4
0.01.181.828 I ggml_metal_init: using embedded metal library
0.01.186.959 I ggml_metal_init: GPU name:   Apple M4
0.01.186.962 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.186.962 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.186.963 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.186.963 I ggml_metal_init: simdgroup reduction   = true
0.01.186.964 I ggml_metal_init: simdgroup matrix mul. = true
0.01.186.964 I ggml_metal_init: has residency sets    = true
0.01.186.964 I ggml_metal_init: has bfloat            = true
0.01.186.964 I ggml_metal_init: use bfloat            = true
0.01.186.965 I ggml_metal_init: hasUnifiedMemory      = true
0.01.186.969 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.202.249 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.259.804 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.259.809 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.259.843 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.264.490 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.264.491 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.264.492 I llama_init_from_model: graph nodes  = 967
0.01.264.492 I llama_init_from_model: graph splits = 2
0.01.264.497 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.264.621 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.264.622 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.322.636 I main: llama threadpool init, n_threads = 4
0.01.322.681 I 
0.01.322.696 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.322.698 I 
0.01.322.873 I sampler seed: 1234
0.01.322.878 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.322.889 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.322.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.322.889 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.423.463 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.02.423.464 I llama_perf_context_print:        load time =    1312.12 ms
0.02.423.464 I llama_perf_context_print: prompt eval time =      49.32 ms /     7 tokens (    7.05 ms per token,   141.92 tokens per second)
0.02.423.465 I llama_perf_context_print:        eval time =    1048.36 ms /    63 runs   (   16.64 ms per token,    60.09 tokens per second)
0.02.423.465 I llama_perf_context_print:       total time =    1101.54 ms /    70 tokens
0.02.423.738 I ggml_metal_free: deallocating

real	0m2.442s
user	0m0.112s
sys	0m0.298s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.282 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.460 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.177 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.178 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.178 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.179 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.180 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.181 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.183 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.184 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.184 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.107 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.023 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.025 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.027 I llama_model_loader: - type  f32:  194 tensors
0.00.026.027 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.028 I print_info: file format = GGUF V3 (latest)
0.00.026.028 I print_info: file type   = Q8_0
0.00.026.031 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.431 I load: special tokens cache size = 25
0.00.040.646 I load: token to piece cache size = 0.2984 MB
0.00.040.651 I print_info: arch             = gptneox
0.00.040.651 I print_info: vocab_only       = 0
0.00.040.652 I print_info: n_ctx_train      = 2048
0.00.040.652 I print_info: n_embd           = 2048
0.00.040.652 I print_info: n_layer          = 24
0.00.040.656 I print_info: n_head           = 16
0.00.040.657 I print_info: n_head_kv        = 16
0.00.040.657 I print_info: n_rot            = 32
0.00.040.657 I print_info: n_swa            = 0
0.00.040.657 I print_info: n_embd_head_k    = 128
0.00.040.658 I print_info: n_embd_head_v    = 128
0.00.040.658 I print_info: n_gqa            = 1
0.00.040.659 I print_info: n_embd_k_gqa     = 2048
0.00.040.660 I print_info: n_embd_v_gqa     = 2048
0.00.040.660 I print_info: f_norm_eps       = 1.0e-05
0.00.040.661 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.661 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.661 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.661 I print_info: f_logit_scale    = 0.0e+00
0.00.040.662 I print_info: n_ff             = 8192
0.00.040.662 I print_info: n_expert         = 0
0.00.040.662 I print_info: n_expert_used    = 0
0.00.040.664 I print_info: causal attn      = 1
0.00.040.665 I print_info: pooling type     = 0
0.00.040.665 I print_info: rope type        = 2
0.00.040.665 I print_info: rope scaling     = linear
0.00.040.665 I print_info: freq_base_train  = 10000.0
0.00.040.665 I print_info: freq_scale_train = 1
0.00.040.666 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.666 I print_info: rope_finetuned   = unknown
0.00.040.666 I print_info: ssm_d_conv       = 0
0.00.040.666 I print_info: ssm_d_inner      = 0
0.00.040.666 I print_info: ssm_d_state      = 0
0.00.040.666 I print_info: ssm_dt_rank      = 0
0.00.040.667 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.668 I print_info: model type       = 1.4B
0.00.040.668 I print_info: model params     = 1.41 B
0.00.040.668 I print_info: general.name     = 1.4B
0.00.040.669 I print_info: vocab type       = BPE
0.00.040.669 I print_info: n_vocab          = 50304
0.00.040.669 I print_info: n_merges         = 50009
0.00.040.669 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.669 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.670 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.670 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.670 I print_info: LF token         = 187 ''
0.00.040.670 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.670 I print_info: max token length = 1024
0.00.040.671 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.933.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.933.137 I load_tensors: offloading output layer to GPU
0.00.933.137 I load_tensors: offloaded 25/25 layers to GPU
0.00.933.171 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.933.174 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.934.667 I llama_init_from_model: n_seq_max     = 1
0.00.934.668 I llama_init_from_model: n_ctx         = 128
0.00.934.669 I llama_init_from_model: n_ctx_per_seq = 128
0.00.934.669 I llama_init_from_model: n_batch       = 128
0.00.934.669 I llama_init_from_model: n_ubatch      = 128
0.00.934.670 I llama_init_from_model: flash_attn    = 0
0.00.934.671 I llama_init_from_model: freq_base     = 10000.0
0.00.934.671 I llama_init_from_model: freq_scale    = 1
0.00.934.672 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.934.673 I ggml_metal_init: allocating
0.00.934.764 I ggml_metal_init: found device: Apple M4
0.00.934.775 I ggml_metal_init: picking default device: Apple M4
0.00.936.234 I ggml_metal_init: using embedded metal library
0.00.941.630 I ggml_metal_init: GPU name:   Apple M4
0.00.941.633 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.941.634 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.941.635 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.941.635 I ggml_metal_init: simdgroup reduction   = true
0.00.941.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.941.636 I ggml_metal_init: has residency sets    = true
0.00.941.636 I ggml_metal_init: has bfloat            = true
0.00.941.636 I ggml_metal_init: use bfloat            = true
0.00.941.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.941.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.956.710 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.959.983 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.959.987 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.960.029 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.962.926 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.962.927 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.962.928 I llama_init_from_model: graph nodes  = 967
0.00.962.928 I llama_init_from_model: graph splits = 2
0.00.962.931 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.962.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.992.763 I 
0.00.992.826 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.992.836 I perplexity: tokenizing the input ..
0.00.999.995 I perplexity: tokenization took 7.157 ms
0.01.000.003 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.139.548 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.140.888 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.140.916 I llama_perf_context_print:        load time =     982.29 ms
0.01.140.917 I llama_perf_context_print: prompt eval time =     138.62 ms /   128 tokens (    1.08 ms per token,   923.38 tokens per second)
0.01.140.917 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.140.918 I llama_perf_context_print:       total time =     148.16 ms /   129 tokens
0.01.141.313 I ggml_metal_free: deallocating

real	0m1.159s
user	0m0.077s
sys	0m0.182s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.015.363 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.808 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.809 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.811 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.811 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.812 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.812 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.812 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.813 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.815 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.815 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.815 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.018 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.019 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.020 I llama_model_loader: - type  f32:  194 tensors
0.00.033.020 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.020 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.021 I print_info: file format = GGUF V3 (latest)
0.00.033.021 I print_info: file type   = Q4_0
0.00.033.022 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.042.906 I load: special tokens cache size = 25
0.00.050.554 I load: token to piece cache size = 0.2984 MB
0.00.050.559 I print_info: arch             = gptneox
0.00.050.559 I print_info: vocab_only       = 0
0.00.050.559 I print_info: n_ctx_train      = 2048
0.00.050.559 I print_info: n_embd           = 2048
0.00.050.560 I print_info: n_layer          = 24
0.00.050.564 I print_info: n_head           = 16
0.00.050.567 I print_info: n_head_kv        = 16
0.00.050.568 I print_info: n_rot            = 32
0.00.050.568 I print_info: n_swa            = 0
0.00.050.568 I print_info: n_embd_head_k    = 128
0.00.050.569 I print_info: n_embd_head_v    = 128
0.00.050.570 I print_info: n_gqa            = 1
0.00.050.571 I print_info: n_embd_k_gqa     = 2048
0.00.050.572 I print_info: n_embd_v_gqa     = 2048
0.00.050.572 I print_info: f_norm_eps       = 1.0e-05
0.00.050.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.574 I print_info: f_logit_scale    = 0.0e+00
0.00.050.574 I print_info: n_ff             = 8192
0.00.050.574 I print_info: n_expert         = 0
0.00.050.575 I print_info: n_expert_used    = 0
0.00.050.575 I print_info: causal attn      = 1
0.00.050.576 I print_info: pooling type     = 0
0.00.050.576 I print_info: rope type        = 2
0.00.050.577 I print_info: rope scaling     = linear
0.00.050.577 I print_info: freq_base_train  = 10000.0
0.00.050.577 I print_info: freq_scale_train = 1
0.00.050.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.578 I print_info: rope_finetuned   = unknown
0.00.050.578 I print_info: ssm_d_conv       = 0
0.00.050.578 I print_info: ssm_d_inner      = 0
0.00.050.578 I print_info: ssm_d_state      = 0
0.00.050.578 I print_info: ssm_dt_rank      = 0
0.00.050.578 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.579 I print_info: model type       = 1.4B
0.00.050.579 I print_info: model params     = 1.41 B
0.00.050.583 I print_info: general.name     = 1.4B
0.00.050.584 I print_info: vocab type       = BPE
0.00.050.585 I print_info: n_vocab          = 50304
0.00.050.585 I print_info: n_merges         = 50009
0.00.050.585 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.585 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.588 I print_info: LF token         = 187 ''
0.00.050.588 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.589 I print_info: max token length = 1024
0.00.050.589 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.113 I load_tensors: offloading output layer to GPU
0.00.637.114 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.150 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.637.151 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.638.634 I llama_init_from_model: n_seq_max     = 1
0.00.638.637 I llama_init_from_model: n_ctx         = 2048
0.00.638.638 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.638 I llama_init_from_model: n_batch       = 2048
0.00.638.639 I llama_init_from_model: n_ubatch      = 512
0.00.638.639 I llama_init_from_model: flash_attn    = 0
0.00.638.642 I llama_init_from_model: freq_base     = 10000.0
0.00.638.642 I llama_init_from_model: freq_scale    = 1
0.00.638.645 I ggml_metal_init: allocating
0.00.638.740 I ggml_metal_init: found device: Apple M4
0.00.638.754 I ggml_metal_init: picking default device: Apple M4
0.00.640.700 I ggml_metal_init: using embedded metal library
0.00.646.388 I ggml_metal_init: GPU name:   Apple M4
0.00.646.394 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.397 I ggml_metal_init: simdgroup reduction   = true
0.00.646.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.398 I ggml_metal_init: has residency sets    = true
0.00.646.398 I ggml_metal_init: has bfloat            = true
0.00.646.398 I ggml_metal_init: use bfloat            = true
0.00.646.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.358 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.398 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.723.406 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.723.456 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.694 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.696 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.697 I llama_init_from_model: graph nodes  = 967
0.00.728.697 I llama_init_from_model: graph splits = 2
0.00.728.704 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.825 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.904 I main: llama threadpool init, n_threads = 4
0.00.786.948 I 
0.00.786.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.968 I 
0.00.787.138 I sampler seed: 1234
0.00.787.143 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.787.163 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.787.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.787.163 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.464.933 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49067.04 tokens per second)
0.01.464.934 I llama_perf_context_print:        load time =     770.77 ms
0.01.464.935 I llama_perf_context_print: prompt eval time =      45.22 ms /     7 tokens (    6.46 ms per token,   154.79 tokens per second)
0.01.464.935 I llama_perf_context_print:        eval time =     629.67 ms /    63 runs   (    9.99 ms per token,   100.05 tokens per second)
0.01.464.936 I llama_perf_context_print:       total time =     678.79 ms /    70 tokens
0.01.465.146 I ggml_metal_free: deallocating

real	0m1.490s
user	0m0.114s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.283 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.938 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.947 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.948 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.948 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.950 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.612 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.616 I llama_model_loader: - type  f32:  194 tensors
0.00.025.616 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.616 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.617 I print_info: file format = GGUF V3 (latest)
0.00.025.617 I print_info: file type   = Q4_0
0.00.025.619 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.650 I load: special tokens cache size = 25
0.00.039.779 I load: token to piece cache size = 0.2984 MB
0.00.039.783 I print_info: arch             = gptneox
0.00.039.783 I print_info: vocab_only       = 0
0.00.039.783 I print_info: n_ctx_train      = 2048
0.00.039.784 I print_info: n_embd           = 2048
0.00.039.784 I print_info: n_layer          = 24
0.00.039.788 I print_info: n_head           = 16
0.00.039.789 I print_info: n_head_kv        = 16
0.00.039.789 I print_info: n_rot            = 32
0.00.039.789 I print_info: n_swa            = 0
0.00.039.791 I print_info: n_embd_head_k    = 128
0.00.039.791 I print_info: n_embd_head_v    = 128
0.00.039.792 I print_info: n_gqa            = 1
0.00.039.793 I print_info: n_embd_k_gqa     = 2048
0.00.039.793 I print_info: n_embd_v_gqa     = 2048
0.00.039.794 I print_info: f_norm_eps       = 1.0e-05
0.00.039.794 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.795 I print_info: f_logit_scale    = 0.0e+00
0.00.039.795 I print_info: n_ff             = 8192
0.00.039.796 I print_info: n_expert         = 0
0.00.039.796 I print_info: n_expert_used    = 0
0.00.039.796 I print_info: causal attn      = 1
0.00.039.796 I print_info: pooling type     = 0
0.00.039.796 I print_info: rope type        = 2
0.00.039.796 I print_info: rope scaling     = linear
0.00.039.797 I print_info: freq_base_train  = 10000.0
0.00.039.797 I print_info: freq_scale_train = 1
0.00.039.797 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.799 I print_info: rope_finetuned   = unknown
0.00.039.799 I print_info: ssm_d_conv       = 0
0.00.039.799 I print_info: ssm_d_inner      = 0
0.00.039.799 I print_info: ssm_d_state      = 0
0.00.039.800 I print_info: ssm_dt_rank      = 0
0.00.039.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.800 I print_info: model type       = 1.4B
0.00.039.800 I print_info: model params     = 1.41 B
0.00.039.800 I print_info: general.name     = 1.4B
0.00.039.801 I print_info: vocab type       = BPE
0.00.039.801 I print_info: n_vocab          = 50304
0.00.039.801 I print_info: n_merges         = 50009
0.00.039.801 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.802 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.803 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.803 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.803 I print_info: LF token         = 187 ''
0.00.039.803 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.803 I print_info: max token length = 1024
0.00.039.806 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.629.738 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.756 I load_tensors: offloading output layer to GPU
0.00.629.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.629.792 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.629.793 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.631.337 I llama_init_from_model: n_seq_max     = 1
0.00.631.345 I llama_init_from_model: n_ctx         = 128
0.00.631.346 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.346 I llama_init_from_model: n_batch       = 128
0.00.631.346 I llama_init_from_model: n_ubatch      = 128
0.00.631.347 I llama_init_from_model: flash_attn    = 0
0.00.631.349 I llama_init_from_model: freq_base     = 10000.0
0.00.631.349 I llama_init_from_model: freq_scale    = 1
0.00.631.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.352 I ggml_metal_init: allocating
0.00.631.432 I ggml_metal_init: found device: Apple M4
0.00.631.443 I ggml_metal_init: picking default device: Apple M4
0.00.633.999 I ggml_metal_init: using embedded metal library
0.00.641.214 I ggml_metal_init: GPU name:   Apple M4
0.00.641.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.224 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.224 I ggml_metal_init: simdgroup reduction   = true
0.00.641.225 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.225 I ggml_metal_init: has residency sets    = true
0.00.641.225 I ggml_metal_init: has bfloat            = true
0.00.641.225 I ggml_metal_init: use bfloat            = true
0.00.641.227 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.659.414 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.119 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.122 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.154 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.273 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.275 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.276 I llama_init_from_model: graph nodes  = 967
0.00.666.276 I llama_init_from_model: graph splits = 2
0.00.666.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.280 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.323 I 
0.00.692.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.392 I perplexity: tokenizing the input ..
0.00.699.930 I perplexity: tokenization took 7.535 ms
0.00.699.937 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.280 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.835.703 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.835.723 I llama_perf_context_print:        load time =     682.56 ms
0.00.835.724 I llama_perf_context_print: prompt eval time =     133.28 ms /   128 tokens (    1.04 ms per token,   960.36 tokens per second)
0.00.835.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.726 I llama_perf_context_print:       total time =     143.40 ms /   129 tokens
0.00.836.078 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.080s
sys	0m0.139s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.031.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.958 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.959 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.959 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.917 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.962 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.862 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.040.862 I llama_model_loader: - type  f32:  194 tensors
0.00.040.862 I llama_model_loader: - type q4_1:   97 tensors
0.00.040.862 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.863 I print_info: file format = GGUF V3 (latest)
0.00.040.863 I print_info: file type   = Q4_1
0.00.040.864 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.049.973 I load: special tokens cache size = 25
0.00.056.610 I load: token to piece cache size = 0.2984 MB
0.00.056.613 I print_info: arch             = gptneox
0.00.056.613 I print_info: vocab_only       = 0
0.00.056.613 I print_info: n_ctx_train      = 2048
0.00.056.613 I print_info: n_embd           = 2048
0.00.056.613 I print_info: n_layer          = 24
0.00.056.616 I print_info: n_head           = 16
0.00.056.616 I print_info: n_head_kv        = 16
0.00.056.617 I print_info: n_rot            = 32
0.00.056.617 I print_info: n_swa            = 0
0.00.056.617 I print_info: n_embd_head_k    = 128
0.00.056.617 I print_info: n_embd_head_v    = 128
0.00.056.618 I print_info: n_gqa            = 1
0.00.056.619 I print_info: n_embd_k_gqa     = 2048
0.00.056.619 I print_info: n_embd_v_gqa     = 2048
0.00.056.620 I print_info: f_norm_eps       = 1.0e-05
0.00.056.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.621 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.621 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.621 I print_info: f_logit_scale    = 0.0e+00
0.00.056.622 I print_info: n_ff             = 8192
0.00.056.622 I print_info: n_expert         = 0
0.00.056.622 I print_info: n_expert_used    = 0
0.00.056.622 I print_info: causal attn      = 1
0.00.056.622 I print_info: pooling type     = 0
0.00.056.622 I print_info: rope type        = 2
0.00.056.623 I print_info: rope scaling     = linear
0.00.056.623 I print_info: freq_base_train  = 10000.0
0.00.056.623 I print_info: freq_scale_train = 1
0.00.056.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.624 I print_info: rope_finetuned   = unknown
0.00.056.624 I print_info: ssm_d_conv       = 0
0.00.056.624 I print_info: ssm_d_inner      = 0
0.00.056.624 I print_info: ssm_d_state      = 0
0.00.056.624 I print_info: ssm_dt_rank      = 0
0.00.056.624 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.625 I print_info: model type       = 1.4B
0.00.056.625 I print_info: model params     = 1.41 B
0.00.056.625 I print_info: general.name     = 1.4B
0.00.056.626 I print_info: vocab type       = BPE
0.00.056.626 I print_info: n_vocab          = 50304
0.00.056.626 I print_info: n_merges         = 50009
0.00.056.627 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.627 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.627 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.627 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.627 I print_info: LF token         = 187 ''
0.00.056.628 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.628 I print_info: max token length = 1024
0.00.056.628 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.742.886 I load_tensors: offloading 24 repeating layers to GPU
0.00.742.900 I load_tensors: offloading output layer to GPU
0.00.742.901 I load_tensors: offloaded 25/25 layers to GPU
0.00.742.935 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.742.936 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.744.466 I llama_init_from_model: n_seq_max     = 1
0.00.744.468 I llama_init_from_model: n_ctx         = 2048
0.00.744.469 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.744.469 I llama_init_from_model: n_batch       = 2048
0.00.744.470 I llama_init_from_model: n_ubatch      = 512
0.00.744.470 I llama_init_from_model: flash_attn    = 0
0.00.744.472 I llama_init_from_model: freq_base     = 10000.0
0.00.744.473 I llama_init_from_model: freq_scale    = 1
0.00.744.477 I ggml_metal_init: allocating
0.00.744.545 I ggml_metal_init: found device: Apple M4
0.00.744.559 I ggml_metal_init: picking default device: Apple M4
0.00.746.453 I ggml_metal_init: using embedded metal library
0.00.753.193 I ggml_metal_init: GPU name:   Apple M4
0.00.753.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.753.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.753.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.753.200 I ggml_metal_init: simdgroup reduction   = true
0.00.753.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.753.201 I ggml_metal_init: has residency sets    = true
0.00.753.201 I ggml_metal_init: has bfloat            = true
0.00.753.201 I ggml_metal_init: use bfloat            = true
0.00.753.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.753.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.522 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.826.236 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.826.243 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.826.285 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.830.859 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.830.861 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.830.861 I llama_init_from_model: graph nodes  = 967
0.00.830.861 I llama_init_from_model: graph splits = 2
0.00.830.868 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.831.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.831.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.890.819 I main: llama threadpool init, n_threads = 4
0.00.890.859 I 
0.00.890.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.890.873 I 
0.00.891.052 I sampler seed: 1234
0.00.891.056 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.891.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.891.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.891.100 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.618.252 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.618.252 I llama_perf_context_print:        load time =     881.27 ms
0.01.618.253 I llama_perf_context_print: prompt eval time =      47.51 ms /     7 tokens (    6.79 ms per token,   147.33 tokens per second)
0.01.618.253 I llama_perf_context_print:        eval time =     676.83 ms /    63 runs   (   10.74 ms per token,    93.08 tokens per second)
0.01.618.254 I llama_perf_context_print:       total time =     728.14 ms /    70 tokens
0.01.618.550 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.111s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.128 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.067 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.069 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.070 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.070 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.071 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.073 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.073 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.076 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.654 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.655 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.656 I llama_model_loader: - type  f32:  194 tensors
0.00.024.656 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.656 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.657 I print_info: file format = GGUF V3 (latest)
0.00.024.658 I print_info: file type   = Q4_1
0.00.024.659 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.825 I load: special tokens cache size = 25
0.00.038.871 I load: token to piece cache size = 0.2984 MB
0.00.038.876 I print_info: arch             = gptneox
0.00.038.876 I print_info: vocab_only       = 0
0.00.038.876 I print_info: n_ctx_train      = 2048
0.00.038.876 I print_info: n_embd           = 2048
0.00.038.876 I print_info: n_layer          = 24
0.00.038.881 I print_info: n_head           = 16
0.00.038.881 I print_info: n_head_kv        = 16
0.00.038.882 I print_info: n_rot            = 32
0.00.038.882 I print_info: n_swa            = 0
0.00.038.882 I print_info: n_embd_head_k    = 128
0.00.038.882 I print_info: n_embd_head_v    = 128
0.00.038.883 I print_info: n_gqa            = 1
0.00.038.883 I print_info: n_embd_k_gqa     = 2048
0.00.038.884 I print_info: n_embd_v_gqa     = 2048
0.00.038.885 I print_info: f_norm_eps       = 1.0e-05
0.00.038.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.885 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.885 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.885 I print_info: f_logit_scale    = 0.0e+00
0.00.038.887 I print_info: n_ff             = 8192
0.00.038.887 I print_info: n_expert         = 0
0.00.038.887 I print_info: n_expert_used    = 0
0.00.038.888 I print_info: causal attn      = 1
0.00.038.888 I print_info: pooling type     = 0
0.00.038.888 I print_info: rope type        = 2
0.00.038.895 I print_info: rope scaling     = linear
0.00.038.895 I print_info: freq_base_train  = 10000.0
0.00.038.895 I print_info: freq_scale_train = 1
0.00.038.896 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.896 I print_info: rope_finetuned   = unknown
0.00.038.896 I print_info: ssm_d_conv       = 0
0.00.038.896 I print_info: ssm_d_inner      = 0
0.00.038.896 I print_info: ssm_d_state      = 0
0.00.038.896 I print_info: ssm_dt_rank      = 0
0.00.038.896 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.896 I print_info: model type       = 1.4B
0.00.038.897 I print_info: model params     = 1.41 B
0.00.038.897 I print_info: general.name     = 1.4B
0.00.038.898 I print_info: vocab type       = BPE
0.00.038.898 I print_info: n_vocab          = 50304
0.00.038.898 I print_info: n_merges         = 50009
0.00.038.898 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.900 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.900 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.901 I print_info: LF token         = 187 ''
0.00.038.901 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.901 I print_info: max token length = 1024
0.00.038.901 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.690.834 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.849 I load_tensors: offloading output layer to GPU
0.00.690.850 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.880 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.690.884 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.692.542 I llama_init_from_model: n_seq_max     = 1
0.00.692.550 I llama_init_from_model: n_ctx         = 128
0.00.692.551 I llama_init_from_model: n_ctx_per_seq = 128
0.00.692.551 I llama_init_from_model: n_batch       = 128
0.00.692.551 I llama_init_from_model: n_ubatch      = 128
0.00.692.552 I llama_init_from_model: flash_attn    = 0
0.00.692.567 I llama_init_from_model: freq_base     = 10000.0
0.00.692.568 I llama_init_from_model: freq_scale    = 1
0.00.692.568 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.692.572 I ggml_metal_init: allocating
0.00.692.696 I ggml_metal_init: found device: Apple M4
0.00.692.752 I ggml_metal_init: picking default device: Apple M4
0.00.694.533 I ggml_metal_init: using embedded metal library
0.00.701.461 I ggml_metal_init: GPU name:   Apple M4
0.00.701.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.701.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.701.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.701.471 I ggml_metal_init: simdgroup reduction   = true
0.00.701.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.701.472 I ggml_metal_init: has residency sets    = true
0.00.701.472 I ggml_metal_init: has bfloat            = true
0.00.701.473 I ggml_metal_init: use bfloat            = true
0.00.701.474 I ggml_metal_init: hasUnifiedMemory      = true
0.00.701.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.720.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.833 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.723.837 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.723.883 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.289 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.727.291 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.727.291 I llama_init_from_model: graph nodes  = 967
0.00.727.292 I llama_init_from_model: graph splits = 2
0.00.727.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.727.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.594 I 
0.00.753.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.663 I perplexity: tokenizing the input ..
0.00.759.459 I perplexity: tokenization took 5.795 ms
0.00.759.463 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.895.020 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.896.438 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.896.461 I llama_perf_context_print:        load time =     744.45 ms
0.00.896.461 I llama_perf_context_print: prompt eval time =     135.31 ms /   128 tokens (    1.06 ms per token,   945.95 tokens per second)
0.00.896.462 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.896.462 I llama_perf_context_print:       total time =     142.87 ms /   129 tokens
0.00.896.817 I ggml_metal_free: deallocating

real	0m0.911s
user	0m0.078s
sys	0m0.118s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.036.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.279 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.279 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.248 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.045.835 I llama_model_loader: - type  f32:  194 tensors
0.00.045.836 I llama_model_loader: - type q5_0:   97 tensors
0.00.045.836 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.837 I print_info: file format = GGUF V3 (latest)
0.00.045.837 I print_info: file type   = Q5_0
0.00.045.838 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.055.100 I load: special tokens cache size = 25
0.00.062.695 I load: token to piece cache size = 0.2984 MB
0.00.062.698 I print_info: arch             = gptneox
0.00.062.699 I print_info: vocab_only       = 0
0.00.062.699 I print_info: n_ctx_train      = 2048
0.00.062.699 I print_info: n_embd           = 2048
0.00.062.699 I print_info: n_layer          = 24
0.00.062.702 I print_info: n_head           = 16
0.00.062.703 I print_info: n_head_kv        = 16
0.00.062.704 I print_info: n_rot            = 32
0.00.062.705 I print_info: n_swa            = 0
0.00.062.705 I print_info: n_embd_head_k    = 128
0.00.062.705 I print_info: n_embd_head_v    = 128
0.00.062.706 I print_info: n_gqa            = 1
0.00.062.707 I print_info: n_embd_k_gqa     = 2048
0.00.062.707 I print_info: n_embd_v_gqa     = 2048
0.00.062.708 I print_info: f_norm_eps       = 1.0e-05
0.00.062.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.709 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.709 I print_info: f_logit_scale    = 0.0e+00
0.00.062.709 I print_info: n_ff             = 8192
0.00.062.710 I print_info: n_expert         = 0
0.00.062.711 I print_info: n_expert_used    = 0
0.00.062.712 I print_info: causal attn      = 1
0.00.062.712 I print_info: pooling type     = 0
0.00.062.712 I print_info: rope type        = 2
0.00.062.712 I print_info: rope scaling     = linear
0.00.062.713 I print_info: freq_base_train  = 10000.0
0.00.062.713 I print_info: freq_scale_train = 1
0.00.062.713 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.713 I print_info: rope_finetuned   = unknown
0.00.062.714 I print_info: ssm_d_conv       = 0
0.00.062.714 I print_info: ssm_d_inner      = 0
0.00.062.715 I print_info: ssm_d_state      = 0
0.00.062.715 I print_info: ssm_dt_rank      = 0
0.00.062.715 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.716 I print_info: model type       = 1.4B
0.00.062.716 I print_info: model params     = 1.41 B
0.00.062.716 I print_info: general.name     = 1.4B
0.00.062.717 I print_info: vocab type       = BPE
0.00.062.717 I print_info: n_vocab          = 50304
0.00.062.717 I print_info: n_merges         = 50009
0.00.062.717 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.717 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.718 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.718 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.718 I print_info: LF token         = 187 ''
0.00.062.723 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.723 I print_info: max token length = 1024
0.00.062.723 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.746.314 I load_tensors: offloading 24 repeating layers to GPU
0.00.746.330 I load_tensors: offloading output layer to GPU
0.00.746.330 I load_tensors: offloaded 25/25 layers to GPU
0.00.746.358 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.746.359 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.747.860 I llama_init_from_model: n_seq_max     = 1
0.00.747.863 I llama_init_from_model: n_ctx         = 2048
0.00.747.864 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.747.865 I llama_init_from_model: n_batch       = 2048
0.00.747.865 I llama_init_from_model: n_ubatch      = 512
0.00.747.865 I llama_init_from_model: flash_attn    = 0
0.00.747.868 I llama_init_from_model: freq_base     = 10000.0
0.00.747.868 I llama_init_from_model: freq_scale    = 1
0.00.747.880 I ggml_metal_init: allocating
0.00.747.970 I ggml_metal_init: found device: Apple M4
0.00.747.986 I ggml_metal_init: picking default device: Apple M4
0.00.749.848 I ggml_metal_init: using embedded metal library
0.00.755.896 I ggml_metal_init: GPU name:   Apple M4
0.00.755.901 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.755.902 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.755.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.755.904 I ggml_metal_init: simdgroup reduction   = true
0.00.755.904 I ggml_metal_init: simdgroup matrix mul. = true
0.00.755.905 I ggml_metal_init: has residency sets    = true
0.00.755.905 I ggml_metal_init: has bfloat            = true
0.00.755.905 I ggml_metal_init: use bfloat            = true
0.00.755.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.755.915 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.774.790 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.832.138 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.832.144 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.832.178 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.836.310 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.836.311 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.836.312 I llama_init_from_model: graph nodes  = 967
0.00.836.312 I llama_init_from_model: graph splits = 2
0.00.836.316 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.836.441 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.836.441 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.892.782 I main: llama threadpool init, n_threads = 4
0.00.892.821 I 
0.00.892.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.892.836 I 
0.00.892.990 I sampler seed: 1234
0.00.892.994 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.893.005 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.893.006 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.893.006 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.678.132 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.678.132 I llama_perf_context_print:        load time =     881.34 ms
0.01.678.133 I llama_perf_context_print: prompt eval time =      42.79 ms /     7 tokens (    6.11 ms per token,   163.58 tokens per second)
0.01.678.134 I llama_perf_context_print:        eval time =     739.49 ms /    63 runs   (   11.74 ms per token,    85.19 tokens per second)
0.01.678.134 I llama_perf_context_print:       total time =     786.12 ms /    70 tokens
0.01.678.398 I ggml_metal_free: deallocating

real	0m1.697s
user	0m0.116s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.361 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.696 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.701 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.425 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.426 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.427 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.427 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.429 I llama_model_loader: - type  f32:  194 tensors
0.00.026.429 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.429 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.430 I print_info: file format = GGUF V3 (latest)
0.00.026.430 I print_info: file type   = Q5_0
0.00.026.432 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.684 I load: special tokens cache size = 25
0.00.040.467 I load: token to piece cache size = 0.2984 MB
0.00.040.471 I print_info: arch             = gptneox
0.00.040.471 I print_info: vocab_only       = 0
0.00.040.471 I print_info: n_ctx_train      = 2048
0.00.040.471 I print_info: n_embd           = 2048
0.00.040.472 I print_info: n_layer          = 24
0.00.040.475 I print_info: n_head           = 16
0.00.040.476 I print_info: n_head_kv        = 16
0.00.040.476 I print_info: n_rot            = 32
0.00.040.476 I print_info: n_swa            = 0
0.00.040.477 I print_info: n_embd_head_k    = 128
0.00.040.477 I print_info: n_embd_head_v    = 128
0.00.040.477 I print_info: n_gqa            = 1
0.00.040.478 I print_info: n_embd_k_gqa     = 2048
0.00.040.479 I print_info: n_embd_v_gqa     = 2048
0.00.040.480 I print_info: f_norm_eps       = 1.0e-05
0.00.040.480 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.480 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.480 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.481 I print_info: f_logit_scale    = 0.0e+00
0.00.040.481 I print_info: n_ff             = 8192
0.00.040.481 I print_info: n_expert         = 0
0.00.040.482 I print_info: n_expert_used    = 0
0.00.040.482 I print_info: causal attn      = 1
0.00.040.482 I print_info: pooling type     = 0
0.00.040.482 I print_info: rope type        = 2
0.00.040.482 I print_info: rope scaling     = linear
0.00.040.482 I print_info: freq_base_train  = 10000.0
0.00.040.483 I print_info: freq_scale_train = 1
0.00.040.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.483 I print_info: rope_finetuned   = unknown
0.00.040.483 I print_info: ssm_d_conv       = 0
0.00.040.484 I print_info: ssm_d_inner      = 0
0.00.040.484 I print_info: ssm_d_state      = 0
0.00.040.484 I print_info: ssm_dt_rank      = 0
0.00.040.486 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.486 I print_info: model type       = 1.4B
0.00.040.486 I print_info: model params     = 1.41 B
0.00.040.487 I print_info: general.name     = 1.4B
0.00.040.487 I print_info: vocab type       = BPE
0.00.040.487 I print_info: n_vocab          = 50304
0.00.040.487 I print_info: n_merges         = 50009
0.00.040.488 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.488 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.488 I print_info: LF token         = 187 ''
0.00.040.488 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.489 I print_info: max token length = 1024
0.00.040.489 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.775.398 I load_tensors: offloading 24 repeating layers to GPU
0.00.775.412 I load_tensors: offloading output layer to GPU
0.00.775.413 I load_tensors: offloaded 25/25 layers to GPU
0.00.775.444 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.775.446 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.776.881 I llama_init_from_model: n_seq_max     = 1
0.00.776.889 I llama_init_from_model: n_ctx         = 128
0.00.776.890 I llama_init_from_model: n_ctx_per_seq = 128
0.00.776.890 I llama_init_from_model: n_batch       = 128
0.00.776.891 I llama_init_from_model: n_ubatch      = 128
0.00.776.891 I llama_init_from_model: flash_attn    = 0
0.00.776.893 I llama_init_from_model: freq_base     = 10000.0
0.00.776.893 I llama_init_from_model: freq_scale    = 1
0.00.776.894 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.776.896 I ggml_metal_init: allocating
0.00.776.979 I ggml_metal_init: found device: Apple M4
0.00.776.992 I ggml_metal_init: picking default device: Apple M4
0.00.778.773 I ggml_metal_init: using embedded metal library
0.00.784.057 I ggml_metal_init: GPU name:   Apple M4
0.00.784.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.784.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.784.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.784.067 I ggml_metal_init: simdgroup reduction   = true
0.00.784.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.784.068 I ggml_metal_init: has residency sets    = true
0.00.784.068 I ggml_metal_init: has bfloat            = true
0.00.784.068 I ggml_metal_init: use bfloat            = true
0.00.784.069 I ggml_metal_init: hasUnifiedMemory      = true
0.00.784.073 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.795.914 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.717 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.797.720 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.797.747 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.799.425 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.799.427 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.799.427 I llama_init_from_model: graph nodes  = 967
0.00.799.427 I llama_init_from_model: graph splits = 2
0.00.799.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.799.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.824.334 I 
0.00.824.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.824.364 I perplexity: tokenizing the input ..
0.00.828.166 I perplexity: tokenization took 3.801 ms
0.00.828.169 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.962.488 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.967.542 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.967.589 I llama_perf_context_print:        load time =     813.96 ms
0.00.967.590 I llama_perf_context_print: prompt eval time =     134.09 ms /   128 tokens (    1.05 ms per token,   954.57 tokens per second)
0.00.967.592 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.967.592 I llama_perf_context_print:       total time =     143.26 ms /   129 tokens
0.00.968.287 I ggml_metal_free: deallocating

real	0m0.992s
user	0m0.089s
sys	0m0.123s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.580 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.581 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.582 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.583 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.583 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.585 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.586 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.588 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.483 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.309 I llama_model_loader: - type  f32:  194 tensors
0.00.025.309 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.309 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.310 I print_info: file format = GGUF V3 (latest)
0.00.025.310 I print_info: file type   = Q5_1
0.00.025.311 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.430 I load: special tokens cache size = 25
0.00.039.383 I load: token to piece cache size = 0.2984 MB
0.00.039.386 I print_info: arch             = gptneox
0.00.039.387 I print_info: vocab_only       = 0
0.00.039.387 I print_info: n_ctx_train      = 2048
0.00.039.387 I print_info: n_embd           = 2048
0.00.039.387 I print_info: n_layer          = 24
0.00.039.390 I print_info: n_head           = 16
0.00.039.391 I print_info: n_head_kv        = 16
0.00.039.391 I print_info: n_rot            = 32
0.00.039.391 I print_info: n_swa            = 0
0.00.039.391 I print_info: n_embd_head_k    = 128
0.00.039.392 I print_info: n_embd_head_v    = 128
0.00.039.392 I print_info: n_gqa            = 1
0.00.039.393 I print_info: n_embd_k_gqa     = 2048
0.00.039.394 I print_info: n_embd_v_gqa     = 2048
0.00.039.394 I print_info: f_norm_eps       = 1.0e-05
0.00.039.395 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.395 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.395 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.395 I print_info: f_logit_scale    = 0.0e+00
0.00.039.398 I print_info: n_ff             = 8192
0.00.039.398 I print_info: n_expert         = 0
0.00.039.398 I print_info: n_expert_used    = 0
0.00.039.398 I print_info: causal attn      = 1
0.00.039.398 I print_info: pooling type     = 0
0.00.039.398 I print_info: rope type        = 2
0.00.039.400 I print_info: rope scaling     = linear
0.00.039.401 I print_info: freq_base_train  = 10000.0
0.00.039.401 I print_info: freq_scale_train = 1
0.00.039.401 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.402 I print_info: rope_finetuned   = unknown
0.00.039.402 I print_info: ssm_d_conv       = 0
0.00.039.402 I print_info: ssm_d_inner      = 0
0.00.039.402 I print_info: ssm_d_state      = 0
0.00.039.402 I print_info: ssm_dt_rank      = 0
0.00.039.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.402 I print_info: model type       = 1.4B
0.00.039.403 I print_info: model params     = 1.41 B
0.00.039.403 I print_info: general.name     = 1.4B
0.00.039.403 I print_info: vocab type       = BPE
0.00.039.404 I print_info: n_vocab          = 50304
0.00.039.404 I print_info: n_merges         = 50009
0.00.039.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.408 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.408 I print_info: LF token         = 187 ''
0.00.039.408 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.409 I print_info: max token length = 1024
0.00.039.409 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.975 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.992 I load_tensors: offloading output layer to GPU
0.00.610.993 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.028 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.611.030 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.612.610 I llama_init_from_model: n_seq_max     = 1
0.00.612.613 I llama_init_from_model: n_ctx         = 2048
0.00.612.614 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.612.614 I llama_init_from_model: n_batch       = 2048
0.00.612.615 I llama_init_from_model: n_ubatch      = 512
0.00.612.615 I llama_init_from_model: flash_attn    = 0
0.00.612.617 I llama_init_from_model: freq_base     = 10000.0
0.00.612.617 I llama_init_from_model: freq_scale    = 1
0.00.612.620 I ggml_metal_init: allocating
0.00.612.697 I ggml_metal_init: found device: Apple M4
0.00.612.712 I ggml_metal_init: picking default device: Apple M4
0.00.614.325 I ggml_metal_init: using embedded metal library
0.00.620.766 I ggml_metal_init: GPU name:   Apple M4
0.00.620.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.772 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.772 I ggml_metal_init: simdgroup reduction   = true
0.00.620.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.773 I ggml_metal_init: has residency sets    = true
0.00.620.773 I ggml_metal_init: has bfloat            = true
0.00.620.773 I ggml_metal_init: use bfloat            = true
0.00.620.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.252 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.353 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.696.360 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.696.395 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.520 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.701.523 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.701.523 I llama_init_from_model: graph nodes  = 967
0.00.701.523 I llama_init_from_model: graph splits = 2
0.00.701.529 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.701.654 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.701.655 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.548 I main: llama threadpool init, n_threads = 4
0.00.759.591 I 
0.00.759.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.607 I 
0.00.759.775 I sampler seed: 1234
0.00.759.779 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.820 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.829 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.830 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.616.676 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46496.40 tokens per second)
0.01.616.677 I llama_perf_context_print:        load time =     750.00 ms
0.01.616.678 I llama_perf_context_print: prompt eval time =      51.84 ms /     7 tokens (    7.41 ms per token,   135.02 tokens per second)
0.01.616.679 I llama_perf_context_print:        eval time =     802.34 ms /    63 runs   (   12.74 ms per token,    78.52 tokens per second)
0.01.616.679 I llama_perf_context_print:       total time =     857.83 ms /    70 tokens
0.01.616.945 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.109s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.516 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.363 I llama_model_loader: - type  f32:  194 tensors
0.00.024.363 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.364 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.364 I print_info: file format = GGUF V3 (latest)
0.00.024.365 I print_info: file type   = Q5_1
0.00.024.366 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.600 I load: special tokens cache size = 25
0.00.038.764 I load: token to piece cache size = 0.2984 MB
0.00.038.769 I print_info: arch             = gptneox
0.00.038.769 I print_info: vocab_only       = 0
0.00.038.769 I print_info: n_ctx_train      = 2048
0.00.038.770 I print_info: n_embd           = 2048
0.00.038.770 I print_info: n_layer          = 24
0.00.038.774 I print_info: n_head           = 16
0.00.038.775 I print_info: n_head_kv        = 16
0.00.038.778 I print_info: n_rot            = 32
0.00.038.778 I print_info: n_swa            = 0
0.00.038.778 I print_info: n_embd_head_k    = 128
0.00.038.778 I print_info: n_embd_head_v    = 128
0.00.038.779 I print_info: n_gqa            = 1
0.00.038.779 I print_info: n_embd_k_gqa     = 2048
0.00.038.780 I print_info: n_embd_v_gqa     = 2048
0.00.038.780 I print_info: f_norm_eps       = 1.0e-05
0.00.038.781 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.781 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.781 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.781 I print_info: f_logit_scale    = 0.0e+00
0.00.038.782 I print_info: n_ff             = 8192
0.00.038.784 I print_info: n_expert         = 0
0.00.038.784 I print_info: n_expert_used    = 0
0.00.038.784 I print_info: causal attn      = 1
0.00.038.784 I print_info: pooling type     = 0
0.00.038.784 I print_info: rope type        = 2
0.00.038.784 I print_info: rope scaling     = linear
0.00.038.811 I print_info: freq_base_train  = 10000.0
0.00.038.813 I print_info: freq_scale_train = 1
0.00.038.813 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.814 I print_info: rope_finetuned   = unknown
0.00.038.814 I print_info: ssm_d_conv       = 0
0.00.038.814 I print_info: ssm_d_inner      = 0
0.00.038.814 I print_info: ssm_d_state      = 0
0.00.038.814 I print_info: ssm_dt_rank      = 0
0.00.038.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.815 I print_info: model type       = 1.4B
0.00.038.815 I print_info: model params     = 1.41 B
0.00.038.815 I print_info: general.name     = 1.4B
0.00.038.816 I print_info: vocab type       = BPE
0.00.038.816 I print_info: n_vocab          = 50304
0.00.038.816 I print_info: n_merges         = 50009
0.00.038.816 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.816 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.817 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.817 I print_info: LF token         = 187 ''
0.00.038.817 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.817 I print_info: max token length = 1024
0.00.038.818 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.882 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.889 I load_tensors: offloading output layer to GPU
0.00.601.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.919 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.601.922 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.603.380 I llama_init_from_model: n_seq_max     = 1
0.00.603.382 I llama_init_from_model: n_ctx         = 128
0.00.603.383 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.383 I llama_init_from_model: n_batch       = 128
0.00.603.384 I llama_init_from_model: n_ubatch      = 128
0.00.603.384 I llama_init_from_model: flash_attn    = 0
0.00.603.385 I llama_init_from_model: freq_base     = 10000.0
0.00.603.386 I llama_init_from_model: freq_scale    = 1
0.00.603.387 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.397 I ggml_metal_init: allocating
0.00.603.442 I ggml_metal_init: found device: Apple M4
0.00.603.455 I ggml_metal_init: picking default device: Apple M4
0.00.604.978 I ggml_metal_init: using embedded metal library
0.00.610.997 I ggml_metal_init: GPU name:   Apple M4
0.00.611.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.017 I ggml_metal_init: simdgroup reduction   = true
0.00.611.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.018 I ggml_metal_init: has residency sets    = true
0.00.611.018 I ggml_metal_init: has bfloat            = true
0.00.611.018 I ggml_metal_init: use bfloat            = true
0.00.611.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.924 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.311 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.318 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.367 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.505 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.507 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.507 I llama_init_from_model: graph nodes  = 967
0.00.634.508 I llama_init_from_model: graph splits = 2
0.00.634.510 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.896 I 
0.00.663.958 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.967 I perplexity: tokenizing the input ..
0.00.671.816 I perplexity: tokenization took 7.845 ms
0.00.671.851 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.605 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.973 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.993 I llama_perf_context_print:        load time =     655.00 ms
0.00.821.994 I llama_perf_context_print: prompt eval time =     147.88 ms /   128 tokens (    1.16 ms per token,   865.55 tokens per second)
0.00.821.995 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.995 I llama_perf_context_print:       total time =     158.10 ms /   129 tokens
0.00.822.346 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.081s
sys	0m0.131s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.779 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.781 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.781 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.783 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.787 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.794 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.794 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.610 I llama_model_loader: - type  f32:  194 tensors
0.00.025.610 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.610 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.611 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.611 I print_info: file format = GGUF V3 (latest)
0.00.025.617 I print_info: file type   = Q2_K - Medium
0.00.025.619 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.021 I load: special tokens cache size = 25
0.00.040.138 I load: token to piece cache size = 0.2984 MB
0.00.040.144 I print_info: arch             = gptneox
0.00.040.144 I print_info: vocab_only       = 0
0.00.040.144 I print_info: n_ctx_train      = 2048
0.00.040.146 I print_info: n_embd           = 2048
0.00.040.146 I print_info: n_layer          = 24
0.00.040.151 I print_info: n_head           = 16
0.00.040.152 I print_info: n_head_kv        = 16
0.00.040.152 I print_info: n_rot            = 32
0.00.040.152 I print_info: n_swa            = 0
0.00.040.152 I print_info: n_embd_head_k    = 128
0.00.040.152 I print_info: n_embd_head_v    = 128
0.00.040.153 I print_info: n_gqa            = 1
0.00.040.154 I print_info: n_embd_k_gqa     = 2048
0.00.040.154 I print_info: n_embd_v_gqa     = 2048
0.00.040.155 I print_info: f_norm_eps       = 1.0e-05
0.00.040.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.155 I print_info: f_logit_scale    = 0.0e+00
0.00.040.156 I print_info: n_ff             = 8192
0.00.040.158 I print_info: n_expert         = 0
0.00.040.158 I print_info: n_expert_used    = 0
0.00.040.158 I print_info: causal attn      = 1
0.00.040.158 I print_info: pooling type     = 0
0.00.040.158 I print_info: rope type        = 2
0.00.040.159 I print_info: rope scaling     = linear
0.00.040.159 I print_info: freq_base_train  = 10000.0
0.00.040.159 I print_info: freq_scale_train = 1
0.00.040.159 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.160 I print_info: rope_finetuned   = unknown
0.00.040.160 I print_info: ssm_d_conv       = 0
0.00.040.160 I print_info: ssm_d_inner      = 0
0.00.040.160 I print_info: ssm_d_state      = 0
0.00.040.160 I print_info: ssm_dt_rank      = 0
0.00.040.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.160 I print_info: model type       = 1.4B
0.00.040.161 I print_info: model params     = 1.41 B
0.00.040.162 I print_info: general.name     = 1.4B
0.00.040.163 I print_info: vocab type       = BPE
0.00.040.163 I print_info: n_vocab          = 50304
0.00.040.163 I print_info: n_merges         = 50009
0.00.040.163 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.164 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: LF token         = 187 ''
0.00.040.166 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.166 I print_info: max token length = 1024
0.00.040.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.537 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.552 I load_tensors: offloading output layer to GPU
0.00.336.553 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.587 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.598 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.758 I llama_init_from_model: n_seq_max     = 1
0.00.337.762 I llama_init_from_model: n_ctx         = 2048
0.00.337.763 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.337.764 I llama_init_from_model: n_batch       = 2048
0.00.337.764 I llama_init_from_model: n_ubatch      = 512
0.00.337.764 I llama_init_from_model: flash_attn    = 0
0.00.337.766 I llama_init_from_model: freq_base     = 10000.0
0.00.337.767 I llama_init_from_model: freq_scale    = 1
0.00.337.769 I ggml_metal_init: allocating
0.00.337.903 I ggml_metal_init: found device: Apple M4
0.00.337.917 I ggml_metal_init: picking default device: Apple M4
0.00.339.896 I ggml_metal_init: using embedded metal library
0.00.345.114 I ggml_metal_init: GPU name:   Apple M4
0.00.345.124 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.125 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.126 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.126 I ggml_metal_init: simdgroup reduction   = true
0.00.345.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.127 I ggml_metal_init: has residency sets    = true
0.00.345.127 I ggml_metal_init: has bfloat            = true
0.00.345.128 I ggml_metal_init: use bfloat            = true
0.00.345.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.298 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.500 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.429.506 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.429.544 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.158 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.434.160 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.434.160 I llama_init_from_model: graph nodes  = 967
0.00.434.161 I llama_init_from_model: graph splits = 2
0.00.434.167 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.434.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.434.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.945 I main: llama threadpool init, n_threads = 4
0.00.491.991 I 
0.00.492.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.492.007 I 
0.00.492.188 I sampler seed: 1234
0.00.492.193 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.492.213 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.492.213 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.492.213 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.166.224 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.01.166.224 I llama_perf_context_print:        load time =     481.30 ms
0.01.166.225 I llama_perf_context_print: prompt eval time =      35.45 ms /     7 tokens (    5.06 ms per token,   197.44 tokens per second)
0.01.166.227 I llama_perf_context_print:        eval time =     635.79 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.166.227 I llama_perf_context_print:       total time =     674.99 ms /    70 tokens
0.01.166.422 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.114s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.751 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.763 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.764 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.765 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.766 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.769 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.769 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.402 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.403 I llama_model_loader: - type  f32:  194 tensors
0.00.025.403 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.403 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.403 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.404 I print_info: file format = GGUF V3 (latest)
0.00.025.405 I print_info: file type   = Q2_K - Medium
0.00.025.406 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.616 I load: special tokens cache size = 25
0.00.039.742 I load: token to piece cache size = 0.2984 MB
0.00.039.746 I print_info: arch             = gptneox
0.00.039.747 I print_info: vocab_only       = 0
0.00.039.747 I print_info: n_ctx_train      = 2048
0.00.039.747 I print_info: n_embd           = 2048
0.00.039.747 I print_info: n_layer          = 24
0.00.039.751 I print_info: n_head           = 16
0.00.039.752 I print_info: n_head_kv        = 16
0.00.039.752 I print_info: n_rot            = 32
0.00.039.752 I print_info: n_swa            = 0
0.00.039.752 I print_info: n_embd_head_k    = 128
0.00.039.753 I print_info: n_embd_head_v    = 128
0.00.039.753 I print_info: n_gqa            = 1
0.00.039.754 I print_info: n_embd_k_gqa     = 2048
0.00.039.755 I print_info: n_embd_v_gqa     = 2048
0.00.039.755 I print_info: f_norm_eps       = 1.0e-05
0.00.039.756 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.756 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.756 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.759 I print_info: f_logit_scale    = 0.0e+00
0.00.039.760 I print_info: n_ff             = 8192
0.00.039.760 I print_info: n_expert         = 0
0.00.039.760 I print_info: n_expert_used    = 0
0.00.039.760 I print_info: causal attn      = 1
0.00.039.760 I print_info: pooling type     = 0
0.00.039.760 I print_info: rope type        = 2
0.00.039.761 I print_info: rope scaling     = linear
0.00.039.761 I print_info: freq_base_train  = 10000.0
0.00.039.761 I print_info: freq_scale_train = 1
0.00.039.761 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.762 I print_info: rope_finetuned   = unknown
0.00.039.762 I print_info: ssm_d_conv       = 0
0.00.039.762 I print_info: ssm_d_inner      = 0
0.00.039.762 I print_info: ssm_d_state      = 0
0.00.039.762 I print_info: ssm_dt_rank      = 0
0.00.039.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.763 I print_info: model type       = 1.4B
0.00.039.763 I print_info: model params     = 1.41 B
0.00.039.763 I print_info: general.name     = 1.4B
0.00.039.764 I print_info: vocab type       = BPE
0.00.039.764 I print_info: n_vocab          = 50304
0.00.039.764 I print_info: n_merges         = 50009
0.00.039.764 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: LF token         = 187 ''
0.00.039.765 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.765 I print_info: max token length = 1024
0.00.039.767 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.355.873 I load_tensors: offloading 24 repeating layers to GPU
0.00.355.884 I load_tensors: offloading output layer to GPU
0.00.355.885 I load_tensors: offloaded 25/25 layers to GPU
0.00.355.912 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.355.914 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.357.289 I llama_init_from_model: n_seq_max     = 1
0.00.357.294 I llama_init_from_model: n_ctx         = 128
0.00.357.295 I llama_init_from_model: n_ctx_per_seq = 128
0.00.357.295 I llama_init_from_model: n_batch       = 128
0.00.357.296 I llama_init_from_model: n_ubatch      = 128
0.00.357.296 I llama_init_from_model: flash_attn    = 0
0.00.357.297 I llama_init_from_model: freq_base     = 10000.0
0.00.357.298 I llama_init_from_model: freq_scale    = 1
0.00.357.298 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.357.312 I ggml_metal_init: allocating
0.00.357.370 I ggml_metal_init: found device: Apple M4
0.00.357.384 I ggml_metal_init: picking default device: Apple M4
0.00.359.285 I ggml_metal_init: using embedded metal library
0.00.364.816 I ggml_metal_init: GPU name:   Apple M4
0.00.364.829 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.364.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.364.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.364.831 I ggml_metal_init: simdgroup reduction   = true
0.00.364.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.364.831 I ggml_metal_init: has residency sets    = true
0.00.364.832 I ggml_metal_init: has bfloat            = true
0.00.364.832 I ggml_metal_init: use bfloat            = true
0.00.364.834 I ggml_metal_init: hasUnifiedMemory      = true
0.00.364.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.386.368 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.390.265 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.390.273 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.390.333 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.394.112 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.394.114 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.394.115 I llama_init_from_model: graph nodes  = 967
0.00.394.115 I llama_init_from_model: graph splits = 2
0.00.394.118 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.394.119 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.425.920 I 
0.00.425.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.003 I perplexity: tokenizing the input ..
0.00.432.627 I perplexity: tokenization took 6.622 ms
0.00.432.631 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.564.052 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.565.391 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.565.415 I llama_perf_context_print:        load time =     416.05 ms
0.00.565.416 I llama_perf_context_print: prompt eval time =     131.18 ms /   128 tokens (    1.02 ms per token,   975.77 tokens per second)
0.00.565.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.565.417 I llama_perf_context_print:       total time =     139.50 ms /   129 tokens
0.00.565.838 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.080s
sys	0m0.105s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.631 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.641 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.641 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.642 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.642 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.643 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.646 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.647 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.647 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.476 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.267 I llama_model_loader: - type  f32:  194 tensors
0.00.025.268 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.268 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.268 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.268 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.269 I print_info: file format = GGUF V3 (latest)
0.00.025.269 I print_info: file type   = Q3_K - Medium
0.00.025.270 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.980 I load: special tokens cache size = 25
0.00.038.885 I load: token to piece cache size = 0.2984 MB
0.00.038.887 I print_info: arch             = gptneox
0.00.038.888 I print_info: vocab_only       = 0
0.00.038.888 I print_info: n_ctx_train      = 2048
0.00.038.888 I print_info: n_embd           = 2048
0.00.038.888 I print_info: n_layer          = 24
0.00.038.891 I print_info: n_head           = 16
0.00.038.892 I print_info: n_head_kv        = 16
0.00.038.892 I print_info: n_rot            = 32
0.00.038.892 I print_info: n_swa            = 0
0.00.038.892 I print_info: n_embd_head_k    = 128
0.00.038.894 I print_info: n_embd_head_v    = 128
0.00.038.894 I print_info: n_gqa            = 1
0.00.038.895 I print_info: n_embd_k_gqa     = 2048
0.00.038.896 I print_info: n_embd_v_gqa     = 2048
0.00.038.896 I print_info: f_norm_eps       = 1.0e-05
0.00.038.897 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.897 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.897 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.897 I print_info: f_logit_scale    = 0.0e+00
0.00.038.898 I print_info: n_ff             = 8192
0.00.038.898 I print_info: n_expert         = 0
0.00.038.898 I print_info: n_expert_used    = 0
0.00.038.900 I print_info: causal attn      = 1
0.00.038.901 I print_info: pooling type     = 0
0.00.038.902 I print_info: rope type        = 2
0.00.038.902 I print_info: rope scaling     = linear
0.00.038.902 I print_info: freq_base_train  = 10000.0
0.00.038.903 I print_info: freq_scale_train = 1
0.00.038.904 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.905 I print_info: rope_finetuned   = unknown
0.00.038.905 I print_info: ssm_d_conv       = 0
0.00.038.905 I print_info: ssm_d_inner      = 0
0.00.038.905 I print_info: ssm_d_state      = 0
0.00.038.905 I print_info: ssm_dt_rank      = 0
0.00.038.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.906 I print_info: model type       = 1.4B
0.00.038.907 I print_info: model params     = 1.41 B
0.00.038.907 I print_info: general.name     = 1.4B
0.00.038.908 I print_info: vocab type       = BPE
0.00.038.908 I print_info: n_vocab          = 50304
0.00.038.908 I print_info: n_merges         = 50009
0.00.038.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.909 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.910 I print_info: LF token         = 187 ''
0.00.038.911 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.911 I print_info: max token length = 1024
0.00.038.911 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.770 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.779 I load_tensors: offloading output layer to GPU
0.00.440.780 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.811 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.815 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.536 I llama_init_from_model: n_seq_max     = 1
0.00.442.538 I llama_init_from_model: n_ctx         = 2048
0.00.442.539 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.442.540 I llama_init_from_model: n_batch       = 2048
0.00.442.540 I llama_init_from_model: n_ubatch      = 512
0.00.442.540 I llama_init_from_model: flash_attn    = 0
0.00.442.543 I llama_init_from_model: freq_base     = 10000.0
0.00.442.543 I llama_init_from_model: freq_scale    = 1
0.00.442.546 I ggml_metal_init: allocating
0.00.442.598 I ggml_metal_init: found device: Apple M4
0.00.442.611 I ggml_metal_init: picking default device: Apple M4
0.00.444.482 I ggml_metal_init: using embedded metal library
0.00.450.345 I ggml_metal_init: GPU name:   Apple M4
0.00.450.350 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.353 I ggml_metal_init: simdgroup reduction   = true
0.00.450.353 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.353 I ggml_metal_init: has residency sets    = true
0.00.450.354 I ggml_metal_init: has bfloat            = true
0.00.450.354 I ggml_metal_init: use bfloat            = true
0.00.450.355 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.357 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.469.362 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.526.745 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.526.751 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.526.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.531.027 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.531.029 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.531.029 I llama_init_from_model: graph nodes  = 967
0.00.531.030 I llama_init_from_model: graph splits = 2
0.00.531.035 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.531.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.531.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.349 I main: llama threadpool init, n_threads = 4
0.00.587.393 I 
0.00.587.411 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.411 I 
0.00.587.588 I sampler seed: 1234
0.00.587.593 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.587.661 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.587.672 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.587.672 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.331.984 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.01.331.985 I llama_perf_context_print:        load time =     577.89 ms
0.01.331.986 I llama_perf_context_print: prompt eval time =      50.01 ms /     7 tokens (    7.14 ms per token,   139.97 tokens per second)
0.01.331.986 I llama_perf_context_print:        eval time =     691.40 ms /    63 runs   (   10.97 ms per token,    91.12 tokens per second)
0.01.331.987 I llama_perf_context_print:       total time =     745.34 ms /    70 tokens
0.01.332.259 I ggml_metal_free: deallocating

real	0m1.348s
user	0m0.110s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.832 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.667 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.674 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.690 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.690 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.578 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.328 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.329 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.330 I llama_model_loader: - type  f32:  194 tensors
0.00.024.331 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.331 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.331 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.331 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.332 I print_info: file format = GGUF V3 (latest)
0.00.024.333 I print_info: file type   = Q3_K - Medium
0.00.024.334 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.328 I load: special tokens cache size = 25
0.00.038.346 I load: token to piece cache size = 0.2984 MB
0.00.038.350 I print_info: arch             = gptneox
0.00.038.350 I print_info: vocab_only       = 0
0.00.038.350 I print_info: n_ctx_train      = 2048
0.00.038.351 I print_info: n_embd           = 2048
0.00.038.351 I print_info: n_layer          = 24
0.00.038.355 I print_info: n_head           = 16
0.00.038.355 I print_info: n_head_kv        = 16
0.00.038.355 I print_info: n_rot            = 32
0.00.038.356 I print_info: n_swa            = 0
0.00.038.356 I print_info: n_embd_head_k    = 128
0.00.038.356 I print_info: n_embd_head_v    = 128
0.00.038.357 I print_info: n_gqa            = 1
0.00.038.357 I print_info: n_embd_k_gqa     = 2048
0.00.038.358 I print_info: n_embd_v_gqa     = 2048
0.00.038.359 I print_info: f_norm_eps       = 1.0e-05
0.00.038.359 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.359 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.360 I print_info: f_logit_scale    = 0.0e+00
0.00.038.360 I print_info: n_ff             = 8192
0.00.038.361 I print_info: n_expert         = 0
0.00.038.361 I print_info: n_expert_used    = 0
0.00.038.361 I print_info: causal attn      = 1
0.00.038.361 I print_info: pooling type     = 0
0.00.038.361 I print_info: rope type        = 2
0.00.038.361 I print_info: rope scaling     = linear
0.00.038.362 I print_info: freq_base_train  = 10000.0
0.00.038.362 I print_info: freq_scale_train = 1
0.00.038.362 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.362 I print_info: rope_finetuned   = unknown
0.00.038.363 I print_info: ssm_d_conv       = 0
0.00.038.363 I print_info: ssm_d_inner      = 0
0.00.038.363 I print_info: ssm_d_state      = 0
0.00.038.363 I print_info: ssm_dt_rank      = 0
0.00.038.363 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.363 I print_info: model type       = 1.4B
0.00.038.363 I print_info: model params     = 1.41 B
0.00.038.364 I print_info: general.name     = 1.4B
0.00.038.364 I print_info: vocab type       = BPE
0.00.038.364 I print_info: n_vocab          = 50304
0.00.038.365 I print_info: n_merges         = 50009
0.00.038.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.365 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: LF token         = 187 ''
0.00.038.366 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.366 I print_info: max token length = 1024
0.00.038.367 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.438.870 I load_tensors: offloading 24 repeating layers to GPU
0.00.438.885 I load_tensors: offloading output layer to GPU
0.00.438.886 I load_tensors: offloaded 25/25 layers to GPU
0.00.438.921 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.438.923 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.440.598 I llama_init_from_model: n_seq_max     = 1
0.00.440.600 I llama_init_from_model: n_ctx         = 128
0.00.440.601 I llama_init_from_model: n_ctx_per_seq = 128
0.00.440.601 I llama_init_from_model: n_batch       = 128
0.00.440.602 I llama_init_from_model: n_ubatch      = 128
0.00.440.602 I llama_init_from_model: flash_attn    = 0
0.00.440.604 I llama_init_from_model: freq_base     = 10000.0
0.00.440.605 I llama_init_from_model: freq_scale    = 1
0.00.440.605 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.440.611 I ggml_metal_init: allocating
0.00.440.695 I ggml_metal_init: found device: Apple M4
0.00.440.709 I ggml_metal_init: picking default device: Apple M4
0.00.442.616 I ggml_metal_init: using embedded metal library
0.00.448.006 I ggml_metal_init: GPU name:   Apple M4
0.00.448.019 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.448.020 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.448.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.448.021 I ggml_metal_init: simdgroup reduction   = true
0.00.448.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.448.022 I ggml_metal_init: has residency sets    = true
0.00.448.022 I ggml_metal_init: has bfloat            = true
0.00.448.023 I ggml_metal_init: use bfloat            = true
0.00.448.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.448.029 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.508 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.472.035 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.472.046 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.472.100 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.475.394 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.475.396 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.475.397 I llama_init_from_model: graph nodes  = 967
0.00.475.397 I llama_init_from_model: graph splits = 2
0.00.475.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.475.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.092 I 
0.00.506.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.152 I perplexity: tokenizing the input ..
0.00.513.520 I perplexity: tokenization took 7.366 ms
0.00.513.527 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.771 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.657.106 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.657.134 I llama_perf_context_print:        load time =     497.24 ms
0.00.657.136 I llama_perf_context_print: prompt eval time =     141.35 ms /   128 tokens (    1.10 ms per token,   905.52 tokens per second)
0.00.657.137 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.657.137 I llama_perf_context_print:       total time =     151.05 ms /   129 tokens
0.00.657.512 I ggml_metal_free: deallocating

real	0m0.671s
user	0m0.081s
sys	0m0.109s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.397 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.143 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.154 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.156 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.160 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.864 I llama_model_loader: - type  f32:  194 tensors
0.00.025.865 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.865 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.865 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.866 I print_info: file format = GGUF V3 (latest)
0.00.025.866 I print_info: file type   = Q4_K - Medium
0.00.025.866 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.660 I load: special tokens cache size = 25
0.00.039.715 I load: token to piece cache size = 0.2984 MB
0.00.039.718 I print_info: arch             = gptneox
0.00.039.718 I print_info: vocab_only       = 0
0.00.039.718 I print_info: n_ctx_train      = 2048
0.00.039.718 I print_info: n_embd           = 2048
0.00.039.718 I print_info: n_layer          = 24
0.00.039.721 I print_info: n_head           = 16
0.00.039.721 I print_info: n_head_kv        = 16
0.00.039.722 I print_info: n_rot            = 32
0.00.039.722 I print_info: n_swa            = 0
0.00.039.722 I print_info: n_embd_head_k    = 128
0.00.039.722 I print_info: n_embd_head_v    = 128
0.00.039.725 I print_info: n_gqa            = 1
0.00.039.726 I print_info: n_embd_k_gqa     = 2048
0.00.039.726 I print_info: n_embd_v_gqa     = 2048
0.00.039.727 I print_info: f_norm_eps       = 1.0e-05
0.00.039.727 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.728 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.728 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.728 I print_info: f_logit_scale    = 0.0e+00
0.00.039.729 I print_info: n_ff             = 8192
0.00.039.729 I print_info: n_expert         = 0
0.00.039.729 I print_info: n_expert_used    = 0
0.00.039.729 I print_info: causal attn      = 1
0.00.039.729 I print_info: pooling type     = 0
0.00.039.729 I print_info: rope type        = 2
0.00.039.730 I print_info: rope scaling     = linear
0.00.039.730 I print_info: freq_base_train  = 10000.0
0.00.039.731 I print_info: freq_scale_train = 1
0.00.039.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.731 I print_info: rope_finetuned   = unknown
0.00.039.731 I print_info: ssm_d_conv       = 0
0.00.039.731 I print_info: ssm_d_inner      = 0
0.00.039.731 I print_info: ssm_d_state      = 0
0.00.039.732 I print_info: ssm_dt_rank      = 0
0.00.039.732 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.732 I print_info: model type       = 1.4B
0.00.039.732 I print_info: model params     = 1.41 B
0.00.039.733 I print_info: general.name     = 1.4B
0.00.039.733 I print_info: vocab type       = BPE
0.00.039.733 I print_info: n_vocab          = 50304
0.00.039.734 I print_info: n_merges         = 50009
0.00.039.734 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.736 I print_info: LF token         = 187 ''
0.00.039.736 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.737 I print_info: max token length = 1024
0.00.039.737 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.529.895 I load_tensors: offloading 24 repeating layers to GPU
0.00.529.910 I load_tensors: offloading output layer to GPU
0.00.529.911 I load_tensors: offloaded 25/25 layers to GPU
0.00.529.947 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.529.948 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.531.409 I llama_init_from_model: n_seq_max     = 1
0.00.531.411 I llama_init_from_model: n_ctx         = 2048
0.00.531.412 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.531.413 I llama_init_from_model: n_batch       = 2048
0.00.531.413 I llama_init_from_model: n_ubatch      = 512
0.00.531.413 I llama_init_from_model: flash_attn    = 0
0.00.531.416 I llama_init_from_model: freq_base     = 10000.0
0.00.531.416 I llama_init_from_model: freq_scale    = 1
0.00.531.418 I ggml_metal_init: allocating
0.00.531.492 I ggml_metal_init: found device: Apple M4
0.00.531.505 I ggml_metal_init: picking default device: Apple M4
0.00.533.381 I ggml_metal_init: using embedded metal library
0.00.539.943 I ggml_metal_init: GPU name:   Apple M4
0.00.539.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.539.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.539.950 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.539.950 I ggml_metal_init: simdgroup reduction   = true
0.00.539.951 I ggml_metal_init: simdgroup matrix mul. = true
0.00.539.951 I ggml_metal_init: has residency sets    = true
0.00.539.951 I ggml_metal_init: has bfloat            = true
0.00.539.952 I ggml_metal_init: use bfloat            = true
0.00.539.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.539.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.558.543 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.063 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.616.073 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.616.121 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.944 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.620.946 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.620.947 I llama_init_from_model: graph nodes  = 967
0.00.620.947 I llama_init_from_model: graph splits = 2
0.00.620.954 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.621.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.621.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.221 I main: llama threadpool init, n_threads = 4
0.00.676.262 I 
0.00.676.278 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.278 I 
0.00.676.435 I sampler seed: 1234
0.00.676.439 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.457 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.458 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.458 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.806 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50141.24 tokens per second)
0.01.426.807 I llama_perf_context_print:        load time =     666.06 ms
0.01.426.809 I llama_perf_context_print: prompt eval time =      47.14 ms /     7 tokens (    6.73 ms per token,   148.50 tokens per second)
0.01.426.811 I llama_perf_context_print:        eval time =     700.28 ms /    63 runs   (   11.12 ms per token,    89.96 tokens per second)
0.01.426.811 I llama_perf_context_print:       total time =     751.35 ms /    70 tokens
0.01.427.031 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.109s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.128 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.131 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.133 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.134 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.904 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.905 I llama_model_loader: - type  f32:  194 tensors
0.00.024.906 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.906 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.906 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.907 I print_info: file format = GGUF V3 (latest)
0.00.024.909 I print_info: file type   = Q4_K - Medium
0.00.024.910 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.180 I load: special tokens cache size = 25
0.00.039.432 I load: token to piece cache size = 0.2984 MB
0.00.039.436 I print_info: arch             = gptneox
0.00.039.437 I print_info: vocab_only       = 0
0.00.039.437 I print_info: n_ctx_train      = 2048
0.00.039.437 I print_info: n_embd           = 2048
0.00.039.437 I print_info: n_layer          = 24
0.00.039.442 I print_info: n_head           = 16
0.00.039.443 I print_info: n_head_kv        = 16
0.00.039.443 I print_info: n_rot            = 32
0.00.039.443 I print_info: n_swa            = 0
0.00.039.443 I print_info: n_embd_head_k    = 128
0.00.039.445 I print_info: n_embd_head_v    = 128
0.00.039.445 I print_info: n_gqa            = 1
0.00.039.446 I print_info: n_embd_k_gqa     = 2048
0.00.039.447 I print_info: n_embd_v_gqa     = 2048
0.00.039.447 I print_info: f_norm_eps       = 1.0e-05
0.00.039.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.448 I print_info: f_logit_scale    = 0.0e+00
0.00.039.449 I print_info: n_ff             = 8192
0.00.039.449 I print_info: n_expert         = 0
0.00.039.449 I print_info: n_expert_used    = 0
0.00.039.449 I print_info: causal attn      = 1
0.00.039.449 I print_info: pooling type     = 0
0.00.039.449 I print_info: rope type        = 2
0.00.039.450 I print_info: rope scaling     = linear
0.00.039.450 I print_info: freq_base_train  = 10000.0
0.00.039.450 I print_info: freq_scale_train = 1
0.00.039.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.451 I print_info: rope_finetuned   = unknown
0.00.039.451 I print_info: ssm_d_conv       = 0
0.00.039.451 I print_info: ssm_d_inner      = 0
0.00.039.451 I print_info: ssm_d_state      = 0
0.00.039.451 I print_info: ssm_dt_rank      = 0
0.00.039.451 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.451 I print_info: model type       = 1.4B
0.00.039.452 I print_info: model params     = 1.41 B
0.00.039.452 I print_info: general.name     = 1.4B
0.00.039.452 I print_info: vocab type       = BPE
0.00.039.453 I print_info: n_vocab          = 50304
0.00.039.453 I print_info: n_merges         = 50009
0.00.039.453 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.454 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.454 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.454 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.454 I print_info: LF token         = 187 ''
0.00.039.454 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.455 I print_info: max token length = 1024
0.00.039.455 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.740 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.755 I load_tensors: offloading output layer to GPU
0.00.511.755 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.791 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.793 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.513.517 I llama_init_from_model: n_seq_max     = 1
0.00.513.519 I llama_init_from_model: n_ctx         = 128
0.00.513.520 I llama_init_from_model: n_ctx_per_seq = 128
0.00.513.521 I llama_init_from_model: n_batch       = 128
0.00.513.521 I llama_init_from_model: n_ubatch      = 128
0.00.513.521 I llama_init_from_model: flash_attn    = 0
0.00.513.524 I llama_init_from_model: freq_base     = 10000.0
0.00.513.525 I llama_init_from_model: freq_scale    = 1
0.00.513.525 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.513.529 I ggml_metal_init: allocating
0.00.513.601 I ggml_metal_init: found device: Apple M4
0.00.513.616 I ggml_metal_init: picking default device: Apple M4
0.00.515.434 I ggml_metal_init: using embedded metal library
0.00.521.404 I ggml_metal_init: GPU name:   Apple M4
0.00.521.411 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.412 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.413 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.413 I ggml_metal_init: simdgroup reduction   = true
0.00.521.414 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.414 I ggml_metal_init: has residency sets    = true
0.00.521.414 I ggml_metal_init: has bfloat            = true
0.00.521.414 I ggml_metal_init: use bfloat            = true
0.00.521.416 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.937 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.461 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.468 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.513 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.748 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.750 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.750 I llama_init_from_model: graph nodes  = 967
0.00.546.751 I llama_init_from_model: graph splits = 2
0.00.546.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.754 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.573.804 I 
0.00.573.860 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.573.867 I perplexity: tokenizing the input ..
0.00.580.732 I perplexity: tokenization took 6.861 ms
0.00.580.741 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.387 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.715.727 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.715.748 I llama_perf_context_print:        load time =     564.99 ms
0.00.715.749 I llama_perf_context_print: prompt eval time =     132.76 ms /   128 tokens (    1.04 ms per token,   964.12 tokens per second)
0.00.715.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.750 I llama_perf_context_print:       total time =     141.95 ms /   129 tokens
0.00.716.122 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.079s
sys	0m0.120s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.011.170 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.656 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.669 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.670 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.670 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.671 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.671 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.671 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.672 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.672 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.674 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.533 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.569 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.341 I llama_model_loader: - type  f32:  194 tensors
0.00.027.341 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.341 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.342 I print_info: file format = GGUF V3 (latest)
0.00.027.342 I print_info: file type   = Q5_K - Medium
0.00.027.343 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.402 I load: special tokens cache size = 25
0.00.041.416 I load: token to piece cache size = 0.2984 MB
0.00.041.419 I print_info: arch             = gptneox
0.00.041.420 I print_info: vocab_only       = 0
0.00.041.420 I print_info: n_ctx_train      = 2048
0.00.041.420 I print_info: n_embd           = 2048
0.00.041.420 I print_info: n_layer          = 24
0.00.041.423 I print_info: n_head           = 16
0.00.041.424 I print_info: n_head_kv        = 16
0.00.041.424 I print_info: n_rot            = 32
0.00.041.424 I print_info: n_swa            = 0
0.00.041.424 I print_info: n_embd_head_k    = 128
0.00.041.424 I print_info: n_embd_head_v    = 128
0.00.041.425 I print_info: n_gqa            = 1
0.00.041.426 I print_info: n_embd_k_gqa     = 2048
0.00.041.426 I print_info: n_embd_v_gqa     = 2048
0.00.041.427 I print_info: f_norm_eps       = 1.0e-05
0.00.041.427 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.428 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.428 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.428 I print_info: f_logit_scale    = 0.0e+00
0.00.041.429 I print_info: n_ff             = 8192
0.00.041.429 I print_info: n_expert         = 0
0.00.041.429 I print_info: n_expert_used    = 0
0.00.041.429 I print_info: causal attn      = 1
0.00.041.429 I print_info: pooling type     = 0
0.00.041.429 I print_info: rope type        = 2
0.00.041.430 I print_info: rope scaling     = linear
0.00.041.431 I print_info: freq_base_train  = 10000.0
0.00.041.431 I print_info: freq_scale_train = 1
0.00.041.431 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.431 I print_info: rope_finetuned   = unknown
0.00.041.432 I print_info: ssm_d_conv       = 0
0.00.041.432 I print_info: ssm_d_inner      = 0
0.00.041.433 I print_info: ssm_d_state      = 0
0.00.041.433 I print_info: ssm_dt_rank      = 0
0.00.041.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.434 I print_info: model type       = 1.4B
0.00.041.434 I print_info: model params     = 1.41 B
0.00.041.434 I print_info: general.name     = 1.4B
0.00.041.435 I print_info: vocab type       = BPE
0.00.041.435 I print_info: n_vocab          = 50304
0.00.041.435 I print_info: n_merges         = 50009
0.00.041.436 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.436 I print_info: LF token         = 187 ''
0.00.041.437 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.437 I print_info: max token length = 1024
0.00.041.437 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.377 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.390 I load_tensors: offloading output layer to GPU
0.00.615.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.422 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.615.426 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.616.798 I llama_init_from_model: n_seq_max     = 1
0.00.616.804 I llama_init_from_model: n_ctx         = 2048
0.00.616.804 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.805 I llama_init_from_model: n_batch       = 2048
0.00.616.805 I llama_init_from_model: n_ubatch      = 512
0.00.616.806 I llama_init_from_model: flash_attn    = 0
0.00.616.807 I llama_init_from_model: freq_base     = 10000.0
0.00.616.807 I llama_init_from_model: freq_scale    = 1
0.00.616.810 I ggml_metal_init: allocating
0.00.616.858 I ggml_metal_init: found device: Apple M4
0.00.616.871 I ggml_metal_init: picking default device: Apple M4
0.00.618.455 I ggml_metal_init: using embedded metal library
0.00.624.925 I ggml_metal_init: GPU name:   Apple M4
0.00.624.929 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.930 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.931 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.932 I ggml_metal_init: simdgroup reduction   = true
0.00.624.932 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.932 I ggml_metal_init: has residency sets    = true
0.00.624.932 I ggml_metal_init: has bfloat            = true
0.00.624.933 I ggml_metal_init: use bfloat            = true
0.00.624.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.554 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.786 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.695.793 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.695.822 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.207 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.208 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.209 I llama_init_from_model: graph nodes  = 967
0.00.700.209 I llama_init_from_model: graph splits = 2
0.00.700.216 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.350 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.763.868 I main: llama threadpool init, n_threads = 4
0.00.763.917 I 
0.00.763.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.934 I 
0.00.764.100 I sampler seed: 1234
0.00.764.104 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.115 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.115 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.115 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.604.956 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.604.959 I llama_perf_context_print:        load time =     751.95 ms
0.01.604.960 I llama_perf_context_print: prompt eval time =      52.70 ms /     7 tokens (    7.53 ms per token,   132.84 tokens per second)
0.01.604.961 I llama_perf_context_print:        eval time =     785.22 ms /    63 runs   (   12.46 ms per token,    80.23 tokens per second)
0.01.604.961 I llama_perf_context_print:       total time =     841.84 ms /    70 tokens
0.01.605.187 I ggml_metal_free: deallocating

real	0m1.624s
user	0m0.109s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.604 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.611 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.614 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.615 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.616 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.617 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.515 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.423 I llama_model_loader: - type  f32:  194 tensors
0.00.025.423 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.424 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.424 I print_info: file format = GGUF V3 (latest)
0.00.025.425 I print_info: file type   = Q5_K - Medium
0.00.025.426 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.923 I load: special tokens cache size = 25
0.00.040.115 I load: token to piece cache size = 0.2984 MB
0.00.040.119 I print_info: arch             = gptneox
0.00.040.119 I print_info: vocab_only       = 0
0.00.040.119 I print_info: n_ctx_train      = 2048
0.00.040.120 I print_info: n_embd           = 2048
0.00.040.120 I print_info: n_layer          = 24
0.00.040.124 I print_info: n_head           = 16
0.00.040.125 I print_info: n_head_kv        = 16
0.00.040.125 I print_info: n_rot            = 32
0.00.040.125 I print_info: n_swa            = 0
0.00.040.125 I print_info: n_embd_head_k    = 128
0.00.040.126 I print_info: n_embd_head_v    = 128
0.00.040.126 I print_info: n_gqa            = 1
0.00.040.127 I print_info: n_embd_k_gqa     = 2048
0.00.040.128 I print_info: n_embd_v_gqa     = 2048
0.00.040.128 I print_info: f_norm_eps       = 1.0e-05
0.00.040.129 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.129 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.129 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.129 I print_info: f_logit_scale    = 0.0e+00
0.00.040.130 I print_info: n_ff             = 8192
0.00.040.130 I print_info: n_expert         = 0
0.00.040.130 I print_info: n_expert_used    = 0
0.00.040.130 I print_info: causal attn      = 1
0.00.040.130 I print_info: pooling type     = 0
0.00.040.131 I print_info: rope type        = 2
0.00.040.131 I print_info: rope scaling     = linear
0.00.040.131 I print_info: freq_base_train  = 10000.0
0.00.040.131 I print_info: freq_scale_train = 1
0.00.040.132 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.132 I print_info: rope_finetuned   = unknown
0.00.040.135 I print_info: ssm_d_conv       = 0
0.00.040.135 I print_info: ssm_d_inner      = 0
0.00.040.135 I print_info: ssm_d_state      = 0
0.00.040.135 I print_info: ssm_dt_rank      = 0
0.00.040.135 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.135 I print_info: model type       = 1.4B
0.00.040.136 I print_info: model params     = 1.41 B
0.00.040.137 I print_info: general.name     = 1.4B
0.00.040.138 I print_info: vocab type       = BPE
0.00.040.138 I print_info: n_vocab          = 50304
0.00.040.138 I print_info: n_merges         = 50009
0.00.040.138 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.138 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.138 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.139 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.139 I print_info: LF token         = 187 ''
0.00.040.139 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.139 I print_info: max token length = 1024
0.00.040.140 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.881 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.893 I load_tensors: offloading output layer to GPU
0.00.617.893 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.920 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.617.921 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.619.519 I llama_init_from_model: n_seq_max     = 1
0.00.619.524 I llama_init_from_model: n_ctx         = 128
0.00.619.524 I llama_init_from_model: n_ctx_per_seq = 128
0.00.619.525 I llama_init_from_model: n_batch       = 128
0.00.619.525 I llama_init_from_model: n_ubatch      = 128
0.00.619.526 I llama_init_from_model: flash_attn    = 0
0.00.619.527 I llama_init_from_model: freq_base     = 10000.0
0.00.619.527 I llama_init_from_model: freq_scale    = 1
0.00.619.528 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.619.531 I ggml_metal_init: allocating
0.00.619.609 I ggml_metal_init: found device: Apple M4
0.00.619.622 I ggml_metal_init: picking default device: Apple M4
0.00.621.411 I ggml_metal_init: using embedded metal library
0.00.628.164 I ggml_metal_init: GPU name:   Apple M4
0.00.628.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.174 I ggml_metal_init: simdgroup reduction   = true
0.00.628.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.174 I ggml_metal_init: has residency sets    = true
0.00.628.175 I ggml_metal_init: has bfloat            = true
0.00.628.175 I ggml_metal_init: use bfloat            = true
0.00.628.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.826 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.649.373 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.423 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.652.590 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.652.592 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.652.593 I llama_init_from_model: graph nodes  = 967
0.00.652.593 I llama_init_from_model: graph splits = 2
0.00.652.596 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.652.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.387 I 
0.00.690.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.467 I perplexity: tokenizing the input ..
0.00.697.264 I perplexity: tokenization took 6.795 ms
0.00.697.268 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.842.695 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.844.000 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.844.028 I llama_perf_context_print:        load time =     680.52 ms
0.00.844.029 I llama_perf_context_print: prompt eval time =     145.18 ms /   128 tokens (    1.13 ms per token,   881.65 tokens per second)
0.00.844.030 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.031 I llama_perf_context_print:       total time =     153.64 ms /   129 tokens
0.00.844.482 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.079s
sys	0m0.152s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.360 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.360 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.361 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.362 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.363 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.798 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.798 I llama_model_loader: - type  f32:  194 tensors
0.00.024.799 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.799 I print_info: file format = GGUF V3 (latest)
0.00.024.800 I print_info: file type   = Q6_K
0.00.024.801 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.485 I load: special tokens cache size = 25
0.00.038.506 I load: token to piece cache size = 0.2984 MB
0.00.038.509 I print_info: arch             = gptneox
0.00.038.509 I print_info: vocab_only       = 0
0.00.038.509 I print_info: n_ctx_train      = 2048
0.00.038.510 I print_info: n_embd           = 2048
0.00.038.510 I print_info: n_layer          = 24
0.00.038.513 I print_info: n_head           = 16
0.00.038.513 I print_info: n_head_kv        = 16
0.00.038.513 I print_info: n_rot            = 32
0.00.038.514 I print_info: n_swa            = 0
0.00.038.514 I print_info: n_embd_head_k    = 128
0.00.038.514 I print_info: n_embd_head_v    = 128
0.00.038.515 I print_info: n_gqa            = 1
0.00.038.515 I print_info: n_embd_k_gqa     = 2048
0.00.038.516 I print_info: n_embd_v_gqa     = 2048
0.00.038.517 I print_info: f_norm_eps       = 1.0e-05
0.00.038.517 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.517 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.517 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.518 I print_info: f_logit_scale    = 0.0e+00
0.00.038.518 I print_info: n_ff             = 8192
0.00.038.519 I print_info: n_expert         = 0
0.00.038.519 I print_info: n_expert_used    = 0
0.00.038.519 I print_info: causal attn      = 1
0.00.038.519 I print_info: pooling type     = 0
0.00.038.519 I print_info: rope type        = 2
0.00.038.519 I print_info: rope scaling     = linear
0.00.038.520 I print_info: freq_base_train  = 10000.0
0.00.038.520 I print_info: freq_scale_train = 1
0.00.038.520 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.521 I print_info: rope_finetuned   = unknown
0.00.038.521 I print_info: ssm_d_conv       = 0
0.00.038.521 I print_info: ssm_d_inner      = 0
0.00.038.521 I print_info: ssm_d_state      = 0
0.00.038.521 I print_info: ssm_dt_rank      = 0
0.00.038.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.522 I print_info: model type       = 1.4B
0.00.038.522 I print_info: model params     = 1.41 B
0.00.038.524 I print_info: general.name     = 1.4B
0.00.038.525 I print_info: vocab type       = BPE
0.00.038.525 I print_info: n_vocab          = 50304
0.00.038.525 I print_info: n_merges         = 50009
0.00.038.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.525 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.526 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.526 I print_info: LF token         = 187 ''
0.00.038.526 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.526 I print_info: max token length = 1024
0.00.038.531 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.310 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.322 I load_tensors: offloading output layer to GPU
0.00.649.323 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.354 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.356 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.650.902 I llama_init_from_model: n_seq_max     = 1
0.00.650.905 I llama_init_from_model: n_ctx         = 2048
0.00.650.906 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.906 I llama_init_from_model: n_batch       = 2048
0.00.650.906 I llama_init_from_model: n_ubatch      = 512
0.00.650.907 I llama_init_from_model: flash_attn    = 0
0.00.650.908 I llama_init_from_model: freq_base     = 10000.0
0.00.650.908 I llama_init_from_model: freq_scale    = 1
0.00.650.910 I ggml_metal_init: allocating
0.00.650.948 I ggml_metal_init: found device: Apple M4
0.00.650.958 I ggml_metal_init: picking default device: Apple M4
0.00.652.513 I ggml_metal_init: using embedded metal library
0.00.658.788 I ggml_metal_init: GPU name:   Apple M4
0.00.658.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.794 I ggml_metal_init: simdgroup reduction   = true
0.00.658.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.795 I ggml_metal_init: has residency sets    = true
0.00.658.795 I ggml_metal_init: has bfloat            = true
0.00.658.795 I ggml_metal_init: use bfloat            = true
0.00.658.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.831 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.047 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.055 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.094 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.641 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.643 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.643 I llama_init_from_model: graph nodes  = 967
0.00.740.643 I llama_init_from_model: graph splits = 2
0.00.740.650 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.173 I main: llama threadpool init, n_threads = 4
0.00.809.213 I 
0.00.809.227 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.227 I 
0.00.809.395 I sampler seed: 1234
0.00.809.399 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.409 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.410 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.410 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.682.682 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.682.682 I llama_perf_context_print:        load time =     799.66 ms
0.01.682.685 I llama_perf_context_print: prompt eval time =      57.59 ms /     7 tokens (    8.23 ms per token,   121.56 tokens per second)
0.01.682.686 I llama_perf_context_print:        eval time =     812.70 ms /    63 runs   (   12.90 ms per token,    77.52 tokens per second)
0.01.682.688 I llama_perf_context_print:       total time =     874.25 ms /    70 tokens
0.01.682.939 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.108s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4741 (9626d935) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.898 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.811 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.820 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.821 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.827 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.828 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.829 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.831 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.833 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.651 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.692 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.496 I llama_model_loader: - type  f32:  194 tensors
0.00.024.496 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.497 I print_info: file format = GGUF V3 (latest)
0.00.024.497 I print_info: file type   = Q6_K
0.00.024.498 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.746 I load: special tokens cache size = 25
0.00.038.782 I load: token to piece cache size = 0.2984 MB
0.00.038.785 I print_info: arch             = gptneox
0.00.038.785 I print_info: vocab_only       = 0
0.00.038.785 I print_info: n_ctx_train      = 2048
0.00.038.785 I print_info: n_embd           = 2048
0.00.038.786 I print_info: n_layer          = 24
0.00.038.789 I print_info: n_head           = 16
0.00.038.790 I print_info: n_head_kv        = 16
0.00.038.790 I print_info: n_rot            = 32
0.00.038.793 I print_info: n_swa            = 0
0.00.038.793 I print_info: n_embd_head_k    = 128
0.00.038.793 I print_info: n_embd_head_v    = 128
0.00.038.794 I print_info: n_gqa            = 1
0.00.038.795 I print_info: n_embd_k_gqa     = 2048
0.00.038.796 I print_info: n_embd_v_gqa     = 2048
0.00.038.796 I print_info: f_norm_eps       = 1.0e-05
0.00.038.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.797 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.797 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.798 I print_info: f_logit_scale    = 0.0e+00
0.00.038.799 I print_info: n_ff             = 8192
0.00.038.799 I print_info: n_expert         = 0
0.00.038.799 I print_info: n_expert_used    = 0
0.00.038.800 I print_info: causal attn      = 1
0.00.038.800 I print_info: pooling type     = 0
0.00.038.800 I print_info: rope type        = 2
0.00.038.800 I print_info: rope scaling     = linear
0.00.038.800 I print_info: freq_base_train  = 10000.0
0.00.038.801 I print_info: freq_scale_train = 1
0.00.038.801 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.801 I print_info: rope_finetuned   = unknown
0.00.038.801 I print_info: ssm_d_conv       = 0
0.00.038.801 I print_info: ssm_d_inner      = 0
0.00.038.801 I print_info: ssm_d_state      = 0
0.00.038.802 I print_info: ssm_dt_rank      = 0
0.00.038.802 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.802 I print_info: model type       = 1.4B
0.00.038.802 I print_info: model params     = 1.41 B
0.00.038.802 I print_info: general.name     = 1.4B
0.00.038.803 I print_info: vocab type       = BPE
0.00.038.803 I print_info: n_vocab          = 50304
0.00.038.803 I print_info: n_merges         = 50009
0.00.038.804 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.804 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.808 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.813 I print_info: LF token         = 187 ''
0.00.038.814 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.814 I print_info: max token length = 1024
0.00.038.814 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.884 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.890 I load_tensors: offloading output layer to GPU
0.00.603.891 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.916 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.603.919 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.605.363 I llama_init_from_model: n_seq_max     = 1
0.00.605.365 I llama_init_from_model: n_ctx         = 128
0.00.605.366 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.366 I llama_init_from_model: n_batch       = 128
0.00.605.366 I llama_init_from_model: n_ubatch      = 128
0.00.605.367 I llama_init_from_model: flash_attn    = 0
0.00.605.367 I llama_init_from_model: freq_base     = 10000.0
0.00.605.368 I llama_init_from_model: freq_scale    = 1
0.00.605.368 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.369 I ggml_metal_init: allocating
0.00.605.387 I ggml_metal_init: found device: Apple M4
0.00.605.395 I ggml_metal_init: picking default device: Apple M4
0.00.606.838 I ggml_metal_init: using embedded metal library
0.00.612.355 I ggml_metal_init: GPU name:   Apple M4
0.00.612.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.359 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.360 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.361 I ggml_metal_init: simdgroup reduction   = true
0.00.612.361 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.361 I ggml_metal_init: has residency sets    = true
0.00.612.361 I ggml_metal_init: has bfloat            = true
0.00.612.361 I ggml_metal_init: use bfloat            = true
0.00.612.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.364 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.387 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.911 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.631.918 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.631.981 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.087 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.089 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.089 I llama_init_from_model: graph nodes  = 967
0.00.635.089 I llama_init_from_model: graph splits = 2
0.00.635.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.877 I 
0.00.668.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.951 I perplexity: tokenizing the input ..
0.00.676.133 I perplexity: tokenization took 7.178 ms
0.00.676.141 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.745 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.810.082 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.810.106 I llama_perf_context_print:        load time =     659.96 ms
0.00.810.107 I llama_perf_context_print: prompt eval time =     131.69 ms /   128 tokens (    1.03 ms per token,   971.98 tokens per second)
0.00.810.108 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.108 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.810.514 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.077s
sys	0m0.136s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4741 (9626d935)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x132e07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x132e08340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x132e088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x132e08ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x132e09450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x132e09a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x132e09fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x132e0a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x132e0ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x132e0b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x132e0b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x132e0ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x132e0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x132e0cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x132e0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x132e0dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x132e0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x132e0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x132e0f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x132e0f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x132e10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x132e10780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x132e10ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x132e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x132e11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x132e12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x132e12730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x132e133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x132e138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x132e13ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x132e14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x132e14300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x132e14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x132e150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x132e15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x132e15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x132e15cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x132e16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x132e16610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x132e16ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x132e16f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x132e173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x132e17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x132e17d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x132e17ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x132e18600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x132e18c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x132e19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x132e19b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x132e1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x132e1a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x132e1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x132e1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x132e1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x132e1c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x132e1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x132e1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x132e1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x132e1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x132e1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x132e1de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x132e1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x132e1e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x132e1ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x132e1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x132e1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x132e1fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x132e1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x132e20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x132e207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x132e20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x132e21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x132e215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x132e21b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x132e22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x132e225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x132e22b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x132e23050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x132e235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x132e23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x132e24040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x132e24590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x132e24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x132e25030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x132e25580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x132e25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x132e26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x132e26570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x132e26ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x132e27010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x132e27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x132e27ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x132e28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x132e28550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x132e28aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x132e28ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x132e29540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x132e19220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x132e299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x132e2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x132e2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x132e2ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x132e2b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x132e2b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x132e2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x132e2c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x132e2c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x132e2cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x132e2d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x132e2d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x132e2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x132e2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x132e2e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x132e2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x132e2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x132e2f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x132e2f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x132e2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x132e30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x132e306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x132e30b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x132e31010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x132e314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x132e31950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x132e31df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x132e32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x132e32730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x132e32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x132e33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x132e33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x132e339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x132e33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x132e342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x132e34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x132e34c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x132e350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x132e35570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x132e35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x132e35eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x132e36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x132e367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x132e36c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x132e37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x132e375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x132e37a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x132e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x132e383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x132e38850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x132e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x132e39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x132e39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x132e39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x132e39f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x132e3a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x132e3a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x132e3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x132e3b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x132e3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x132e3bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x132e3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x132e3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x132e3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x132e3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x132e3d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x132e3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x132e3db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x132e3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x132e3e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x132e3e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x132e3ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x132e3f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x132e3f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x132e3fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x132e40090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x132e40530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x132e409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x132e40e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x132e41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x132e417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x132e41c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x132e420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x132e42590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x132e42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x132e42ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x132e43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x132e43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x132e43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x132e44150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x132e445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x132e44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x132e44f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x132e453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x132e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x132e45dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x132e46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x132e46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x132e46db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x132e47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x132e47680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x132e47c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x132e482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x132e48a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x132e48f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x132e491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x132e49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x132e49e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x132e4a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x132e4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x132e4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x132e4b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x132e4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x132e4c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x132e4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x132e4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x132e4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x132e4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x132e4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x132e4e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132e4e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132e4eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132e4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x132e4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132e4fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132e500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132e505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132e50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132e51090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132e515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x132e51b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132e52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132e525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132e52b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132e53070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132e535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132e53b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x132e54060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132e545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132e54b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132e55050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132e555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132e55af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132e56040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x132e56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132e56ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132e57030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132e57580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x132e57ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132e58020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132e58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x132e58ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132e59010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132e59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132e59ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x132e5a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x132e5a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x132e5aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x132e5aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x132e5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x132e5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x132e5bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x132e5c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x132e5ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x132e5cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x132e5d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x132e5da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x132e5dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x132e5e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x132e5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x132e5ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x132e5f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x132e5f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x132e5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x132e600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x132e60570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x132e60a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132e60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x132e61350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132e617f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x132e61c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132e62130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132e625d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132e62a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132e62fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132e636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x132e63e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132e64520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132e64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x132e64f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132e656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132e659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132e65fc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.743.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.244 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116304dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116305240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1163056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116305b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116305f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116306400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116306870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116306ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116307150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1163075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116307a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116308120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116308c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1163093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116309c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11630a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11630aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11630b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11630b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11630bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11630c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11630cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11630d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11630dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11630e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11630e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11630e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11630ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11630f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11630f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11630fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11630ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116310430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1163106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116310b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116310fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116311440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1163118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116311d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116312190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116312600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116312a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116312ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116313350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1163137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116313c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1163140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116314510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116314980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116314df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116315260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1163156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116315b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116315fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116316420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116316890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116316e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116317300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116317770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116317be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116318050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1163184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116318930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116318da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116319210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116319680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116319af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116319f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11631a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11631a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11631acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11631b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11631b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11631ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11631be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11631c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11631c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11631cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11631d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11631d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11631d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11631dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11631e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11631e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11631ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11631ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11631f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11631f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11631fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116320100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116320570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1163209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116320e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1163212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116321730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116321ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116322010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116322480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1163228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116322d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1163231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116323640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116323ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116323f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116324390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116324800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116324c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1163250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116325550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1163259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116325e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1163262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116326710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116326b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116326ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116327460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1163278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116327d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1163281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116328620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116328a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116328f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116329370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1163297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116329c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11632a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11632a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11632a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11632ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11632b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11632b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11632bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11632bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11632c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11632c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11632cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11632d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11632d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11632da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11632dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11632e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11632e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11632ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11632f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11632f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11632f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11632fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116330260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1163306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116330b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116330fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116331420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116331890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116331d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116332170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1163325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116332a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116332ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116333330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1163337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116333c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116334080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1163344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116334960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116334dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116335240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116335e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116336130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1163363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116336860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116336cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116337140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1163375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116337a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116337e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116338300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116338770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116338be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116339050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1163394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116339930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116339da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11633a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11633a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11633aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11633af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11633b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11633b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11633bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11633c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11633c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11633ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11633ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11633d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11633d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11633dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11633e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11633e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11633e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11633ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11633f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11633f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11633fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1163400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116340540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1163409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116340e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116341290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1163417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116341cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116342830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116342af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1163430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116343670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116343c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1163441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1163447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116344d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116345330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1163458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116345eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116346470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116346a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116346ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1163475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116347b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116348130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1163486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116348cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116349270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116349df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11634a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11634a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11634af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11634b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11634bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11634c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11634c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11634cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11634d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11634d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11634dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11634e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11634e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11634ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11634f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11634f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11634ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116350570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116350b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1163510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1163516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116351c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116352230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1163527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116352db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116353370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116353930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116353ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1163544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116354a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116355030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1163555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116355bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116356170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116356730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116356cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1163571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1163576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116357bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1163580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1163585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116358af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116358ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1163594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1163599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116359ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11635a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11635a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11635adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11635b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11635b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11635c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11635c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11635d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11635d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11635da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11635e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11635e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11635eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1439044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143904950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143904dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143905230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1439056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143905b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143905f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1439063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143906860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143906cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143907140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143907820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143908340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143908af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143909300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143909a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14390a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14390a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14390af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14390b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14390be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14390c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14390ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14390d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14390daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14390ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14390e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14390e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14390e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14390edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14390f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14390f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14390fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14390fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143910300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143910770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143910be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143911050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1439114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143911930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143911da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143912210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143912680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143912af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143912f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1439133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143913840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143913cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143914120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143914590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143914a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143914e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1439152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143915750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143915bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143916030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1439165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143916aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143916f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143917380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1439177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143917c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1439180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143918540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1439189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143918e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143919290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143919700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143919b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143919fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14391a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14391a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14391ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14391b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14391b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14391ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14391bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14391c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14391c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14391cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14391d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14391d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14391d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14391de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14391e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14391e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14391eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14391efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14391f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14391f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14391fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143920180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1439205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143920a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143920ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143921340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1439217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143921c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143922090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143922500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143922970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143922de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143923250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1439236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143924030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1439242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143924760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143924bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143925040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1439254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143925920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143925d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143926200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143926670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143926ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143926f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1439273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143927830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143927ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143928110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143928580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1439289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143928e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1439292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143929740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143929bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14392a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14392a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14392a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14392ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14392b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14392b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14392bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14392bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14392c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14392c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14392cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14392d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14392d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14392d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14392de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14392e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14392e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14392eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14392f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14392f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14392f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14392fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1439301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143930630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143930aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143930f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143931380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1439317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143931c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1439320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143932540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1439329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143932e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143933290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143933700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143933b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143933fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143934450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1439348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143934d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1439351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143935610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143935a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143935ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143936360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1439367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143936c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1439370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143937520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143937990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143937e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143938270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1439386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143938b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143938fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143939430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1439398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143939d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14393a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14393a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14393aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14393aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14393b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14393b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14393bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14393c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14393c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14393c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14393cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14393d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117604230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1176046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117604b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117604f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1176053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117605860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117605cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117606140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1176065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117606a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117606e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117607300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117607770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117607be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117608790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117608a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117608d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117609180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1176095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117609a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117609ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11760a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11760a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11760ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11760b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11760b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11760b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11760bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11760c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11760c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11760cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11760cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11760d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11760d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11760dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11760e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11760e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11760ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11760eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11760f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11760f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11760fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117610070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1176104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117610950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117610dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117611230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1176116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117611b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117611f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1176123f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117612860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117612cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117613140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1176135b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117613e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117614300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117614770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117614be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1176154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117615930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117615da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117616210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117616680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117616f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1176173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117617cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117618120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117618590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117618a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117618e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1176192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117619750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117619bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11761a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11761a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11761a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11761ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11761b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11761b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11761bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11761bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11761c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11761ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11761d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11761dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11761e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11761e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11761eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11761f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11761f6c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.783s
user	0m0.278s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4741 (9626d935)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f11090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f11640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f11bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f12750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f12d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f14760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f15280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f17080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f17ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f18690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f18db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f1d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f22890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f23ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f24ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f26b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f27e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f28750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f29090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f29e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f2a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f2e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f2f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f2fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f30800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f31d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f32eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f33400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f33950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f33ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f35930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f36920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f37d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f38640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f38f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f39d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f3b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f3b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f3c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f3d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f3d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f3d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f3de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f3e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f3ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f3f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f40320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f40c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f41a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f41ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f42380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f42820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f42cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f43600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f44d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f45660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f46440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f46d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f47220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f47b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f48de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f49720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f49bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f4a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f4ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f4b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f4c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f4ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f4dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f4e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f4e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f4f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f4f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f50ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f51f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f52550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f52b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f53350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f53c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f56370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f56e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f57e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f58350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f59de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f5a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f5add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f5b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f5bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f5c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f5cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f5d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f5dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f5e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f5e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f5ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f5f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f5fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f62d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f63d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f64290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f65280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f65d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f66270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f66d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f67260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f67700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f67ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f68040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f68980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f68e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f69760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f69c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f6a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f6ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f6b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f6b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f6bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f6c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f6cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f6d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f6d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131f6e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f6e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f6ed10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.517 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131e05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131e06020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131e06490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131e06900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131e06d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131e071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131e07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131e07ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131e07f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131e08440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131e088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131e08f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131e09a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131e0a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131e0aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131e0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131e0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131e0bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131e0c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131e0ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131e0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131e0dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131e0e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131e0eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131e0f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131e0f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131e0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131e10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131e104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131e10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131e10e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131e112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131e115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131e11a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131e11e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131e122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131e12760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131e12bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131e13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131e134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131e13920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131e13d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131e14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131e14670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131e14ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131e14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131e153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131e15830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131e15ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131e16110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131e16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131e169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131e16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131e172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131e17740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131e17cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131e181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131e18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131e18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131e18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131e19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131e197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131e19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131e1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131e1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131e1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131e1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131e1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131e1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131e1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131e1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131e1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131e1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131e1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131e1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131e1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131e1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131e1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131e1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131e1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131e1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131e1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131e1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131e1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131e1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131e20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131e206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131e20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131e20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131e21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131e21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131e21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131e22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131e225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131e22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131e22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131e237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131e23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131e24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131e244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131e24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131e24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131e25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131e256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131e25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131e25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131e26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131e26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131e26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131e27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131e275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131e27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131e27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131e28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131e28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131e28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131e29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131e294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131e29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131e29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131e2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131e2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131e2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131e2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131e2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131e2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131e2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131e2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131e2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131e2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131e2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131e2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131e2d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131e2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131e2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131e2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131e2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131e2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131e2f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131e2f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131e2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131e2ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131e303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131e30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131e30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131e31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131e31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131e319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131e31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131e322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131e32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131e32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131e33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131e33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131e33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131e33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131e341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131e34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131e34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131e34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131e353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131e35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131e35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131e360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131e368a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131e36b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131e36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131e37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131e378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131e37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131e38190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131e38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131e38a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131e38ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131e39350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131e397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131e39c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131e3a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131e3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131e3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131e3adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131e3b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131e3b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131e3bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131e3bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131e3c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131e3c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131e3cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131e3d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131e3d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131e3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131e3dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131e3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131e3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131e3ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131e3f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131e3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131e3f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131e3fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131e40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131e406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131e40b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131e40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131e41400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131e41870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131e41ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131e42150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131e425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131e43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131e43430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131e436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131e43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131e43fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131e44440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131e448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131e44d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131e45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131e45600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131e45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131e45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131e46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131e467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131e46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131e470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131e47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131e47980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131e47df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131e48260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131e486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131e48b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131e48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131e49420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131e49890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131e49d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131e4a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131e4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131e4aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131e4aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131e4b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131e4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131e4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131e4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131e4c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131e4c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131e4cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131e4d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131e4d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131e4db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131e4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131e4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131e4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131e4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131e4f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131e4f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131e4fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131e4fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131e50310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131e50780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131e50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131e51060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131e514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131e51940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131e51db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131e52220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131e52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131e52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131e52f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131e533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131e53850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131e53cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131e54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131e545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131e54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131e54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131e552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131e55760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131e55bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131e56040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131e564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131e56920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131e56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131e57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131e57f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131e58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131e58d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131e59020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131e59490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131e59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131e5a0a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f50690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f6e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f50080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f25d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f52810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f21610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f6df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f1d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f1d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f52e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f1b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f6f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f6f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f6f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f6f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f6fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f6ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f701f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f704b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f70770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f70a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f70cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f70fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f71270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f71530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f717f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f71ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f71d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f72030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f722f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f725b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f72870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f72b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f72df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f730b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f73370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f73630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f738f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f73bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f73e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f74130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f743f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f746b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f74970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f74c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f74ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f751b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f75470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f75730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f759f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f75cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f75f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f76230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f764f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f767b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f76a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f76d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f76ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f772b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f77570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f77830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f77af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f77db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f78070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f78330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f785f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f788b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f78b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f78e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f790f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f793b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f79670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f79930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f79bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f79eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f7a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f7a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f7a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f7a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f7ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f7af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f7b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f7b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f7b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f7ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f7bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f7bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f7c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f7c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f7c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f7cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f7cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f7d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f7d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f7d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f7d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f7db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f7ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f7e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f7e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f7e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f7e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f7ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f7ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f7f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f7f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f7f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f7f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f7fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f7fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f801b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f80470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f80730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f809f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f80cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f80f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f81230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f81a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f81d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f81ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f822b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f82570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f82830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f82af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f82db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f83070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f83330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f835f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f838b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f83b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f83e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f840f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f843b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f84670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f84930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f84bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f84eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f85170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f85430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f856f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f859b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f85c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f85f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f861f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f864b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f86770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f86a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f86cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f86fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f87270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f87530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f877f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f87ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f87d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f88030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f882f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f885b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f88870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f88b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f88df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f890b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f89370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f89630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f898f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f89bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f89e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f8a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f8a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f8a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f8a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f8ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f8aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f8b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f8b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f8b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f8b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f8bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f8bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f8c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f8c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f8c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f8ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f8cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f8cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f8d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f8d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f8d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f8daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f8ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ef04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ef044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ef04ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ef04fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ef054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ef05eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ef06170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ef06730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ef06cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ef072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ef07870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ef07e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ef083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ef089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ef08f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ef09530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ef09af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ef0a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ef0a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ef0ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ef0b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ef0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ef0bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ef0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ef0c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ef0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ef0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ef0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ef0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ef0e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ef0eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ef0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ef0f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ef0fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ef10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ef10830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ef10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ef113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ef11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ef11f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ef124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ef12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ef13070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ef13630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ef13bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ef141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ef14770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ef14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ef152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ef158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ef15e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ef16430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ef169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ef16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ef17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ef17b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ef180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ef186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ef18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ef19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ef197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ef19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ef1a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ef1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ef1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ef1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ef1b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ef1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ef1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ef1c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ef1cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ef1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ef1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ef1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ef1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ef1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ef1e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ef1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ef1f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ef1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ef206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ef20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ef210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ef21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ef21b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ef22160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.953s
user	0m0.229s
sys	0m0.188s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.39 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.81 sec*proc (2 tests)

Total Test time (real) =   1.83 sec
        1.85 real         0.51 user         0.22 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.12 user         0.08 sys
```
