### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.54 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.13 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.43 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.73 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.24 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.36 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.99 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.10 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.19 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.04 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 166.17 sec*proc (29 tests)

Total Test time (real) = 166.18 sec

real	2m46.237s
user	4m37.652s
sys	0m5.764s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.30 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.21 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.86 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.26 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.38 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.35 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.28 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.44 sec*proc (29 tests)

Total Test time (real) =  48.45 sec

real	0m48.466s
user	0m54.801s
sys	0m5.184s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.134 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.579 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.969 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.975 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.978 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.979 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.979 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.980 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.980 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.982 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.986 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.987 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.987 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.988 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.990 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.991 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.991 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.992 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.992 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.993 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.994 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.400 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.554 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.557 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.557 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.558 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.558 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.559 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.029.559 I llama_model_loader: - type  f32:  124 tensors
0.00.029.560 I llama_model_loader: - type  f16:   73 tensors
0.00.029.560 I print_info: file format = GGUF V3 (latest)
0.00.029.561 I print_info: file type   = F16
0.00.029.562 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.033.799 I load: special tokens cache size = 5
0.00.035.791 I load: token to piece cache size = 0.2032 MB
0.00.035.817 I print_info: arch             = bert
0.00.035.819 I print_info: vocab_only       = 0
0.00.035.819 I print_info: n_ctx_train      = 512
0.00.035.820 I print_info: n_embd           = 384
0.00.035.820 I print_info: n_layer          = 12
0.00.035.823 I print_info: n_head           = 12
0.00.035.823 I print_info: n_head_kv        = 12
0.00.035.824 I print_info: n_rot            = 32
0.00.035.824 I print_info: n_swa            = 0
0.00.035.824 I print_info: n_embd_head_k    = 32
0.00.035.824 I print_info: n_embd_head_v    = 32
0.00.035.830 I print_info: n_gqa            = 1
0.00.035.831 I print_info: n_embd_k_gqa     = 384
0.00.035.832 I print_info: n_embd_v_gqa     = 384
0.00.035.833 I print_info: f_norm_eps       = 1.0e-12
0.00.035.834 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.035.834 I print_info: f_clamp_kqv      = 0.0e+00
0.00.035.834 I print_info: f_max_alibi_bias = 0.0e+00
0.00.035.836 I print_info: f_logit_scale    = 0.0e+00
0.00.035.837 I print_info: n_ff             = 1536
0.00.035.838 I print_info: n_expert         = 0
0.00.035.838 I print_info: n_expert_used    = 0
0.00.035.838 I print_info: causal attn      = 0
0.00.035.838 I print_info: pooling type     = 2
0.00.035.838 I print_info: rope type        = 2
0.00.035.840 I print_info: rope scaling     = linear
0.00.035.841 I print_info: freq_base_train  = 10000.0
0.00.035.841 I print_info: freq_scale_train = 1
0.00.035.842 I print_info: n_ctx_orig_yarn  = 512
0.00.035.842 I print_info: rope_finetuned   = unknown
0.00.035.842 I print_info: ssm_d_conv       = 0
0.00.035.842 I print_info: ssm_d_inner      = 0
0.00.035.842 I print_info: ssm_d_state      = 0
0.00.035.844 I print_info: ssm_dt_rank      = 0
0.00.035.844 I print_info: ssm_dt_b_c_rms   = 0
0.00.035.845 I print_info: model type       = 33M
0.00.035.845 I print_info: model params     = 33.21 M
0.00.035.846 I print_info: general.name     = Bge Small
0.00.035.846 I print_info: vocab type       = WPM
0.00.035.846 I print_info: n_vocab          = 30522
0.00.035.847 I print_info: n_merges         = 0
0.00.035.847 I print_info: BOS token        = 101 '[CLS]'
0.00.035.847 I print_info: UNK token        = 100 '[UNK]'
0.00.035.848 I print_info: SEP token        = 102 '[SEP]'
0.00.035.848 I print_info: PAD token        = 0 '[PAD]'
0.00.035.852 I print_info: MASK token       = 103 '[MASK]'
0.00.035.853 I print_info: LF token         = 0 '[PAD]'
0.00.035.853 I print_info: max token length = 21
0.00.035.854 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.038.843 I load_tensors: offloading 12 repeating layers to GPU
0.00.038.845 I load_tensors: offloading output layer to GPU
0.00.038.845 I load_tensors: offloaded 13/13 layers to GPU
0.00.038.868 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.870 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.144 I llama_init_from_model: n_seq_max     = 1
0.00.039.146 I llama_init_from_model: n_ctx         = 512
0.00.039.146 I llama_init_from_model: n_ctx_per_seq = 512
0.00.039.146 I llama_init_from_model: n_batch       = 2048
0.00.039.147 I llama_init_from_model: n_ubatch      = 2048
0.00.039.147 I llama_init_from_model: flash_attn    = 0
0.00.039.147 I llama_init_from_model: freq_base     = 10000.0
0.00.039.148 I llama_init_from_model: freq_scale    = 1
0.00.039.148 I ggml_metal_init: allocating
0.00.039.154 I ggml_metal_init: found device: Apple M4
0.00.039.160 I ggml_metal_init: picking default device: Apple M4
0.00.039.981 I ggml_metal_init: using embedded metal library
0.00.043.790 I ggml_metal_init: GPU name:   Apple M4
0.00.043.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.043.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.043.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.043.794 I ggml_metal_init: simdgroup reduction   = true
0.00.043.794 I ggml_metal_init: simdgroup matrix mul. = true
0.00.043.794 I ggml_metal_init: has residency sets    = true
0.00.043.794 I ggml_metal_init: has bfloat            = true
0.00.043.794 I ggml_metal_init: use bfloat            = true
0.00.043.795 I ggml_metal_init: hasUnifiedMemory      = true
0.00.043.795 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.246 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.056.903 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.056.905 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.056.907 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.058.072 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.058.074 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.058.074 I llama_init_from_model: graph nodes  = 429
0.00.058.075 I llama_init_from_model: graph splits = 2
0.00.058.076 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.058.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.063.642 I 
0.00.063.668 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.064.286 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.069.386 I llama_perf_context_print:        load time =      45.06 ms
0.00.069.387 I llama_perf_context_print: prompt eval time =       4.95 ms /     9 tokens (    0.55 ms per token,  1817.08 tokens per second)
0.00.069.388 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.069.388 I llama_perf_context_print:       total time =       5.74 ms /    10 tokens
0.00.069.508 I ggml_metal_free: deallocating

real	0m0.291s
user	0m0.051s
sys	0m0.035s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.234 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.013.038 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.013.042 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.043 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.013.044 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.044 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.013.044 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.013.045 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.013.045 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.013.046 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.013.046 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.013.046 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.013.047 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.013.049 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.013.049 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.013.050 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.013.050 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.013.050 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.013.051 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.560 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.016.221 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.016.222 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.016.222 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.016.223 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.016.223 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.016.223 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.016.224 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.016.224 I llama_model_loader: - type  f32:  124 tensors
0.00.016.224 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.225 I print_info: file format = GGUF V3 (latest)
0.00.016.225 I print_info: file type   = Q8_0
0.00.016.226 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.639 I load: special tokens cache size = 5
0.00.019.865 I load: token to piece cache size = 0.2032 MB
0.00.019.873 I print_info: arch             = bert
0.00.019.874 I print_info: vocab_only       = 0
0.00.019.874 I print_info: n_ctx_train      = 512
0.00.019.874 I print_info: n_embd           = 384
0.00.019.875 I print_info: n_layer          = 12
0.00.019.877 I print_info: n_head           = 12
0.00.019.878 I print_info: n_head_kv        = 12
0.00.019.878 I print_info: n_rot            = 32
0.00.019.878 I print_info: n_swa            = 0
0.00.019.878 I print_info: n_embd_head_k    = 32
0.00.019.878 I print_info: n_embd_head_v    = 32
0.00.019.879 I print_info: n_gqa            = 1
0.00.019.879 I print_info: n_embd_k_gqa     = 384
0.00.019.881 I print_info: n_embd_v_gqa     = 384
0.00.019.881 I print_info: f_norm_eps       = 1.0e-12
0.00.019.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.882 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.882 I print_info: f_logit_scale    = 0.0e+00
0.00.019.883 I print_info: n_ff             = 1536
0.00.019.883 I print_info: n_expert         = 0
0.00.019.883 I print_info: n_expert_used    = 0
0.00.019.883 I print_info: causal attn      = 0
0.00.019.883 I print_info: pooling type     = 2
0.00.019.884 I print_info: rope type        = 2
0.00.019.884 I print_info: rope scaling     = linear
0.00.019.884 I print_info: freq_base_train  = 10000.0
0.00.019.884 I print_info: freq_scale_train = 1
0.00.019.885 I print_info: n_ctx_orig_yarn  = 512
0.00.019.885 I print_info: rope_finetuned   = unknown
0.00.019.885 I print_info: ssm_d_conv       = 0
0.00.019.885 I print_info: ssm_d_inner      = 0
0.00.019.885 I print_info: ssm_d_state      = 0
0.00.019.885 I print_info: ssm_dt_rank      = 0
0.00.019.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.885 I print_info: model type       = 33M
0.00.019.886 I print_info: model params     = 33.21 M
0.00.019.886 I print_info: general.name     = Bge Small
0.00.019.887 I print_info: vocab type       = WPM
0.00.019.887 I print_info: n_vocab          = 30522
0.00.019.887 I print_info: n_merges         = 0
0.00.019.887 I print_info: BOS token        = 101 '[CLS]'
0.00.019.887 I print_info: UNK token        = 100 '[UNK]'
0.00.019.888 I print_info: SEP token        = 102 '[SEP]'
0.00.019.888 I print_info: PAD token        = 0 '[PAD]'
0.00.019.888 I print_info: MASK token       = 103 '[MASK]'
0.00.019.888 I print_info: LF token         = 0 '[PAD]'
0.00.019.888 I print_info: max token length = 21
0.00.019.889 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.642 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.643 I load_tensors: offloading output layer to GPU
0.00.021.643 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.649 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.649 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.825 I llama_init_from_model: n_seq_max     = 1
0.00.021.826 I llama_init_from_model: n_ctx         = 512
0.00.021.826 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.826 I llama_init_from_model: n_batch       = 2048
0.00.021.826 I llama_init_from_model: n_ubatch      = 2048
0.00.021.826 I llama_init_from_model: flash_attn    = 0
0.00.021.827 I llama_init_from_model: freq_base     = 10000.0
0.00.021.827 I llama_init_from_model: freq_scale    = 1
0.00.021.828 I ggml_metal_init: allocating
0.00.021.831 I ggml_metal_init: found device: Apple M4
0.00.021.835 I ggml_metal_init: picking default device: Apple M4
0.00.022.328 I ggml_metal_init: using embedded metal library
0.00.024.904 I ggml_metal_init: GPU name:   Apple M4
0.00.024.906 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.906 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.907 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.907 I ggml_metal_init: simdgroup reduction   = true
0.00.024.907 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.907 I ggml_metal_init: has residency sets    = true
0.00.024.907 I ggml_metal_init: has bfloat            = true
0.00.024.908 I ggml_metal_init: use bfloat            = true
0.00.024.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.586 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.036.179 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.036.181 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.036.182 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.037.198 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.037.199 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.037.199 I llama_init_from_model: graph nodes  = 429
0.00.037.199 I llama_init_from_model: graph splits = 2
0.00.037.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.037.201 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.232 I 
0.00.041.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.776 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.222 I llama_perf_context_print:        load time =      30.99 ms
0.00.046.223 I llama_perf_context_print: prompt eval time =       4.32 ms /     9 tokens (    0.48 ms per token,  2085.26 tokens per second)
0.00.046.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.225 I llama_perf_context_print:       total time =       4.99 ms /    10 tokens
0.00.046.445 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.231 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.672 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.799 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.804 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.807 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.815 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.816 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.818 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.819 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.820 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.821 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.821 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.822 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.825 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.826 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.826 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.132 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.245 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.470 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.473 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.474 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.474 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.474 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.475 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.475 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.476 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.476 I llama_model_loader: - type  f32:   40 tensors
0.00.048.476 I llama_model_loader: - type  f16:   30 tensors
0.00.048.482 I print_info: file format = GGUF V3 (latest)
0.00.048.483 I print_info: file type   = F16
0.00.048.484 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.843 W load: empty token at index 5
0.00.058.039 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.458 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.493 I load: special tokens cache size = 5
0.00.321.938 I load: token to piece cache size = 1.5060 MB
0.00.321.967 I print_info: arch             = jina-bert-v2
0.00.321.968 I print_info: vocab_only       = 0
0.00.321.968 I print_info: n_ctx_train      = 8192
0.00.321.969 I print_info: n_embd           = 384
0.00.321.969 I print_info: n_layer          = 4
0.00.321.975 I print_info: n_head           = 12
0.00.321.976 I print_info: n_head_kv        = 12
0.00.321.978 I print_info: n_rot            = 32
0.00.321.978 I print_info: n_swa            = 0
0.00.321.978 I print_info: n_embd_head_k    = 32
0.00.321.978 I print_info: n_embd_head_v    = 32
0.00.321.979 I print_info: n_gqa            = 1
0.00.321.980 I print_info: n_embd_k_gqa     = 384
0.00.321.981 I print_info: n_embd_v_gqa     = 384
0.00.321.981 I print_info: f_norm_eps       = 1.0e-12
0.00.321.982 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.321.982 I print_info: f_clamp_kqv      = 0.0e+00
0.00.321.983 I print_info: f_max_alibi_bias = 8.0e+00
0.00.321.983 I print_info: f_logit_scale    = 0.0e+00
0.00.321.985 I print_info: n_ff             = 1536
0.00.321.985 I print_info: n_expert         = 0
0.00.321.985 I print_info: n_expert_used    = 0
0.00.321.985 I print_info: causal attn      = 0
0.00.321.986 I print_info: pooling type     = -1
0.00.321.986 I print_info: rope type        = -1
0.00.321.986 I print_info: rope scaling     = linear
0.00.321.986 I print_info: freq_base_train  = 10000.0
0.00.321.986 I print_info: freq_scale_train = 1
0.00.321.987 I print_info: n_ctx_orig_yarn  = 8192
0.00.321.987 I print_info: rope_finetuned   = unknown
0.00.321.987 I print_info: ssm_d_conv       = 0
0.00.321.987 I print_info: ssm_d_inner      = 0
0.00.321.987 I print_info: ssm_d_state      = 0
0.00.321.987 I print_info: ssm_dt_rank      = 0
0.00.321.987 I print_info: ssm_dt_b_c_rms   = 0
0.00.321.987 I print_info: model type       = 33M
0.00.321.992 I print_info: model params     = 32.90 M
0.00.321.992 I print_info: general.name     = Jina Bert Implementation
0.00.321.993 I print_info: vocab type       = BPE
0.00.321.993 I print_info: n_vocab          = 61056
0.00.321.994 I print_info: n_merges         = 39382
0.00.321.994 I print_info: BOS token        = 0 '<s>'
0.00.321.994 I print_info: EOS token        = 2 '</s>'
0.00.321.994 I print_info: UNK token        = 3 '<unk>'
0.00.321.994 I print_info: SEP token        = 2 '</s>'
0.00.321.995 I print_info: PAD token        = 1 '<pad>'
0.00.321.995 I print_info: MASK token       = 4 '<mask>'
0.00.321.995 I print_info: EOG token        = 2 '</s>'
0.00.321.995 I print_info: max token length = 45
0.00.321.996 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.077 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.078 I load_tensors: offloading output layer to GPU
0.00.324.078 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.103 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.104 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.443 I llama_init_from_model: n_seq_max     = 1
0.00.324.444 I llama_init_from_model: n_ctx         = 8192
0.00.324.445 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.445 I llama_init_from_model: n_batch       = 2048
0.00.324.445 I llama_init_from_model: n_ubatch      = 2048
0.00.324.445 I llama_init_from_model: flash_attn    = 0
0.00.324.446 I llama_init_from_model: freq_base     = 10000.0
0.00.324.446 I llama_init_from_model: freq_scale    = 1
0.00.324.446 I ggml_metal_init: allocating
0.00.324.455 I ggml_metal_init: found device: Apple M4
0.00.324.460 I ggml_metal_init: picking default device: Apple M4
0.00.325.209 I ggml_metal_init: using embedded metal library
0.00.327.703 I ggml_metal_init: GPU name:   Apple M4
0.00.327.705 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.705 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.706 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.706 I ggml_metal_init: simdgroup reduction   = true
0.00.327.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.706 I ggml_metal_init: has residency sets    = true
0.00.327.706 I ggml_metal_init: has bfloat            = true
0.00.327.706 I ggml_metal_init: use bfloat            = true
0.00.327.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.570 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.527 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.529 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.530 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.062 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.064 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.064 I llama_init_from_model: graph nodes  = 154
0.00.347.064 I llama_init_from_model: graph splits = 2
0.00.347.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.066 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.353.616 I 
0.00.353.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.353.748 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.353.748 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.353.751 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.353.751 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.353.754 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.353.754 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.354.289 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.357.704 I llama_perf_context_print:        load time =     331.94 ms
0.00.357.705 I llama_perf_context_print: prompt eval time =       3.41 ms /    62 tokens (    0.05 ms per token, 18197.83 tokens per second)
0.00.357.706 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.357.710 I llama_perf_context_print:       total time =       4.09 ms /    63 tokens
0.00.357.961 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.334s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.206 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.363 I main: llama backend init
0.00.000.373 I main: load the model and apply lora adapter, if any
0.00.051.347 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.064.956 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.983 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.984 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.984 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.989 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.991 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.073.569 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.075.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.082.988 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.082.990 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.082.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.082.991 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.082.991 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.082.992 I llama_model_loader: - type  f32:  194 tensors
0.00.082.992 I llama_model_loader: - type  f16:   98 tensors
0.00.082.993 I print_info: file format = GGUF V3 (latest)
0.00.082.995 I print_info: file type   = all F32 (guessed)
0.00.082.996 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.512 I load: special tokens cache size = 25
0.00.105.474 I load: token to piece cache size = 0.2984 MB
0.00.105.497 I print_info: arch             = gptneox
0.00.105.498 I print_info: vocab_only       = 0
0.00.105.499 I print_info: n_ctx_train      = 2048
0.00.105.499 I print_info: n_embd           = 2048
0.00.105.499 I print_info: n_layer          = 24
0.00.105.502 I print_info: n_head           = 16
0.00.105.508 I print_info: n_head_kv        = 16
0.00.105.508 I print_info: n_rot            = 32
0.00.105.508 I print_info: n_swa            = 0
0.00.105.509 I print_info: n_embd_head_k    = 128
0.00.105.509 I print_info: n_embd_head_v    = 128
0.00.105.511 I print_info: n_gqa            = 1
0.00.105.512 I print_info: n_embd_k_gqa     = 2048
0.00.105.514 I print_info: n_embd_v_gqa     = 2048
0.00.105.515 I print_info: f_norm_eps       = 1.0e-05
0.00.105.515 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.516 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.516 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.516 I print_info: f_logit_scale    = 0.0e+00
0.00.105.517 I print_info: n_ff             = 8192
0.00.105.517 I print_info: n_expert         = 0
0.00.105.517 I print_info: n_expert_used    = 0
0.00.105.517 I print_info: causal attn      = 1
0.00.105.517 I print_info: pooling type     = 0
0.00.105.517 I print_info: rope type        = 2
0.00.105.518 I print_info: rope scaling     = linear
0.00.105.519 I print_info: freq_base_train  = 10000.0
0.00.105.520 I print_info: freq_scale_train = 1
0.00.105.520 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.520 I print_info: rope_finetuned   = unknown
0.00.105.520 I print_info: ssm_d_conv       = 0
0.00.105.520 I print_info: ssm_d_inner      = 0
0.00.105.521 I print_info: ssm_d_state      = 0
0.00.105.521 I print_info: ssm_dt_rank      = 0
0.00.105.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.521 I print_info: model type       = 1.4B
0.00.105.521 I print_info: model params     = 1.41 B
0.00.105.522 I print_info: general.name     = 1.4B
0.00.105.522 I print_info: vocab type       = BPE
0.00.105.522 I print_info: n_vocab          = 50304
0.00.105.523 I print_info: n_merges         = 50009
0.00.105.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.526 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.526 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.526 I print_info: LF token         = 187 ''
0.00.105.527 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.530 I print_info: max token length = 1024
0.00.105.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.191.959 I load_tensors: offloading 24 repeating layers to GPU
0.00.191.964 I load_tensors: offloading output layer to GPU
0.00.191.964 I load_tensors: offloaded 25/25 layers to GPU
0.00.191.993 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.191.995 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.192.650 I llama_init_from_model: n_seq_max     = 1
0.00.192.651 I llama_init_from_model: n_ctx         = 2048
0.00.192.651 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.192.651 I llama_init_from_model: n_batch       = 2048
0.00.192.651 I llama_init_from_model: n_ubatch      = 512
0.00.192.651 I llama_init_from_model: flash_attn    = 0
0.00.192.652 I llama_init_from_model: freq_base     = 10000.0
0.00.192.652 I llama_init_from_model: freq_scale    = 1
0.00.192.654 I ggml_metal_init: allocating
0.00.192.807 I ggml_metal_init: found device: Apple M4
0.00.192.812 I ggml_metal_init: picking default device: Apple M4
0.00.193.739 I ggml_metal_init: using embedded metal library
0.00.228.156 I ggml_metal_init: GPU name:   Apple M4
0.00.228.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.228.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.228.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.228.159 I ggml_metal_init: simdgroup reduction   = true
0.00.228.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.228.160 I ggml_metal_init: has residency sets    = true
0.00.228.160 I ggml_metal_init: has bfloat            = true
0.00.228.160 I ggml_metal_init: use bfloat            = true
0.00.228.160 I ggml_metal_init: hasUnifiedMemory      = true
0.00.228.161 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.261.419 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.292.289 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.292.295 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.292.319 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.296.033 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.296.035 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.296.036 I llama_init_from_model: graph nodes  = 967
0.00.296.036 I llama_init_from_model: graph splits = 2
0.00.296.041 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.296.171 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.296.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.362.790 I main: llama threadpool init, n_threads = 4
0.00.362.852 I 
0.00.362.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.362.882 I 
0.00.363.073 I sampler seed: 1234
0.00.363.078 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.363.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.363.113 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.363.113 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.191.351 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.02.191.352 I llama_perf_context_print:        load time =     310.54 ms
0.02.191.352 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.22 tokens per second)
0.02.191.354 I llama_perf_context_print:        eval time =    1781.63 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.191.355 I llama_perf_context_print:       total time =    1829.45 ms /    70 tokens
0.02.191.563 I ggml_metal_free: deallocating

real	0m2.596s
user	0m0.132s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.520 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.074 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.781 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.818 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.819 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.822 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.823 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.824 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.831 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.833 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.243 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.820 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.820 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.821 I llama_model_loader: - type  f32:  194 tensors
0.00.055.821 I llama_model_loader: - type  f16:   98 tensors
0.00.055.823 I print_info: file format = GGUF V3 (latest)
0.00.055.824 I print_info: file type   = all F32 (guessed)
0.00.055.826 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.955 I load: special tokens cache size = 25
0.00.077.698 I load: token to piece cache size = 0.2984 MB
0.00.077.713 I print_info: arch             = gptneox
0.00.077.714 I print_info: vocab_only       = 0
0.00.077.714 I print_info: n_ctx_train      = 2048
0.00.077.714 I print_info: n_embd           = 2048
0.00.077.715 I print_info: n_layer          = 24
0.00.077.718 I print_info: n_head           = 16
0.00.077.719 I print_info: n_head_kv        = 16
0.00.077.719 I print_info: n_rot            = 32
0.00.077.719 I print_info: n_swa            = 0
0.00.077.719 I print_info: n_embd_head_k    = 128
0.00.077.720 I print_info: n_embd_head_v    = 128
0.00.077.720 I print_info: n_gqa            = 1
0.00.077.721 I print_info: n_embd_k_gqa     = 2048
0.00.077.722 I print_info: n_embd_v_gqa     = 2048
0.00.077.723 I print_info: f_norm_eps       = 1.0e-05
0.00.077.723 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.724 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.724 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.724 I print_info: f_logit_scale    = 0.0e+00
0.00.077.725 I print_info: n_ff             = 8192
0.00.077.725 I print_info: n_expert         = 0
0.00.077.725 I print_info: n_expert_used    = 0
0.00.077.725 I print_info: causal attn      = 1
0.00.077.726 I print_info: pooling type     = 0
0.00.077.726 I print_info: rope type        = 2
0.00.077.726 I print_info: rope scaling     = linear
0.00.077.726 I print_info: freq_base_train  = 10000.0
0.00.077.727 I print_info: freq_scale_train = 1
0.00.077.727 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.727 I print_info: rope_finetuned   = unknown
0.00.077.727 I print_info: ssm_d_conv       = 0
0.00.077.727 I print_info: ssm_d_inner      = 0
0.00.077.728 I print_info: ssm_d_state      = 0
0.00.077.728 I print_info: ssm_dt_rank      = 0
0.00.077.728 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.728 I print_info: model type       = 1.4B
0.00.077.729 I print_info: model params     = 1.41 B
0.00.077.729 I print_info: general.name     = 1.4B
0.00.077.732 I print_info: vocab type       = BPE
0.00.077.732 I print_info: n_vocab          = 50304
0.00.077.732 I print_info: n_merges         = 50009
0.00.077.733 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.733 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.733 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.734 I print_info: LF token         = 187 ''
0.00.077.734 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.738 I print_info: max token length = 1024
0.00.077.739 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.307.399 I load_tensors: offloading 24 repeating layers to GPU
0.01.307.404 I load_tensors: offloading output layer to GPU
0.01.307.404 I load_tensors: offloaded 25/25 layers to GPU
0.01.307.434 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.307.436 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.308.191 I llama_init_from_model: n_seq_max     = 1
0.01.308.192 I llama_init_from_model: n_ctx         = 128
0.01.308.192 I llama_init_from_model: n_ctx_per_seq = 128
0.01.308.193 I llama_init_from_model: n_batch       = 128
0.01.308.193 I llama_init_from_model: n_ubatch      = 128
0.01.308.193 I llama_init_from_model: flash_attn    = 0
0.01.308.194 I llama_init_from_model: freq_base     = 10000.0
0.01.308.194 I llama_init_from_model: freq_scale    = 1
0.01.308.194 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.308.195 I ggml_metal_init: allocating
0.01.308.282 I ggml_metal_init: found device: Apple M4
0.01.308.291 I ggml_metal_init: picking default device: Apple M4
0.01.309.561 I ggml_metal_init: using embedded metal library
0.01.313.362 I ggml_metal_init: GPU name:   Apple M4
0.01.313.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.313.365 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.313.366 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.313.366 I ggml_metal_init: simdgroup reduction   = true
0.01.313.366 I ggml_metal_init: simdgroup matrix mul. = true
0.01.313.366 I ggml_metal_init: has residency sets    = true
0.01.313.366 I ggml_metal_init: has bfloat            = true
0.01.313.367 I ggml_metal_init: use bfloat            = true
0.01.313.367 I ggml_metal_init: hasUnifiedMemory      = true
0.01.313.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.324.080 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.325.925 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.325.928 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.325.941 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.327.546 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.327.547 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.327.547 I llama_init_from_model: graph nodes  = 967
0.01.327.548 I llama_init_from_model: graph splits = 2
0.01.327.549 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.327.549 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.363.057 I 
0.01.363.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.363.104 I perplexity: tokenizing the input ..
0.01.367.815 I perplexity: tokenization took 4.709 ms
0.01.367.819 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.486.705 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.488.055 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.488.089 I llama_perf_context_print:        load time =    1338.97 ms
0.01.488.090 I llama_perf_context_print: prompt eval time =     118.63 ms /   128 tokens (    0.93 ms per token,  1079.02 tokens per second)
0.01.488.091 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.488.091 I llama_perf_context_print:       total time =     125.03 ms /   129 tokens
0.01.488.484 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.098s
sys	0m0.260s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.233 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.211 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.216 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.218 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.219 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.219 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.221 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.221 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.222 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.222 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.225 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.225 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.227 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.227 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.228 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.808 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.810 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.810 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.810 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.811 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.811 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.812 I llama_model_loader: - type  f32:  194 tensors
0.00.029.812 I llama_model_loader: - type q8_0:   98 tensors
0.00.029.813 I print_info: file format = GGUF V3 (latest)
0.00.029.814 I print_info: file type   = Q8_0
0.00.029.815 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.038.278 I load: special tokens cache size = 25
0.00.044.668 I load: token to piece cache size = 0.2984 MB
0.00.044.686 I print_info: arch             = gptneox
0.00.044.687 I print_info: vocab_only       = 0
0.00.044.687 I print_info: n_ctx_train      = 2048
0.00.044.687 I print_info: n_embd           = 2048
0.00.044.688 I print_info: n_layer          = 24
0.00.044.694 I print_info: n_head           = 16
0.00.044.694 I print_info: n_head_kv        = 16
0.00.044.694 I print_info: n_rot            = 32
0.00.044.695 I print_info: n_swa            = 0
0.00.044.695 I print_info: n_embd_head_k    = 128
0.00.044.695 I print_info: n_embd_head_v    = 128
0.00.044.695 I print_info: n_gqa            = 1
0.00.044.696 I print_info: n_embd_k_gqa     = 2048
0.00.044.697 I print_info: n_embd_v_gqa     = 2048
0.00.044.698 I print_info: f_norm_eps       = 1.0e-05
0.00.044.698 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.699 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.699 I print_info: f_logit_scale    = 0.0e+00
0.00.044.700 I print_info: n_ff             = 8192
0.00.044.700 I print_info: n_expert         = 0
0.00.044.700 I print_info: n_expert_used    = 0
0.00.044.700 I print_info: causal attn      = 1
0.00.044.700 I print_info: pooling type     = 0
0.00.044.701 I print_info: rope type        = 2
0.00.044.703 I print_info: rope scaling     = linear
0.00.044.703 I print_info: freq_base_train  = 10000.0
0.00.044.704 I print_info: freq_scale_train = 1
0.00.044.704 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.704 I print_info: rope_finetuned   = unknown
0.00.044.704 I print_info: ssm_d_conv       = 0
0.00.044.704 I print_info: ssm_d_inner      = 0
0.00.044.704 I print_info: ssm_d_state      = 0
0.00.044.705 I print_info: ssm_dt_rank      = 0
0.00.044.705 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.705 I print_info: model type       = 1.4B
0.00.044.706 I print_info: model params     = 1.41 B
0.00.044.706 I print_info: general.name     = 1.4B
0.00.044.707 I print_info: vocab type       = BPE
0.00.044.707 I print_info: n_vocab          = 50304
0.00.044.707 I print_info: n_merges         = 50009
0.00.044.707 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.708 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.708 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: LF token         = 187 ''
0.00.044.710 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.710 I print_info: max token length = 1024
0.00.044.711 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.167.697 I load_tensors: offloading 24 repeating layers to GPU
0.01.167.702 I load_tensors: offloading output layer to GPU
0.01.167.703 I load_tensors: offloaded 25/25 layers to GPU
0.01.167.728 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.167.732 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.168.886 I llama_init_from_model: n_seq_max     = 1
0.01.168.889 I llama_init_from_model: n_ctx         = 2048
0.01.168.889 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.168.889 I llama_init_from_model: n_batch       = 2048
0.01.168.890 I llama_init_from_model: n_ubatch      = 512
0.01.168.890 I llama_init_from_model: flash_attn    = 0
0.01.168.891 I llama_init_from_model: freq_base     = 10000.0
0.01.168.892 I llama_init_from_model: freq_scale    = 1
0.01.168.892 I ggml_metal_init: allocating
0.01.168.904 I ggml_metal_init: found device: Apple M4
0.01.168.912 I ggml_metal_init: picking default device: Apple M4
0.01.170.364 I ggml_metal_init: using embedded metal library
0.01.175.986 I ggml_metal_init: GPU name:   Apple M4
0.01.175.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.175.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.175.991 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.175.991 I ggml_metal_init: simdgroup reduction   = true
0.01.175.992 I ggml_metal_init: simdgroup matrix mul. = true
0.01.175.992 I ggml_metal_init: has residency sets    = true
0.01.175.992 I ggml_metal_init: has bfloat            = true
0.01.175.992 I ggml_metal_init: use bfloat            = true
0.01.175.993 I ggml_metal_init: hasUnifiedMemory      = true
0.01.175.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.192.398 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.245.029 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.245.037 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.245.059 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.249.853 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.249.854 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.249.855 I llama_init_from_model: graph nodes  = 967
0.01.249.855 I llama_init_from_model: graph splits = 2
0.01.249.860 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.249.989 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.249.990 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.307.401 I main: llama threadpool init, n_threads = 4
0.01.307.453 I 
0.01.307.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.307.474 I 
0.01.307.631 I sampler seed: 1234
0.01.307.635 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.307.685 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.307.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.307.694 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.394.061 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51005.75 tokens per second)
0.02.394.061 I llama_perf_context_print:        load time =    1295.44 ms
0.02.394.064 I llama_perf_context_print: prompt eval time =      49.38 ms /     7 tokens (    7.05 ms per token,   141.77 tokens per second)
0.02.394.064 I llama_perf_context_print:        eval time =    1033.98 ms /    63 runs   (   16.41 ms per token,    60.93 tokens per second)
0.02.394.065 I llama_perf_context_print:       total time =    1087.39 ms /    70 tokens
0.02.394.331 I ggml_metal_free: deallocating

real	0m2.413s
user	0m0.109s
sys	0m0.290s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.228 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.413 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.422 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.425 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.427 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.198 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.006 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.008 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.008 I llama_model_loader: - type  f32:  194 tensors
0.00.025.009 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.009 I print_info: file format = GGUF V3 (latest)
0.00.025.010 I print_info: file type   = Q8_0
0.00.025.013 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.372 I load: special tokens cache size = 25
0.00.039.744 I load: token to piece cache size = 0.2984 MB
0.00.039.761 I print_info: arch             = gptneox
0.00.039.762 I print_info: vocab_only       = 0
0.00.039.762 I print_info: n_ctx_train      = 2048
0.00.039.762 I print_info: n_embd           = 2048
0.00.039.763 I print_info: n_layer          = 24
0.00.039.767 I print_info: n_head           = 16
0.00.039.767 I print_info: n_head_kv        = 16
0.00.039.768 I print_info: n_rot            = 32
0.00.039.768 I print_info: n_swa            = 0
0.00.039.768 I print_info: n_embd_head_k    = 128
0.00.039.768 I print_info: n_embd_head_v    = 128
0.00.039.769 I print_info: n_gqa            = 1
0.00.039.769 I print_info: n_embd_k_gqa     = 2048
0.00.039.770 I print_info: n_embd_v_gqa     = 2048
0.00.039.770 I print_info: f_norm_eps       = 1.0e-05
0.00.039.774 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.774 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.774 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.774 I print_info: f_logit_scale    = 0.0e+00
0.00.039.775 I print_info: n_ff             = 8192
0.00.039.775 I print_info: n_expert         = 0
0.00.039.775 I print_info: n_expert_used    = 0
0.00.039.776 I print_info: causal attn      = 1
0.00.039.776 I print_info: pooling type     = 0
0.00.039.776 I print_info: rope type        = 2
0.00.039.777 I print_info: rope scaling     = linear
0.00.039.777 I print_info: freq_base_train  = 10000.0
0.00.039.777 I print_info: freq_scale_train = 1
0.00.039.777 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.778 I print_info: rope_finetuned   = unknown
0.00.039.778 I print_info: ssm_d_conv       = 0
0.00.039.778 I print_info: ssm_d_inner      = 0
0.00.039.778 I print_info: ssm_d_state      = 0
0.00.039.778 I print_info: ssm_dt_rank      = 0
0.00.039.778 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.778 I print_info: model type       = 1.4B
0.00.039.779 I print_info: model params     = 1.41 B
0.00.039.779 I print_info: general.name     = 1.4B
0.00.039.779 I print_info: vocab type       = BPE
0.00.039.779 I print_info: n_vocab          = 50304
0.00.039.780 I print_info: n_merges         = 50009
0.00.039.780 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.780 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.780 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.780 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.780 I print_info: LF token         = 187 ''
0.00.039.781 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.781 I print_info: max token length = 1024
0.00.039.781 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.878.219 I load_tensors: offloading 24 repeating layers to GPU
0.00.878.224 I load_tensors: offloading output layer to GPU
0.00.878.225 I load_tensors: offloaded 25/25 layers to GPU
0.00.878.248 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.878.249 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.879.518 I llama_init_from_model: n_seq_max     = 1
0.00.879.520 I llama_init_from_model: n_ctx         = 128
0.00.879.520 I llama_init_from_model: n_ctx_per_seq = 128
0.00.879.521 I llama_init_from_model: n_batch       = 128
0.00.879.521 I llama_init_from_model: n_ubatch      = 128
0.00.879.521 I llama_init_from_model: flash_attn    = 0
0.00.879.522 I llama_init_from_model: freq_base     = 10000.0
0.00.879.523 I llama_init_from_model: freq_scale    = 1
0.00.879.523 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.879.524 I ggml_metal_init: allocating
0.00.879.570 I ggml_metal_init: found device: Apple M4
0.00.879.580 I ggml_metal_init: picking default device: Apple M4
0.00.880.935 I ggml_metal_init: using embedded metal library
0.00.886.400 I ggml_metal_init: GPU name:   Apple M4
0.00.886.404 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.886.405 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.886.406 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.886.406 I ggml_metal_init: simdgroup reduction   = true
0.00.886.406 I ggml_metal_init: simdgroup matrix mul. = true
0.00.886.407 I ggml_metal_init: has residency sets    = true
0.00.886.407 I ggml_metal_init: has bfloat            = true
0.00.886.407 I ggml_metal_init: use bfloat            = true
0.00.886.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.886.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.902.228 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.905.590 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.905.593 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.905.619 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.908.883 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.908.885 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.908.885 I llama_init_from_model: graph nodes  = 967
0.00.908.886 I llama_init_from_model: graph splits = 2
0.00.908.888 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.908.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.703 I 
0.00.935.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.787 I perplexity: tokenizing the input ..
0.00.943.007 I perplexity: tokenization took 7.217 ms
0.00.943.016 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.081.506 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.082.839 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.082.863 I llama_perf_context_print:        load time =     926.47 ms
0.01.082.865 I llama_perf_context_print: prompt eval time =     137.59 ms /   128 tokens (    1.07 ms per token,   930.33 tokens per second)
0.01.082.866 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.082.866 I llama_perf_context_print:       total time =     147.16 ms /   129 tokens
0.01.083.243 I ggml_metal_free: deallocating

real	0m1.097s
user	0m0.077s
sys	0m0.167s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.012.155 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.595 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.602 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.602 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.603 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.605 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.605 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.606 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.606 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.607 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.610 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.374 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.375 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.376 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.376 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.376 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.031.377 I llama_model_loader: - type  f32:  194 tensors
0.00.031.377 I llama_model_loader: - type q4_0:   97 tensors
0.00.031.378 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.379 I print_info: file format = GGUF V3 (latest)
0.00.031.379 I print_info: file type   = Q4_0
0.00.031.380 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.039.301 I load: special tokens cache size = 25
0.00.045.666 I load: token to piece cache size = 0.2984 MB
0.00.045.681 I print_info: arch             = gptneox
0.00.045.682 I print_info: vocab_only       = 0
0.00.045.682 I print_info: n_ctx_train      = 2048
0.00.045.682 I print_info: n_embd           = 2048
0.00.045.683 I print_info: n_layer          = 24
0.00.045.686 I print_info: n_head           = 16
0.00.045.687 I print_info: n_head_kv        = 16
0.00.045.688 I print_info: n_rot            = 32
0.00.045.688 I print_info: n_swa            = 0
0.00.045.688 I print_info: n_embd_head_k    = 128
0.00.045.690 I print_info: n_embd_head_v    = 128
0.00.045.691 I print_info: n_gqa            = 1
0.00.045.692 I print_info: n_embd_k_gqa     = 2048
0.00.045.692 I print_info: n_embd_v_gqa     = 2048
0.00.045.693 I print_info: f_norm_eps       = 1.0e-05
0.00.045.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.696 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.697 I print_info: f_logit_scale    = 0.0e+00
0.00.045.697 I print_info: n_ff             = 8192
0.00.045.697 I print_info: n_expert         = 0
0.00.045.697 I print_info: n_expert_used    = 0
0.00.045.698 I print_info: causal attn      = 1
0.00.045.698 I print_info: pooling type     = 0
0.00.045.698 I print_info: rope type        = 2
0.00.045.698 I print_info: rope scaling     = linear
0.00.045.698 I print_info: freq_base_train  = 10000.0
0.00.045.698 I print_info: freq_scale_train = 1
0.00.045.699 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.699 I print_info: rope_finetuned   = unknown
0.00.045.699 I print_info: ssm_d_conv       = 0
0.00.045.699 I print_info: ssm_d_inner      = 0
0.00.045.699 I print_info: ssm_d_state      = 0
0.00.045.699 I print_info: ssm_dt_rank      = 0
0.00.045.699 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.699 I print_info: model type       = 1.4B
0.00.045.700 I print_info: model params     = 1.41 B
0.00.045.700 I print_info: general.name     = 1.4B
0.00.045.700 I print_info: vocab type       = BPE
0.00.045.700 I print_info: n_vocab          = 50304
0.00.045.700 I print_info: n_merges         = 50009
0.00.045.701 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.701 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.701 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.701 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.701 I print_info: LF token         = 187 ''
0.00.045.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.702 I print_info: max token length = 1024
0.00.045.702 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.205 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.213 I load_tensors: offloading output layer to GPU
0.00.669.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.239 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.669.242 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.670.825 I llama_init_from_model: n_seq_max     = 1
0.00.670.829 I llama_init_from_model: n_ctx         = 2048
0.00.670.829 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.830 I llama_init_from_model: n_batch       = 2048
0.00.670.830 I llama_init_from_model: n_ubatch      = 512
0.00.670.830 I llama_init_from_model: flash_attn    = 0
0.00.670.833 I llama_init_from_model: freq_base     = 10000.0
0.00.670.833 I llama_init_from_model: freq_scale    = 1
0.00.670.836 I ggml_metal_init: allocating
0.00.670.896 I ggml_metal_init: found device: Apple M4
0.00.670.910 I ggml_metal_init: picking default device: Apple M4
0.00.672.642 I ggml_metal_init: using embedded metal library
0.00.678.306 I ggml_metal_init: GPU name:   Apple M4
0.00.678.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.318 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.319 I ggml_metal_init: simdgroup reduction   = true
0.00.678.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.320 I ggml_metal_init: has residency sets    = true
0.00.678.320 I ggml_metal_init: has bfloat            = true
0.00.678.320 I ggml_metal_init: use bfloat            = true
0.00.678.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.330 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.835 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.759.439 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.759.447 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.759.471 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.763.929 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.763.931 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.763.931 I llama_init_from_model: graph nodes  = 967
0.00.763.932 I llama_init_from_model: graph splits = 2
0.00.763.944 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.764.068 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.764.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.250 I main: llama threadpool init, n_threads = 4
0.00.822.300 I 
0.00.822.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.325 I 
0.00.822.506 I sampler seed: 1234
0.00.822.511 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.526 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.526 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.508.732 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.508.733 I llama_perf_context_print:        load time =     809.36 ms
0.01.508.733 I llama_perf_context_print: prompt eval time =      49.29 ms /     7 tokens (    7.04 ms per token,   142.03 tokens per second)
0.01.508.735 I llama_perf_context_print:        eval time =     634.01 ms /    63 runs   (   10.06 ms per token,    99.37 tokens per second)
0.01.508.735 I llama_perf_context_print:       total time =     687.21 ms /    70 tokens
0.01.508.970 I ggml_metal_free: deallocating

real	0m1.531s
user	0m0.110s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.993 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.710 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.716 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.717 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.723 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.725 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.725 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.726 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.726 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.726 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.727 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.727 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.729 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.486 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.275 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.276 I llama_model_loader: - type  f32:  194 tensors
0.00.025.276 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.277 I print_info: file format = GGUF V3 (latest)
0.00.025.278 I print_info: file type   = Q4_0
0.00.025.279 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.354 I load: special tokens cache size = 25
0.00.039.711 I load: token to piece cache size = 0.2984 MB
0.00.039.728 I print_info: arch             = gptneox
0.00.039.729 I print_info: vocab_only       = 0
0.00.039.730 I print_info: n_ctx_train      = 2048
0.00.039.730 I print_info: n_embd           = 2048
0.00.039.730 I print_info: n_layer          = 24
0.00.039.734 I print_info: n_head           = 16
0.00.039.738 I print_info: n_head_kv        = 16
0.00.039.738 I print_info: n_rot            = 32
0.00.039.738 I print_info: n_swa            = 0
0.00.039.738 I print_info: n_embd_head_k    = 128
0.00.039.738 I print_info: n_embd_head_v    = 128
0.00.039.739 I print_info: n_gqa            = 1
0.00.039.740 I print_info: n_embd_k_gqa     = 2048
0.00.039.741 I print_info: n_embd_v_gqa     = 2048
0.00.039.741 I print_info: f_norm_eps       = 1.0e-05
0.00.039.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.744 I print_info: f_logit_scale    = 0.0e+00
0.00.039.746 I print_info: n_ff             = 8192
0.00.039.746 I print_info: n_expert         = 0
0.00.039.746 I print_info: n_expert_used    = 0
0.00.039.746 I print_info: causal attn      = 1
0.00.039.747 I print_info: pooling type     = 0
0.00.039.747 I print_info: rope type        = 2
0.00.039.747 I print_info: rope scaling     = linear
0.00.039.747 I print_info: freq_base_train  = 10000.0
0.00.039.748 I print_info: freq_scale_train = 1
0.00.039.748 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.748 I print_info: rope_finetuned   = unknown
0.00.039.748 I print_info: ssm_d_conv       = 0
0.00.039.749 I print_info: ssm_d_inner      = 0
0.00.039.749 I print_info: ssm_d_state      = 0
0.00.039.749 I print_info: ssm_dt_rank      = 0
0.00.039.749 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.749 I print_info: model type       = 1.4B
0.00.039.750 I print_info: model params     = 1.41 B
0.00.039.750 I print_info: general.name     = 1.4B
0.00.039.750 I print_info: vocab type       = BPE
0.00.039.751 I print_info: n_vocab          = 50304
0.00.039.751 I print_info: n_merges         = 50009
0.00.039.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.752 I print_info: LF token         = 187 ''
0.00.039.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.752 I print_info: max token length = 1024
0.00.039.753 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.709 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.723 I load_tensors: offloading output layer to GPU
0.00.588.724 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.771 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.773 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.590.527 I llama_init_from_model: n_seq_max     = 1
0.00.590.535 I llama_init_from_model: n_ctx         = 128
0.00.590.535 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.536 I llama_init_from_model: n_batch       = 128
0.00.590.536 I llama_init_from_model: n_ubatch      = 128
0.00.590.536 I llama_init_from_model: flash_attn    = 0
0.00.590.538 I llama_init_from_model: freq_base     = 10000.0
0.00.590.538 I llama_init_from_model: freq_scale    = 1
0.00.590.539 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.542 I ggml_metal_init: allocating
0.00.590.656 I ggml_metal_init: found device: Apple M4
0.00.590.676 I ggml_metal_init: picking default device: Apple M4
0.00.593.123 I ggml_metal_init: using embedded metal library
0.00.600.345 I ggml_metal_init: GPU name:   Apple M4
0.00.600.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.355 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.355 I ggml_metal_init: simdgroup reduction   = true
0.00.600.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.356 I ggml_metal_init: has residency sets    = true
0.00.600.356 I ggml_metal_init: has bfloat            = true
0.00.600.356 I ggml_metal_init: use bfloat            = true
0.00.600.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.362 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.276 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.622.895 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.622.902 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.622.931 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.141 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.143 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.144 I llama_init_from_model: graph nodes  = 967
0.00.626.144 I llama_init_from_model: graph splits = 2
0.00.626.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.057 I 
0.00.653.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.147 I perplexity: tokenizing the input ..
0.00.660.453 I perplexity: tokenization took 7.304 ms
0.00.660.467 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.063 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.798.741 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.798.761 I llama_perf_context_print:        load time =     643.06 ms
0.00.798.762 I llama_perf_context_print: prompt eval time =     135.65 ms /   128 tokens (    1.06 ms per token,   943.64 tokens per second)
0.00.798.763 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.763 I llama_perf_context_print:       total time =     145.71 ms /   129 tokens
0.00.799.166 I ggml_metal_free: deallocating

real	0m0.815s
user	0m0.079s
sys	0m0.127s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.379 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.387 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.389 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.393 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.393 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.396 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.397 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.042 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.787 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.788 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.788 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.789 I llama_model_loader: - type  f32:  194 tensors
0.00.035.789 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.789 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.790 I print_info: file format = GGUF V3 (latest)
0.00.035.790 I print_info: file type   = Q4_1
0.00.035.791 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.905 I load: special tokens cache size = 25
0.00.051.146 I load: token to piece cache size = 0.2984 MB
0.00.051.160 I print_info: arch             = gptneox
0.00.051.161 I print_info: vocab_only       = 0
0.00.051.161 I print_info: n_ctx_train      = 2048
0.00.051.161 I print_info: n_embd           = 2048
0.00.051.162 I print_info: n_layer          = 24
0.00.051.164 I print_info: n_head           = 16
0.00.051.165 I print_info: n_head_kv        = 16
0.00.051.165 I print_info: n_rot            = 32
0.00.051.165 I print_info: n_swa            = 0
0.00.051.165 I print_info: n_embd_head_k    = 128
0.00.051.166 I print_info: n_embd_head_v    = 128
0.00.051.166 I print_info: n_gqa            = 1
0.00.051.167 I print_info: n_embd_k_gqa     = 2048
0.00.051.168 I print_info: n_embd_v_gqa     = 2048
0.00.051.170 I print_info: f_norm_eps       = 1.0e-05
0.00.051.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.170 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.171 I print_info: f_logit_scale    = 0.0e+00
0.00.051.172 I print_info: n_ff             = 8192
0.00.051.172 I print_info: n_expert         = 0
0.00.051.172 I print_info: n_expert_used    = 0
0.00.051.173 I print_info: causal attn      = 1
0.00.051.173 I print_info: pooling type     = 0
0.00.051.174 I print_info: rope type        = 2
0.00.051.175 I print_info: rope scaling     = linear
0.00.051.175 I print_info: freq_base_train  = 10000.0
0.00.051.176 I print_info: freq_scale_train = 1
0.00.051.176 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.176 I print_info: rope_finetuned   = unknown
0.00.051.176 I print_info: ssm_d_conv       = 0
0.00.051.176 I print_info: ssm_d_inner      = 0
0.00.051.176 I print_info: ssm_d_state      = 0
0.00.051.176 I print_info: ssm_dt_rank      = 0
0.00.051.177 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.177 I print_info: model type       = 1.4B
0.00.051.181 I print_info: model params     = 1.41 B
0.00.051.181 I print_info: general.name     = 1.4B
0.00.051.181 I print_info: vocab type       = BPE
0.00.051.182 I print_info: n_vocab          = 50304
0.00.051.182 I print_info: n_merges         = 50009
0.00.051.182 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.182 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.183 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.183 I print_info: LF token         = 187 ''
0.00.051.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.183 I print_info: max token length = 1024
0.00.051.184 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.696.126 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.142 I load_tensors: offloading output layer to GPU
0.00.696.143 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.177 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.696.178 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.697.849 I llama_init_from_model: n_seq_max     = 1
0.00.697.853 I llama_init_from_model: n_ctx         = 2048
0.00.697.853 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.697.854 I llama_init_from_model: n_batch       = 2048
0.00.697.854 I llama_init_from_model: n_ubatch      = 512
0.00.697.854 I llama_init_from_model: flash_attn    = 0
0.00.697.857 I llama_init_from_model: freq_base     = 10000.0
0.00.697.857 I llama_init_from_model: freq_scale    = 1
0.00.697.866 I ggml_metal_init: allocating
0.00.697.935 I ggml_metal_init: found device: Apple M4
0.00.697.948 I ggml_metal_init: picking default device: Apple M4
0.00.699.832 I ggml_metal_init: using embedded metal library
0.00.706.650 I ggml_metal_init: GPU name:   Apple M4
0.00.706.655 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.706.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.706.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.706.657 I ggml_metal_init: simdgroup reduction   = true
0.00.706.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.706.657 I ggml_metal_init: has residency sets    = true
0.00.706.658 I ggml_metal_init: has bfloat            = true
0.00.706.658 I ggml_metal_init: use bfloat            = true
0.00.706.659 I ggml_metal_init: hasUnifiedMemory      = true
0.00.706.660 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.725.344 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.784.163 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.784.171 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.784.199 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.354 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.788.356 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.788.357 I llama_init_from_model: graph nodes  = 967
0.00.788.357 I llama_init_from_model: graph splits = 2
0.00.788.363 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.788.484 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.788.484 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.845.581 I main: llama threadpool init, n_threads = 4
0.00.845.632 I 
0.00.845.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.845.655 I 
0.00.845.823 I sampler seed: 1234
0.00.845.828 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.845.844 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.845.845 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.845.845 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.587.867 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.587.868 I llama_perf_context_print:        load time =     835.90 ms
0.01.587.869 I llama_perf_context_print: prompt eval time =      49.14 ms /     7 tokens (    7.02 ms per token,   142.46 tokens per second)
0.01.587.869 I llama_perf_context_print:        eval time =     690.11 ms /    63 runs   (   10.95 ms per token,    91.29 tokens per second)
0.01.587.870 I llama_perf_context_print:       total time =     743.00 ms /    70 tokens
0.01.588.137 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.111s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.140 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.321 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.327 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.274 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.257 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.130 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.131 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.132 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.132 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.132 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.133 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.133 I llama_model_loader: - type  f32:  194 tensors
0.00.026.133 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.134 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.135 I print_info: file format = GGUF V3 (latest)
0.00.026.135 I print_info: file type   = Q4_1
0.00.026.136 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.587 I load: special tokens cache size = 25
0.00.041.025 I load: token to piece cache size = 0.2984 MB
0.00.041.038 I print_info: arch             = gptneox
0.00.041.039 I print_info: vocab_only       = 0
0.00.041.040 I print_info: n_ctx_train      = 2048
0.00.041.040 I print_info: n_embd           = 2048
0.00.041.040 I print_info: n_layer          = 24
0.00.041.046 I print_info: n_head           = 16
0.00.041.046 I print_info: n_head_kv        = 16
0.00.041.046 I print_info: n_rot            = 32
0.00.041.047 I print_info: n_swa            = 0
0.00.041.047 I print_info: n_embd_head_k    = 128
0.00.041.047 I print_info: n_embd_head_v    = 128
0.00.041.047 I print_info: n_gqa            = 1
0.00.041.050 I print_info: n_embd_k_gqa     = 2048
0.00.041.050 I print_info: n_embd_v_gqa     = 2048
0.00.041.051 I print_info: f_norm_eps       = 1.0e-05
0.00.041.051 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.051 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.052 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.052 I print_info: f_logit_scale    = 0.0e+00
0.00.041.052 I print_info: n_ff             = 8192
0.00.041.053 I print_info: n_expert         = 0
0.00.041.053 I print_info: n_expert_used    = 0
0.00.041.053 I print_info: causal attn      = 1
0.00.041.053 I print_info: pooling type     = 0
0.00.041.053 I print_info: rope type        = 2
0.00.041.053 I print_info: rope scaling     = linear
0.00.041.054 I print_info: freq_base_train  = 10000.0
0.00.041.054 I print_info: freq_scale_train = 1
0.00.041.054 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.054 I print_info: rope_finetuned   = unknown
0.00.041.054 I print_info: ssm_d_conv       = 0
0.00.041.055 I print_info: ssm_d_inner      = 0
0.00.041.055 I print_info: ssm_d_state      = 0
0.00.041.055 I print_info: ssm_dt_rank      = 0
0.00.041.055 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.055 I print_info: model type       = 1.4B
0.00.041.055 I print_info: model params     = 1.41 B
0.00.041.055 I print_info: general.name     = 1.4B
0.00.041.056 I print_info: vocab type       = BPE
0.00.041.056 I print_info: n_vocab          = 50304
0.00.041.056 I print_info: n_merges         = 50009
0.00.041.057 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.057 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.057 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.057 I print_info: LF token         = 187 ''
0.00.041.058 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.058 I print_info: max token length = 1024
0.00.041.058 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.300 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.314 I load_tensors: offloading output layer to GPU
0.00.610.315 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.352 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.610.354 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.611.830 I llama_init_from_model: n_seq_max     = 1
0.00.611.832 I llama_init_from_model: n_ctx         = 128
0.00.611.833 I llama_init_from_model: n_ctx_per_seq = 128
0.00.611.833 I llama_init_from_model: n_batch       = 128
0.00.611.834 I llama_init_from_model: n_ubatch      = 128
0.00.611.834 I llama_init_from_model: flash_attn    = 0
0.00.611.836 I llama_init_from_model: freq_base     = 10000.0
0.00.611.837 I llama_init_from_model: freq_scale    = 1
0.00.611.837 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.611.839 I ggml_metal_init: allocating
0.00.611.923 I ggml_metal_init: found device: Apple M4
0.00.611.936 I ggml_metal_init: picking default device: Apple M4
0.00.613.767 I ggml_metal_init: using embedded metal library
0.00.620.607 I ggml_metal_init: GPU name:   Apple M4
0.00.620.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.620.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.620.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.620.618 I ggml_metal_init: simdgroup reduction   = true
0.00.620.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.620.619 I ggml_metal_init: has residency sets    = true
0.00.620.619 I ggml_metal_init: has bfloat            = true
0.00.620.619 I ggml_metal_init: use bfloat            = true
0.00.620.620 I ggml_metal_init: hasUnifiedMemory      = true
0.00.620.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.602 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.033 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.037 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.063 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.645.366 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.645.368 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.645.369 I llama_init_from_model: graph nodes  = 967
0.00.645.369 I llama_init_from_model: graph splits = 2
0.00.645.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.380 I 
0.00.671.480 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.489 I perplexity: tokenizing the input ..
0.00.678.672 I perplexity: tokenization took 7.181 ms
0.00.678.680 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.119 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.812.457 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.812.484 I llama_perf_context_print:        load time =     662.23 ms
0.00.812.485 I llama_perf_context_print: prompt eval time =     131.57 ms /   128 tokens (    1.03 ms per token,   972.87 tokens per second)
0.00.812.485 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.812.487 I llama_perf_context_print:       total time =     141.11 ms /   129 tokens
0.00.812.871 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.081s
sys	0m0.115s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.516 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.369 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.374 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.381 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.390 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.076 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.778 I llama_model_loader: - type  f32:  194 tensors
0.00.026.778 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.778 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.779 I print_info: file format = GGUF V3 (latest)
0.00.026.779 I print_info: file type   = Q5_0
0.00.026.780 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.587 I load: special tokens cache size = 25
0.00.040.791 I load: token to piece cache size = 0.2984 MB
0.00.040.805 I print_info: arch             = gptneox
0.00.040.806 I print_info: vocab_only       = 0
0.00.040.807 I print_info: n_ctx_train      = 2048
0.00.040.807 I print_info: n_embd           = 2048
0.00.040.807 I print_info: n_layer          = 24
0.00.040.810 I print_info: n_head           = 16
0.00.040.811 I print_info: n_head_kv        = 16
0.00.040.811 I print_info: n_rot            = 32
0.00.040.812 I print_info: n_swa            = 0
0.00.040.812 I print_info: n_embd_head_k    = 128
0.00.040.812 I print_info: n_embd_head_v    = 128
0.00.040.813 I print_info: n_gqa            = 1
0.00.040.813 I print_info: n_embd_k_gqa     = 2048
0.00.040.814 I print_info: n_embd_v_gqa     = 2048
0.00.040.817 I print_info: f_norm_eps       = 1.0e-05
0.00.040.817 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.817 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.817 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.817 I print_info: f_logit_scale    = 0.0e+00
0.00.040.818 I print_info: n_ff             = 8192
0.00.040.818 I print_info: n_expert         = 0
0.00.040.818 I print_info: n_expert_used    = 0
0.00.040.818 I print_info: causal attn      = 1
0.00.040.819 I print_info: pooling type     = 0
0.00.040.819 I print_info: rope type        = 2
0.00.040.819 I print_info: rope scaling     = linear
0.00.040.819 I print_info: freq_base_train  = 10000.0
0.00.040.819 I print_info: freq_scale_train = 1
0.00.040.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.820 I print_info: rope_finetuned   = unknown
0.00.040.820 I print_info: ssm_d_conv       = 0
0.00.040.820 I print_info: ssm_d_inner      = 0
0.00.040.820 I print_info: ssm_d_state      = 0
0.00.040.820 I print_info: ssm_dt_rank      = 0
0.00.040.821 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.821 I print_info: model type       = 1.4B
0.00.040.821 I print_info: model params     = 1.41 B
0.00.040.821 I print_info: general.name     = 1.4B
0.00.040.822 I print_info: vocab type       = BPE
0.00.040.822 I print_info: n_vocab          = 50304
0.00.040.822 I print_info: n_merges         = 50009
0.00.040.823 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.823 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.823 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.823 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.823 I print_info: LF token         = 187 ''
0.00.040.824 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: max token length = 1024
0.00.040.824 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.479 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.495 I load_tensors: offloading output layer to GPU
0.00.668.496 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.536 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.668.537 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.670.224 I llama_init_from_model: n_seq_max     = 1
0.00.670.227 I llama_init_from_model: n_ctx         = 2048
0.00.670.227 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.228 I llama_init_from_model: n_batch       = 2048
0.00.670.228 I llama_init_from_model: n_ubatch      = 512
0.00.670.229 I llama_init_from_model: flash_attn    = 0
0.00.670.231 I llama_init_from_model: freq_base     = 10000.0
0.00.670.231 I llama_init_from_model: freq_scale    = 1
0.00.670.236 I ggml_metal_init: allocating
0.00.670.380 I ggml_metal_init: found device: Apple M4
0.00.670.394 I ggml_metal_init: picking default device: Apple M4
0.00.672.279 I ggml_metal_init: using embedded metal library
0.00.679.212 I ggml_metal_init: GPU name:   Apple M4
0.00.679.219 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.220 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.222 I ggml_metal_init: simdgroup reduction   = true
0.00.679.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.222 I ggml_metal_init: has residency sets    = true
0.00.679.222 I ggml_metal_init: has bfloat            = true
0.00.679.223 I ggml_metal_init: use bfloat            = true
0.00.679.224 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.227 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.871 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.751.713 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.751.720 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.751.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.740 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.743 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.743 I llama_init_from_model: graph nodes  = 967
0.00.756.744 I llama_init_from_model: graph splits = 2
0.00.756.750 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.879 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.910 I main: llama threadpool init, n_threads = 4
0.00.813.962 I 
0.00.813.994 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.997 I 
0.00.814.243 I sampler seed: 1234
0.00.814.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.273 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.275 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.275 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.605.017 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.605.018 I llama_perf_context_print:        load time =     802.68 ms
0.01.605.019 I llama_perf_context_print: prompt eval time =      43.17 ms /     7 tokens (    6.17 ms per token,   162.15 tokens per second)
0.01.605.019 I llama_perf_context_print:        eval time =     744.64 ms /    63 runs   (   11.82 ms per token,    84.60 tokens per second)
0.01.605.020 I llama_perf_context_print:       total time =     791.82 ms /    70 tokens
0.01.605.258 I ggml_metal_free: deallocating

real	0m1.624s
user	0m0.110s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.884 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.884 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.455 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.458 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.459 I llama_model_loader: - type  f32:  194 tensors
0.00.025.459 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.459 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.460 I print_info: file format = GGUF V3 (latest)
0.00.025.461 I print_info: file type   = Q5_0
0.00.025.462 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.034 I load: special tokens cache size = 25
0.00.040.520 I load: token to piece cache size = 0.2984 MB
0.00.040.537 I print_info: arch             = gptneox
0.00.040.538 I print_info: vocab_only       = 0
0.00.040.538 I print_info: n_ctx_train      = 2048
0.00.040.538 I print_info: n_embd           = 2048
0.00.040.539 I print_info: n_layer          = 24
0.00.040.543 I print_info: n_head           = 16
0.00.040.543 I print_info: n_head_kv        = 16
0.00.040.544 I print_info: n_rot            = 32
0.00.040.544 I print_info: n_swa            = 0
0.00.040.544 I print_info: n_embd_head_k    = 128
0.00.040.544 I print_info: n_embd_head_v    = 128
0.00.040.545 I print_info: n_gqa            = 1
0.00.040.545 I print_info: n_embd_k_gqa     = 2048
0.00.040.546 I print_info: n_embd_v_gqa     = 2048
0.00.040.546 I print_info: f_norm_eps       = 1.0e-05
0.00.040.547 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.547 I print_info: f_logit_scale    = 0.0e+00
0.00.040.549 I print_info: n_ff             = 8192
0.00.040.549 I print_info: n_expert         = 0
0.00.040.549 I print_info: n_expert_used    = 0
0.00.040.549 I print_info: causal attn      = 1
0.00.040.550 I print_info: pooling type     = 0
0.00.040.550 I print_info: rope type        = 2
0.00.040.550 I print_info: rope scaling     = linear
0.00.040.550 I print_info: freq_base_train  = 10000.0
0.00.040.551 I print_info: freq_scale_train = 1
0.00.040.551 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.551 I print_info: rope_finetuned   = unknown
0.00.040.551 I print_info: ssm_d_conv       = 0
0.00.040.551 I print_info: ssm_d_inner      = 0
0.00.040.551 I print_info: ssm_d_state      = 0
0.00.040.551 I print_info: ssm_dt_rank      = 0
0.00.040.552 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.552 I print_info: model type       = 1.4B
0.00.040.552 I print_info: model params     = 1.41 B
0.00.040.552 I print_info: general.name     = 1.4B
0.00.040.553 I print_info: vocab type       = BPE
0.00.040.553 I print_info: n_vocab          = 50304
0.00.040.553 I print_info: n_merges         = 50009
0.00.040.553 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.553 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.553 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.554 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.554 I print_info: LF token         = 187 ''
0.00.040.554 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.554 I print_info: max token length = 1024
0.00.040.555 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.511 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.524 I load_tensors: offloading output layer to GPU
0.00.679.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.561 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.679.562 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.681.327 I llama_init_from_model: n_seq_max     = 1
0.00.681.329 I llama_init_from_model: n_ctx         = 128
0.00.681.330 I llama_init_from_model: n_ctx_per_seq = 128
0.00.681.331 I llama_init_from_model: n_batch       = 128
0.00.681.331 I llama_init_from_model: n_ubatch      = 128
0.00.681.332 I llama_init_from_model: flash_attn    = 0
0.00.681.334 I llama_init_from_model: freq_base     = 10000.0
0.00.681.334 I llama_init_from_model: freq_scale    = 1
0.00.681.335 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.681.338 I ggml_metal_init: allocating
0.00.681.431 I ggml_metal_init: found device: Apple M4
0.00.681.445 I ggml_metal_init: picking default device: Apple M4
0.00.683.237 I ggml_metal_init: using embedded metal library
0.00.689.729 I ggml_metal_init: GPU name:   Apple M4
0.00.689.736 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.737 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.738 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.738 I ggml_metal_init: simdgroup reduction   = true
0.00.689.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.739 I ggml_metal_init: has residency sets    = true
0.00.689.739 I ggml_metal_init: has bfloat            = true
0.00.689.739 I ggml_metal_init: use bfloat            = true
0.00.689.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.707.085 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.710.611 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.710.644 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.883 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.713.884 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.713.885 I llama_init_from_model: graph nodes  = 967
0.00.713.886 I llama_init_from_model: graph splits = 2
0.00.713.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.713.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.222 I 
0.00.743.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.329 I perplexity: tokenizing the input ..
0.00.750.570 I perplexity: tokenization took 7.238 ms
0.00.750.578 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.895.737 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.897.191 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.897.218 I llama_perf_context_print:        load time =     733.34 ms
0.00.897.219 I llama_perf_context_print: prompt eval time =     144.22 ms /   128 tokens (    1.13 ms per token,   887.51 tokens per second)
0.00.897.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.897.220 I llama_perf_context_print:       total time =     154.00 ms /   129 tokens
0.00.897.614 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.080s
sys	0m0.128s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.427 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.437 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.442 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.860 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.861 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.862 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.862 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.863 I llama_model_loader: - type  f32:  194 tensors
0.00.024.863 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.864 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.864 I print_info: file format = GGUF V3 (latest)
0.00.024.865 I print_info: file type   = Q5_1
0.00.024.866 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.629 I load: special tokens cache size = 25
0.00.038.994 I load: token to piece cache size = 0.2984 MB
0.00.039.008 I print_info: arch             = gptneox
0.00.039.009 I print_info: vocab_only       = 0
0.00.039.009 I print_info: n_ctx_train      = 2048
0.00.039.010 I print_info: n_embd           = 2048
0.00.039.010 I print_info: n_layer          = 24
0.00.039.012 I print_info: n_head           = 16
0.00.039.013 I print_info: n_head_kv        = 16
0.00.039.013 I print_info: n_rot            = 32
0.00.039.013 I print_info: n_swa            = 0
0.00.039.013 I print_info: n_embd_head_k    = 128
0.00.039.013 I print_info: n_embd_head_v    = 128
0.00.039.014 I print_info: n_gqa            = 1
0.00.039.015 I print_info: n_embd_k_gqa     = 2048
0.00.039.015 I print_info: n_embd_v_gqa     = 2048
0.00.039.016 I print_info: f_norm_eps       = 1.0e-05
0.00.039.016 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.017 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.017 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.017 I print_info: f_logit_scale    = 0.0e+00
0.00.039.018 I print_info: n_ff             = 8192
0.00.039.018 I print_info: n_expert         = 0
0.00.039.018 I print_info: n_expert_used    = 0
0.00.039.018 I print_info: causal attn      = 1
0.00.039.018 I print_info: pooling type     = 0
0.00.039.020 I print_info: rope type        = 2
0.00.039.021 I print_info: rope scaling     = linear
0.00.039.022 I print_info: freq_base_train  = 10000.0
0.00.039.022 I print_info: freq_scale_train = 1
0.00.039.022 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.022 I print_info: rope_finetuned   = unknown
0.00.039.022 I print_info: ssm_d_conv       = 0
0.00.039.023 I print_info: ssm_d_inner      = 0
0.00.039.023 I print_info: ssm_d_state      = 0
0.00.039.023 I print_info: ssm_dt_rank      = 0
0.00.039.023 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.023 I print_info: model type       = 1.4B
0.00.039.023 I print_info: model params     = 1.41 B
0.00.039.027 I print_info: general.name     = 1.4B
0.00.039.027 I print_info: vocab type       = BPE
0.00.039.027 I print_info: n_vocab          = 50304
0.00.039.027 I print_info: n_merges         = 50009
0.00.039.028 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.028 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.031 I print_info: LF token         = 187 ''
0.00.039.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.032 I print_info: max token length = 1024
0.00.039.032 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.610.105 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.110 I load_tensors: offloading output layer to GPU
0.00.610.111 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.134 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.610.135 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.611.583 I llama_init_from_model: n_seq_max     = 1
0.00.611.586 I llama_init_from_model: n_ctx         = 2048
0.00.611.586 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.611.586 I llama_init_from_model: n_batch       = 2048
0.00.611.587 I llama_init_from_model: n_ubatch      = 512
0.00.611.588 I llama_init_from_model: flash_attn    = 0
0.00.611.589 I llama_init_from_model: freq_base     = 10000.0
0.00.611.590 I llama_init_from_model: freq_scale    = 1
0.00.611.591 I ggml_metal_init: allocating
0.00.611.612 I ggml_metal_init: found device: Apple M4
0.00.611.622 I ggml_metal_init: picking default device: Apple M4
0.00.613.176 I ggml_metal_init: using embedded metal library
0.00.619.376 I ggml_metal_init: GPU name:   Apple M4
0.00.619.380 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.619.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.619.382 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.619.382 I ggml_metal_init: simdgroup reduction   = true
0.00.619.383 I ggml_metal_init: simdgroup matrix mul. = true
0.00.619.383 I ggml_metal_init: has residency sets    = true
0.00.619.383 I ggml_metal_init: has bfloat            = true
0.00.619.383 I ggml_metal_init: use bfloat            = true
0.00.619.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.619.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.948 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.960 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.991 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.702.530 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.702.532 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.702.532 I llama_init_from_model: graph nodes  = 967
0.00.702.533 I llama_init_from_model: graph splits = 2
0.00.702.538 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.702.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.173 I main: llama threadpool init, n_threads = 4
0.00.759.224 I 
0.00.759.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.248 I 
0.00.759.421 I sampler seed: 1234
0.00.759.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.442 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.443 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.443 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.596.123 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46465.97 tokens per second)
0.01.596.124 I llama_perf_context_print:        load time =     749.58 ms
0.01.596.125 I llama_perf_context_print: prompt eval time =      41.91 ms /     7 tokens (    5.99 ms per token,   167.01 tokens per second)
0.01.596.126 I llama_perf_context_print:        eval time =     792.16 ms /    63 runs   (   12.57 ms per token,    79.53 tokens per second)
0.01.596.126 I llama_perf_context_print:       total time =     837.68 ms /    70 tokens
0.01.596.412 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.110s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.141 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.645 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.650 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.653 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.653 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.654 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.655 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.655 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.656 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.658 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.658 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.659 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.659 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.661 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.664 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.664 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.436 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.195 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.196 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.197 I llama_model_loader: - type  f32:  194 tensors
0.00.025.197 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.198 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.198 I print_info: file format = GGUF V3 (latest)
0.00.025.199 I print_info: file type   = Q5_1
0.00.025.200 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.256 I load: special tokens cache size = 25
0.00.039.666 I load: token to piece cache size = 0.2984 MB
0.00.039.683 I print_info: arch             = gptneox
0.00.039.684 I print_info: vocab_only       = 0
0.00.039.684 I print_info: n_ctx_train      = 2048
0.00.039.684 I print_info: n_embd           = 2048
0.00.039.684 I print_info: n_layer          = 24
0.00.039.688 I print_info: n_head           = 16
0.00.039.688 I print_info: n_head_kv        = 16
0.00.039.689 I print_info: n_rot            = 32
0.00.039.689 I print_info: n_swa            = 0
0.00.039.689 I print_info: n_embd_head_k    = 128
0.00.039.691 I print_info: n_embd_head_v    = 128
0.00.039.692 I print_info: n_gqa            = 1
0.00.039.692 I print_info: n_embd_k_gqa     = 2048
0.00.039.693 I print_info: n_embd_v_gqa     = 2048
0.00.039.694 I print_info: f_norm_eps       = 1.0e-05
0.00.039.694 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.694 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.694 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.694 I print_info: f_logit_scale    = 0.0e+00
0.00.039.695 I print_info: n_ff             = 8192
0.00.039.695 I print_info: n_expert         = 0
0.00.039.695 I print_info: n_expert_used    = 0
0.00.039.695 I print_info: causal attn      = 1
0.00.039.695 I print_info: pooling type     = 0
0.00.039.695 I print_info: rope type        = 2
0.00.039.696 I print_info: rope scaling     = linear
0.00.039.696 I print_info: freq_base_train  = 10000.0
0.00.039.696 I print_info: freq_scale_train = 1
0.00.039.696 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.697 I print_info: rope_finetuned   = unknown
0.00.039.697 I print_info: ssm_d_conv       = 0
0.00.039.697 I print_info: ssm_d_inner      = 0
0.00.039.697 I print_info: ssm_d_state      = 0
0.00.039.697 I print_info: ssm_dt_rank      = 0
0.00.039.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.697 I print_info: model type       = 1.4B
0.00.039.698 I print_info: model params     = 1.41 B
0.00.039.698 I print_info: general.name     = 1.4B
0.00.039.698 I print_info: vocab type       = BPE
0.00.039.698 I print_info: n_vocab          = 50304
0.00.039.699 I print_info: n_merges         = 50009
0.00.039.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: LF token         = 187 ''
0.00.039.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: max token length = 1024
0.00.039.700 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.898 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.911 I load_tensors: offloading output layer to GPU
0.00.638.912 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.945 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.638.946 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.640.514 I llama_init_from_model: n_seq_max     = 1
0.00.640.520 I llama_init_from_model: n_ctx         = 128
0.00.640.520 I llama_init_from_model: n_ctx_per_seq = 128
0.00.640.521 I llama_init_from_model: n_batch       = 128
0.00.640.521 I llama_init_from_model: n_ubatch      = 128
0.00.640.522 I llama_init_from_model: flash_attn    = 0
0.00.640.523 I llama_init_from_model: freq_base     = 10000.0
0.00.640.523 I llama_init_from_model: freq_scale    = 1
0.00.640.524 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.640.526 I ggml_metal_init: allocating
0.00.640.610 I ggml_metal_init: found device: Apple M4
0.00.640.626 I ggml_metal_init: picking default device: Apple M4
0.00.642.430 I ggml_metal_init: using embedded metal library
0.00.649.257 I ggml_metal_init: GPU name:   Apple M4
0.00.649.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.268 I ggml_metal_init: simdgroup reduction   = true
0.00.649.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.268 I ggml_metal_init: has residency sets    = true
0.00.649.269 I ggml_metal_init: has bfloat            = true
0.00.649.269 I ggml_metal_init: use bfloat            = true
0.00.649.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.065 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.670.646 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.670.673 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.941 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.673.943 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.673.944 I llama_init_from_model: graph nodes  = 967
0.00.673.944 I llama_init_from_model: graph splits = 2
0.00.673.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.673.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.509 I 
0.00.707.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.604 I perplexity: tokenizing the input ..
0.00.714.186 I perplexity: tokenization took 6.58 ms
0.00.714.190 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.856.239 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.857.659 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.857.681 I llama_perf_context_print:        load time =     697.66 ms
0.00.857.681 I llama_perf_context_print: prompt eval time =     141.49 ms /   128 tokens (    1.11 ms per token,   904.68 tokens per second)
0.00.857.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.857.682 I llama_perf_context_print:       total time =     150.18 ms /   129 tokens
0.00.858.051 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.078s
sys	0m0.155s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.011.007 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.896 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.909 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.909 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.909 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.910 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.910 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.911 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.912 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.914 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.915 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.730 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.918 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.918 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.919 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.919 I llama_model_loader: - type  f32:  194 tensors
0.00.026.920 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.920 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.921 I print_info: file format = GGUF V3 (latest)
0.00.026.922 I print_info: file type   = Q2_K - Medium
0.00.026.922 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.109 I load: special tokens cache size = 25
0.00.041.530 I load: token to piece cache size = 0.2984 MB
0.00.041.547 I print_info: arch             = gptneox
0.00.041.548 I print_info: vocab_only       = 0
0.00.041.548 I print_info: n_ctx_train      = 2048
0.00.041.549 I print_info: n_embd           = 2048
0.00.041.549 I print_info: n_layer          = 24
0.00.041.553 I print_info: n_head           = 16
0.00.041.553 I print_info: n_head_kv        = 16
0.00.041.553 I print_info: n_rot            = 32
0.00.041.554 I print_info: n_swa            = 0
0.00.041.554 I print_info: n_embd_head_k    = 128
0.00.041.554 I print_info: n_embd_head_v    = 128
0.00.041.554 I print_info: n_gqa            = 1
0.00.041.555 I print_info: n_embd_k_gqa     = 2048
0.00.041.556 I print_info: n_embd_v_gqa     = 2048
0.00.041.556 I print_info: f_norm_eps       = 1.0e-05
0.00.041.557 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.557 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.557 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.557 I print_info: f_logit_scale    = 0.0e+00
0.00.041.566 I print_info: n_ff             = 8192
0.00.041.566 I print_info: n_expert         = 0
0.00.041.566 I print_info: n_expert_used    = 0
0.00.041.567 I print_info: causal attn      = 1
0.00.041.568 I print_info: pooling type     = 0
0.00.041.568 I print_info: rope type        = 2
0.00.041.569 I print_info: rope scaling     = linear
0.00.041.569 I print_info: freq_base_train  = 10000.0
0.00.041.569 I print_info: freq_scale_train = 1
0.00.041.569 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.570 I print_info: rope_finetuned   = unknown
0.00.041.570 I print_info: ssm_d_conv       = 0
0.00.041.570 I print_info: ssm_d_inner      = 0
0.00.041.570 I print_info: ssm_d_state      = 0
0.00.041.570 I print_info: ssm_dt_rank      = 0
0.00.041.570 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.571 I print_info: model type       = 1.4B
0.00.041.571 I print_info: model params     = 1.41 B
0.00.041.571 I print_info: general.name     = 1.4B
0.00.041.572 I print_info: vocab type       = BPE
0.00.041.574 I print_info: n_vocab          = 50304
0.00.041.574 I print_info: n_merges         = 50009
0.00.041.574 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.574 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.574 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.574 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.574 I print_info: LF token         = 187 ''
0.00.041.575 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.575 I print_info: max token length = 1024
0.00.041.576 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.339.307 I load_tensors: offloading 24 repeating layers to GPU
0.00.339.315 I load_tensors: offloading output layer to GPU
0.00.339.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.339.354 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.339.356 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.340.546 I llama_init_from_model: n_seq_max     = 1
0.00.340.550 I llama_init_from_model: n_ctx         = 2048
0.00.340.551 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.551 I llama_init_from_model: n_batch       = 2048
0.00.340.552 I llama_init_from_model: n_ubatch      = 512
0.00.340.552 I llama_init_from_model: flash_attn    = 0
0.00.340.555 I llama_init_from_model: freq_base     = 10000.0
0.00.340.555 I llama_init_from_model: freq_scale    = 1
0.00.340.558 I ggml_metal_init: allocating
0.00.340.615 I ggml_metal_init: found device: Apple M4
0.00.340.641 I ggml_metal_init: picking default device: Apple M4
0.00.342.530 I ggml_metal_init: using embedded metal library
0.00.348.680 I ggml_metal_init: GPU name:   Apple M4
0.00.348.701 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.348.702 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.348.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.348.703 I ggml_metal_init: simdgroup reduction   = true
0.00.348.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.348.704 I ggml_metal_init: has residency sets    = true
0.00.348.704 I ggml_metal_init: has bfloat            = true
0.00.348.704 I ggml_metal_init: use bfloat            = true
0.00.348.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.348.714 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.173 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.431.782 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.431.791 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.431.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.079 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.437.081 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.437.081 I llama_init_from_model: graph nodes  = 967
0.00.437.081 I llama_init_from_model: graph splits = 2
0.00.437.088 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.437.207 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.437.208 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.081 I main: llama threadpool init, n_threads = 4
0.00.495.127 I 
0.00.495.146 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.146 I 
0.00.495.333 I sampler seed: 1234
0.00.495.339 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.495.367 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.495.367 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.495.367 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.161.869 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49650.35 tokens per second)
0.01.161.870 I llama_perf_context_print:        load time =     483.35 ms
0.01.161.870 I llama_perf_context_print: prompt eval time =      35.49 ms /     7 tokens (    5.07 ms per token,   197.24 tokens per second)
0.01.161.871 I llama_perf_context_print:        eval time =     628.39 ms /    63 runs   (    9.97 ms per token,   100.26 tokens per second)
0.01.161.871 I llama_perf_context_print:       total time =     667.51 ms /    70 tokens
0.01.162.083 I ggml_metal_free: deallocating

real	0m1.181s
user	0m0.113s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.375 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.376 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.378 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.379 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.382 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.057 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.060 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.060 I llama_model_loader: - type  f32:  194 tensors
0.00.026.060 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.061 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.061 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.062 I print_info: file format = GGUF V3 (latest)
0.00.026.062 I print_info: file type   = Q2_K - Medium
0.00.026.063 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.457 I load: special tokens cache size = 25
0.00.040.769 I load: token to piece cache size = 0.2984 MB
0.00.040.786 I print_info: arch             = gptneox
0.00.040.788 I print_info: vocab_only       = 0
0.00.040.788 I print_info: n_ctx_train      = 2048
0.00.040.788 I print_info: n_embd           = 2048
0.00.040.788 I print_info: n_layer          = 24
0.00.040.792 I print_info: n_head           = 16
0.00.040.793 I print_info: n_head_kv        = 16
0.00.040.793 I print_info: n_rot            = 32
0.00.040.793 I print_info: n_swa            = 0
0.00.040.793 I print_info: n_embd_head_k    = 128
0.00.040.793 I print_info: n_embd_head_v    = 128
0.00.040.794 I print_info: n_gqa            = 1
0.00.040.794 I print_info: n_embd_k_gqa     = 2048
0.00.040.795 I print_info: n_embd_v_gqa     = 2048
0.00.040.795 I print_info: f_norm_eps       = 1.0e-05
0.00.040.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.796 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.796 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.796 I print_info: f_logit_scale    = 0.0e+00
0.00.040.796 I print_info: n_ff             = 8192
0.00.040.797 I print_info: n_expert         = 0
0.00.040.797 I print_info: n_expert_used    = 0
0.00.040.797 I print_info: causal attn      = 1
0.00.040.797 I print_info: pooling type     = 0
0.00.040.797 I print_info: rope type        = 2
0.00.040.797 I print_info: rope scaling     = linear
0.00.040.798 I print_info: freq_base_train  = 10000.0
0.00.040.798 I print_info: freq_scale_train = 1
0.00.040.798 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.798 I print_info: rope_finetuned   = unknown
0.00.040.798 I print_info: ssm_d_conv       = 0
0.00.040.799 I print_info: ssm_d_inner      = 0
0.00.040.799 I print_info: ssm_d_state      = 0
0.00.040.799 I print_info: ssm_dt_rank      = 0
0.00.040.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.799 I print_info: model type       = 1.4B
0.00.040.800 I print_info: model params     = 1.41 B
0.00.040.800 I print_info: general.name     = 1.4B
0.00.040.809 I print_info: vocab type       = BPE
0.00.040.809 I print_info: n_vocab          = 50304
0.00.040.809 I print_info: n_merges         = 50009
0.00.040.810 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.810 I print_info: LF token         = 187 ''
0.00.040.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.811 I print_info: max token length = 1024
0.00.040.811 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.323.743 I load_tensors: offloading 24 repeating layers to GPU
0.00.323.750 I load_tensors: offloading output layer to GPU
0.00.323.750 I load_tensors: offloaded 25/25 layers to GPU
0.00.323.767 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.323.768 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.324.669 I llama_init_from_model: n_seq_max     = 1
0.00.324.674 I llama_init_from_model: n_ctx         = 128
0.00.324.674 I llama_init_from_model: n_ctx_per_seq = 128
0.00.324.674 I llama_init_from_model: n_batch       = 128
0.00.324.675 I llama_init_from_model: n_ubatch      = 128
0.00.324.675 I llama_init_from_model: flash_attn    = 0
0.00.324.676 I llama_init_from_model: freq_base     = 10000.0
0.00.324.677 I llama_init_from_model: freq_scale    = 1
0.00.324.677 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.324.679 I ggml_metal_init: allocating
0.00.324.720 I ggml_metal_init: found device: Apple M4
0.00.324.731 I ggml_metal_init: picking default device: Apple M4
0.00.325.853 I ggml_metal_init: using embedded metal library
0.00.330.149 I ggml_metal_init: GPU name:   Apple M4
0.00.330.157 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.330.158 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.330.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.330.159 I ggml_metal_init: simdgroup reduction   = true
0.00.330.159 I ggml_metal_init: simdgroup matrix mul. = true
0.00.330.160 I ggml_metal_init: has residency sets    = true
0.00.330.160 I ggml_metal_init: has bfloat            = true
0.00.330.160 I ggml_metal_init: use bfloat            = true
0.00.330.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.330.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.346 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.350.121 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.350.124 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.350.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.351.760 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.351.761 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.351.761 I llama_init_from_model: graph nodes  = 967
0.00.351.761 I llama_init_from_model: graph splits = 2
0.00.351.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.351.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.357 I 
0.00.381.393 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.397 I perplexity: tokenizing the input ..
0.00.385.327 I perplexity: tokenization took 3.928 ms
0.00.385.330 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.530.578 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.531.920 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.531.947 I llama_perf_context_print:        load time =     371.23 ms
0.00.531.948 I llama_perf_context_print: prompt eval time =     145.01 ms /   128 tokens (    1.13 ms per token,   882.72 tokens per second)
0.00.531.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.531.951 I llama_perf_context_print:       total time =     150.59 ms /   129 tokens
0.00.532.318 I ggml_metal_free: deallocating

real	0m0.548s
user	0m0.073s
sys	0m0.062s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.031.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.622 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.626 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.523 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.331 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.332 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.333 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.333 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.040.334 I llama_model_loader: - type  f32:  194 tensors
0.00.040.334 I llama_model_loader: - type q3_K:   25 tensors
0.00.040.334 I llama_model_loader: - type q4_K:   71 tensors
0.00.040.335 I llama_model_loader: - type q5_K:    1 tensors
0.00.040.335 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.335 I print_info: file format = GGUF V3 (latest)
0.00.040.336 I print_info: file type   = Q3_K - Medium
0.00.040.337 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.049.053 I load: special tokens cache size = 25
0.00.056.447 I load: token to piece cache size = 0.2984 MB
0.00.056.461 I print_info: arch             = gptneox
0.00.056.462 I print_info: vocab_only       = 0
0.00.056.462 I print_info: n_ctx_train      = 2048
0.00.056.462 I print_info: n_embd           = 2048
0.00.056.463 I print_info: n_layer          = 24
0.00.056.465 I print_info: n_head           = 16
0.00.056.466 I print_info: n_head_kv        = 16
0.00.056.466 I print_info: n_rot            = 32
0.00.056.466 I print_info: n_swa            = 0
0.00.056.466 I print_info: n_embd_head_k    = 128
0.00.056.467 I print_info: n_embd_head_v    = 128
0.00.056.467 I print_info: n_gqa            = 1
0.00.056.468 I print_info: n_embd_k_gqa     = 2048
0.00.056.468 I print_info: n_embd_v_gqa     = 2048
0.00.056.469 I print_info: f_norm_eps       = 1.0e-05
0.00.056.469 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.470 I print_info: f_logit_scale    = 0.0e+00
0.00.056.470 I print_info: n_ff             = 8192
0.00.056.470 I print_info: n_expert         = 0
0.00.056.471 I print_info: n_expert_used    = 0
0.00.056.472 I print_info: causal attn      = 1
0.00.056.472 I print_info: pooling type     = 0
0.00.056.472 I print_info: rope type        = 2
0.00.056.472 I print_info: rope scaling     = linear
0.00.056.473 I print_info: freq_base_train  = 10000.0
0.00.056.473 I print_info: freq_scale_train = 1
0.00.056.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.473 I print_info: rope_finetuned   = unknown
0.00.056.473 I print_info: ssm_d_conv       = 0
0.00.056.473 I print_info: ssm_d_inner      = 0
0.00.056.473 I print_info: ssm_d_state      = 0
0.00.056.473 I print_info: ssm_dt_rank      = 0
0.00.056.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.474 I print_info: model type       = 1.4B
0.00.056.474 I print_info: model params     = 1.41 B
0.00.056.474 I print_info: general.name     = 1.4B
0.00.056.475 I print_info: vocab type       = BPE
0.00.056.475 I print_info: n_vocab          = 50304
0.00.056.475 I print_info: n_merges         = 50009
0.00.056.475 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.475 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.476 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.476 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.476 I print_info: LF token         = 187 ''
0.00.056.477 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.477 I print_info: max token length = 1024
0.00.056.477 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.466.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.466.860 I load_tensors: offloading output layer to GPU
0.00.466.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.466.891 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.466.892 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.468.654 I llama_init_from_model: n_seq_max     = 1
0.00.468.656 I llama_init_from_model: n_ctx         = 2048
0.00.468.657 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.468.657 I llama_init_from_model: n_batch       = 2048
0.00.468.658 I llama_init_from_model: n_ubatch      = 512
0.00.468.658 I llama_init_from_model: flash_attn    = 0
0.00.468.661 I llama_init_from_model: freq_base     = 10000.0
0.00.468.661 I llama_init_from_model: freq_scale    = 1
0.00.468.665 I ggml_metal_init: allocating
0.00.468.730 I ggml_metal_init: found device: Apple M4
0.00.468.743 I ggml_metal_init: picking default device: Apple M4
0.00.470.613 I ggml_metal_init: using embedded metal library
0.00.476.480 I ggml_metal_init: GPU name:   Apple M4
0.00.476.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.476.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.476.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.476.498 I ggml_metal_init: simdgroup reduction   = true
0.00.476.498 I ggml_metal_init: simdgroup matrix mul. = true
0.00.476.499 I ggml_metal_init: has residency sets    = true
0.00.476.499 I ggml_metal_init: has bfloat            = true
0.00.476.499 I ggml_metal_init: use bfloat            = true
0.00.476.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.476.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.498.070 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.558.997 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.559.007 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.559.040 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.393 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.563.395 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.563.395 I llama_init_from_model: graph nodes  = 967
0.00.563.395 I llama_init_from_model: graph splits = 2
0.00.563.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.563.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.563.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.014 I main: llama threadpool init, n_threads = 4
0.00.622.060 I 
0.00.622.079 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.079 I 
0.00.622.243 I sampler seed: 1234
0.00.622.248 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.263 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.265 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.265 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.368.607 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48864.42 tokens per second)
0.01.368.609 I llama_perf_context_print:        load time =     612.35 ms
0.01.368.609 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.99 tokens per second)
0.01.368.610 I llama_perf_context_print:        eval time =     699.41 ms /    63 runs   (   11.10 ms per token,    90.08 tokens per second)
0.01.368.610 I llama_perf_context_print:       total time =     747.38 ms /    70 tokens
0.01.368.834 I ggml_metal_free: deallocating

real	0m1.385s
user	0m0.114s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.725 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.893 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.901 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.902 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.904 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.904 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.905 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.906 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.908 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.641 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.445 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.446 I llama_model_loader: - type  f32:  194 tensors
0.00.024.446 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.447 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.447 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.447 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.448 I print_info: file format = GGUF V3 (latest)
0.00.024.448 I print_info: file type   = Q3_K - Medium
0.00.024.449 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.554 I load: special tokens cache size = 25
0.00.039.110 I load: token to piece cache size = 0.2984 MB
0.00.039.126 I print_info: arch             = gptneox
0.00.039.127 I print_info: vocab_only       = 0
0.00.039.127 I print_info: n_ctx_train      = 2048
0.00.039.127 I print_info: n_embd           = 2048
0.00.039.128 I print_info: n_layer          = 24
0.00.039.132 I print_info: n_head           = 16
0.00.039.132 I print_info: n_head_kv        = 16
0.00.039.132 I print_info: n_rot            = 32
0.00.039.132 I print_info: n_swa            = 0
0.00.039.133 I print_info: n_embd_head_k    = 128
0.00.039.133 I print_info: n_embd_head_v    = 128
0.00.039.133 I print_info: n_gqa            = 1
0.00.039.134 I print_info: n_embd_k_gqa     = 2048
0.00.039.135 I print_info: n_embd_v_gqa     = 2048
0.00.039.135 I print_info: f_norm_eps       = 1.0e-05
0.00.039.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.136 I print_info: f_logit_scale    = 0.0e+00
0.00.039.137 I print_info: n_ff             = 8192
0.00.039.137 I print_info: n_expert         = 0
0.00.039.139 I print_info: n_expert_used    = 0
0.00.039.139 I print_info: causal attn      = 1
0.00.039.139 I print_info: pooling type     = 0
0.00.039.139 I print_info: rope type        = 2
0.00.039.139 I print_info: rope scaling     = linear
0.00.039.142 I print_info: freq_base_train  = 10000.0
0.00.039.142 I print_info: freq_scale_train = 1
0.00.039.142 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.142 I print_info: rope_finetuned   = unknown
0.00.039.142 I print_info: ssm_d_conv       = 0
0.00.039.142 I print_info: ssm_d_inner      = 0
0.00.039.142 I print_info: ssm_d_state      = 0
0.00.039.142 I print_info: ssm_dt_rank      = 0
0.00.039.143 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.143 I print_info: model type       = 1.4B
0.00.039.143 I print_info: model params     = 1.41 B
0.00.039.143 I print_info: general.name     = 1.4B
0.00.039.144 I print_info: vocab type       = BPE
0.00.039.144 I print_info: n_vocab          = 50304
0.00.039.144 I print_info: n_merges         = 50009
0.00.039.150 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.151 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.151 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.151 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.151 I print_info: LF token         = 187 ''
0.00.039.151 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.152 I print_info: max token length = 1024
0.00.039.152 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.426.496 I load_tensors: offloading 24 repeating layers to GPU
0.00.426.509 I load_tensors: offloading output layer to GPU
0.00.426.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.426.542 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.426.543 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.428.176 I llama_init_from_model: n_seq_max     = 1
0.00.428.182 I llama_init_from_model: n_ctx         = 128
0.00.428.183 I llama_init_from_model: n_ctx_per_seq = 128
0.00.428.184 I llama_init_from_model: n_batch       = 128
0.00.428.184 I llama_init_from_model: n_ubatch      = 128
0.00.428.184 I llama_init_from_model: flash_attn    = 0
0.00.428.186 I llama_init_from_model: freq_base     = 10000.0
0.00.428.187 I llama_init_from_model: freq_scale    = 1
0.00.428.187 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.428.190 I ggml_metal_init: allocating
0.00.428.279 I ggml_metal_init: found device: Apple M4
0.00.428.293 I ggml_metal_init: picking default device: Apple M4
0.00.430.200 I ggml_metal_init: using embedded metal library
0.00.436.098 I ggml_metal_init: GPU name:   Apple M4
0.00.436.107 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.436.108 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.436.109 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.436.109 I ggml_metal_init: simdgroup reduction   = true
0.00.436.110 I ggml_metal_init: simdgroup matrix mul. = true
0.00.436.110 I ggml_metal_init: has residency sets    = true
0.00.436.110 I ggml_metal_init: has bfloat            = true
0.00.436.111 I ggml_metal_init: use bfloat            = true
0.00.436.112 I ggml_metal_init: hasUnifiedMemory      = true
0.00.436.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.456.444 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.460.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.460.133 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.460.182 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.463.507 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.463.509 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.463.510 I llama_init_from_model: graph nodes  = 967
0.00.463.510 I llama_init_from_model: graph splits = 2
0.00.463.514 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.463.514 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.490.690 I 
0.00.490.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.490.788 I perplexity: tokenizing the input ..
0.00.497.765 I perplexity: tokenization took 6.973 ms
0.00.497.776 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.639.668 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.641.010 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.641.031 I llama_perf_context_print:        load time =     481.95 ms
0.00.641.032 I llama_perf_context_print: prompt eval time =     140.93 ms /   128 tokens (    1.10 ms per token,   908.25 tokens per second)
0.00.641.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.641.033 I llama_perf_context_print:       total time =     150.35 ms /   129 tokens
0.00.641.408 I ggml_metal_free: deallocating

real	0m0.655s
user	0m0.080s
sys	0m0.106s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.073 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.024.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.312 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.319 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.323 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.910 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.032.911 I llama_model_loader: - type  f32:  194 tensors
0.00.032.911 I llama_model_loader: - type q4_K:   61 tensors
0.00.032.911 I llama_model_loader: - type q5_K:   24 tensors
0.00.032.911 I llama_model_loader: - type q6_K:   13 tensors
0.00.032.912 I print_info: file format = GGUF V3 (latest)
0.00.032.912 I print_info: file type   = Q4_K - Medium
0.00.032.913 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.041.049 I load: special tokens cache size = 25
0.00.048.413 I load: token to piece cache size = 0.2984 MB
0.00.048.428 I print_info: arch             = gptneox
0.00.048.429 I print_info: vocab_only       = 0
0.00.048.429 I print_info: n_ctx_train      = 2048
0.00.048.429 I print_info: n_embd           = 2048
0.00.048.429 I print_info: n_layer          = 24
0.00.048.432 I print_info: n_head           = 16
0.00.048.432 I print_info: n_head_kv        = 16
0.00.048.433 I print_info: n_rot            = 32
0.00.048.433 I print_info: n_swa            = 0
0.00.048.433 I print_info: n_embd_head_k    = 128
0.00.048.433 I print_info: n_embd_head_v    = 128
0.00.048.434 I print_info: n_gqa            = 1
0.00.048.435 I print_info: n_embd_k_gqa     = 2048
0.00.048.436 I print_info: n_embd_v_gqa     = 2048
0.00.048.437 I print_info: f_norm_eps       = 1.0e-05
0.00.048.437 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.437 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.437 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.437 I print_info: f_logit_scale    = 0.0e+00
0.00.048.438 I print_info: n_ff             = 8192
0.00.048.438 I print_info: n_expert         = 0
0.00.048.438 I print_info: n_expert_used    = 0
0.00.048.438 I print_info: causal attn      = 1
0.00.048.440 I print_info: pooling type     = 0
0.00.048.441 I print_info: rope type        = 2
0.00.048.442 I print_info: rope scaling     = linear
0.00.048.442 I print_info: freq_base_train  = 10000.0
0.00.048.442 I print_info: freq_scale_train = 1
0.00.048.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.446 I print_info: rope_finetuned   = unknown
0.00.048.447 I print_info: ssm_d_conv       = 0
0.00.048.447 I print_info: ssm_d_inner      = 0
0.00.048.448 I print_info: ssm_d_state      = 0
0.00.048.449 I print_info: ssm_dt_rank      = 0
0.00.048.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.449 I print_info: model type       = 1.4B
0.00.048.449 I print_info: model params     = 1.41 B
0.00.048.449 I print_info: general.name     = 1.4B
0.00.048.453 I print_info: vocab type       = BPE
0.00.048.453 I print_info: n_vocab          = 50304
0.00.048.454 I print_info: n_merges         = 50009
0.00.048.455 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.455 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.455 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.455 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.455 I print_info: LF token         = 187 ''
0.00.048.456 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.456 I print_info: max token length = 1024
0.00.048.456 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.177 I load_tensors: offloading output layer to GPU
0.00.547.177 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.211 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.547.212 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.548.917 I llama_init_from_model: n_seq_max     = 1
0.00.548.920 I llama_init_from_model: n_ctx         = 2048
0.00.548.920 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.548.921 I llama_init_from_model: n_batch       = 2048
0.00.548.921 I llama_init_from_model: n_ubatch      = 512
0.00.548.921 I llama_init_from_model: flash_attn    = 0
0.00.548.924 I llama_init_from_model: freq_base     = 10000.0
0.00.548.924 I llama_init_from_model: freq_scale    = 1
0.00.548.927 I ggml_metal_init: allocating
0.00.549.006 I ggml_metal_init: found device: Apple M4
0.00.549.019 I ggml_metal_init: picking default device: Apple M4
0.00.550.909 I ggml_metal_init: using embedded metal library
0.00.557.642 I ggml_metal_init: GPU name:   Apple M4
0.00.557.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.648 I ggml_metal_init: simdgroup reduction   = true
0.00.557.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.649 I ggml_metal_init: has residency sets    = true
0.00.557.649 I ggml_metal_init: has bfloat            = true
0.00.557.649 I ggml_metal_init: use bfloat            = true
0.00.557.650 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.652 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.897 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.896 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.628.905 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.628.935 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.711 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.633.713 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.633.714 I llama_init_from_model: graph nodes  = 967
0.00.633.714 I llama_init_from_model: graph splits = 2
0.00.633.719 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.633.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.633.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.698 I main: llama threadpool init, n_threads = 4
0.00.691.750 I 
0.00.691.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.771 I 
0.00.691.936 I sampler seed: 1234
0.00.691.941 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.957 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.966 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.966 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.460.422 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.460.423 I llama_perf_context_print:        load time =     681.90 ms
0.01.460.424 I llama_perf_context_print: prompt eval time =      58.36 ms /     7 tokens (    8.34 ms per token,   119.94 tokens per second)
0.01.460.428 I llama_perf_context_print:        eval time =     707.33 ms /    63 runs   (   11.23 ms per token,    89.07 tokens per second)
0.01.460.428 I llama_perf_context_print:       total time =     769.44 ms /    70 tokens
0.01.460.719 I ggml_metal_free: deallocating

real	0m1.478s
user	0m0.111s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.861 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.867 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.874 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.881 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.881 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.665 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.453 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.454 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.455 I llama_model_loader: - type  f32:  194 tensors
0.00.024.455 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.455 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.455 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.456 I print_info: file format = GGUF V3 (latest)
0.00.024.460 I print_info: file type   = Q4_K - Medium
0.00.024.462 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.866 I load: special tokens cache size = 25
0.00.039.312 I load: token to piece cache size = 0.2984 MB
0.00.039.329 I print_info: arch             = gptneox
0.00.039.330 I print_info: vocab_only       = 0
0.00.039.330 I print_info: n_ctx_train      = 2048
0.00.039.331 I print_info: n_embd           = 2048
0.00.039.331 I print_info: n_layer          = 24
0.00.039.335 I print_info: n_head           = 16
0.00.039.335 I print_info: n_head_kv        = 16
0.00.039.335 I print_info: n_rot            = 32
0.00.039.335 I print_info: n_swa            = 0
0.00.039.336 I print_info: n_embd_head_k    = 128
0.00.039.336 I print_info: n_embd_head_v    = 128
0.00.039.336 I print_info: n_gqa            = 1
0.00.039.337 I print_info: n_embd_k_gqa     = 2048
0.00.039.337 I print_info: n_embd_v_gqa     = 2048
0.00.039.338 I print_info: f_norm_eps       = 1.0e-05
0.00.039.338 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.338 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.339 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.339 I print_info: f_logit_scale    = 0.0e+00
0.00.039.339 I print_info: n_ff             = 8192
0.00.039.340 I print_info: n_expert         = 0
0.00.039.340 I print_info: n_expert_used    = 0
0.00.039.340 I print_info: causal attn      = 1
0.00.039.340 I print_info: pooling type     = 0
0.00.039.340 I print_info: rope type        = 2
0.00.039.340 I print_info: rope scaling     = linear
0.00.039.341 I print_info: freq_base_train  = 10000.0
0.00.039.341 I print_info: freq_scale_train = 1
0.00.039.341 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.341 I print_info: rope_finetuned   = unknown
0.00.039.342 I print_info: ssm_d_conv       = 0
0.00.039.342 I print_info: ssm_d_inner      = 0
0.00.039.342 I print_info: ssm_d_state      = 0
0.00.039.342 I print_info: ssm_dt_rank      = 0
0.00.039.342 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.342 I print_info: model type       = 1.4B
0.00.039.343 I print_info: model params     = 1.41 B
0.00.039.343 I print_info: general.name     = 1.4B
0.00.039.343 I print_info: vocab type       = BPE
0.00.039.343 I print_info: n_vocab          = 50304
0.00.039.344 I print_info: n_merges         = 50009
0.00.039.344 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.345 I print_info: LF token         = 187 ''
0.00.039.345 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.345 I print_info: max token length = 1024
0.00.039.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.902 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.917 I load_tensors: offloading output layer to GPU
0.00.538.918 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.951 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.952 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.658 I llama_init_from_model: n_seq_max     = 1
0.00.540.661 I llama_init_from_model: n_ctx         = 128
0.00.540.661 I llama_init_from_model: n_ctx_per_seq = 128
0.00.540.662 I llama_init_from_model: n_batch       = 128
0.00.540.662 I llama_init_from_model: n_ubatch      = 128
0.00.540.662 I llama_init_from_model: flash_attn    = 0
0.00.540.665 I llama_init_from_model: freq_base     = 10000.0
0.00.540.665 I llama_init_from_model: freq_scale    = 1
0.00.540.666 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.540.668 I ggml_metal_init: allocating
0.00.540.758 I ggml_metal_init: found device: Apple M4
0.00.540.772 I ggml_metal_init: picking default device: Apple M4
0.00.542.733 I ggml_metal_init: using embedded metal library
0.00.549.344 I ggml_metal_init: GPU name:   Apple M4
0.00.549.349 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.350 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.351 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.351 I ggml_metal_init: simdgroup reduction   = true
0.00.549.351 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.352 I ggml_metal_init: has residency sets    = true
0.00.549.352 I ggml_metal_init: has bfloat            = true
0.00.549.352 I ggml_metal_init: use bfloat            = true
0.00.549.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.570.326 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.570.331 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.570.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.573.583 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.573.585 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.573.585 I llama_init_from_model: graph nodes  = 967
0.00.573.586 I llama_init_from_model: graph splits = 2
0.00.573.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.573.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.691 I 
0.00.602.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.793 I perplexity: tokenizing the input ..
0.00.609.999 I perplexity: tokenization took 7.202 ms
0.00.610.011 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.752 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.745.101 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.745.124 I llama_perf_context_print:        load time =     593.71 ms
0.00.745.124 I llama_perf_context_print: prompt eval time =     132.80 ms /   128 tokens (    1.04 ms per token,   963.88 tokens per second)
0.00.745.125 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.125 I llama_perf_context_print:       total time =     142.44 ms /   129 tokens
0.00.745.480 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.080s
sys	0m0.141s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.012.832 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.314 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.317 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.317 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.318 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.322 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.146 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.872 I llama_model_loader: - type  f32:  194 tensors
0.00.028.872 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.872 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.873 I print_info: file format = GGUF V3 (latest)
0.00.028.873 I print_info: file type   = Q5_K - Medium
0.00.028.874 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.037.083 I load: special tokens cache size = 25
0.00.043.376 I load: token to piece cache size = 0.2984 MB
0.00.043.390 I print_info: arch             = gptneox
0.00.043.392 I print_info: vocab_only       = 0
0.00.043.392 I print_info: n_ctx_train      = 2048
0.00.043.392 I print_info: n_embd           = 2048
0.00.043.392 I print_info: n_layer          = 24
0.00.043.395 I print_info: n_head           = 16
0.00.043.397 I print_info: n_head_kv        = 16
0.00.043.397 I print_info: n_rot            = 32
0.00.043.397 I print_info: n_swa            = 0
0.00.043.397 I print_info: n_embd_head_k    = 128
0.00.043.397 I print_info: n_embd_head_v    = 128
0.00.043.398 I print_info: n_gqa            = 1
0.00.043.399 I print_info: n_embd_k_gqa     = 2048
0.00.043.400 I print_info: n_embd_v_gqa     = 2048
0.00.043.400 I print_info: f_norm_eps       = 1.0e-05
0.00.043.400 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.407 I print_info: f_logit_scale    = 0.0e+00
0.00.043.413 I print_info: n_ff             = 8192
0.00.043.414 I print_info: n_expert         = 0
0.00.043.414 I print_info: n_expert_used    = 0
0.00.043.414 I print_info: causal attn      = 1
0.00.043.414 I print_info: pooling type     = 0
0.00.043.414 I print_info: rope type        = 2
0.00.043.414 I print_info: rope scaling     = linear
0.00.043.415 I print_info: freq_base_train  = 10000.0
0.00.043.415 I print_info: freq_scale_train = 1
0.00.043.415 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.415 I print_info: rope_finetuned   = unknown
0.00.043.415 I print_info: ssm_d_conv       = 0
0.00.043.416 I print_info: ssm_d_inner      = 0
0.00.043.416 I print_info: ssm_d_state      = 0
0.00.043.416 I print_info: ssm_dt_rank      = 0
0.00.043.417 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.417 I print_info: model type       = 1.4B
0.00.043.417 I print_info: model params     = 1.41 B
0.00.043.418 I print_info: general.name     = 1.4B
0.00.043.419 I print_info: vocab type       = BPE
0.00.043.419 I print_info: n_vocab          = 50304
0.00.043.419 I print_info: n_merges         = 50009
0.00.043.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.420 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.420 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.421 I print_info: LF token         = 187 ''
0.00.043.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.422 I print_info: max token length = 1024
0.00.043.422 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.991 I load_tensors: offloading 24 repeating layers to GPU
0.00.602.000 I load_tensors: offloading output layer to GPU
0.00.602.001 I load_tensors: offloaded 25/25 layers to GPU
0.00.602.034 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.602.037 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.603.365 I llama_init_from_model: n_seq_max     = 1
0.00.603.367 I llama_init_from_model: n_ctx         = 2048
0.00.603.368 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.368 I llama_init_from_model: n_batch       = 2048
0.00.603.368 I llama_init_from_model: n_ubatch      = 512
0.00.603.369 I llama_init_from_model: flash_attn    = 0
0.00.603.370 I llama_init_from_model: freq_base     = 10000.0
0.00.603.371 I llama_init_from_model: freq_scale    = 1
0.00.603.372 I ggml_metal_init: allocating
0.00.603.387 I ggml_metal_init: found device: Apple M4
0.00.603.396 I ggml_metal_init: picking default device: Apple M4
0.00.604.931 I ggml_metal_init: using embedded metal library
0.00.611.210 I ggml_metal_init: GPU name:   Apple M4
0.00.611.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.216 I ggml_metal_init: simdgroup reduction   = true
0.00.611.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.216 I ggml_metal_init: has residency sets    = true
0.00.611.217 I ggml_metal_init: has bfloat            = true
0.00.611.217 I ggml_metal_init: use bfloat            = true
0.00.611.218 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.219 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.382 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.572 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.689.583 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.689.608 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.694.007 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.694.010 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.694.010 I llama_init_from_model: graph nodes  = 967
0.00.694.010 I llama_init_from_model: graph splits = 2
0.00.694.015 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.079 I main: llama threadpool init, n_threads = 4
0.00.761.130 I 
0.00.761.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.153 I 
0.00.761.313 I sampler seed: 1234
0.00.761.318 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.362 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.365 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.606.755 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.606.756 I llama_perf_context_print:        load time =     747.53 ms
0.01.606.756 I llama_perf_context_print: prompt eval time =      57.20 ms /     7 tokens (    8.17 ms per token,   122.37 tokens per second)
0.01.606.759 I llama_perf_context_print:        eval time =     785.23 ms /    63 runs   (   12.46 ms per token,    80.23 tokens per second)
0.01.606.759 I llama_perf_context_print:       total time =     846.39 ms /    70 tokens
0.01.607.020 I ggml_metal_free: deallocating

real	0m1.624s
user	0m0.107s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.599 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.600 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.600 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.601 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.601 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.602 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.602 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.602 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.603 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.603 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.604 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.606 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.355 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.158 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.160 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.160 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.161 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.162 I llama_model_loader: - type  f32:  194 tensors
0.00.025.162 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.162 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.163 I print_info: file format = GGUF V3 (latest)
0.00.025.164 I print_info: file type   = Q5_K - Medium
0.00.025.165 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.156 I load: special tokens cache size = 25
0.00.039.576 I load: token to piece cache size = 0.2984 MB
0.00.039.593 I print_info: arch             = gptneox
0.00.039.594 I print_info: vocab_only       = 0
0.00.039.594 I print_info: n_ctx_train      = 2048
0.00.039.594 I print_info: n_embd           = 2048
0.00.039.594 I print_info: n_layer          = 24
0.00.039.598 I print_info: n_head           = 16
0.00.039.599 I print_info: n_head_kv        = 16
0.00.039.599 I print_info: n_rot            = 32
0.00.039.599 I print_info: n_swa            = 0
0.00.039.599 I print_info: n_embd_head_k    = 128
0.00.039.599 I print_info: n_embd_head_v    = 128
0.00.039.600 I print_info: n_gqa            = 1
0.00.039.600 I print_info: n_embd_k_gqa     = 2048
0.00.039.601 I print_info: n_embd_v_gqa     = 2048
0.00.039.601 I print_info: f_norm_eps       = 1.0e-05
0.00.039.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.602 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.602 I print_info: f_logit_scale    = 0.0e+00
0.00.039.603 I print_info: n_ff             = 8192
0.00.039.603 I print_info: n_expert         = 0
0.00.039.603 I print_info: n_expert_used    = 0
0.00.039.603 I print_info: causal attn      = 1
0.00.039.603 I print_info: pooling type     = 0
0.00.039.604 I print_info: rope type        = 2
0.00.039.604 I print_info: rope scaling     = linear
0.00.039.604 I print_info: freq_base_train  = 10000.0
0.00.039.604 I print_info: freq_scale_train = 1
0.00.039.610 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.610 I print_info: rope_finetuned   = unknown
0.00.039.610 I print_info: ssm_d_conv       = 0
0.00.039.610 I print_info: ssm_d_inner      = 0
0.00.039.610 I print_info: ssm_d_state      = 0
0.00.039.610 I print_info: ssm_dt_rank      = 0
0.00.039.610 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.611 I print_info: model type       = 1.4B
0.00.039.611 I print_info: model params     = 1.41 B
0.00.039.611 I print_info: general.name     = 1.4B
0.00.039.612 I print_info: vocab type       = BPE
0.00.039.612 I print_info: n_vocab          = 50304
0.00.039.612 I print_info: n_merges         = 50009
0.00.039.612 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: LF token         = 187 ''
0.00.039.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.613 I print_info: max token length = 1024
0.00.039.614 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.562 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.577 I load_tensors: offloading output layer to GPU
0.00.589.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.614 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.616 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.259 I llama_init_from_model: n_seq_max     = 1
0.00.591.261 I llama_init_from_model: n_ctx         = 128
0.00.591.262 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.262 I llama_init_from_model: n_batch       = 128
0.00.591.263 I llama_init_from_model: n_ubatch      = 128
0.00.591.263 I llama_init_from_model: flash_attn    = 0
0.00.591.265 I llama_init_from_model: freq_base     = 10000.0
0.00.591.266 I llama_init_from_model: freq_scale    = 1
0.00.591.266 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.269 I ggml_metal_init: allocating
0.00.591.413 I ggml_metal_init: found device: Apple M4
0.00.591.436 I ggml_metal_init: picking default device: Apple M4
0.00.593.146 I ggml_metal_init: using embedded metal library
0.00.599.518 I ggml_metal_init: GPU name:   Apple M4
0.00.599.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.524 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.524 I ggml_metal_init: simdgroup reduction   = true
0.00.599.525 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.525 I ggml_metal_init: has residency sets    = true
0.00.599.525 I ggml_metal_init: has bfloat            = true
0.00.599.525 I ggml_metal_init: use bfloat            = true
0.00.599.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.209 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.758 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.762 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.788 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.623.856 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.623.857 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.623.858 I llama_init_from_model: graph nodes  = 967
0.00.623.858 I llama_init_from_model: graph splits = 2
0.00.623.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.623.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.811 I 
0.00.657.899 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.908 I perplexity: tokenizing the input ..
0.00.664.883 I perplexity: tokenization took 6.971 ms
0.00.664.892 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.750 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.803.107 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.803.132 I llama_perf_context_print:        load time =     647.94 ms
0.00.803.133 I llama_perf_context_print: prompt eval time =     136.01 ms /   128 tokens (    1.06 ms per token,   941.13 tokens per second)
0.00.803.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.134 I llama_perf_context_print:       total time =     145.33 ms /   129 tokens
0.00.803.525 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.078s
sys	0m0.139s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.931 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.933 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.934 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.934 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.935 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.937 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.939 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.511 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.512 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.513 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.514 I llama_model_loader: - type  f32:  194 tensors
0.00.025.515 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.515 I print_info: file format = GGUF V3 (latest)
0.00.025.515 I print_info: file type   = Q6_K
0.00.025.516 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.647 I load: special tokens cache size = 25
0.00.039.874 I load: token to piece cache size = 0.2984 MB
0.00.039.889 I print_info: arch             = gptneox
0.00.039.890 I print_info: vocab_only       = 0
0.00.039.890 I print_info: n_ctx_train      = 2048
0.00.039.890 I print_info: n_embd           = 2048
0.00.039.890 I print_info: n_layer          = 24
0.00.039.893 I print_info: n_head           = 16
0.00.039.894 I print_info: n_head_kv        = 16
0.00.039.894 I print_info: n_rot            = 32
0.00.039.894 I print_info: n_swa            = 0
0.00.039.894 I print_info: n_embd_head_k    = 128
0.00.039.894 I print_info: n_embd_head_v    = 128
0.00.039.895 I print_info: n_gqa            = 1
0.00.039.896 I print_info: n_embd_k_gqa     = 2048
0.00.039.897 I print_info: n_embd_v_gqa     = 2048
0.00.039.897 I print_info: f_norm_eps       = 1.0e-05
0.00.039.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.900 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.900 I print_info: f_logit_scale    = 0.0e+00
0.00.039.900 I print_info: n_ff             = 8192
0.00.039.901 I print_info: n_expert         = 0
0.00.039.901 I print_info: n_expert_used    = 0
0.00.039.901 I print_info: causal attn      = 1
0.00.039.901 I print_info: pooling type     = 0
0.00.039.901 I print_info: rope type        = 2
0.00.039.901 I print_info: rope scaling     = linear
0.00.039.903 I print_info: freq_base_train  = 10000.0
0.00.039.903 I print_info: freq_scale_train = 1
0.00.039.903 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.903 I print_info: rope_finetuned   = unknown
0.00.039.903 I print_info: ssm_d_conv       = 0
0.00.039.903 I print_info: ssm_d_inner      = 0
0.00.039.904 I print_info: ssm_d_state      = 0
0.00.039.904 I print_info: ssm_dt_rank      = 0
0.00.039.904 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.904 I print_info: model type       = 1.4B
0.00.039.904 I print_info: model params     = 1.41 B
0.00.039.904 I print_info: general.name     = 1.4B
0.00.039.905 I print_info: vocab type       = BPE
0.00.039.905 I print_info: n_vocab          = 50304
0.00.039.905 I print_info: n_merges         = 50009
0.00.039.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.905 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.909 I print_info: LF token         = 187 ''
0.00.039.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.910 I print_info: max token length = 1024
0.00.039.910 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.671.592 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.595 I load_tensors: offloading output layer to GPU
0.00.671.596 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.620 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.671.621 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.673.160 I llama_init_from_model: n_seq_max     = 1
0.00.673.162 I llama_init_from_model: n_ctx         = 2048
0.00.673.163 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.673.163 I llama_init_from_model: n_batch       = 2048
0.00.673.164 I llama_init_from_model: n_ubatch      = 512
0.00.673.164 I llama_init_from_model: flash_attn    = 0
0.00.673.165 I llama_init_from_model: freq_base     = 10000.0
0.00.673.165 I llama_init_from_model: freq_scale    = 1
0.00.673.167 I ggml_metal_init: allocating
0.00.673.211 I ggml_metal_init: found device: Apple M4
0.00.673.222 I ggml_metal_init: picking default device: Apple M4
0.00.674.693 I ggml_metal_init: using embedded metal library
0.00.680.755 I ggml_metal_init: GPU name:   Apple M4
0.00.680.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.680.760 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.680.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.680.762 I ggml_metal_init: simdgroup reduction   = true
0.00.680.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.680.762 I ggml_metal_init: has residency sets    = true
0.00.680.762 I ggml_metal_init: has bfloat            = true
0.00.680.763 I ggml_metal_init: use bfloat            = true
0.00.680.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.680.768 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.697.863 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.753.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.753.988 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.754.013 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.758.555 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.758.558 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.758.558 I llama_init_from_model: graph nodes  = 967
0.00.758.559 I llama_init_from_model: graph splits = 2
0.00.758.565 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.758.681 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.758.681 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.360 I main: llama threadpool init, n_threads = 4
0.00.827.412 I 
0.00.827.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.437 I 
0.00.827.601 I sampler seed: 1234
0.00.827.606 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.827.621 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.827.623 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.827.623 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.700.606 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.700.607 I llama_perf_context_print:        load time =     817.73 ms
0.01.700.608 I llama_perf_context_print: prompt eval time =      57.46 ms /     7 tokens (    8.21 ms per token,   121.83 tokens per second)
0.01.700.609 I llama_perf_context_print:        eval time =     812.63 ms /    63 runs   (   12.90 ms per token,    77.53 tokens per second)
0.01.700.609 I llama_perf_context_print:       total time =     873.97 ms /    70 tokens
0.01.700.875 I ggml_metal_free: deallocating

real	0m1.719s
user	0m0.108s
sys	0m0.235s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4870 (96e12808) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.041 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.060 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.061 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.062 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.062 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.063 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.064 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.064 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.064 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.065 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.067 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.616 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.617 I llama_model_loader: - type  f32:  194 tensors
0.00.024.617 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.618 I print_info: file format = GGUF V3 (latest)
0.00.024.618 I print_info: file type   = Q6_K
0.00.024.619 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.939 I load: special tokens cache size = 25
0.00.039.495 I load: token to piece cache size = 0.2984 MB
0.00.039.511 I print_info: arch             = gptneox
0.00.039.512 I print_info: vocab_only       = 0
0.00.039.512 I print_info: n_ctx_train      = 2048
0.00.039.512 I print_info: n_embd           = 2048
0.00.039.512 I print_info: n_layer          = 24
0.00.039.518 I print_info: n_head           = 16
0.00.039.519 I print_info: n_head_kv        = 16
0.00.039.519 I print_info: n_rot            = 32
0.00.039.519 I print_info: n_swa            = 0
0.00.039.519 I print_info: n_embd_head_k    = 128
0.00.039.519 I print_info: n_embd_head_v    = 128
0.00.039.520 I print_info: n_gqa            = 1
0.00.039.521 I print_info: n_embd_k_gqa     = 2048
0.00.039.521 I print_info: n_embd_v_gqa     = 2048
0.00.039.522 I print_info: f_norm_eps       = 1.0e-05
0.00.039.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.524 I print_info: f_logit_scale    = 0.0e+00
0.00.039.524 I print_info: n_ff             = 8192
0.00.039.524 I print_info: n_expert         = 0
0.00.039.524 I print_info: n_expert_used    = 0
0.00.039.525 I print_info: causal attn      = 1
0.00.039.525 I print_info: pooling type     = 0
0.00.039.525 I print_info: rope type        = 2
0.00.039.525 I print_info: rope scaling     = linear
0.00.039.525 I print_info: freq_base_train  = 10000.0
0.00.039.525 I print_info: freq_scale_train = 1
0.00.039.526 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.526 I print_info: rope_finetuned   = unknown
0.00.039.526 I print_info: ssm_d_conv       = 0
0.00.039.526 I print_info: ssm_d_inner      = 0
0.00.039.526 I print_info: ssm_d_state      = 0
0.00.039.526 I print_info: ssm_dt_rank      = 0
0.00.039.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.527 I print_info: model type       = 1.4B
0.00.039.527 I print_info: model params     = 1.41 B
0.00.039.527 I print_info: general.name     = 1.4B
0.00.039.528 I print_info: vocab type       = BPE
0.00.039.528 I print_info: n_vocab          = 50304
0.00.039.528 I print_info: n_merges         = 50009
0.00.039.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.529 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.529 I print_info: LF token         = 187 ''
0.00.039.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.534 I print_info: max token length = 1024
0.00.039.535 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.481.141 I load_tensors: offloading 24 repeating layers to GPU
0.00.481.149 I load_tensors: offloading output layer to GPU
0.00.481.149 I load_tensors: offloaded 25/25 layers to GPU
0.00.481.182 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.481.185 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.482.853 I llama_init_from_model: n_seq_max     = 1
0.00.482.855 I llama_init_from_model: n_ctx         = 128
0.00.482.855 I llama_init_from_model: n_ctx_per_seq = 128
0.00.482.856 I llama_init_from_model: n_batch       = 128
0.00.482.856 I llama_init_from_model: n_ubatch      = 128
0.00.482.856 I llama_init_from_model: flash_attn    = 0
0.00.482.858 I llama_init_from_model: freq_base     = 10000.0
0.00.482.858 I llama_init_from_model: freq_scale    = 1
0.00.482.859 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.482.860 I ggml_metal_init: allocating
0.00.482.925 I ggml_metal_init: found device: Apple M4
0.00.482.938 I ggml_metal_init: picking default device: Apple M4
0.00.484.526 I ggml_metal_init: using embedded metal library
0.00.490.551 I ggml_metal_init: GPU name:   Apple M4
0.00.490.554 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.490.555 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.490.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.490.557 I ggml_metal_init: simdgroup reduction   = true
0.00.490.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.490.557 I ggml_metal_init: has residency sets    = true
0.00.490.558 I ggml_metal_init: has bfloat            = true
0.00.490.558 I ggml_metal_init: use bfloat            = true
0.00.490.559 I ggml_metal_init: hasUnifiedMemory      = true
0.00.490.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.507.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.511.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.511.287 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.511.319 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.514.483 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.514.484 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.514.485 I llama_init_from_model: graph nodes  = 967
0.00.514.485 I llama_init_from_model: graph splits = 2
0.00.514.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.514.490 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.795 I 
0.00.544.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.891 I perplexity: tokenizing the input ..
0.00.552.377 I perplexity: tokenization took 7.483 ms
0.00.552.386 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.685.373 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.686.717 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.686.746 I llama_perf_context_print:        load time =     535.75 ms
0.00.686.748 I llama_perf_context_print: prompt eval time =     132.06 ms /   128 tokens (    1.03 ms per token,   969.23 tokens per second)
0.00.686.750 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.686.750 I llama_perf_context_print:       total time =     141.95 ms /   129 tokens
0.00.687.148 I ggml_metal_free: deallocating

real	0m0.701s
user	0m0.080s
sys	0m0.119s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4870 (96e12808)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d104b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d1084b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d108a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d109010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d1095c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d109b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d10a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d10a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d10ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d10b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d10b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d10bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d10c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d10ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d10d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d10dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d10e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d10ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d10f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d10fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d1101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d1108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d111010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d1118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d111fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d112470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d112910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d112fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d113450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d1138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d113bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d1142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d114560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d114a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d114ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d115340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d1157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d115c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d116120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d1165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d116a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d116f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d1173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d117840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d117b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d118010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d118520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d118f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d1193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d119860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d119d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d11a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d11a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d11aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d11af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d11b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d11b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d11bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d11c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d11c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d11ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d11ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d11d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d11d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d11dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d11e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d11e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d11ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d11ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d11f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d11f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d11fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d120190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d1206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d120c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d121180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d1216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d121c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d122170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d1226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d122c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d123160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d1236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d123c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d124150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d1246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d124bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d125140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d125690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d125be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d126130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d126680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d126bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d127120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d127670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d127bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d128110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d118a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d128580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d128d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d129280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d1297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d129d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d12a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d12a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d12ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d12b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d12b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d12bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d12c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d12c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d12ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d12d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d12d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d12db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d12e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d12e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d12e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d12ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d12f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d12f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d12fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d130080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d130520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d1309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d130e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d131300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d1317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d131c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d1320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d132580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d132a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d132ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d133360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d133800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d133ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d134140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d1345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d134a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d134f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d1353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d135860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d135d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d1361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d136640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d136ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d136f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d137420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d1378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d137d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d138200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d1386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d138b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d138fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d139480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d139920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d139dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d13a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d13a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d13aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d13b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d13b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d13b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d13be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d13c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d13c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d13cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d13d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d13d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d13d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d13de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d13e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d13e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d13ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d13f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d13f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d13fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d13fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d140380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d140820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d140cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d141160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d141600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d141aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d141f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d1423e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d142880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d142d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d1431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d143660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d143b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d143fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d144440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d144990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d144ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d145430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d145980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d145e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d1462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d146760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d146c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d1470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d147540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d147a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d147f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d1483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d148870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d148d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d1491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d149650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d149ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d14a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d14a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d14ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d14b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d14b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d14be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d14c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d14c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d14ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d14d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d14d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d14de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d14e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d14e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d14ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d14f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d14f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d14fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d150390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d1508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d150e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d151380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d1518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d151e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d152370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d1528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d152e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d153360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d1538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d153e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d154350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d1548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d154df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d155340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d155890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d155de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d156330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d156880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d156dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d157320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d157870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d157dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d158310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d158860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d158db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d159300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d159850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d159da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d15a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d15a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d15ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d15b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d15b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d15bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d15c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d15c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d15ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d15d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d15d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d15daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d15df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d15e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d15e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d15ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d15f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d15f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d15fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d15ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d160440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d1608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d160d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12d161220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12d1616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12d161b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12d162000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12d1624a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12d162940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12d162de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12d163280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12d163720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12d163bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d164110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d164830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d164f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d165670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d165d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d1662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d166780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d166c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d1670c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.736.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f304d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f3051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f305630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f305aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f305f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f306380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f3067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f306c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f3070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f307540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f3079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f3080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f308bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f309370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f309b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f30a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f30a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f30b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f30b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f30bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f30c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f30cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f30d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f30dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f30e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f30e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f30e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f30ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f30f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f30f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f30fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f310070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f310750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f310bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f311090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f311530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f3119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f311e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f312310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f3127b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f312c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f3130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f313590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f313a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f313ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f314370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f314810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f314cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f315150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f3155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f315a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f315f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f3163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f316870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f316d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f3171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f317650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f317910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f317bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f318040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f3184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f318920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f318d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f319200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f319670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f319ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f319f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f31a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f31a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f31aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f31b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f31b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f31b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f31be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f31c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f31c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f31cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f31d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f31d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f31d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f31dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f31e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f31e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f31eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f31ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f31f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f31f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f31fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f3200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f320560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f3209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f320e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f3212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f321720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f321b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f322000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f322470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f3228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f322d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f3231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f323630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f323aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f323f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f324380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f3247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f324c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f3250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f325540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f3259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f325e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f326290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f326700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f326b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f326fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f327450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f3278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f327d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f3281a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f328610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f328a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f328ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f329360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f3297d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f329c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f32a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f32a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f32a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f32ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f32b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f32b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f32bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f32bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f32c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f32c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f32cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f32d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f32d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f32da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f32ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f32e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f32e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f32ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f32f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f32f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f32f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f32fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f330250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f3306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f330b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f330fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f331410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f331880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f331cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f332160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f3325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f332a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f332eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f333320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f333790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f333c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f334070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f3344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f334950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f334dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f335230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f3356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f335e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f3360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f336550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f3369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f336e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f3372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f337710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f337b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f337ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f338460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f3388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f338d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f3391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f339620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f339a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f339f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f33a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f33a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f33ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f33b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f33b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f33b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f33be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f33c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f33c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f33cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f33cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f33d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f33d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f33dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f33e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f33e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f33ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f33eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f33f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f33f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f33ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f340250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f340800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f340d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f3413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f341880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f341d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f3421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f342a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f342cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f343280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f343830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f343de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f344390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f344940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f344ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f3454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f345a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f346000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f3465b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f346b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f347110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f3476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f347c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f348220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f3487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f348d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f349330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f3498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f349e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f34a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f34a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f34afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f34b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f34bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f34c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f34c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f34cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f34d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f34d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f34dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f34e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f34e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f34ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f34f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f34f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f34ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f3504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f350aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f351050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f351600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f351bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f352160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f352710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f352cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f353270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f353820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f353dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f354380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f354930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f354ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f355490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f355a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f355ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f3565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f356b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f357050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f357550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f357a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f357f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f358450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f358950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f358e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f359350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f359850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f359d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f35a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f35a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f35ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f35b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10f35b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10f35bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10f35c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10f35c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10f35ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10f35cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10f35d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10f35d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10f35de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10f35e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f35e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f35f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f35f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f3600a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f3607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f360a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f361210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f3616b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f361b50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10af044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10af04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10af04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10af05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10af056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10af05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10af05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10af063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10af06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10af06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10af07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10af07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10af08380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10af08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10af09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10af09a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10af0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10af0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10af0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10af0b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10af0be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10af0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10af0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10af0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10af0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10af0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10af0e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10af0e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10af0e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10af0ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10af0f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10af0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10af0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10af103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10af10850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10af10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10af11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10af11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10af11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10af11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10af12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10af128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10af12d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10af131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10af13690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10af13b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10af13fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10af14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10af14910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10af14db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10af15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10af156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10af15b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10af16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10af164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10af16970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10af16e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10af170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10af17390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10af17800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10af17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10af180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10af18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10af189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10af18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10af192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10af19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10af19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10af19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d111550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d10be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d128840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d113e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d167380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d167640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d167900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d167bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d167e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d168140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d168400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d1686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d168980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d168c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d168f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d1691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d169480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d169740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d169a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d169cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d169f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d16a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d16a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d16a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d16aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d16ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d16b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d16b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d16b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d16b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d16bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d16bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d16c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d16c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d16c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d16c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d16cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d16ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d16d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d16d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d16d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d16d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d16dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d16dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d16e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d16e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d16e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d16e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d16ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d16ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d16f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d16f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d16f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d16fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d16fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d16ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d170280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d170540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d170800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d170ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d170d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d171040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d171300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d1715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d171880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d171b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d171e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d1720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d172380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d172640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d172900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d172bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d172e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d173140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d173400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d1736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d173980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d173c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d173f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d1741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d174480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d174740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d174a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d174cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d174f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d175240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d175500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d1757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d175a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d175d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d176000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d1762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d176580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d176840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d176b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d176dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d177080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d177340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d177600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d1778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d177b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d177e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d178100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d1783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d178680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d178940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d178c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d178ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d179180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d179440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d179700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d1799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d179c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d179f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d17a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d17a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d17a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d17aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d17ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d17afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d17b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d17b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d17b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d17bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d17bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d17c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d17c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d17c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d17c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d17cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d17ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d17d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d17d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d17d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d17d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d17dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d17de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d17e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d17e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d17e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d17e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d17eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d17f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d17f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d17f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d17faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d17fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d180070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d180330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d1805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d1808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d180b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d180e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d1810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d1813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d181670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d181930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d181bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d181eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d182170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d182430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d1826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d1829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d182c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d182f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d1831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d1834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d183770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d183a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d183cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d183fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d184270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d184530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d1847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d184ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d184d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d185030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d1852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d1855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d185870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d185b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d185df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d1860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d186370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d186630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d1868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d186bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d186e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d187130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d1873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d1876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d187970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d187c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d187ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d1881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d188470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d188730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d1889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d188cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d188f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d189230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d1894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d1897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d189a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d189d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d189ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d18a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d18a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d18a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d18aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d18adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d18b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d18b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12d18b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12d18b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12d18bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12d18be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12d18c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12d18c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12d18c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12d18c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12d18cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12d18ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d18d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d18d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d18d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d18d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d18dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d18df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d18e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d18e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d18e770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.817s
user	0m0.280s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4870 (96e12808)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15af0d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15af0dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15af0e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15af0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15af0edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15af0f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15af0f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15af0fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15af10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15af10990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15af10e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15af11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15af11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15af12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15af12e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15af13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15af13cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15af143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15af14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15af152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15af159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15af16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15af16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15af170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15af177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15af17c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15af18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15af187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15af18c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15af19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15af193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15af19ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15af19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15af1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15af1a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15af1ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15af1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c004080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c004640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c004b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c005040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c005540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c005a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c005f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c006440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c006950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c0079d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c007e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c008310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c0087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c008a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c008f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c009670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c009b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c009fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c00a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c00a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c00ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c00b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c00b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c00bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c00c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c00c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c00cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c00d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c00d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c00da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c00df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c00e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c00ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c00f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c00f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c00fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c0103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c0109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c011530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c011af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c0120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c012c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c0131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c0137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c013d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c014330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c0148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c014eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c015a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c015ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c0165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c016b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c017130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c0176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c017cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c018270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c007520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c0186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c018b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c018fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c019430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c0198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c019d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c01a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c01a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c01aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c01aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c01b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c01b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c01bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c01c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c01c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c01ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c01ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c01d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c01d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c01dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c01e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c01e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c01e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c01edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c01f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c01f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c01fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c01ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c0203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c020860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c020cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c021140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c0215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c021a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c021e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c022300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c022770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c022be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c023050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c0234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c023930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c024210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c024680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c024f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c0253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c025840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c026a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c0272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c027750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c027bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c028030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c0284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c028910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c0291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c029660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c029ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c02a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c02a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c02ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c02b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c02b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c02b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c02be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c02c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c02c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c02cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c02d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c02d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c02d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c02dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c02e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c02e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c02eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c02ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c02f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c02f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c02fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c0300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c030550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c0309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c0312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c031710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c031ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c0328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c032d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c0331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c033620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c033ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c034460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c0348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c034d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c0357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c035d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15c036290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c036970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c036e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c0372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c037750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c037fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c038260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c038810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c038dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c039370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c039920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c039ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c03a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c03aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c03afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c03b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c03bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c03c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c03c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c03cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c03d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c03d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c03dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c03e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c03e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c03ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c03f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c03f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c03ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c040530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c040ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c041090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c041640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c041bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c0421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c042750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c042d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c0432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c043860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c043e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c0443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c044970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c044f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c0454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c045a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c046030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c0465e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c046b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c047140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c0476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c047ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c048250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c048800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c048db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c049360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c049910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c049ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c04a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c04aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c04afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c04b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c04bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15c04c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15c04c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c04cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c04cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c04d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c04d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c04dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c04e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c04e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c04ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c04f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c04f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c04fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c0501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c0506e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15c050be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15c0510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15c0515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15c051ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15c051fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15c0524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15c0529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15c052ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15c0533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15c0538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c053de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c0547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c054f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c055630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c055d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c056010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c0567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c056c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c0570e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.933 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15c040da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15c03da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15c03b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15c04a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15c047f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15c045d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15c043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15c03be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15c039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15c03e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15c03f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15c044c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15c041900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15c049620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15c03c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15c044680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15c03f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15c045790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15c042a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15c04ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15c03a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15c038ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15c03acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15c0407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15c048ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15c03eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15c041350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15c0451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15c03c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15c046e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15c03b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15c049bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15c03cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15c047400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15c042fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15c04bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15c03a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c04b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c039be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c04a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c0440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c0462f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c049070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c0479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c03fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c0106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c0100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c00fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c015170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c004340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c0173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c011db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c016e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c0117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c014bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c0134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c016870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c011230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c0145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c012ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c0162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c010c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c014030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c015cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c013a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c012370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c0573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c057660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c057920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c057be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c057ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c058160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c0586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c058970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c058c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c058ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c0591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c059470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c059730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c0599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c059cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c059f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c05a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c05a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c05a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c05aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c05ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c05aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c05b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c05b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c05b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c05baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c05bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c05c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c05c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c05c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c05c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c05cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c05ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c05d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c05d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c05d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c05d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c05dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c05deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c05e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c05e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c05e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c05e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c05ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c05ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c05f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c05f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c05f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c05fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c05fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c05ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c060270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c060530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c0607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c060ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c060d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c061030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c0612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c0615b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c061870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c061b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c061df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c0620b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c062370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c062630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c0628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c062bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c062e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c063270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c063530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c0637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c063c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c0640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c064540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c0649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c064e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c065290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c065700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c065b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c065fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c066450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c0668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c066d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c0671a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c067610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c067a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c067ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c068360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c0687d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c068c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c0690b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c069520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c069990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c069e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c06a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c06a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c06ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c06afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c06b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c06b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c06bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c06c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c06c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c06ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c06ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c06d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c06d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c06dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c06e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c06e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c06e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c06ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c06f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c06f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c06fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c06ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c070410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c070880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c070cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c071160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c0715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c071a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c071eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c072320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c072790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c072c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c073070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c0734e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c073950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c073dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c074230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c0746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c074b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c074f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c0753f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c075860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c075dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c076230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15c0766a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c076b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c077050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c077550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c077a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c078670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c078930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c078ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c0794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c079a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c07a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c07a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c07abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c07b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c07b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c07bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c07c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c07c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c07ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c07d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c07d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c07df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c07e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c07eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c07f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c07f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c07fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c0801f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c0807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c080d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c081330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c0818f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c081eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c082470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c082a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c082ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c0835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c083b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c084130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c0846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c084cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c085270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c085830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c085df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c0863b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c086970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c086f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c0874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c087ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c088070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c088630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c088bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c0891b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c089770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c089d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c08a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c08a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c08ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c08b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c08b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c08bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c08c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15c08cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15c08d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c08d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c08da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c08df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c08e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c08e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c08ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c08f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c08f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c08fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c090230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c090730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c090c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c091130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x15c091630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x15c091b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x15c092030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x15c092530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x15c092a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x15c092f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x15c093430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x15c093930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x15c093e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x15c094330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c094830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c095240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c095960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c096080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c0967a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c096a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c0971f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c0974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c0979c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14af046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14af04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14af04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14af05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14af058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14af05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14af06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14af065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14af06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14af06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14af07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14af07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14af08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14af08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14af09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14af09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14af0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14af0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14af0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14af0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14af0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14af0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14af0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14af0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14af0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14af0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14af0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14af0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14af0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14af0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14af0f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14af0fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14af10110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14af105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14af10a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14af10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14af11390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14af11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14af11cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14af12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14af12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14af12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14af12f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14af133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14af13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14af13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14af141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14af14670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14af14b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14af14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14af15450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14af158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14af15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14af16230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14af166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14af16b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14af17010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14af172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14af17590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14af17a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14af17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14af182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14af18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14af18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14af19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14af194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14af19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14af19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14af1a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14af1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14af1aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14af1af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14af1b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14af1b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14af1bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14af1c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14af1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14af1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14af1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14af1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14af1d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14af1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14af1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14af1e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14af1e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14af1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14af1f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14af1f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14af1fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14af1ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14af20390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14af20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14af20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14af210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14af21550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14af219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14af21e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14af222a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14af229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14af22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14af23450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14af23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14af23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14af24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14af24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14af250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14af25670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14af25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14af261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14af26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14af26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14af272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14af27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14af27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14af28340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14af28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14af28d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14af29240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14af29740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14af29c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14af2a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14af2a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14af2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14af2b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14af2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14af2ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14af2bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14af2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14af2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14af2ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14af2d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14af2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14af2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14af2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14af2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14af2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14af2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14af2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14af2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14af30040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14af30540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14af30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14af30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14af31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14af31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14af31e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14af32340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14af32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14af32d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14af33240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14af33740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14af33c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14af34140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14af34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14af34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14af35040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14af35540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14af35a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14af35f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14af36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14af36940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14af36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14af37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14af37840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14af37d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14af38240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14af38740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14af38c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14af39140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14af39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14af39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14af3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14af3a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14af3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14af3af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14af3b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14af3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14af3be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14af3c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14af3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14af3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14af3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14af3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14af3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14af3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14af3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14af3eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14af3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14af3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14af3fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14af3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14af40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14af40940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14af40e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14af413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14af419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14af41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14af42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14af42a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14af42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14af43400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14af43ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14af43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14af44240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14af447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14af44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14af453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14af45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14af45d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14af461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14af46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14af46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14af47270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14af47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14af47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14af48380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14af48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14af48ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14af49490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14af49a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14af49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14af4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14af4ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14af4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14af4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14af4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14af4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14af4c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14af4cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14af4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14af4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14af4de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14af4e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14af4e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14af4ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14af4f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14af4faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14af500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14af50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14af50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14af511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14af51760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14af51d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14af522c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14af52870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14af52e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14af533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14af53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14af53f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14af544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14af54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14af55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14af555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14af55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14af56150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14af56700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14af56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14af57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14af57810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14af57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14af58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14af58920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14af58ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14af59480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14af59a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14af59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14af5a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14af5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14af5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14af5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14af5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14af5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14af5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14af5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14af5ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14af5d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14af5d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14af5dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14af5e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14af5e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14af5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14af5f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14af5f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14af5fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14af60040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14af60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14af60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14af60f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14af61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14af61940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14af61e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14af62340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14af62840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14af63250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14af63970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14af64090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14af647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14af64a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14af65200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14af656a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14af65b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.935s
user	0m0.230s
sys	0m0.170s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.34 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.77 sec*proc (2 tests)

Total Test time (real) =   1.79 sec
        1.81 real         0.51 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
```
